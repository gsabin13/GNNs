+ '[' -z '' ']'
+ case "$-" in
+ __lmod_vx=x
+ '[' -n x ']'
+ set +x
Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/init/bash)
Shell debugging restarted
+ unset __lmod_vx
++ /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda shell.bash hook
+ __conda_setup='export CONDA_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause

__add_sys_prefix_to_path() {
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA}" ] && [ -n "${WINDIR+x}" ]; then
        SYSP=$(\dirname "${CONDA_EXE}")
    else
        SYSP=$(\dirname "${CONDA_EXE}")
        SYSP=$(\dirname "${SYSP}")
    fi

    if [ -n "${WINDIR+x}" ]; then
        PATH="${SYSP}/bin:${PATH}"
        PATH="${SYSP}/Scripts:${PATH}"
        PATH="${SYSP}/Library/bin:${PATH}"
        PATH="${SYSP}/Library/usr/bin:${PATH}"
        PATH="${SYSP}/Library/mingw-w64/bin:${PATH}"
        PATH="${SYSP}:${PATH}"
    else
        PATH="${SYSP}/bin:${PATH}"
    fi
    \export PATH
}

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi

    \local cmd="$1"
    shift
    \local ask_conda
    CONDA_INTERNAL_OLDPATH="${PATH}"
    __add_sys_prefix_to_path
    ask_conda="$(PS1="$PS1" "$CONDA_EXE" $_CE_M $_CE_CONDA shell.posix "$cmd" "$@")" || \return $?
    rc=$?
    PATH="${CONDA_INTERNAL_OLDPATH}"
    \eval "$ask_conda"
    if [ $rc != 0 ]; then
        \export PATH
    fi
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    CONDA_INTERNAL_OLDPATH="${PATH}"
    __add_sys_prefix_to_path
    ask_conda="$(PS1="$PS1" "$CONDA_EXE" $_CE_M $_CE_CONDA shell.posix reactivate)" || \return $?
    PATH="${CONDA_INTERNAL_OLDPATH}"
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    if [ "$#" -lt 1 ]; then
        "$CONDA_EXE" $_CE_M $_CE_CONDA
    else
        \local cmd="$1"
        shift
        case "$cmd" in
            activate|deactivate)
                __conda_activate "$cmd" "$@"
                ;;
            install|update|upgrade|remove|uninstall)
                CONDA_INTERNAL_OLDPATH="${PATH}"
                __add_sys_prefix_to_path
                "$CONDA_EXE" $_CE_M $_CE_CONDA "$cmd" "$@"
                \local t1=$?
                PATH="${CONDA_INTERNAL_OLDPATH}"
                if [ $t1 = 0 ]; then
                    __conda_reactivate
                else
                    return $t1
                fi
                ;;
            *)
                CONDA_INTERNAL_OLDPATH="${PATH}"
                __add_sys_prefix_to_path
                "$CONDA_EXE" $_CE_M $_CE_CONDA "$cmd" "$@"
                \local t1=$?
                PATH="${CONDA_INTERNAL_OLDPATH}"
                return $t1
                ;;
        esac
    fi
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
+ '[' 0 -eq 0 ']'
+ eval 'export CONDA_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python'\''

# Copyright (C) 2012 Anaconda, Inc
# SPDX-License-Identifier: BSD-3-Clause

__add_sys_prefix_to_path() {
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA}" ] && [ -n "${WINDIR+x}" ]; then
        SYSP=$(\dirname "${CONDA_EXE}")
    else
        SYSP=$(\dirname "${CONDA_EXE}")
        SYSP=$(\dirname "${SYSP}")
    fi

    if [ -n "${WINDIR+x}" ]; then
        PATH="${SYSP}/bin:${PATH}"
        PATH="${SYSP}/Scripts:${PATH}"
        PATH="${SYSP}/Library/bin:${PATH}"
        PATH="${SYSP}/Library/usr/bin:${PATH}"
        PATH="${SYSP}/Library/mingw-w64/bin:${PATH}"
        PATH="${SYSP}:${PATH}"
    else
        PATH="${SYSP}/bin:${PATH}"
    fi
    \export PATH
}

__conda_hashr() {
    if [ -n "${ZSH_VERSION:+x}" ]; then
        \rehash
    elif [ -n "${POSH_VERSION:+x}" ]; then
        :  # pass
    else
        \hash -r
    fi
}

__conda_activate() {
    if [ -n "${CONDA_PS1_BACKUP:+x}" ]; then
        # Handle transition from shell activated with conda <= 4.3 to a subsequent activation
        # after conda updated to >= 4.4. See issue #6173.
        PS1="$CONDA_PS1_BACKUP"
        \unset CONDA_PS1_BACKUP
    fi

    \local cmd="$1"
    shift
    \local ask_conda
    CONDA_INTERNAL_OLDPATH="${PATH}"
    __add_sys_prefix_to_path
    ask_conda="$(PS1="$PS1" "$CONDA_EXE" $_CE_M $_CE_CONDA shell.posix "$cmd" "$@")" || \return $?
    rc=$?
    PATH="${CONDA_INTERNAL_OLDPATH}"
    \eval "$ask_conda"
    if [ $rc != 0 ]; then
        \export PATH
    fi
    __conda_hashr
}

__conda_reactivate() {
    \local ask_conda
    CONDA_INTERNAL_OLDPATH="${PATH}"
    __add_sys_prefix_to_path
    ask_conda="$(PS1="$PS1" "$CONDA_EXE" $_CE_M $_CE_CONDA shell.posix reactivate)" || \return $?
    PATH="${CONDA_INTERNAL_OLDPATH}"
    \eval "$ask_conda"
    __conda_hashr
}

conda() {
    if [ "$#" -lt 1 ]; then
        "$CONDA_EXE" $_CE_M $_CE_CONDA
    else
        \local cmd="$1"
        shift
        case "$cmd" in
            activate|deactivate)
                __conda_activate "$cmd" "$@"
                ;;
            install|update|upgrade|remove|uninstall)
                CONDA_INTERNAL_OLDPATH="${PATH}"
                __add_sys_prefix_to_path
                "$CONDA_EXE" $_CE_M $_CE_CONDA "$cmd" "$@"
                \local t1=$?
                PATH="${CONDA_INTERNAL_OLDPATH}"
                if [ $t1 = 0 ]; then
                    __conda_reactivate
                else
                    return $t1
                fi
                ;;
            *)
                CONDA_INTERNAL_OLDPATH="${PATH}"
                __add_sys_prefix_to_path
                "$CONDA_EXE" $_CE_M $_CE_CONDA "$cmd" "$@"
                \local t1=$?
                PATH="${CONDA_INTERNAL_OLDPATH}"
                return $t1
                ;;
        esac
    fi
}

if [ -z "${CONDA_SHLVL+x}" ]; then
    \export CONDA_SHLVL=0
    # In dev-mode CONDA_EXE is python.exe and on Windows
    # it is in a different relative location to condabin.
    if [ -n "${_CE_CONDA:+x}" ] && [ -n "${WINDIR+x}" ]; then
        PATH="$(\dirname "$CONDA_EXE")/condabin${PATH:+":${PATH}"}"
    else
        PATH="$(\dirname "$(\dirname "$CONDA_EXE")")/condabin${PATH:+":${PATH}"}"
    fi
    \export PATH

    # We'\''re not allowing PS1 to be unbound. It must at least be set.
    # However, we'\''re not exporting it, which can cause problems when starting a second shell
    # via a first shell (i.e. starting zsh from bash).
    if [ -z "${PS1+x}" ]; then
        PS1=
    fi
fi

conda activate base'
++ export CONDA_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
++ CONDA_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python
++ CONDA_PYTHON_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python
++ '[' -z x ']'
++ conda activate base
++ '[' 2 -lt 1 ']'
++ local cmd=activate
++ shift
++ case "$cmd" in
++ __conda_activate activate base
++ '[' -n '' ']'
++ local cmd=activate
++ shift
++ local ask_conda
++ CONDA_INTERNAL_OLDPATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ __add_sys_prefix_to_path
++ '[' -n '' ']'
+++ dirname /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
++ SYSP=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin
+++ dirname /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin
++ SYSP=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3
++ '[' -n '' ']'
++ PATH=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ export PATH
+++ PS1=
+++ /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda shell.posix activate base
++ ask_conda='export PATH='\''/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools'\''
unset CONDA_PREFIX_1
PS1='\''(base) '\''
export CONDA_PREFIX='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python'\'''
++ rc=0
++ PATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ eval 'export PATH='\''/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools'\''
unset CONDA_PREFIX_1
PS1='\''(base) '\''
export CONDA_PREFIX='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python'\'''
+++ export PATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
+++ PATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
+++ unset CONDA_PREFIX_1
+++ PS1='(base) '
+++ export CONDA_PREFIX=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3
+++ CONDA_PREFIX=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
+++ CONDA_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python
+++ CONDA_PYTHON_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python
++ '[' 0 '!=' 0 ']'
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ unset __conda_setup
+ conda activate ogb
+ '[' 2 -lt 1 ']'
+ local cmd=activate
+ shift
+ case "$cmd" in
+ __conda_activate activate ogb
+ '[' -n '' ']'
+ local cmd=activate
+ shift
+ local ask_conda
+ CONDA_INTERNAL_OLDPATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
+ __add_sys_prefix_to_path
+ '[' -n '' ']'
++ dirname /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
+ SYSP=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin
++ dirname /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin
+ SYSP=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3
+ '[' -n '' ']'
+ PATH=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
+ export PATH
++ PS1='(base) '
++ /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda shell.posix activate ogb
+ ask_conda='PS1='\''(ogb) '\''
export PATH='\''/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools'\''
export CONDA_PREFIX='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''ogb'\''
export CONDA_PROMPT_MODIFIER='\''(ogb) '\''
export CONDA_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python'\''
export CONDA_PREFIX_1='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3'\'''
+ rc=0
+ PATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
+ eval 'PS1='\''(ogb) '\''
export PATH='\''/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools'\''
export CONDA_PREFIX='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''ogb'\''
export CONDA_PROMPT_MODIFIER='\''(ogb) '\''
export CONDA_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python'\''
export CONDA_PREFIX_1='\''/uufs/chpc.utah.edu/common/home/u1320844/anaconda3'\'''
++ PS1='(ogb) '
++ export PATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ PATH=/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ export CONDA_PREFIX=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb
++ CONDA_PREFIX=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=ogb
++ CONDA_DEFAULT_ENV=ogb
++ export 'CONDA_PROMPT_MODIFIER=(ogb) '
++ CONDA_PROMPT_MODIFIER='(ogb) '
++ export CONDA_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
++ CONDA_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python
++ CONDA_PYTHON_EXE=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/bin/python
++ export CONDA_PREFIX_1=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3
++ CONDA_PREFIX_1=/uufs/chpc.utah.edu/common/home/u1320844/anaconda3
+ '[' 0 '!=' 0 ']'
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ module load gcc/9.2.0
++ /uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/libexec/lmod bash load gcc/9.2.0
+ eval 'GCC_LIB=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64;' export 'GCC_LIB;' 'INCLUDE=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/include;' export 'INCLUDE;' '__LMOD_REF_COUNT_LD_LIBRARY_PATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:1\;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:1\;/uufs/chpc.utah.edu/common/home/u1320844/local/lib:1\;/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:1\;/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib:2;' export '__LMOD_REF_COUNT_LD_LIBRARY_PATH;' 'LD_LIBRARY_PATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:/uufs/chpc.utah.edu/common/home/u1320844/local/lib:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib;' export 'LD_LIBRARY_PATH;' 'LMOD_FAMILY_COMPILER=gcc;' export 'LMOD_FAMILY_COMPILER;' 'LMOD_FAMILY_COMPILER_VERSION=9.2.0;' export 'LMOD_FAMILY_COMPILER_VERSION;' '__LMOD_REF_COUNT_LOADEDMODULES=chpc/1.0:1\;cuda/11.0:1\;gcc/9.2.0:1;' export '__LMOD_REF_COUNT_LOADEDMODULES;' 'LOADEDMODULES=chpc/1.0:cuda/11.0:gcc/9.2.0;' export 'LOADEDMODULES;' 'MANPATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/share/man:/uufs/kingspeak.peaks/sys/pkg/slurm/std/share/man:/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/share/man:/uufs/chpc.utah.edu/sys/man:/usr/local/share/man:/usr/share/man;' export 'MANPATH;' '__LMOD_REF_COUNT_MODULEPATH=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:2\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:1\;/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core:1;' export '__LMOD_REF_COUNT_MODULEPATH;' 'MODULEPATH=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core;' export 'MODULEPATH;' '__LMOD_REF_COUNT_PATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:1\;/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:1\;/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:1\;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:1\;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:1\;/uufs/chpc.utah.edu/sys/bin:1\;/usr/local/bin:1\;/usr/bin:1\;/usr/local/sbin:1\;/usr/sbin:1\;/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:2\;/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools:1;' export '__LMOD_REF_COUNT_PATH;' 'PATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools;' export 'PATH;' '__LMOD_REF_COUNT__LMFILES_=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua:1;' export '__LMOD_REF_COUNT__LMFILES_;' '_LMFILES_=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua;' export '_LMFILES_;' '_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpDb21waWxlciA9ICJnY2MiLApjdWRhID0gImN1ZGEiLAp9LAptVCA9IHsKY2hwYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2NocGMvMS4wLmx1YSIsCmZ1bGxOYW1lID0gImNocGMvMS4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7Cmxtb2QgPSB7CnN0aWNreSA9IDEsCn0sCn0sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY2hwYyIsCndWID0gIjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKY3Vk;' export '_ModuleTable001_;' '_ModuleTable002_=YSA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2N1ZGEvMTEuMC5sdWEiLApmdWxsTmFtZSA9ICJjdWRhLzExLjAiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHsKYXJjaCA9IHsKZ3B1ID0gMSwKfSwKfSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRhLzExLjAiLAp3ViA9ICIwMDAwMDAwMTEuKnpmaW5hbCIsCn0sCmdjYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2djYy85LjIuMC5sdWEiLApmdWxsTmFtZSA9ICJnY2MvOS4yLjAiLApsb2FkT3JkZXIgPSAzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwK;' export '_ModuleTable002_;' '_ModuleTable003_=c3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImdjYy85LjIuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL0NIUEMtcjgvQ29tcGlsZXIva3AvZ2NjLzkuMi4wIgosICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9zcGFjay9saW51eC1jZW50b3M3LXg4Nl82NC9Db21waWxlci9saW51eC1jZW50b3M3LW5laGFsZW0vZ2NjLzkuMi4wIgosICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvbXBpbGVyL2djYy85LjIuMCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1y;' export '_ModuleTable003_;' '_ModuleTable004_=OC9Db21waWxlci9jdWRhLzExLjAiCiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL3NwYWNrL2xpbnV4LXJvY2t5OC14ODZfNjQvQ29yZS9saW51eC1yb2NreTgtbmVoYWxlbSIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlIiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL2luc3RhbGxkaXIvbG1vZC84LjYtcjgvbW9kdWxlZmlsZXMvQ29yZSIsCn0sCnN5c3RlbUJhc2VNUEFUSCA9ICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9zcGFjay9saW51eC1yb2NreTgteDg2XzY0L0NvcmUvbGludXgt;' export '_ModuleTable004_;' '_ModuleTable005_=cm9ja3k4LW5laGFsZW06L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eDovdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvcmU6L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvaW5zdGFsbGRpci9sbW9kLzguNi1yOC9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=;' export '_ModuleTable005_;' '_ModuleTable_Sz_=5;' export '_ModuleTable_Sz_;'
++ GCC_LIB=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64
++ export GCC_LIB
++ INCLUDE=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/include
++ export INCLUDE
++ __LMOD_REF_COUNT_LD_LIBRARY_PATH='/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:1;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:1;/uufs/chpc.utah.edu/common/home/u1320844/local/lib:1;/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:1;/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib:2'
++ export __LMOD_REF_COUNT_LD_LIBRARY_PATH
++ LD_LIBRARY_PATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:/uufs/chpc.utah.edu/common/home/u1320844/local/lib:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib
++ export LD_LIBRARY_PATH
++ LMOD_FAMILY_COMPILER=gcc
++ export LMOD_FAMILY_COMPILER
++ LMOD_FAMILY_COMPILER_VERSION=9.2.0
++ export LMOD_FAMILY_COMPILER_VERSION
++ __LMOD_REF_COUNT_LOADEDMODULES='chpc/1.0:1;cuda/11.0:1;gcc/9.2.0:1'
++ export __LMOD_REF_COUNT_LOADEDMODULES
++ LOADEDMODULES=chpc/1.0:cuda/11.0:gcc/9.2.0
++ export LOADEDMODULES
++ MANPATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/share/man:/uufs/kingspeak.peaks/sys/pkg/slurm/std/share/man:/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/share/man:/uufs/chpc.utah.edu/sys/man:/usr/local/share/man:/usr/share/man
++ export MANPATH
++ __LMOD_REF_COUNT_MODULEPATH='/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:1;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:1;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:2;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:1;/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core:1'
++ export __LMOD_REF_COUNT_MODULEPATH
++ MODULEPATH=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core
++ export MODULEPATH
++ __LMOD_REF_COUNT_PATH='/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:1;/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:1;/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:1;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:1;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:1;/uufs/chpc.utah.edu/sys/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1;/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:2;/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools:1'
++ export __LMOD_REF_COUNT_PATH
++ PATH=/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ export PATH
++ __LMOD_REF_COUNT__LMFILES_='/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua:1'
++ export __LMOD_REF_COUNT__LMFILES_
++ _LMFILES_=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua
++ export _LMFILES_
++ _ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpDb21waWxlciA9ICJnY2MiLApjdWRhID0gImN1ZGEiLAp9LAptVCA9IHsKY2hwYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2NocGMvMS4wLmx1YSIsCmZ1bGxOYW1lID0gImNocGMvMS4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7Cmxtb2QgPSB7CnN0aWNreSA9IDEsCn0sCn0sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY2hwYyIsCndWID0gIjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKY3Vk
++ export _ModuleTable001_
++ _ModuleTable002_=YSA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2N1ZGEvMTEuMC5sdWEiLApmdWxsTmFtZSA9ICJjdWRhLzExLjAiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHsKYXJjaCA9IHsKZ3B1ID0gMSwKfSwKfSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRhLzExLjAiLAp3ViA9ICIwMDAwMDAwMTEuKnpmaW5hbCIsCn0sCmdjYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2djYy85LjIuMC5sdWEiLApmdWxsTmFtZSA9ICJnY2MvOS4yLjAiLApsb2FkT3JkZXIgPSAzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwK
++ export _ModuleTable002_
++ _ModuleTable003_=c3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImdjYy85LjIuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL0NIUEMtcjgvQ29tcGlsZXIva3AvZ2NjLzkuMi4wIgosICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9zcGFjay9saW51eC1jZW50b3M3LXg4Nl82NC9Db21waWxlci9saW51eC1jZW50b3M3LW5laGFsZW0vZ2NjLzkuMi4wIgosICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvbXBpbGVyL2djYy85LjIuMCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1y
++ export _ModuleTable003_
++ _ModuleTable004_=OC9Db21waWxlci9jdWRhLzExLjAiCiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL3NwYWNrL2xpbnV4LXJvY2t5OC14ODZfNjQvQ29yZS9saW51eC1yb2NreTgtbmVoYWxlbSIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlIiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL2luc3RhbGxkaXIvbG1vZC84LjYtcjgvbW9kdWxlZmlsZXMvQ29yZSIsCn0sCnN5c3RlbUJhc2VNUEFUSCA9ICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9zcGFjay9saW51eC1yb2NreTgteDg2XzY0L0NvcmUvbGludXgt
++ export _ModuleTable004_
++ _ModuleTable005_=cm9ja3k4LW5laGFsZW06L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eDovdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvcmU6L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvaW5zdGFsbGRpci9sbW9kLzguNi1yOC9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=
++ export _ModuleTable005_
++ _ModuleTable_Sz_=5
++ export _ModuleTable_Sz_
++ : -s sh
+ eval
+ module load cuda/11.0
++ /uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/libexec/lmod bash load cuda/11.0
+ eval 'CUDA_BINDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin;' export 'CUDA_BINDIR;' 'CUDA_INCDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/include;' export 'CUDA_INCDIR;' 'CUDA_LIBDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/lib64;' export 'CUDA_LIBDIR;' 'CUDA_PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2;' export 'CUDA_PATH;' 'CUDA_ROOT=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2;' export 'CUDA_ROOT;' 'CUDA_ROOTDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2;' export 'CUDA_ROOTDIR;' '__LMOD_REF_COUNT_LD_LIBRARY_PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:1\;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:1\;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:1\;/uufs/chpc.utah.edu/common/home/u1320844/local/lib:1\;/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib:2;' export '__LMOD_REF_COUNT_LD_LIBRARY_PATH;' 'LD_LIBRARY_PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:/uufs/chpc.utah.edu/common/home/u1320844/local/lib:/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib;' export 'LD_LIBRARY_PATH;' 'LMOD_FAMILY_CUDA=cuda;' export 'LMOD_FAMILY_CUDA;' 'LMOD_FAMILY_CUDA_VERSION=11.0;' export 'LMOD_FAMILY_CUDA_VERSION;' '__LMOD_REF_COUNT_LOADEDMODULES=chpc/1.0:1\;gcc/9.2.0:1\;cuda/11.0:1;' export '__LMOD_REF_COUNT_LOADEDMODULES;' 'LOADEDMODULES=chpc/1.0:gcc/9.2.0:cuda/11.0;' export 'LOADEDMODULES;' '__LMOD_REF_COUNT_MODULEPATH=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:1\;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:2\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:1\;/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core:1;' export '__LMOD_REF_COUNT_MODULEPATH;' 'MODULEPATH=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core;' export 'MODULEPATH;' '__LMOD_REF_COUNT_PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:1\;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:1\;/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:1\;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:1\;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:1\;/uufs/chpc.utah.edu/sys/bin:1\;/usr/local/bin:1\;/usr/bin:1\;/usr/local/sbin:1\;/usr/sbin:1\;/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:2\;/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools:1;' export '__LMOD_REF_COUNT_PATH;' 'PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools;' export 'PATH;' '__LMOD_REF_COUNT__LMFILES_=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua:1\;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua:1;' export '__LMOD_REF_COUNT__LMFILES_;' '_LMFILES_=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua;' export '_LMFILES_;' '_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpDb21waWxlciA9ICJnY2MiLApjdWRhID0gImN1ZGEiLAp9LAptVCA9IHsKY2hwYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2NocGMvMS4wLmx1YSIsCmZ1bGxOYW1lID0gImNocGMvMS4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7Cmxtb2QgPSB7CnN0aWNreSA9IDEsCn0sCn0sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY2hwYyIsCndWID0gIjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKY3Vk;' export '_ModuleTable001_;' '_ModuleTable002_=YSA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2N1ZGEvMTEuMC5sdWEiLApmdWxsTmFtZSA9ICJjdWRhLzExLjAiLApsb2FkT3JkZXIgPSAzLApwcm9wVCA9IHsKYXJjaCA9IHsKZ3B1ID0gMSwKfSwKfSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRhLzExLjAiLAp3ViA9ICIwMDAwMDAwMTEuKnpmaW5hbCIsCn0sCmdjYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2djYy85LjIuMC5sdWEiLApmdWxsTmFtZSA9ICJnY2MvOS4yLjAiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwK;' export '_ModuleTable002_;' '_ModuleTable003_=c3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImdjYy85LjIuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL0NIUEMtcjgvQ29tcGlsZXIvY3VkYS8xMS4wIgosICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvbXBpbGVyL2twL2djYy85LjIuMCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvc3BhY2svbGludXgtY2VudG9zNy14ODZfNjQvQ29tcGlsZXIvbGludXgtY2VudG9zNy1uZWhhbGVtL2djYy85LjIuMCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1y;' export '_ModuleTable003_;' '_ModuleTable004_=OC9Db21waWxlci9nY2MvOS4yLjAiCiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL3NwYWNrL2xpbnV4LXJvY2t5OC14ODZfNjQvQ29yZS9saW51eC1yb2NreTgtbmVoYWxlbSIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlIiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL2luc3RhbGxkaXIvbG1vZC84LjYtcjgvbW9kdWxlZmlsZXMvQ29yZSIsCn0sCnN5c3RlbUJhc2VNUEFUSCA9ICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9zcGFjay9saW51eC1yb2NreTgteDg2XzY0L0NvcmUvbGludXgt;' export '_ModuleTable004_;' '_ModuleTable005_=cm9ja3k4LW5laGFsZW06L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eDovdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvcmU6L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvaW5zdGFsbGRpci9sbW9kLzguNi1yOC9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=;' export '_ModuleTable005_;' '_ModuleTable_Sz_=5;' export '_ModuleTable_Sz_;'
++ CUDA_BINDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin
++ export CUDA_BINDIR
++ CUDA_INCDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/include
++ export CUDA_INCDIR
++ CUDA_LIBDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/lib64
++ export CUDA_LIBDIR
++ CUDA_PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2
++ export CUDA_PATH
++ CUDA_ROOT=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2
++ export CUDA_ROOT
++ CUDA_ROOTDIR=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2
++ export CUDA_ROOTDIR
++ __LMOD_REF_COUNT_LD_LIBRARY_PATH='/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:1;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:1;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:1;/uufs/chpc.utah.edu/common/home/u1320844/local/lib:1;/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib:2'
++ export __LMOD_REF_COUNT_LD_LIBRARY_PATH
++ LD_LIBRARY_PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/targets/x86_64-linux/lib:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib64:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/lib:/uufs/chpc.utah.edu/common/home/u1320844/local/lib:/uufs/kingspeak.peaks/sys/pkg/slurm/std/lib
++ export LD_LIBRARY_PATH
++ LMOD_FAMILY_CUDA=cuda
++ export LMOD_FAMILY_CUDA
++ LMOD_FAMILY_CUDA_VERSION=11.0
++ export LMOD_FAMILY_CUDA_VERSION
++ __LMOD_REF_COUNT_LOADEDMODULES='chpc/1.0:1;gcc/9.2.0:1;cuda/11.0:1'
++ export __LMOD_REF_COUNT_LOADEDMODULES
++ LOADEDMODULES=chpc/1.0:gcc/9.2.0:cuda/11.0
++ export LOADEDMODULES
++ __LMOD_REF_COUNT_MODULEPATH='/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:1;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:1;/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:2;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:1;/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core:1'
++ export __LMOD_REF_COUNT_MODULEPATH
++ MODULEPATH=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/cuda/11.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/kp/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-centos7-x86_64/Compiler/linux-centos7-nehalem/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Compiler/gcc/9.2.0:/uufs/chpc.utah.edu/sys/modulefiles/spack/linux-rocky8-x86_64/Core/linux-rocky8-nehalem:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Linux:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core:/uufs/chpc.utah.edu/sys/installdir/lmod/8.6-r8/modulefiles/Core
++ export MODULEPATH
++ __LMOD_REF_COUNT_PATH='/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:1;/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:1;/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:1;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:1;/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:1;/uufs/chpc.utah.edu/sys/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1;/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:2;/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools:1'
++ export __LMOD_REF_COUNT_PATH
++ PATH=/uufs/chpc.utah.edu/sys/installdir/cuda/11.0.2/bin:/uufs/chpc.utah.edu/sys/installdir/gcc/9.2.0/bin:/uufs/chpc.utah.edu/common/home/u1320844/vision_transformer/google-cloud-sdk/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin:/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/condabin:/uufs/chpc.utah.edu/sys/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/uufs/kingspeak.peaks/sys/pkg/slurm/std/bin:/uufs/chpc.utah.edu/common/home/u1320844/.dotnet/tools
++ export PATH
++ __LMOD_REF_COUNT__LMFILES_='/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua:1;/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua:1'
++ export __LMOD_REF_COUNT__LMFILES_
++ _LMFILES_=/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/chpc/1.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/gcc/9.2.0.lua:/uufs/chpc.utah.edu/sys/modulefiles/CHPC-r8/Core/cuda/11.0.lua
++ export _LMFILES_
++ _ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpDb21waWxlciA9ICJnY2MiLApjdWRhID0gImN1ZGEiLAp9LAptVCA9IHsKY2hwYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2NocGMvMS4wLmx1YSIsCmZ1bGxOYW1lID0gImNocGMvMS4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7Cmxtb2QgPSB7CnN0aWNreSA9IDEsCn0sCn0sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY2hwYyIsCndWID0gIjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKY3Vk
++ export _ModuleTable001_
++ _ModuleTable002_=YSA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2N1ZGEvMTEuMC5sdWEiLApmdWxsTmFtZSA9ICJjdWRhLzExLjAiLApsb2FkT3JkZXIgPSAzLApwcm9wVCA9IHsKYXJjaCA9IHsKZ3B1ID0gMSwKfSwKfSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRhLzExLjAiLAp3ViA9ICIwMDAwMDAwMTEuKnpmaW5hbCIsCn0sCmdjYyA9IHsKZm4gPSAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlL2djYy85LjIuMC5sdWEiLApmdWxsTmFtZSA9ICJnY2MvOS4yLjAiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwK
++ export _ModuleTable002_
++ _ModuleTable003_=c3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImdjYy85LjIuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL0NIUEMtcjgvQ29tcGlsZXIvY3VkYS8xMS4wIgosICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvbXBpbGVyL2twL2djYy85LjIuMCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvc3BhY2svbGludXgtY2VudG9zNy14ODZfNjQvQ29tcGlsZXIvbGludXgtY2VudG9zNy1uZWhhbGVtL2djYy85LjIuMCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1y
++ export _ModuleTable003_
++ _ModuleTable004_=OC9Db21waWxlci9nY2MvOS4yLjAiCiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL21vZHVsZWZpbGVzL3NwYWNrL2xpbnV4LXJvY2t5OC14ODZfNjQvQ29yZS9saW51eC1yb2NreTgtbmVoYWxlbSIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eCIKLCAiL3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9Db3JlIiwgIi91dWZzL2NocGMudXRhaC5lZHUvc3lzL2luc3RhbGxkaXIvbG1vZC84LjYtcjgvbW9kdWxlZmlsZXMvQ29yZSIsCn0sCnN5c3RlbUJhc2VNUEFUSCA9ICIvdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9zcGFjay9saW51eC1yb2NreTgteDg2XzY0L0NvcmUvbGludXgt
++ export _ModuleTable004_
++ _ModuleTable005_=cm9ja3k4LW5laGFsZW06L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvbW9kdWxlZmlsZXMvQ0hQQy1yOC9MaW51eDovdXVmcy9jaHBjLnV0YWguZWR1L3N5cy9tb2R1bGVmaWxlcy9DSFBDLXI4L0NvcmU6L3V1ZnMvY2hwYy51dGFoLmVkdS9zeXMvaW5zdGFsbGRpci9sbW9kLzguNi1yOC9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=
++ export _ModuleTable005_
++ _ModuleTable_Sz_=5
++ export _ModuleTable_Sz_
++ : -s sh
+ eval
+ sh run.sh sh dgl/run_dgl.sh full_new.csv
Date:  Jul12-02:02:30-MDT2022
/uufs/chpc.utah.edu/common/home/u1320844/GNNs_log/Jul12-02:02:30-MDT2022_env_info.txt
/uufs/chpc.utah.edu/common/home/u1320844/GNNs_log/Jul12-02:02:30-MDT2022_running_log.txt
rank:0
Namespace(csv='full_new.csv', dataset='ogbn-arxiv', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
Inited proc group
Max label: tensor(39)
----Data statistics------'
    #Nodes 169343
    #Edges 2484941
    #Classes/Labels (multi binary labels) 40
    #Train samples 90941
    #Val samples 0
    #Test samples 48603
Running on: 0
GCN(
  (layers): ModuleList(
    (0): GraphConv(in=128, out=128, normalization=both, activation=<function relu at 0x155097750f80>)
    (1): GraphConv(in=128, out=40, normalization=both, activation=None)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
Namespace(csv='full_new.csv', dataset='ogbn-arxiv', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
epoch:1/50, training loss:3.701838254928589
Train Acc 0.0151
 Acc 0.0171
new best val f1: 0.017075531936806113
ogbn-arxiv,dgl,1,0,1.3235,0.0171

epoch:2/50, training loss:3.6710195541381836
Train Acc 0.0235
 Acc 0.0514
new best val f1: 0.05141197552987703
ogbn-arxiv,dgl,1,1,1.3367,0.0514

epoch:3/50, training loss:3.6410698890686035
Train Acc 0.0952
 Acc 0.0621
new best val f1: 0.062081608272055035
ogbn-arxiv,dgl,1,2,1.3493,0.0621

epoch:4/50, training loss:3.6117868423461914
Train Acc 0.1236
 Acc 0.0591
ogbn-arxiv,dgl,1,3,1.3621,0.0591

epoch:5/50, training loss:3.583113670349121
Train Acc 0.1265
 Acc 0.0589
ogbn-arxiv,dgl,1,4,1.3747,0.0589

epoch:6/50, training loss:3.554832696914673
Train Acc 0.1269
 Acc 0.0588
ogbn-arxiv,dgl,1,5,1.3874,0.0588

epoch:7/50, training loss:3.526895761489868
Train Acc 0.1269
 Acc 0.0587
ogbn-arxiv,dgl,1,6,1.3999,0.0587

epoch:8/50, training loss:3.4992363452911377
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,7,1.4125,0.0587

epoch:9/50, training loss:3.4717092514038086
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,8,1.4263,0.0587

epoch:10/50, training loss:3.4442245960235596
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,9,1.4390,0.0587

epoch:11/50, training loss:3.416670560836792
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,10,1.4515,0.0587

epoch:12/50, training loss:3.3890843391418457
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,11,1.4640,0.0587

epoch:13/50, training loss:3.361524820327759
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,12,1.4764,0.0587

epoch:14/50, training loss:3.3340699672698975
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,13,1.4890,0.0587

epoch:15/50, training loss:3.306934118270874
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,14,1.5014,0.0587

epoch:16/50, training loss:3.2802040576934814
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,15,1.5140,0.0587

epoch:17/50, training loss:3.254098892211914
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,16,1.5264,0.0587

epoch:18/50, training loss:3.2288079261779785
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,17,1.5389,0.0587

epoch:19/50, training loss:3.2045106887817383
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,18,1.5513,0.0587

epoch:20/50, training loss:3.181326389312744
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,19,1.5637,0.0587

epoch:21/50, training loss:3.159383535385132
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,20,1.5761,0.0587

epoch:22/50, training loss:3.1386942863464355
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,21,1.5886,0.0587

epoch:23/50, training loss:3.1191649436950684
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,22,1.6060,0.0587

epoch:24/50, training loss:3.100778579711914
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,23,1.6185,0.0587

epoch:25/50, training loss:3.083425998687744
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,24,1.6310,0.0587

epoch:26/50, training loss:3.0669636726379395
Train Acc 0.1268
 Acc 0.0587
ogbn-arxiv,dgl,1,25,1.6435,0.0587

epoch:27/50, training loss:3.0513830184936523
Train Acc 0.1269
 Acc 0.0587
ogbn-arxiv,dgl,1,26,1.6559,0.0587

epoch:28/50, training loss:3.0366525650024414
Train Acc 0.1269
 Acc 0.0588
ogbn-arxiv,dgl,1,27,1.6683,0.0588

epoch:29/50, training loss:3.022839307785034
Train Acc 0.1270
 Acc 0.0590
ogbn-arxiv,dgl,1,28,1.6807,0.0590

epoch:30/50, training loss:3.0099422931671143
Train Acc 0.1273
 Acc 0.0600
ogbn-arxiv,dgl,1,29,1.6933,0.0600

epoch:31/50, training loss:2.9979612827301025
Train Acc 0.1285
 Acc 0.0636
new best val f1: 0.06360584152093761
ogbn-arxiv,dgl,1,30,1.7056,0.0636

epoch:32/50, training loss:2.9868781566619873
Train Acc 0.1324
 Acc 0.0923
new best val f1: 0.09227790479721519
ogbn-arxiv,dgl,1,31,1.7182,0.0923

epoch:33/50, training loss:2.976580858230591
Train Acc 0.1537
 Acc 0.1480
new best val f1: 0.14801540711446168
ogbn-arxiv,dgl,1,32,1.7307,0.1480

epoch:34/50, training loss:2.966905117034912
Train Acc 0.1934
 Acc 0.2017
new best val f1: 0.20169313477105605
ogbn-arxiv,dgl,1,33,1.7432,0.2017

epoch:35/50, training loss:2.9576475620269775
Train Acc 0.2311
 Acc 0.2359
new best val f1: 0.2358647963912748
ogbn-arxiv,dgl,1,34,1.7555,0.2359

epoch:36/50, training loss:2.9485630989074707
Train Acc 0.2541
 Acc 0.2491
new best val f1: 0.2490679519660549
ogbn-arxiv,dgl,1,35,1.7680,0.2491

epoch:37/50, training loss:2.939480781555176
Train Acc 0.2637
 Acc 0.2543
new best val f1: 0.2542791818575048
ogbn-arxiv,dgl,1,36,1.7804,0.2543

epoch:38/50, training loss:2.9302895069122314
Train Acc 0.2681
 Acc 0.2566
new best val f1: 0.25658612947743514
ogbn-arxiv,dgl,1,37,1.7929,0.2566

epoch:39/50, training loss:2.9209494590759277
Train Acc 0.2700
 Acc 0.2572
new best val f1: 0.25722465962223734
ogbn-arxiv,dgl,1,38,1.8053,0.2572

epoch:40/50, training loss:2.9114937782287598
Train Acc 0.2704
 Acc 0.2567
ogbn-arxiv,dgl,1,39,1.8178,0.2567

epoch:41/50, training loss:2.901960849761963
Train Acc 0.2699
 Acc 0.2557
ogbn-arxiv,dgl,1,40,1.8301,0.2557

epoch:42/50, training loss:2.8923728466033936
Train Acc 0.2690
 Acc 0.2539
ogbn-arxiv,dgl,1,41,1.8427,0.2539

epoch:43/50, training loss:2.882772922515869
Train Acc 0.2675
 Acc 0.2522
ogbn-arxiv,dgl,1,42,1.8550,0.2522

epoch:44/50, training loss:2.873178005218506
Train Acc 0.2659
 Acc 0.2506
ogbn-arxiv,dgl,1,43,1.8676,0.2506

epoch:45/50, training loss:2.8635575771331787
Train Acc 0.2647
 Acc 0.2499
ogbn-arxiv,dgl,1,44,1.8799,0.2499

epoch:46/50, training loss:2.8538618087768555
Train Acc 0.2641
 Acc 0.2500
ogbn-arxiv,dgl,1,45,1.8925,0.2500

epoch:47/50, training loss:2.8440396785736084
Train Acc 0.2643
 Acc 0.2514
ogbn-arxiv,dgl,1,46,1.9049,0.2514

epoch:48/50, training loss:2.834075689315796
Train Acc 0.2653
 Acc 0.2533
ogbn-arxiv,dgl,1,47,1.9189,0.2533

epoch:49/50, training loss:2.824009418487549
Train Acc 0.2670
 Acc 0.2556
ogbn-arxiv,dgl,1,48,1.9313,0.2556

epoch:50/50, training loss:2.813950777053833
Train Acc 0.2692
 Acc 0.2582
new best val f1: 0.2581515582195308
ogbn-arxiv,dgl,1,49,1.9439,0.2582

epoch:51/50, training loss:2.8039438724517822
Train Acc 0.2716
 Acc 0.2606
new best val f1: 0.2606438855589199
ogbn-arxiv,dgl,1,50,1.9563,0.2606

epoch:52/50, training loss:2.7939951419830322
Train Acc 0.2742
 Acc 0.2637
new best val f1: 0.26365115656347193
ogbn-arxiv,dgl,1,51,1.9688,0.2637

epoch:53/50, training loss:2.784062147140503
Train Acc 0.2771
 Acc 0.2664
new best val f1: 0.2664112546087458
ogbn-arxiv,dgl,1,52,1.9814,0.2664

epoch:54/50, training loss:2.7740917205810547
Train Acc 0.2799
 Acc 0.2692
new best val f1: 0.26923314589383923
ogbn-arxiv,dgl,1,53,1.9940,0.2692

epoch:55/50, training loss:2.764069080352783
Train Acc 0.2829
 Acc 0.2718
new best val f1: 0.27184905971286744
ogbn-arxiv,dgl,1,54,2.0064,0.2718

epoch:56/50, training loss:2.7539918422698975
Train Acc 0.2858
 Acc 0.2740
new best val f1: 0.27403242085315865
ogbn-arxiv,dgl,1,55,2.0188,0.2740

epoch:57/50, training loss:2.74383807182312
Train Acc 0.2887
 Acc 0.2763
new best val f1: 0.276298172979876
ogbn-arxiv,dgl,1,56,2.0313,0.2763

epoch:58/50, training loss:2.7336511611938477
Train Acc 0.2911
 Acc 0.2782
new best val f1: 0.27819316566767593
ogbn-arxiv,dgl,1,57,2.0439,0.2782

epoch:59/50, training loss:2.723410129547119
Train Acc 0.2935
 Acc 0.2796
new best val f1: 0.2795732146903129
ogbn-arxiv,dgl,1,58,2.0563,0.2796

epoch:60/50, training loss:2.713125705718994
Train Acc 0.2954
 Acc 0.2813
new best val f1: 0.28128282765865414
ogbn-arxiv,dgl,1,59,2.0688,0.2813

epoch:61/50, training loss:2.702840805053711
Train Acc 0.2974
 Acc 0.2829
new best val f1: 0.2828688541473563
ogbn-arxiv,dgl,1,60,2.0813,0.2829

epoch:62/50, training loss:2.6925110816955566
Train Acc 0.2995
 Acc 0.2847
new best val f1: 0.28468145584873017
ogbn-arxiv,dgl,1,61,2.0939,0.2847

epoch:63/50, training loss:2.682138442993164
Train Acc 0.3019
 Acc 0.2864
new best val f1: 0.28643226431028446
ogbn-arxiv,dgl,1,62,2.1064,0.2864

epoch:64/50, training loss:2.671736478805542
Train Acc 0.3044
 Acc 0.2886
new best val f1: 0.28857442995736265
ogbn-arxiv,dgl,1,63,2.1190,0.2886

epoch:65/50, training loss:2.6612770557403564
Train Acc 0.3072
 Acc 0.2906
new best val f1: 0.2905518136315887
ogbn-arxiv,dgl,1,64,2.1314,0.2906

epoch:66/50, training loss:2.650803565979004
Train Acc 0.3102
 Acc 0.2935
new best val f1: 0.2934560959031082
ogbn-arxiv,dgl,1,65,2.1440,0.2935

epoch:67/50, training loss:2.64029860496521
Train Acc 0.3136
 Acc 0.2964
new best val f1: 0.29642217141444727
ogbn-arxiv,dgl,1,66,2.1565,0.2964

epoch:68/50, training loss:2.629795789718628
Train Acc 0.3175
 Acc 0.3002
new best val f1: 0.3002327545366537
ogbn-arxiv,dgl,1,67,2.1692,0.3002

epoch:69/50, training loss:2.6192739009857178
Train Acc 0.3222
 Acc 0.3043
new best val f1: 0.3042905106181384
ogbn-arxiv,dgl,1,68,2.1816,0.3043

epoch:70/50, training loss:2.6086649894714355
Train Acc 0.3270
 Acc 0.3089
new best val f1: 0.3089044058579991
ogbn-arxiv,dgl,1,69,2.1943,0.3089

epoch:71/50, training loss:2.5979795455932617
Train Acc 0.3319
 Acc 0.3133
new best val f1: 0.31329172588518817
ogbn-arxiv,dgl,1,70,2.2068,0.3133

epoch:72/50, training loss:2.5871570110321045
Train Acc 0.3369
 Acc 0.3185
new best val f1: 0.3185441512698511
ogbn-arxiv,dgl,1,71,2.2195,0.3185

epoch:73/50, training loss:2.5762100219726562
Train Acc 0.3421
 Acc 0.3241
new best val f1: 0.3240849451070053
ogbn-arxiv,dgl,1,72,2.2320,0.3241

epoch:74/50, training loss:2.565180540084839
Train Acc 0.3477
 Acc 0.3297
new best val f1: 0.32972872767719213
ogbn-arxiv,dgl,1,73,2.2447,0.3297

epoch:75/50, training loss:2.5541863441467285
Train Acc 0.3530
 Acc 0.3353
new best val f1: 0.3353313147541659
ogbn-arxiv,dgl,1,74,2.2571,0.3353

epoch:76/50, training loss:2.543276071548462
Train Acc 0.3583
 Acc 0.3407
new best val f1: 0.3406661311252549
ogbn-arxiv,dgl,1,75,2.2697,0.3407

epoch:77/50, training loss:2.5324180126190186
Train Acc 0.3631
 Acc 0.3456
new best val f1: 0.3456301880574265
ogbn-arxiv,dgl,1,76,2.2821,0.3456

epoch:78/50, training loss:2.521646499633789
Train Acc 0.3677
 Acc 0.3493
new best val f1: 0.34927598920678077
ogbn-arxiv,dgl,1,77,2.2946,0.3493

epoch:79/50, training loss:2.5108933448791504
Train Acc 0.3715
 Acc 0.3518
new best val f1: 0.3518301097859894
ogbn-arxiv,dgl,1,78,2.3070,0.3518

epoch:80/50, training loss:2.500135660171509
Train Acc 0.3747
 Acc 0.3532
new best val f1: 0.3531895610620198
ogbn-arxiv,dgl,1,79,2.3196,0.3532

epoch:81/50, training loss:2.4893715381622314
Train Acc 0.3769
 Acc 0.3543
new best val f1: 0.35430183937877197
ogbn-arxiv,dgl,1,80,2.3320,0.3543

epoch:82/50, training loss:2.4786009788513184
Train Acc 0.3791
 Acc 0.3549
new best val f1: 0.3548785762837546
ogbn-arxiv,dgl,1,81,2.3445,0.3549

epoch:83/50, training loss:2.467853307723999
Train Acc 0.3810
 Acc 0.3558
new best val f1: 0.35582607262765453
ogbn-arxiv,dgl,1,82,2.3569,0.3558

epoch:84/50, training loss:2.457115888595581
Train Acc 0.3830
 Acc 0.3579
new best val f1: 0.3579064450349132
ogbn-arxiv,dgl,1,83,2.3694,0.3579

epoch:85/50, training loss:2.4464268684387207
Train Acc 0.3855
 Acc 0.3610
new best val f1: 0.36097550927928485
ogbn-arxiv,dgl,1,84,2.3819,0.3610

epoch:86/50, training loss:2.435763359069824
Train Acc 0.3885
 Acc 0.3643
new best val f1: 0.36427114873632827
ogbn-arxiv,dgl,1,85,2.3945,0.3643

epoch:87/50, training loss:2.4251391887664795
Train Acc 0.3918
 Acc 0.3680
new best val f1: 0.3679993408721086
ogbn-arxiv,dgl,1,86,2.4069,0.3680

epoch:88/50, training loss:2.414544105529785
Train Acc 0.3954
 Acc 0.3724
new best val f1: 0.37242785639251064
ogbn-arxiv,dgl,1,87,2.4195,0.3724

epoch:89/50, training loss:2.403986692428589
Train Acc 0.3994
 Acc 0.3777
new best val f1: 0.37774207501699314
ogbn-arxiv,dgl,1,88,2.4319,0.3777

epoch:90/50, training loss:2.3934710025787354
Train Acc 0.4037
 Acc 0.3836
new best val f1: 0.38357123730663867
ogbn-arxiv,dgl,1,89,2.4446,0.3836

epoch:91/50, training loss:2.382992744445801
Train Acc 0.4083
 Acc 0.3891
new best val f1: 0.3890914333971863
ogbn-arxiv,dgl,1,90,2.4569,0.3891

epoch:92/50, training loss:2.372554302215576
Train Acc 0.4128
 Acc 0.3945
new best val f1: 0.3944674452614884
ogbn-arxiv,dgl,1,91,2.4695,0.3945

epoch:93/50, training loss:2.362169027328491
Train Acc 0.4170
 Acc 0.3994
new best val f1: 0.3994109044470535
ogbn-arxiv,dgl,1,92,2.4819,0.3994

epoch:94/50, training loss:2.35184907913208
Train Acc 0.4211
 Acc 0.4045
new best val f1: 0.40453974335207726
ogbn-arxiv,dgl,1,93,2.4945,0.4045

epoch:95/50, training loss:2.341578483581543
Train Acc 0.4249
 Acc 0.4087
new best val f1: 0.4087004881665946
ogbn-arxiv,dgl,1,94,2.5068,0.4087

epoch:96/50, training loss:2.3313379287719727
Train Acc 0.4282
 Acc 0.4123
new best val f1: 0.41234628931594886
ogbn-arxiv,dgl,1,95,2.5193,0.4123

epoch:97/50, training loss:2.321181535720825
Train Acc 0.4314
 Acc 0.4156
new best val f1: 0.4156007332797792
ogbn-arxiv,dgl,1,96,2.5317,0.4156

epoch:98/50, training loss:2.311030626296997
Train Acc 0.4343
 Acc 0.4184
new best val f1: 0.4184432223114791
ogbn-arxiv,dgl,1,97,2.5442,0.4184

epoch:99/50, training loss:2.300952434539795
Train Acc 0.4368
 Acc 0.4206
new best val f1: 0.4205647902119508
ogbn-arxiv,dgl,1,98,2.5566,0.4206

epoch:100/50, training loss:2.290916681289673
Train Acc 0.4391
 Acc 0.4229
new best val f1: 0.42293353107170073
ogbn-arxiv,dgl,1,99,2.5691,0.4229

epoch:101/50, training loss:2.280902147293091
Train Acc 0.4412
 Acc 0.4252
new best val f1: 0.4251992831984181
ogbn-arxiv,dgl,1,100,2.5816,0.4252

epoch:102/50, training loss:2.270949125289917
Train Acc 0.4435
 Acc 0.4275
new best val f1: 0.4275062308183485
ogbn-arxiv,dgl,1,101,2.5941,0.4275

epoch:103/50, training loss:2.2610270977020264
Train Acc 0.4455
 Acc 0.4306
new best val f1: 0.43057529506272013
ogbn-arxiv,dgl,1,102,2.6066,0.4306

epoch:104/50, training loss:2.25117564201355
Train Acc 0.4482
 Acc 0.4336
new best val f1: 0.43364435930709183
ogbn-arxiv,dgl,1,103,2.6191,0.4336

epoch:105/50, training loss:2.241405963897705
Train Acc 0.4510
 Acc 0.4364
new best val f1: 0.4364456528455787
ogbn-arxiv,dgl,1,104,2.6316,0.4364

epoch:106/50, training loss:2.2317066192626953
Train Acc 0.4536
 Acc 0.4395
new best val f1: 0.4394529238501308
ogbn-arxiv,dgl,1,105,2.6441,0.4395

epoch:107/50, training loss:2.222115993499756
Train Acc 0.4563
 Acc 0.4425
new best val f1: 0.44250139034789593
ogbn-arxiv,dgl,1,106,2.6566,0.4425

epoch:108/50, training loss:2.212590217590332
Train Acc 0.4591
 Acc 0.4460
new best val f1: 0.44602360501761107
ogbn-arxiv,dgl,1,107,2.6692,0.4460

epoch:109/50, training loss:2.203134536743164
Train Acc 0.4619
 Acc 0.4500
new best val f1: 0.4500401656058827
ogbn-arxiv,dgl,1,108,2.6816,0.4500

epoch:110/50, training loss:2.193751573562622
Train Acc 0.4650
 Acc 0.4532
new best val f1: 0.453212218583287
ogbn-arxiv,dgl,1,109,2.6941,0.4532

epoch:111/50, training loss:2.1844465732574463
Train Acc 0.4677
 Acc 0.4569
new best val f1: 0.4569198129724608
ogbn-arxiv,dgl,1,110,2.7065,0.4569

epoch:112/50, training loss:2.175203323364258
Train Acc 0.4706
 Acc 0.4603
new best val f1: 0.4602978434159303
ogbn-arxiv,dgl,1,111,2.7195,0.4603

epoch:113/50, training loss:2.166024923324585
Train Acc 0.4733
 Acc 0.4634
new best val f1: 0.4634287009001215
ogbn-arxiv,dgl,1,112,2.7336,0.4634

epoch:114/50, training loss:2.1568949222564697
Train Acc 0.4756
 Acc 0.4664
new best val f1: 0.4664359719046736
ogbn-arxiv,dgl,1,113,2.7472,0.4664

epoch:115/50, training loss:2.1478517055511475
Train Acc 0.4780
 Acc 0.4694
new best val f1: 0.46944324290922573
ogbn-arxiv,dgl,1,114,2.7597,0.4694

epoch:116/50, training loss:2.1388745307922363
Train Acc 0.4804
 Acc 0.4721
new best val f1: 0.47212094996807347
ogbn-arxiv,dgl,1,115,2.7722,0.4721

epoch:117/50, training loss:2.1299450397491455
Train Acc 0.4828
 Acc 0.4747
new best val f1: 0.47473686378710167
ogbn-arxiv,dgl,1,116,2.7863,0.4747

epoch:118/50, training loss:2.1210970878601074
Train Acc 0.4850
 Acc 0.4766
new best val f1: 0.47661125872829513
ogbn-arxiv,dgl,1,117,2.7989,0.4766

epoch:119/50, training loss:2.112330675125122
Train Acc 0.4869
 Acc 0.4786
new best val f1: 0.4786092401491277
ogbn-arxiv,dgl,1,118,2.8114,0.4786

epoch:120/50, training loss:2.1036462783813477
Train Acc 0.4889
 Acc 0.4809
new best val f1: 0.480874992275845
ogbn-arxiv,dgl,1,119,2.8240,0.4809

epoch:121/50, training loss:2.095032215118408
Train Acc 0.4909
 Acc 0.4829
new best val f1: 0.4829347669364972
ogbn-arxiv,dgl,1,120,2.8364,0.4829

epoch:122/50, training loss:2.086481809616089
Train Acc 0.4928
 Acc 0.4848
new best val f1: 0.4848297596242971
ogbn-arxiv,dgl,1,121,2.8490,0.4848

epoch:123/50, training loss:2.078023910522461
Train Acc 0.4945
 Acc 0.4867
new best val f1: 0.48668355681888403
ogbn-arxiv,dgl,1,122,2.8614,0.4867

epoch:124/50, training loss:2.0696256160736084
Train Acc 0.4964
 Acc 0.4891
new best val f1: 0.48907289542524046
ogbn-arxiv,dgl,1,123,2.8739,0.4891

epoch:125/50, training loss:2.0612757205963135
Train Acc 0.4983
 Acc 0.4911
new best val f1: 0.49109147459267954
ogbn-arxiv,dgl,1,124,2.8863,0.4911

epoch:126/50, training loss:2.053013324737549
Train Acc 0.5001
 Acc 0.4932
new best val f1: 0.4931718469999382
ogbn-arxiv,dgl,1,125,2.8989,0.4932

epoch:127/50, training loss:2.04481840133667
Train Acc 0.5019
 Acc 0.4951
new best val f1: 0.4951492306741642
ogbn-arxiv,dgl,1,126,2.9113,0.4951

epoch:128/50, training loss:2.0366806983947754
Train Acc 0.5037
 Acc 0.4975
new best val f1: 0.4974973737873077
ogbn-arxiv,dgl,1,127,2.9239,0.4975

epoch:129/50, training loss:2.0286154747009277
Train Acc 0.5056
 Acc 0.4999
new best val f1: 0.4998661146470576
ogbn-arxiv,dgl,1,128,2.9383,0.4999

epoch:130/50, training loss:2.0205910205841064
Train Acc 0.5076
 Acc 0.5017
new best val f1: 0.501740509588251
ogbn-arxiv,dgl,1,129,2.9508,0.5017

epoch:131/50, training loss:2.012641668319702
Train Acc 0.5093
 Acc 0.5038
new best val f1: 0.5037796865022967
ogbn-arxiv,dgl,1,130,2.9632,0.5038

epoch:132/50, training loss:2.004762649536133
Train Acc 0.5109
 Acc 0.5056
new best val f1: 0.5055922882036705
ogbn-arxiv,dgl,1,131,2.9757,0.5056

epoch:133/50, training loss:1.9969221353530884
Train Acc 0.5125
 Acc 0.5075
new best val f1: 0.5075284763846836
ogbn-arxiv,dgl,1,132,2.9881,0.5075

epoch:134/50, training loss:1.9891725778579712
Train Acc 0.5144
 Acc 0.5096
new best val f1: 0.5095676532987291
ogbn-arxiv,dgl,1,133,3.0007,0.5096

epoch:135/50, training loss:1.9814814329147339
Train Acc 0.5160
 Acc 0.5117
new best val f1: 0.5117098189458074
ogbn-arxiv,dgl,1,134,3.0131,0.5117

epoch:136/50, training loss:1.973855972290039
Train Acc 0.5178
 Acc 0.5133
new best val f1: 0.513316443181116
ogbn-arxiv,dgl,1,135,3.0256,0.5133

epoch:137/50, training loss:1.9663145542144775
Train Acc 0.5195
 Acc 0.5148
new best val f1: 0.5148200786833921
ogbn-arxiv,dgl,1,136,3.0380,0.5148

epoch:138/50, training loss:1.9588292837142944
Train Acc 0.5210
 Acc 0.5166
new best val f1: 0.5165914848915528
ogbn-arxiv,dgl,1,137,3.0506,0.5166

epoch:139/50, training loss:1.9514087438583374
Train Acc 0.5225
 Acc 0.5179
new best val f1: 0.5179097406743702
ogbn-arxiv,dgl,1,138,3.0630,0.5179

epoch:140/50, training loss:1.9440605640411377
Train Acc 0.5238
 Acc 0.5200
new best val f1: 0.5199901130816289
ogbn-arxiv,dgl,1,139,3.0756,0.5200

epoch:141/50, training loss:1.936766505241394
Train Acc 0.5253
 Acc 0.5217
new best val f1: 0.5217203237965766
ogbn-arxiv,dgl,1,140,3.0880,0.5217

epoch:142/50, training loss:1.9295326471328735
Train Acc 0.5269
 Acc 0.5230
new best val f1: 0.5229561885929679
ogbn-arxiv,dgl,1,141,3.1005,0.5230

epoch:143/50, training loss:1.9223722219467163
Train Acc 0.5282
 Acc 0.5245
new best val f1: 0.5245010195884571
ogbn-arxiv,dgl,1,142,3.1129,0.5245

epoch:144/50, training loss:1.915260672569275
Train Acc 0.5296
 Acc 0.5262
new best val f1: 0.5261694370635852
ogbn-arxiv,dgl,1,143,3.1254,0.5262

epoch:145/50, training loss:1.908200740814209
Train Acc 0.5310
 Acc 0.5278
new best val f1: 0.5277554635522874
ogbn-arxiv,dgl,1,144,3.1379,0.5278

epoch:146/50, training loss:1.9011961221694946
Train Acc 0.5323
 Acc 0.5293
new best val f1: 0.5293414900409895
ogbn-arxiv,dgl,1,145,3.1505,0.5293

epoch:147/50, training loss:1.8942540884017944
Train Acc 0.5336
 Acc 0.5306
new best val f1: 0.5306185503305938
ogbn-arxiv,dgl,1,146,3.1629,0.5306

epoch:148/50, training loss:1.887377142906189
Train Acc 0.5349
 Acc 0.5321
new best val f1: 0.5321015880862634
ogbn-arxiv,dgl,1,147,3.1755,0.5321

epoch:149/50, training loss:1.880558729171753
Train Acc 0.5362
 Acc 0.5331
new best val f1: 0.5330696821767699
ogbn-arxiv,dgl,1,148,3.1879,0.5331

epoch:150/50, training loss:1.8738024234771729
Train Acc 0.5373
 Acc 0.5343
new best val f1: 0.5342643514799481
ogbn-arxiv,dgl,1,149,3.2005,0.5343

epoch:151/50, training loss:1.8671175241470337
Train Acc 0.5386
 Acc 0.5355
new best val f1: 0.5355414117695524
ogbn-arxiv,dgl,1,150,3.2129,0.5355

epoch:152/50, training loss:1.8604897260665894
Train Acc 0.5399
 Acc 0.5366
new best val f1: 0.536591896846485
ogbn-arxiv,dgl,1,151,3.2254,0.5366

epoch:153/50, training loss:1.8539148569107056
Train Acc 0.5413
 Acc 0.5382
new best val f1: 0.5381573255885806
ogbn-arxiv,dgl,1,152,3.2379,0.5382

epoch:154/50, training loss:1.8473920822143555
Train Acc 0.5427
 Acc 0.5401
new best val f1: 0.5400523182763806
ogbn-arxiv,dgl,1,153,3.2504,0.5401

epoch:155/50, training loss:1.8409028053283691
Train Acc 0.5441
 Acc 0.5412
new best val f1: 0.5412469875795588
ogbn-arxiv,dgl,1,154,3.2628,0.5412

epoch:156/50, training loss:1.8344709873199463
Train Acc 0.5454
 Acc 0.5428
new best val f1: 0.5427712208284413
ogbn-arxiv,dgl,1,155,3.2753,0.5428

epoch:157/50, training loss:1.8280974626541138
Train Acc 0.5467
 Acc 0.5442
new best val f1: 0.5441924653442913
ogbn-arxiv,dgl,1,156,3.2878,0.5442

epoch:158/50, training loss:1.8217500448226929
Train Acc 0.5480
 Acc 0.5454
new best val f1: 0.5454283301406826
ogbn-arxiv,dgl,1,157,3.3003,0.5454

epoch:159/50, training loss:1.815468668937683
Train Acc 0.5492
 Acc 0.5468
new best val f1: 0.5468083791633195
ogbn-arxiv,dgl,1,158,3.3128,0.5468

epoch:160/50, training loss:1.809246301651001
Train Acc 0.5505
 Acc 0.5478
new best val f1: 0.5478176687470391
ogbn-arxiv,dgl,1,159,3.3253,0.5478

epoch:161/50, training loss:1.8030842542648315
Train Acc 0.5517
 Acc 0.5496
new best val f1: 0.5495684772085934
ogbn-arxiv,dgl,1,160,3.3378,0.5496

epoch:162/50, training loss:1.7969770431518555
Train Acc 0.5531
 Acc 0.5504
new best val f1: 0.5504335825660672
ogbn-arxiv,dgl,1,161,3.3503,0.5504

epoch:163/50, training loss:1.7909284830093384
Train Acc 0.5541
 Acc 0.5518
new best val f1: 0.5518136315887042
ogbn-arxiv,dgl,1,162,3.3627,0.5518

epoch:164/50, training loss:1.7849184274673462
Train Acc 0.5553
 Acc 0.5532
new best val f1: 0.5532142783579477
ogbn-arxiv,dgl,1,163,3.3754,0.5532

epoch:165/50, training loss:1.7789770364761353
Train Acc 0.5566
 Acc 0.5545
new best val f1: 0.5545119363941585
ogbn-arxiv,dgl,1,164,3.3897,0.5545

epoch:166/50, training loss:1.7730867862701416
Train Acc 0.5578
 Acc 0.5555
new best val f1: 0.5555418237244846
ogbn-arxiv,dgl,1,165,3.4023,0.5555

epoch:167/50, training loss:1.7672384977340698
Train Acc 0.5588
 Acc 0.5567
new best val f1: 0.5566746997878432
ogbn-arxiv,dgl,1,166,3.4147,0.5567

epoch:168/50, training loss:1.7614433765411377
Train Acc 0.5598
 Acc 0.5579
new best val f1: 0.5578693690910215
ogbn-arxiv,dgl,1,167,3.4272,0.5579

epoch:169/50, training loss:1.7556999921798706
Train Acc 0.5610
 Acc 0.5588
new best val f1: 0.5587756699417084
ogbn-arxiv,dgl,1,168,3.4397,0.5588

epoch:170/50, training loss:1.7500182390213013
Train Acc 0.5620
 Acc 0.5601
new best val f1: 0.5601145234711322
ogbn-arxiv,dgl,1,169,3.4522,0.5601

epoch:171/50, training loss:1.7443844079971313
Train Acc 0.5632
 Acc 0.5615
new best val f1: 0.5615151702403757
ogbn-arxiv,dgl,1,170,3.4646,0.5615

epoch:172/50, training loss:1.7388015985488892
Train Acc 0.5645
 Acc 0.5623
new best val f1: 0.5622978846114235
ogbn-arxiv,dgl,1,171,3.4772,0.5623

epoch:173/50, training loss:1.7332714796066284
Train Acc 0.5657
 Acc 0.5632
new best val f1: 0.5632453809553235
ogbn-arxiv,dgl,1,172,3.4901,0.5632

epoch:174/50, training loss:1.727777361869812
Train Acc 0.5667
 Acc 0.5640
new best val f1: 0.5639868998331583
ogbn-arxiv,dgl,1,173,3.5028,0.5640

epoch:175/50, training loss:1.7223546504974365
Train Acc 0.5680
 Acc 0.5649
new best val f1: 0.5648520051906322
ogbn-arxiv,dgl,1,174,3.5153,0.5649

epoch:176/50, training loss:1.7169698476791382
Train Acc 0.5690
 Acc 0.5658
new best val f1: 0.5658406970277452
ogbn-arxiv,dgl,1,175,3.5279,0.5658

epoch:177/50, training loss:1.7116295099258423
Train Acc 0.5700
 Acc 0.5672
new best val f1: 0.5672001483037755
ogbn-arxiv,dgl,1,176,3.5404,0.5672

epoch:178/50, training loss:1.706355333328247
Train Acc 0.5713
 Acc 0.5680
new best val f1: 0.5680034604214299
ogbn-arxiv,dgl,1,177,3.5530,0.5680

epoch:179/50, training loss:1.701122522354126
Train Acc 0.5723
 Acc 0.5686
new best val f1: 0.568600795073019
ogbn-arxiv,dgl,1,178,3.5655,0.5686

epoch:180/50, training loss:1.6959497928619385
Train Acc 0.5734
 Acc 0.5692
new best val f1: 0.5691981297246081
ogbn-arxiv,dgl,1,179,3.5780,0.5692

epoch:181/50, training loss:1.6908198595046997
Train Acc 0.5743
 Acc 0.5702
new best val f1: 0.5701662238151146
ogbn-arxiv,dgl,1,180,3.5904,0.5702

epoch:182/50, training loss:1.6857390403747559
Train Acc 0.5754
 Acc 0.5711
new best val f1: 0.5710725246658016
ogbn-arxiv,dgl,1,181,3.6030,0.5711

epoch:183/50, training loss:1.6807191371917725
Train Acc 0.5764
 Acc 0.5717
new best val f1: 0.5717110548106037
ogbn-arxiv,dgl,1,182,3.6154,0.5717

epoch:184/50, training loss:1.6757335662841797
Train Acc 0.5776
 Acc 0.5727
new best val f1: 0.5726585511545037
ogbn-arxiv,dgl,1,183,3.6279,0.5727

epoch:185/50, training loss:1.6707994937896729
Train Acc 0.5786
 Acc 0.5739
new best val f1: 0.573935611444108
ogbn-arxiv,dgl,1,184,3.6402,0.5739

epoch:186/50, training loss:1.6659284830093384
Train Acc 0.5796
 Acc 0.5750
new best val f1: 0.5749860965210406
ogbn-arxiv,dgl,1,185,3.6527,0.5750

epoch:187/50, training loss:1.6610920429229736
Train Acc 0.5807
 Acc 0.5759
new best val f1: 0.5759335928649406
ogbn-arxiv,dgl,1,186,3.6651,0.5759

epoch:188/50, training loss:1.656302571296692
Train Acc 0.5817
 Acc 0.5768
new best val f1: 0.5767575027292015
ogbn-arxiv,dgl,1,187,3.6776,0.5768

epoch:189/50, training loss:1.6515614986419678
Train Acc 0.5826
 Acc 0.5778
new best val f1: 0.5778285855527405
ogbn-arxiv,dgl,1,188,3.6900,0.5778

epoch:190/50, training loss:1.6468698978424072
Train Acc 0.5837
 Acc 0.5787
new best val f1: 0.5786936909102144
ogbn-arxiv,dgl,1,189,3.7026,0.5787

epoch:191/50, training loss:1.642230749130249
Train Acc 0.5846
 Acc 0.5796
new best val f1: 0.5795793940142948
ogbn-arxiv,dgl,1,190,3.7150,0.5796

epoch:192/50, training loss:1.6376237869262695
Train Acc 0.5855
 Acc 0.5805
new best val f1: 0.5804650971183752
ogbn-arxiv,dgl,1,191,3.7275,0.5805

epoch:193/50, training loss:1.6330844163894653
Train Acc 0.5864
 Acc 0.5812
new best val f1: 0.5811860182496035
ogbn-arxiv,dgl,1,192,3.7399,0.5812

epoch:194/50, training loss:1.6285679340362549
Train Acc 0.5873
 Acc 0.5821
new best val f1: 0.5821335145935035
ogbn-arxiv,dgl,1,193,3.7524,0.5821

epoch:195/50, training loss:1.6241114139556885
Train Acc 0.5881
 Acc 0.5830
new best val f1: 0.5829780222043709
ogbn-arxiv,dgl,1,194,3.7647,0.5830

epoch:196/50, training loss:1.6197012662887573
Train Acc 0.5889
 Acc 0.5838
new best val f1: 0.5837813343220252
ogbn-arxiv,dgl,1,195,3.7773,0.5838

epoch:197/50, training loss:1.6153188943862915
Train Acc 0.5896
 Acc 0.5843
new best val f1: 0.5842550824939752
ogbn-arxiv,dgl,1,196,3.7897,0.5843

epoch:198/50, training loss:1.6109899282455444
Train Acc 0.5901
 Acc 0.5852
new best val f1: 0.5852025788378752
ogbn-arxiv,dgl,1,197,3.8022,0.5852

epoch:199/50, training loss:1.6067179441452026
Train Acc 0.5910
 Acc 0.5860
new best val f1: 0.5859852932089229
ogbn-arxiv,dgl,1,198,3.8146,0.5860

epoch:200/50, training loss:1.6024742126464844
Train Acc 0.5916
 Acc 0.5868
new best val f1: 0.5867680075799707
ogbn-arxiv,dgl,1,199,3.8272,0.5868

epoch:201/50, training loss:1.5982723236083984
Train Acc 0.5925
 Acc 0.5878
new best val f1: 0.5878184926569033
ogbn-arxiv,dgl,1,200,3.8397,0.5878

epoch:202/50, training loss:1.5941288471221924
Train Acc 0.5934
 Acc 0.5886
new best val f1: 0.5886012070279512
ogbn-arxiv,dgl,1,201,3.8522,0.5886

epoch:203/50, training loss:1.5900131464004517
Train Acc 0.5941
 Acc 0.5894
new best val f1: 0.5894045191456054
ogbn-arxiv,dgl,1,202,3.8645,0.5894

epoch:204/50, training loss:1.5859529972076416
Train Acc 0.5950
 Acc 0.5902
new best val f1: 0.5901872335166533
ogbn-arxiv,dgl,1,203,3.8776,0.5902

epoch:205/50, training loss:1.5819226503372192
Train Acc 0.5956
 Acc 0.5906
new best val f1: 0.5906197861953902
ogbn-arxiv,dgl,1,204,3.8900,0.5906

epoch:206/50, training loss:1.5779404640197754
Train Acc 0.5963
 Acc 0.5913
new best val f1: 0.5912583163401924
ogbn-arxiv,dgl,1,205,3.9025,0.5913

epoch:207/50, training loss:1.5739885568618774
Train Acc 0.5969
 Acc 0.5918
new best val f1: 0.5918350532451749
ogbn-arxiv,dgl,1,206,3.9149,0.5918

epoch:208/50, training loss:1.5700783729553223
Train Acc 0.5975
 Acc 0.5923
new best val f1: 0.592349996910338
ogbn-arxiv,dgl,1,207,3.9293,0.5923

epoch:209/50, training loss:1.5662249326705933
Train Acc 0.5982
 Acc 0.5929
new best val f1: 0.5929267338153206
ogbn-arxiv,dgl,1,208,3.9416,0.5929

epoch:210/50, training loss:1.562402367591858
Train Acc 0.5987
 Acc 0.5933
new best val f1: 0.593338688747451
ogbn-arxiv,dgl,1,209,3.9541,0.5933

epoch:211/50, training loss:1.5586124658584595
Train Acc 0.5993
 Acc 0.5941
new best val f1: 0.5941420008651054
ogbn-arxiv,dgl,1,210,3.9665,0.5941

epoch:212/50, training loss:1.5548748970031738
Train Acc 0.5999
 Acc 0.5946
new best val f1: 0.5946157490370554
ogbn-arxiv,dgl,1,211,3.9790,0.5946

epoch:213/50, training loss:1.5511696338653564
Train Acc 0.6005
 Acc 0.5950
new best val f1: 0.5949659107293662
ogbn-arxiv,dgl,1,212,3.9913,0.5950

epoch:214/50, training loss:1.5474882125854492
Train Acc 0.6012
 Acc 0.5954
new best val f1: 0.5953778656614966
ogbn-arxiv,dgl,1,213,4.0037,0.5954

epoch:215/50, training loss:1.5438644886016846
Train Acc 0.6016
 Acc 0.5958
new best val f1: 0.5958310160868401
ogbn-arxiv,dgl,1,214,4.0161,0.5958

epoch:216/50, training loss:1.5402681827545166
Train Acc 0.6023
 Acc 0.5969
new best val f1: 0.5968815011637727
ogbn-arxiv,dgl,1,215,4.0287,0.5969

epoch:217/50, training loss:1.5367119312286377
Train Acc 0.6030
 Acc 0.5975
new best val f1: 0.5974788358153618
ogbn-arxiv,dgl,1,216,4.0410,0.5975

epoch:218/50, training loss:1.5331916809082031
Train Acc 0.6036
 Acc 0.5981
new best val f1: 0.5981379637067705
ogbn-arxiv,dgl,1,217,4.0535,0.5981

epoch:219/50, training loss:1.5297051668167114
Train Acc 0.6042
 Acc 0.5987
new best val f1: 0.59867350511854
ogbn-arxiv,dgl,1,218,4.0658,0.5987

epoch:220/50, training loss:1.5262545347213745
Train Acc 0.6048
 Acc 0.5992
new best val f1: 0.599188448783703
ogbn-arxiv,dgl,1,219,4.0784,0.5992

epoch:221/50, training loss:1.5228426456451416
Train Acc 0.6053
 Acc 0.5996
new best val f1: 0.5996004037158335
ogbn-arxiv,dgl,1,220,4.0908,0.5996

epoch:222/50, training loss:1.5194576978683472
Train Acc 0.6057
 Acc 0.6003
new best val f1: 0.6002801293538487
ogbn-arxiv,dgl,1,221,4.1033,0.6003

epoch:223/50, training loss:1.5161211490631104
Train Acc 0.6061
 Acc 0.6008
new best val f1: 0.6008362685122247
ogbn-arxiv,dgl,1,222,4.1156,0.6008

epoch:224/50, training loss:1.5128002166748047
Train Acc 0.6068
 Acc 0.6012
new best val f1: 0.6012070279511421
ogbn-arxiv,dgl,1,223,4.1282,0.6012

epoch:225/50, training loss:1.5095218420028687
Train Acc 0.6073
 Acc 0.6018
new best val f1: 0.6018249603493377
ogbn-arxiv,dgl,1,224,4.1406,0.6018

epoch:226/50, training loss:1.506281852722168
Train Acc 0.6078
 Acc 0.6025
new best val f1: 0.6025252837339595
ogbn-arxiv,dgl,1,225,4.1531,0.6025

epoch:227/50, training loss:1.5030676126480103
Train Acc 0.6083
 Acc 0.6030
new best val f1: 0.6029990319059095
ogbn-arxiv,dgl,1,226,4.1654,0.6030

epoch:228/50, training loss:1.4998811483383179
Train Acc 0.6087
 Acc 0.6031
new best val f1: 0.6031226183855486
ogbn-arxiv,dgl,1,227,4.1780,0.6031

epoch:229/50, training loss:1.4967314004898071
Train Acc 0.6090
 Acc 0.6037
new best val f1: 0.6036993552905312
ogbn-arxiv,dgl,1,228,4.1925,0.6037

epoch:230/50, training loss:1.4936091899871826
Train Acc 0.6094
 Acc 0.6043
new best val f1: 0.6042966899421204
ogbn-arxiv,dgl,1,229,4.2050,0.6043

epoch:231/50, training loss:1.490525484085083
Train Acc 0.6099
 Acc 0.6045
new best val f1: 0.604482069661579
ogbn-arxiv,dgl,1,230,4.2174,0.6045

epoch:232/50, training loss:1.4874727725982666
Train Acc 0.6104
 Acc 0.6048
new best val f1: 0.6047910358606768
ogbn-arxiv,dgl,1,231,4.2299,0.6048

epoch:233/50, training loss:1.4844415187835693
Train Acc 0.6107
 Acc 0.6051
new best val f1: 0.6051205998063812
ogbn-arxiv,dgl,1,232,4.2423,0.6051

epoch:234/50, training loss:1.4814454317092896
Train Acc 0.6111
 Acc 0.6054
new best val f1: 0.6053883705122659
ogbn-arxiv,dgl,1,233,4.2565,0.6054

epoch:235/50, training loss:1.4784754514694214
Train Acc 0.6115
 Acc 0.6058
new best val f1: 0.6057797276977899
ogbn-arxiv,dgl,1,234,4.2689,0.6058

epoch:236/50, training loss:1.4755359888076782
Train Acc 0.6119
 Acc 0.6064
new best val f1: 0.6063564646027725
ogbn-arxiv,dgl,1,235,4.2814,0.6064

epoch:237/50, training loss:1.4726290702819824
Train Acc 0.6124
 Acc 0.6069
new best val f1: 0.6068714082679355
ogbn-arxiv,dgl,1,236,4.2938,0.6069

epoch:238/50, training loss:1.4697455167770386
Train Acc 0.6129
 Acc 0.6077
new best val f1: 0.6076541226389833
ogbn-arxiv,dgl,1,237,4.3063,0.6077

epoch:239/50, training loss:1.4668947458267212
Train Acc 0.6134
 Acc 0.6080
new best val f1: 0.6080042843312942
ogbn-arxiv,dgl,1,238,4.3187,0.6080

epoch:240/50, training loss:1.4640570878982544
Train Acc 0.6138
 Acc 0.6081
new best val f1: 0.6081484685575398
ogbn-arxiv,dgl,1,239,4.3312,0.6081

epoch:241/50, training loss:1.4612371921539307
Train Acc 0.6142
 Acc 0.6087
new best val f1: 0.6087252054625224
ogbn-arxiv,dgl,1,240,4.3435,0.6087

epoch:242/50, training loss:1.4584450721740723
Train Acc 0.6146
 Acc 0.6090
new best val f1: 0.6089929761684072
ogbn-arxiv,dgl,1,241,4.3561,0.6090

epoch:243/50, training loss:1.4557056427001953
Train Acc 0.6149
 Acc 0.6095
new best val f1: 0.6095285175801767
ogbn-arxiv,dgl,1,242,4.3685,0.6095

epoch:244/50, training loss:1.4529707431793213
Train Acc 0.6154
 Acc 0.6101
new best val f1: 0.6100846567385528
ogbn-arxiv,dgl,1,243,4.3810,0.6101

epoch:245/50, training loss:1.4502569437026978
Train Acc 0.6159
 Acc 0.6105
new best val f1: 0.6104760139240767
ogbn-arxiv,dgl,1,244,4.3934,0.6105

epoch:246/50, training loss:1.447572112083435
Train Acc 0.6163
 Acc 0.6110
new best val f1: 0.6109703598426333
ogbn-arxiv,dgl,1,245,4.4059,0.6110

epoch:247/50, training loss:1.4449058771133423
Train Acc 0.6167
 Acc 0.6113
new best val f1: 0.6112999237883375
ogbn-arxiv,dgl,1,246,4.4183,0.6113

epoch:248/50, training loss:1.442280888557434
Train Acc 0.6170
 Acc 0.6117
new best val f1: 0.6116500854806484
ogbn-arxiv,dgl,1,247,4.4307,0.6117

epoch:249/50, training loss:1.4396576881408691
Train Acc 0.6174
 Acc 0.6121
new best val f1: 0.6121444313992049
ogbn-arxiv,dgl,1,248,4.4431,0.6121

epoch:250/50, training loss:1.437067985534668
Train Acc 0.6179
 Acc 0.6127
new best val f1: 0.612700570557581
ogbn-arxiv,dgl,1,249,4.4556,0.6127

epoch:251/50, training loss:1.434495449066162
Train Acc 0.6184
 Acc 0.6128
new best val f1: 0.6128241570372202
ogbn-arxiv,dgl,1,250,4.4680,0.6128

epoch:252/50, training loss:1.4319568872451782
Train Acc 0.6186
 Acc 0.6131
new best val f1: 0.613133123236318
ogbn-arxiv,dgl,1,251,4.4805,0.6131

epoch:253/50, training loss:1.429438591003418
Train Acc 0.6189
 Acc 0.6135
new best val f1: 0.6135244804218418
ogbn-arxiv,dgl,1,252,4.4929,0.6135

epoch:254/50, training loss:1.4269384145736694
Train Acc 0.6194
 Acc 0.6140
new best val f1: 0.6139982285937918
ogbn-arxiv,dgl,1,253,4.5054,0.6140

epoch:255/50, training loss:1.4244530200958252
Train Acc 0.6198
 Acc 0.6144
new best val f1: 0.6144101835259222
ogbn-arxiv,dgl,1,254,4.5177,0.6144

epoch:256/50, training loss:1.4220000505447388
Train Acc 0.6201
 Acc 0.6149
new best val f1: 0.6149045294444788
ogbn-arxiv,dgl,1,255,4.5302,0.6149

epoch:257/50, training loss:1.4195634126663208
Train Acc 0.6206
 Acc 0.6154
new best val f1: 0.6154194731096418
ogbn-arxiv,dgl,1,256,4.5426,0.6154

epoch:258/50, training loss:1.4171528816223145
Train Acc 0.6211
 Acc 0.6160
new best val f1: 0.6159756122680179
ogbn-arxiv,dgl,1,257,4.5551,0.6160

epoch:259/50, training loss:1.4147545099258423
Train Acc 0.6215
 Acc 0.6163
new best val f1: 0.6162845784671157
ogbn-arxiv,dgl,1,258,4.5675,0.6163

epoch:260/50, training loss:1.4123798608779907
Train Acc 0.6218
 Acc 0.6167
new best val f1: 0.6167377288924591
ogbn-arxiv,dgl,1,259,4.5800,0.6167

epoch:261/50, training loss:1.4100310802459717
Train Acc 0.6223
 Acc 0.6170
new best val f1: 0.6170260973449505
ogbn-arxiv,dgl,1,260,4.5923,0.6170

epoch:262/50, training loss:1.407691478729248
Train Acc 0.6226
 Acc 0.6175
new best val f1: 0.6174586500236874
ogbn-arxiv,dgl,1,261,4.6048,0.6175

epoch:263/50, training loss:1.4053733348846436
Train Acc 0.6229
 Acc 0.6178
new best val f1: 0.6177882139693918
ogbn-arxiv,dgl,1,262,4.6171,0.6178

epoch:264/50, training loss:1.4030698537826538
Train Acc 0.6232
 Acc 0.6181
new best val f1: 0.6180559846752766
ogbn-arxiv,dgl,1,263,4.6296,0.6181

epoch:265/50, training loss:1.4007834196090698
Train Acc 0.6235
 Acc 0.6184
new best val f1: 0.6183649508743744
ogbn-arxiv,dgl,1,264,4.6419,0.6184

epoch:266/50, training loss:1.3985108137130737
Train Acc 0.6239
 Acc 0.6188
new best val f1: 0.6187975035531112
ogbn-arxiv,dgl,1,265,4.6544,0.6188

epoch:267/50, training loss:1.3962574005126953
Train Acc 0.6241
 Acc 0.6190
new best val f1: 0.6190446765123896
ogbn-arxiv,dgl,1,266,4.6667,0.6190

epoch:268/50, training loss:1.3940151929855347
Train Acc 0.6245
 Acc 0.6197
new best val f1: 0.6196626089105852
ogbn-arxiv,dgl,1,267,4.6793,0.6197

epoch:269/50, training loss:1.3917968273162842
Train Acc 0.6250
 Acc 0.6204
new best val f1: 0.6204041277884199
ogbn-arxiv,dgl,1,268,4.6916,0.6204

epoch:270/50, training loss:1.3895885944366455
Train Acc 0.6253
 Acc 0.6207
new best val f1: 0.6206718984943047
ogbn-arxiv,dgl,1,269,4.7041,0.6207

epoch:271/50, training loss:1.3874057531356812
Train Acc 0.6257
 Acc 0.6211
new best val f1: 0.6211456466662547
ogbn-arxiv,dgl,1,270,4.7164,0.6211

epoch:272/50, training loss:1.3852356672286987
Train Acc 0.6260
 Acc 0.6214
new best val f1: 0.621434015118746
ogbn-arxiv,dgl,1,271,4.7290,0.6214

epoch:273/50, training loss:1.3830777406692505
Train Acc 0.6263
 Acc 0.6218
new best val f1: 0.6217841768110569
ogbn-arxiv,dgl,1,272,4.7419,0.6218

epoch:274/50, training loss:1.3809361457824707
Train Acc 0.6265
 Acc 0.6221
new best val f1: 0.6220931430101547
ogbn-arxiv,dgl,1,273,4.7595,0.6221

epoch:275/50, training loss:1.3788104057312012
Train Acc 0.6268
 Acc 0.6223
new best val f1: 0.6222785227296134
ogbn-arxiv,dgl,1,274,4.7718,0.6223

epoch:276/50, training loss:1.3766978979110718
Train Acc 0.6271
 Acc 0.6226
new best val f1: 0.6226080866753178
ogbn-arxiv,dgl,1,275,4.7844,0.6226

epoch:277/50, training loss:1.3746029138565063
Train Acc 0.6275
 Acc 0.6228
new best val f1: 0.6228346618879894
ogbn-arxiv,dgl,1,276,4.7969,0.6228

epoch:278/50, training loss:1.3725265264511108
Train Acc 0.6278
 Acc 0.6233
new best val f1: 0.6233496055531524
ogbn-arxiv,dgl,1,277,4.8094,0.6233

epoch:279/50, training loss:1.3704485893249512
Train Acc 0.6281
 Acc 0.6237
new best val f1: 0.6236997672454634
ogbn-arxiv,dgl,1,278,4.8217,0.6237

epoch:280/50, training loss:1.3683940172195435
Train Acc 0.6284
 Acc 0.6239
new best val f1: 0.6239057447115286
ogbn-arxiv,dgl,1,279,4.8342,0.6239

epoch:281/50, training loss:1.3663469552993774
Train Acc 0.6287
 Acc 0.6243
new best val f1: 0.6242559064038394
ogbn-arxiv,dgl,1,280,4.8466,0.6243

epoch:282/50, training loss:1.3643131256103516
Train Acc 0.6291
 Acc 0.6246
new best val f1: 0.6246060680961503
ogbn-arxiv,dgl,1,281,4.8590,0.6246

epoch:283/50, training loss:1.3622899055480957
Train Acc 0.6294
 Acc 0.6248
new best val f1: 0.624791447815609
ogbn-arxiv,dgl,1,282,4.8714,0.6248

epoch:284/50, training loss:1.360278606414795
Train Acc 0.6297
 Acc 0.6251
new best val f1: 0.6250798162681003
ogbn-arxiv,dgl,1,283,4.8839,0.6251

epoch:285/50, training loss:1.3582853078842163
Train Acc 0.6300
 Acc 0.6255
new best val f1: 0.6255123689468373
ogbn-arxiv,dgl,1,284,4.8962,0.6255

epoch:286/50, training loss:1.3563064336776733
Train Acc 0.6303
 Acc 0.6259
new best val f1: 0.6259243238789677
ogbn-arxiv,dgl,1,285,4.9087,0.6259

epoch:287/50, training loss:1.354346752166748
Train Acc 0.6307
 Acc 0.6262
new best val f1: 0.6261714968382459
ogbn-arxiv,dgl,1,286,4.9211,0.6262

epoch:288/50, training loss:1.352401852607727
Train Acc 0.6310
 Acc 0.6267
new best val f1: 0.6267070382500155
ogbn-arxiv,dgl,1,287,4.9336,0.6267

epoch:289/50, training loss:1.3504691123962402
Train Acc 0.6315
 Acc 0.6270
new best val f1: 0.6269748089559002
ogbn-arxiv,dgl,1,288,4.9460,0.6270

epoch:290/50, training loss:1.348557472229004
Train Acc 0.6317
 Acc 0.6271
new best val f1: 0.6270983954355394
ogbn-arxiv,dgl,1,289,4.9585,0.6271

epoch:291/50, training loss:1.3466628789901733
Train Acc 0.6320
 Acc 0.6273
new best val f1: 0.627283775154998
ogbn-arxiv,dgl,1,290,4.9727,0.6273

epoch:292/50, training loss:1.3447766304016113
Train Acc 0.6324
 Acc 0.6278
new best val f1: 0.6278193165667676
ogbn-arxiv,dgl,1,291,4.9852,0.6278

epoch:293/50, training loss:1.3429081439971924
Train Acc 0.6328
 Acc 0.6279
new best val f1: 0.6279223052998002
ogbn-arxiv,dgl,1,292,4.9976,0.6279

epoch:294/50, training loss:1.3410534858703613
Train Acc 0.6330
 Acc 0.6283
new best val f1: 0.628313662485324
ogbn-arxiv,dgl,1,293,5.0100,0.6283

epoch:295/50, training loss:1.339206337928772
Train Acc 0.6334
 Acc 0.6285
new best val f1: 0.6284578467115698
ogbn-arxiv,dgl,1,294,5.0223,0.6285

epoch:296/50, training loss:1.3373816013336182
Train Acc 0.6338
 Acc 0.6286
new best val f1: 0.6286020309378154
ogbn-arxiv,dgl,1,295,5.0349,0.6286

epoch:297/50, training loss:1.3355646133422852
Train Acc 0.6340
 Acc 0.6288
new best val f1: 0.6288286061504872
ogbn-arxiv,dgl,1,296,5.0472,0.6288

epoch:298/50, training loss:1.3337562084197998
Train Acc 0.6344
 Acc 0.6294
new best val f1: 0.6293847453088632
ogbn-arxiv,dgl,1,297,5.0597,0.6294

epoch:299/50, training loss:1.3319658041000366
Train Acc 0.6347
 Acc 0.6298
new best val f1: 0.6298172979876001
ogbn-arxiv,dgl,1,298,5.0721,0.6298

epoch:300/50, training loss:1.3301875591278076
Train Acc 0.6350
 Acc 0.6302
new best val f1: 0.6302086551731241
ogbn-arxiv,dgl,1,299,5.0845,0.6302

epoch:301/50, training loss:1.3284186124801636
Train Acc 0.6353
 Acc 0.6306
new best val f1: 0.6306000123586479
ogbn-arxiv,dgl,1,300,5.0969,0.6306

epoch:302/50, training loss:1.3266699314117432
Train Acc 0.6356
 Acc 0.6310
new best val f1: 0.6310119672907784
ogbn-arxiv,dgl,1,301,5.1094,0.6310

epoch:303/50, training loss:1.3249284029006958
Train Acc 0.6359
 Acc 0.6314
new best val f1: 0.6313621289830893
ogbn-arxiv,dgl,1,302,5.1217,0.6314

epoch:304/50, training loss:1.32319974899292
Train Acc 0.6362
 Acc 0.6318
new best val f1: 0.6317740839152197
ogbn-arxiv,dgl,1,303,5.1342,0.6318

epoch:305/50, training loss:1.3214830160140991
Train Acc 0.6365
 Acc 0.6321
new best val f1: 0.6320830501143175
ogbn-arxiv,dgl,1,304,5.1467,0.6321

epoch:306/50, training loss:1.3197791576385498
Train Acc 0.6369
 Acc 0.6324
new best val f1: 0.6324126140600218
ogbn-arxiv,dgl,1,305,5.1592,0.6324

epoch:307/50, training loss:1.3180896043777466
Train Acc 0.6372
 Acc 0.6326
new best val f1: 0.6325979937794806
ogbn-arxiv,dgl,1,306,5.1715,0.6326

epoch:308/50, training loss:1.3164091110229492
Train Acc 0.6375
 Acc 0.6328
new best val f1: 0.6328245689921522
ogbn-arxiv,dgl,1,307,5.1841,0.6328

epoch:309/50, training loss:1.3147392272949219
Train Acc 0.6378
 Acc 0.6331
new best val f1: 0.633051144204824
ogbn-arxiv,dgl,1,308,5.1965,0.6331

epoch:310/50, training loss:1.3130812644958496
Train Acc 0.6380
 Acc 0.6334
new best val f1: 0.6334425013903479
ogbn-arxiv,dgl,1,309,5.2090,0.6334

epoch:311/50, training loss:1.311440110206604
Train Acc 0.6383
 Acc 0.6338
new best val f1: 0.6338338585758718
ogbn-arxiv,dgl,1,310,5.2214,0.6338

epoch:312/50, training loss:1.3098022937774658
Train Acc 0.6386
 Acc 0.6341
new best val f1: 0.6341222270283631
ogbn-arxiv,dgl,1,311,5.2339,0.6341

epoch:313/50, training loss:1.3081889152526855
Train Acc 0.6389
 Acc 0.6344
new best val f1: 0.6344311932274609
ogbn-arxiv,dgl,1,312,5.2463,0.6344

epoch:314/50, training loss:1.3065835237503052
Train Acc 0.6393
 Acc 0.6348
new best val f1: 0.6347607571731653
ogbn-arxiv,dgl,1,313,5.2588,0.6348

epoch:315/50, training loss:1.3049792051315308
Train Acc 0.6395
 Acc 0.6350
new best val f1: 0.634987332385837
ogbn-arxiv,dgl,1,314,5.2712,0.6350

epoch:316/50, training loss:1.3033908605575562
Train Acc 0.6397
 Acc 0.6352
new best val f1: 0.6351933098519023
ogbn-arxiv,dgl,1,315,5.2837,0.6352

epoch:317/50, training loss:1.3018171787261963
Train Acc 0.6401
 Acc 0.6355
new best val f1: 0.6354816783043935
ogbn-arxiv,dgl,1,316,5.2961,0.6355

epoch:318/50, training loss:1.3002506494522095
Train Acc 0.6403
 Acc 0.6360
new best val f1: 0.6359554264763435
ogbn-arxiv,dgl,1,317,5.3086,0.6360

epoch:319/50, training loss:1.298695683479309
Train Acc 0.6406
 Acc 0.6362
new best val f1: 0.6362231971822283
ogbn-arxiv,dgl,1,318,5.3210,0.6362

epoch:320/50, training loss:1.2971559762954712
Train Acc 0.6408
 Acc 0.6366
new best val f1: 0.6365939566211456
ogbn-arxiv,dgl,1,319,5.3334,0.6366

epoch:321/50, training loss:1.2956225872039795
Train Acc 0.6411
 Acc 0.6367
new best val f1: 0.6366969453541782
ogbn-arxiv,dgl,1,320,5.3458,0.6367

epoch:322/50, training loss:1.2940982580184937
Train Acc 0.6413
 Acc 0.6369
new best val f1: 0.6369029228202434
ogbn-arxiv,dgl,1,321,5.3583,0.6369

epoch:323/50, training loss:1.2925827503204346
Train Acc 0.6415
 Acc 0.6371
new best val f1: 0.6370677047930956
ogbn-arxiv,dgl,1,322,5.3705,0.6371

epoch:324/50, training loss:1.2910795211791992
Train Acc 0.6417
 Acc 0.6374
new best val f1: 0.6373766709921934
ogbn-arxiv,dgl,1,323,5.3830,0.6374

epoch:325/50, training loss:1.2895839214324951
Train Acc 0.6419
 Acc 0.6375
new best val f1: 0.6375208552184392
ogbn-arxiv,dgl,1,324,5.3954,0.6375

epoch:326/50, training loss:1.2880969047546387
Train Acc 0.6422
 Acc 0.6379
new best val f1: 0.6378916146573564
ogbn-arxiv,dgl,1,325,5.4079,0.6379

epoch:327/50, training loss:1.2866214513778687
Train Acc 0.6425
 Acc 0.6381
new best val f1: 0.6381181898700282
ogbn-arxiv,dgl,1,326,5.4203,0.6381

epoch:328/50, training loss:1.2851595878601074
Train Acc 0.6426
 Acc 0.6384
new best val f1: 0.6384477538157326
ogbn-arxiv,dgl,1,327,5.4328,0.6384

epoch:329/50, training loss:1.2837083339691162
Train Acc 0.6429
 Acc 0.6387
new best val f1: 0.6387155245216173
ogbn-arxiv,dgl,1,328,5.4452,0.6387

epoch:330/50, training loss:1.2822701930999756
Train Acc 0.6432
 Acc 0.6390
new best val f1: 0.6390038929741086
ogbn-arxiv,dgl,1,329,5.4577,0.6390

epoch:331/50, training loss:1.2808408737182617
Train Acc 0.6435
 Acc 0.6393
new best val f1: 0.639333456919813
ogbn-arxiv,dgl,1,330,5.4701,0.6393

epoch:332/50, training loss:1.2794214487075806
Train Acc 0.6438
 Acc 0.6397
new best val f1: 0.6396836186121239
ogbn-arxiv,dgl,1,331,5.4826,0.6397

epoch:333/50, training loss:1.2780156135559082
Train Acc 0.6441
 Acc 0.6397
new best val f1: 0.6397248141053369
ogbn-arxiv,dgl,1,332,5.4950,0.6397

epoch:334/50, training loss:1.2766129970550537
Train Acc 0.6442
 Acc 0.6401
new best val f1: 0.6400749757976477
ogbn-arxiv,dgl,1,333,5.5075,0.6401

epoch:335/50, training loss:1.275225281715393
Train Acc 0.6446
 Acc 0.6403
new best val f1: 0.6403015510103195
ogbn-arxiv,dgl,1,334,5.5199,0.6403

epoch:336/50, training loss:1.2738438844680786
Train Acc 0.6449
 Acc 0.6403
ogbn-arxiv,dgl,1,335,5.5324,0.6403

epoch:337/50, training loss:1.2724748849868774
Train Acc 0.6451
 Acc 0.6405
new best val f1: 0.6405487239695977
ogbn-arxiv,dgl,1,336,5.5447,0.6405

epoch:338/50, training loss:1.2711131572723389
Train Acc 0.6454
 Acc 0.6407
new best val f1: 0.6406929081958433
ogbn-arxiv,dgl,1,337,5.5572,0.6407

epoch:339/50, training loss:1.2697594165802002
Train Acc 0.6456
 Acc 0.6407
ogbn-arxiv,dgl,1,338,5.5696,0.6407

epoch:340/50, training loss:1.2684235572814941
Train Acc 0.6458
 Acc 0.6410
new best val f1: 0.6410224721415477
ogbn-arxiv,dgl,1,339,5.5822,0.6410

epoch:341/50, training loss:1.2670881748199463
Train Acc 0.6461
 Acc 0.6412
new best val f1: 0.6412078518610064
ogbn-arxiv,dgl,1,340,5.5946,0.6412

epoch:342/50, training loss:1.2657616138458252
Train Acc 0.6463
 Acc 0.6416
new best val f1: 0.6415580135533173
ogbn-arxiv,dgl,1,341,5.6071,0.6416

epoch:343/50, training loss:1.2644509077072144
Train Acc 0.6466
 Acc 0.6417
new best val f1: 0.6416816000329564
ogbn-arxiv,dgl,1,342,5.6195,0.6417

epoch:344/50, training loss:1.263140082359314
Train Acc 0.6468
 Acc 0.6419
new best val f1: 0.6418875774990216
ogbn-arxiv,dgl,1,343,5.6320,0.6419

epoch:345/50, training loss:1.2618443965911865
Train Acc 0.6470
 Acc 0.6423
new best val f1: 0.642258336937939
ogbn-arxiv,dgl,1,344,5.6444,0.6423

epoch:346/50, training loss:1.260561227798462
Train Acc 0.6472
 Acc 0.6428
new best val f1: 0.6427526828564954
ogbn-arxiv,dgl,1,345,5.6568,0.6428

epoch:347/50, training loss:1.2592859268188477
Train Acc 0.6476
 Acc 0.6430
new best val f1: 0.6430204535623802
ogbn-arxiv,dgl,1,346,5.6693,0.6430

epoch:348/50, training loss:1.2580163478851318
Train Acc 0.6478
 Acc 0.6432
new best val f1: 0.643205833281839
ogbn-arxiv,dgl,1,347,5.6818,0.6432

epoch:349/50, training loss:1.2567545175552368
Train Acc 0.6480
 Acc 0.6434
new best val f1: 0.6434324084945107
ogbn-arxiv,dgl,1,348,5.6942,0.6434

epoch:350/50, training loss:1.25550377368927
Train Acc 0.6484
 Acc 0.6439
new best val f1: 0.6438649611732477
ogbn-arxiv,dgl,1,349,5.7067,0.6439

epoch:351/50, training loss:1.2542608976364136
Train Acc 0.6487
 Acc 0.6441
new best val f1: 0.6440503408927063
ogbn-arxiv,dgl,1,350,5.7191,0.6441

epoch:352/50, training loss:1.2530244588851929
Train Acc 0.6490
 Acc 0.6441
new best val f1: 0.6441327318791324
ogbn-arxiv,dgl,1,351,5.7317,0.6441

epoch:353/50, training loss:1.2518025636672974
Train Acc 0.6492
 Acc 0.6443
new best val f1: 0.6443387093451977
ogbn-arxiv,dgl,1,352,5.7442,0.6443

epoch:354/50, training loss:1.2505847215652466
Train Acc 0.6494
 Acc 0.6448
new best val f1: 0.6447506642773281
ogbn-arxiv,dgl,1,353,5.7566,0.6448

epoch:355/50, training loss:1.2493700981140137
Train Acc 0.6498
 Acc 0.6449
new best val f1: 0.6449360439967867
ogbn-arxiv,dgl,1,354,5.7690,0.6449

epoch:356/50, training loss:1.2481738328933716
Train Acc 0.6499
 Acc 0.6452
new best val f1: 0.645183216956065
ogbn-arxiv,dgl,1,355,5.7815,0.6452

epoch:357/50, training loss:1.2469782829284668
Train Acc 0.6502
 Acc 0.6455
new best val f1: 0.6454921831551629
ogbn-arxiv,dgl,1,356,5.7939,0.6455

epoch:358/50, training loss:1.2457941770553589
Train Acc 0.6504
 Acc 0.6455
new best val f1: 0.6455127809017693
ogbn-arxiv,dgl,1,357,5.8064,0.6455

epoch:359/50, training loss:1.244617223739624
Train Acc 0.6506
 Acc 0.6457
new best val f1: 0.6457393561144411
ogbn-arxiv,dgl,1,358,5.8188,0.6457

epoch:360/50, training loss:1.2434453964233398
Train Acc 0.6508
 Acc 0.6460
new best val f1: 0.6460277245669324
ogbn-arxiv,dgl,1,359,5.8313,0.6460

epoch:361/50, training loss:1.2422846555709839
Train Acc 0.6511
 Acc 0.6462
new best val f1: 0.646171908793178
ogbn-arxiv,dgl,1,360,5.8437,0.6462

epoch:362/50, training loss:1.2411319017410278
Train Acc 0.6512
 Acc 0.6465
new best val f1: 0.6465220704854889
ogbn-arxiv,dgl,1,361,5.8562,0.6465

epoch:363/50, training loss:1.2399766445159912
Train Acc 0.6515
 Acc 0.6469
new best val f1: 0.6468722321777998
ogbn-arxiv,dgl,1,362,5.8686,0.6469

epoch:364/50, training loss:1.2388365268707275
Train Acc 0.6519
 Acc 0.6472
new best val f1: 0.647160600630291
ogbn-arxiv,dgl,1,363,5.8811,0.6472

epoch:365/50, training loss:1.2376974821090698
Train Acc 0.6521
 Acc 0.6474
new best val f1: 0.6474077735895692
ogbn-arxiv,dgl,1,364,5.8935,0.6474

epoch:366/50, training loss:1.2365666627883911
Train Acc 0.6523
 Acc 0.6477
new best val f1: 0.647675544295454
ogbn-arxiv,dgl,1,365,5.9061,0.6477

epoch:367/50, training loss:1.23543381690979
Train Acc 0.6525
 Acc 0.6478
new best val f1: 0.6478403262683062
ogbn-arxiv,dgl,1,366,5.9184,0.6478

epoch:368/50, training loss:1.2343003749847412
Train Acc 0.6527
 Acc 0.6476
ogbn-arxiv,dgl,1,367,5.9309,0.6476

epoch:369/50, training loss:1.2331699132919312
Train Acc 0.6528
 Acc 0.6478
ogbn-arxiv,dgl,1,368,5.9433,0.6478

epoch:370/50, training loss:1.2320544719696045
Train Acc 0.6530
 Acc 0.6481
new best val f1: 0.648066901480978
ogbn-arxiv,dgl,1,369,5.9558,0.6481

epoch:371/50, training loss:1.2309378385543823
Train Acc 0.6533
 Acc 0.6485
new best val f1: 0.6484994541597149
ogbn-arxiv,dgl,1,370,5.9681,0.6485

epoch:372/50, training loss:1.2298229932785034
Train Acc 0.6536
 Acc 0.6489
new best val f1: 0.6489320068384519
ogbn-arxiv,dgl,1,371,5.9806,0.6489

epoch:373/50, training loss:1.228729009628296
Train Acc 0.6538
 Acc 0.6491
new best val f1: 0.6490967888113041
ogbn-arxiv,dgl,1,372,5.9929,0.6491

epoch:374/50, training loss:1.2276391983032227
Train Acc 0.6539
 Acc 0.6491
ogbn-arxiv,dgl,1,373,6.0054,0.6491

epoch:375/50, training loss:1.2265536785125732
Train Acc 0.6542
 Acc 0.6489
ogbn-arxiv,dgl,1,374,6.0178,0.6489

epoch:376/50, training loss:1.225475788116455
Train Acc 0.6543
 Acc 0.6491
ogbn-arxiv,dgl,1,375,6.0304,0.6491

epoch:377/50, training loss:1.2244051694869995
Train Acc 0.6544
 Acc 0.6494
new best val f1: 0.6494469505036149
ogbn-arxiv,dgl,1,376,6.0428,0.6494

epoch:378/50, training loss:1.2233436107635498
Train Acc 0.6547
 Acc 0.6500
new best val f1: 0.6500442851552041
ogbn-arxiv,dgl,1,377,6.0553,0.6500

epoch:379/50, training loss:1.2222881317138672
Train Acc 0.6550
 Acc 0.6504
new best val f1: 0.6503738491009083
ogbn-arxiv,dgl,1,378,6.0676,0.6504

epoch:380/50, training loss:1.221236228942871
Train Acc 0.6553
 Acc 0.6504
new best val f1: 0.6504356423407279
ogbn-arxiv,dgl,1,379,6.0801,0.6504

epoch:381/50, training loss:1.22019362449646
Train Acc 0.6554
 Acc 0.6506
new best val f1: 0.6505592288203671
ogbn-arxiv,dgl,1,380,6.0925,0.6506

epoch:382/50, training loss:1.21915602684021
Train Acc 0.6556
 Acc 0.6507
new best val f1: 0.6507446085398257
ogbn-arxiv,dgl,1,381,6.1050,0.6507

epoch:383/50, training loss:1.2181310653686523
Train Acc 0.6558
 Acc 0.6510
new best val f1: 0.650950586005891
ogbn-arxiv,dgl,1,382,6.1174,0.6510

epoch:384/50, training loss:1.2171075344085693
Train Acc 0.6561
 Acc 0.6512
new best val f1: 0.6512183567117757
ogbn-arxiv,dgl,1,383,6.1298,0.6512

epoch:385/50, training loss:1.2160892486572266
Train Acc 0.6564
 Acc 0.6515
new best val f1: 0.651465529671054
ogbn-arxiv,dgl,1,384,6.1422,0.6515

epoch:386/50, training loss:1.21507728099823
Train Acc 0.6566
 Acc 0.6518
new best val f1: 0.6517744958701518
ogbn-arxiv,dgl,1,385,6.1546,0.6518

epoch:387/50, training loss:1.2140803337097168
Train Acc 0.6568
 Acc 0.6522
new best val f1: 0.6522070485488888
ogbn-arxiv,dgl,1,386,6.1670,0.6522

epoch:388/50, training loss:1.2130777835845947
Train Acc 0.6572
 Acc 0.6523
new best val f1: 0.6523306350285278
ogbn-arxiv,dgl,1,387,6.1795,0.6523

epoch:389/50, training loss:1.2120871543884277
Train Acc 0.6572
 Acc 0.6526
new best val f1: 0.6525572102411996
ogbn-arxiv,dgl,1,388,6.1918,0.6526

epoch:390/50, training loss:1.2111005783081055
Train Acc 0.6575
 Acc 0.6525
ogbn-arxiv,dgl,1,389,6.2044,0.6525

epoch:391/50, training loss:1.2101263999938965
Train Acc 0.6577
 Acc 0.6527
new best val f1: 0.6526601989742322
ogbn-arxiv,dgl,1,390,6.2168,0.6527

epoch:392/50, training loss:1.2091511487960815
Train Acc 0.6578
 Acc 0.6528
new best val f1: 0.6528455786936909
ogbn-arxiv,dgl,1,391,6.2293,0.6528

epoch:393/50, training loss:1.2081868648529053
Train Acc 0.6580
 Acc 0.6532
new best val f1: 0.6531957403860018
ogbn-arxiv,dgl,1,392,6.2417,0.6532

epoch:394/50, training loss:1.2072200775146484
Train Acc 0.6582
 Acc 0.6533
new best val f1: 0.6532781313724279
ogbn-arxiv,dgl,1,393,6.2542,0.6533

epoch:395/50, training loss:1.206269383430481
Train Acc 0.6584
 Acc 0.6535
new best val f1: 0.6535047065850996
ogbn-arxiv,dgl,1,394,6.2665,0.6535

epoch:396/50, training loss:1.2053236961364746
Train Acc 0.6586
 Acc 0.6539
new best val f1: 0.6538548682774105
ogbn-arxiv,dgl,1,395,6.2791,0.6539

epoch:397/50, training loss:1.2043819427490234
Train Acc 0.6589
 Acc 0.6540
new best val f1: 0.6539990525036561
ogbn-arxiv,dgl,1,396,6.2914,0.6540

epoch:398/50, training loss:1.203447937965393
Train Acc 0.6590
 Acc 0.6543
new best val f1: 0.6542874209561474
ogbn-arxiv,dgl,1,397,6.3040,0.6543

epoch:399/50, training loss:1.2025165557861328
Train Acc 0.6592
 Acc 0.6545
new best val f1: 0.6544933984222127
ogbn-arxiv,dgl,1,398,6.3164,0.6545

epoch:400/50, training loss:1.2015959024429321
Train Acc 0.6595
 Acc 0.6545
ogbn-arxiv,dgl,1,399,6.3289,0.6545

epoch:401/50, training loss:1.2006803750991821
Train Acc 0.6596
 Acc 0.6547
new best val f1: 0.6546581803950647
ogbn-arxiv,dgl,1,400,6.3413,0.6547

epoch:402/50, training loss:1.1997648477554321
Train Acc 0.6598
 Acc 0.6547
new best val f1: 0.6546993758882779
ogbn-arxiv,dgl,1,401,6.3538,0.6547

epoch:403/50, training loss:1.1988592147827148
Train Acc 0.6600
 Acc 0.6552
new best val f1: 0.6552349173000473
ogbn-arxiv,dgl,1,402,6.3662,0.6552

epoch:404/50, training loss:1.1979576349258423
Train Acc 0.6603
 Acc 0.6555
new best val f1: 0.6554614925127191
ogbn-arxiv,dgl,1,403,6.3786,0.6555

epoch:405/50, training loss:1.1970621347427368
Train Acc 0.6604
 Acc 0.6556
new best val f1: 0.6556056767389647
ogbn-arxiv,dgl,1,404,6.3909,0.6556

epoch:406/50, training loss:1.196170687675476
Train Acc 0.6607
 Acc 0.6557
new best val f1: 0.6557292632186039
ogbn-arxiv,dgl,1,405,6.4034,0.6557

epoch:407/50, training loss:1.19529128074646
Train Acc 0.6608
 Acc 0.6560
new best val f1: 0.6559764361778821
ogbn-arxiv,dgl,1,406,6.4175,0.6560

epoch:408/50, training loss:1.194411039352417
Train Acc 0.6610
 Acc 0.6560
new best val f1: 0.6560382294177017
ogbn-arxiv,dgl,1,407,6.4300,0.6560

epoch:409/50, training loss:1.193537712097168
Train Acc 0.6612
 Acc 0.6562
new best val f1: 0.6562236091371604
ogbn-arxiv,dgl,1,408,6.4433,0.6562

epoch:410/50, training loss:1.1926724910736084
Train Acc 0.6613
 Acc 0.6565
new best val f1: 0.6565325753362582
ogbn-arxiv,dgl,1,409,6.4559,0.6565

epoch:411/50, training loss:1.191811203956604
Train Acc 0.6616
 Acc 0.6568
new best val f1: 0.656841541535356
ogbn-arxiv,dgl,1,410,6.4682,0.6568

epoch:412/50, training loss:1.1909559965133667
Train Acc 0.6618
 Acc 0.6571
new best val f1: 0.6570887144946342
ogbn-arxiv,dgl,1,411,6.4808,0.6571

epoch:413/50, training loss:1.1901060342788696
Train Acc 0.6621
 Acc 0.6572
new best val f1: 0.65723289872088
ogbn-arxiv,dgl,1,412,6.4932,0.6572

epoch:414/50, training loss:1.1892597675323486
Train Acc 0.6622
 Acc 0.6573
new best val f1: 0.6572946919606995
ogbn-arxiv,dgl,1,413,6.5057,0.6573

epoch:415/50, training loss:1.1884194612503052
Train Acc 0.6623
 Acc 0.6576
new best val f1: 0.6575624626665842
ogbn-arxiv,dgl,1,414,6.5180,0.6576

epoch:416/50, training loss:1.1875845193862915
Train Acc 0.6625
 Acc 0.6579
new best val f1: 0.6579126243588951
ogbn-arxiv,dgl,1,415,6.5305,0.6579

epoch:417/50, training loss:1.186753511428833
Train Acc 0.6628
 Acc 0.6581
new best val f1: 0.6580980040783538
ogbn-arxiv,dgl,1,416,6.5428,0.6581

epoch:418/50, training loss:1.1859349012374878
Train Acc 0.6630
 Acc 0.6584
new best val f1: 0.6583863725308451
ogbn-arxiv,dgl,1,417,6.5553,0.6584

epoch:419/50, training loss:1.1851166486740112
Train Acc 0.6632
 Acc 0.6587
new best val f1: 0.6586541432367299
ogbn-arxiv,dgl,1,418,6.5678,0.6587

epoch:420/50, training loss:1.1843042373657227
Train Acc 0.6634
 Acc 0.6589
new best val f1: 0.6588807184494017
ogbn-arxiv,dgl,1,419,6.5803,0.6589

epoch:421/50, training loss:1.1834919452667236
Train Acc 0.6636
 Acc 0.6589
new best val f1: 0.6589013161960081
ogbn-arxiv,dgl,1,420,6.5926,0.6589

epoch:422/50, training loss:1.1826950311660767
Train Acc 0.6637
 Acc 0.6592
new best val f1: 0.6591896846484995
ogbn-arxiv,dgl,1,421,6.6062,0.6592

epoch:423/50, training loss:1.1818925142288208
Train Acc 0.6640
 Acc 0.6592
ogbn-arxiv,dgl,1,422,6.6185,0.6592

epoch:424/50, training loss:1.1811062097549438
Train Acc 0.6642
 Acc 0.6594
new best val f1: 0.6593956621145647
ogbn-arxiv,dgl,1,423,6.6310,0.6594

epoch:425/50, training loss:1.180314540863037
Train Acc 0.6643
 Acc 0.6595
new best val f1: 0.6594780531009907
ogbn-arxiv,dgl,1,424,6.6452,0.6595

epoch:426/50, training loss:1.179539442062378
Train Acc 0.6644
 Acc 0.6596
new best val f1: 0.6596016395806299
ogbn-arxiv,dgl,1,425,6.6578,0.6596

epoch:427/50, training loss:1.1787543296813965
Train Acc 0.6645
 Acc 0.6597
new best val f1: 0.6596634328204495
ogbn-arxiv,dgl,1,426,6.6702,0.6597

epoch:428/50, training loss:1.1779837608337402
Train Acc 0.6647
 Acc 0.6597
new best val f1: 0.6597458238068755
ogbn-arxiv,dgl,1,427,6.6826,0.6597

epoch:429/50, training loss:1.1772167682647705
Train Acc 0.6648
 Acc 0.6598
new best val f1: 0.6598488125399081
ogbn-arxiv,dgl,1,428,6.6950,0.6598

epoch:430/50, training loss:1.1764570474624634
Train Acc 0.6649
 Acc 0.6599
new best val f1: 0.6599312035263342
ogbn-arxiv,dgl,1,429,6.7075,0.6599

epoch:431/50, training loss:1.1756986379623413
Train Acc 0.6651
 Acc 0.6601
new best val f1: 0.6601165832457929
ogbn-arxiv,dgl,1,430,6.7198,0.6601

epoch:432/50, training loss:1.1749439239501953
Train Acc 0.6652
 Acc 0.6603
new best val f1: 0.6603431584584647
ogbn-arxiv,dgl,1,431,6.7323,0.6603

epoch:433/50, training loss:1.1741971969604492
Train Acc 0.6653
 Acc 0.6608
new best val f1: 0.6607963088838081
ogbn-arxiv,dgl,1,432,6.7447,0.6608

epoch:434/50, training loss:1.173456072807312
Train Acc 0.6656
 Acc 0.6607
ogbn-arxiv,dgl,1,433,6.7571,0.6607

epoch:435/50, training loss:1.1727145910263062
Train Acc 0.6657
 Acc 0.6609
new best val f1: 0.6608992976168407
ogbn-arxiv,dgl,1,434,6.7695,0.6609

epoch:436/50, training loss:1.1719870567321777
Train Acc 0.6658
 Acc 0.6609
new best val f1: 0.6609198953634472
ogbn-arxiv,dgl,1,435,6.7820,0.6609

epoch:437/50, training loss:1.1712524890899658
Train Acc 0.6659
 Acc 0.6612
new best val f1: 0.6612494593091516
ogbn-arxiv,dgl,1,436,6.7944,0.6612

epoch:438/50, training loss:1.1705282926559448
Train Acc 0.6661
 Acc 0.6614
new best val f1: 0.6614142412820038
ogbn-arxiv,dgl,1,437,6.8069,0.6614

epoch:439/50, training loss:1.1698098182678223
Train Acc 0.6663
 Acc 0.6615
new best val f1: 0.6614760345218234
ogbn-arxiv,dgl,1,438,6.8193,0.6615

epoch:440/50, training loss:1.1690911054611206
Train Acc 0.6664
 Acc 0.6615
new best val f1: 0.6615378277616428
ogbn-arxiv,dgl,1,439,6.8318,0.6615

epoch:441/50, training loss:1.1683839559555054
Train Acc 0.6667
 Acc 0.6618
new best val f1: 0.6618055984675276
ogbn-arxiv,dgl,1,440,6.8441,0.6618

epoch:442/50, training loss:1.167676329612732
Train Acc 0.6668
 Acc 0.6617
ogbn-arxiv,dgl,1,441,6.8566,0.6617

epoch:443/50, training loss:1.1669739484786987
Train Acc 0.6668
 Acc 0.6619
new best val f1: 0.6618673917073472
ogbn-arxiv,dgl,1,442,6.8690,0.6619

epoch:444/50, training loss:1.1662753820419312
Train Acc 0.6670
 Acc 0.6619
new best val f1: 0.6619291849471668
ogbn-arxiv,dgl,1,443,6.8814,0.6619

epoch:445/50, training loss:1.1655845642089844
Train Acc 0.6671
 Acc 0.6621
new best val f1: 0.6620733691734124
ogbn-arxiv,dgl,1,444,6.8937,0.6621

epoch:446/50, training loss:1.1648967266082764
Train Acc 0.6673
 Acc 0.6623
new best val f1: 0.6623205421326906
ogbn-arxiv,dgl,1,445,6.9062,0.6623

epoch:447/50, training loss:1.164210557937622
Train Acc 0.6674
 Acc 0.6625
new best val f1: 0.6624853241055428
ogbn-arxiv,dgl,1,446,6.9186,0.6625

epoch:448/50, training loss:1.1635292768478394
Train Acc 0.6676
 Acc 0.6626
new best val f1: 0.6625883128385754
ogbn-arxiv,dgl,1,447,6.9312,0.6626

epoch:449/50, training loss:1.1628526449203491
Train Acc 0.6678
 Acc 0.6627
new best val f1: 0.6627324970648211
ogbn-arxiv,dgl,1,448,6.9435,0.6627

epoch:450/50, training loss:1.162183403968811
Train Acc 0.6678
 Acc 0.6628
new best val f1: 0.6628148880512472
ogbn-arxiv,dgl,1,449,6.9561,0.6628

epoch:451/50, training loss:1.1615148782730103
Train Acc 0.6680
 Acc 0.6632
new best val f1: 0.663165049743558
ogbn-arxiv,dgl,1,450,6.9684,0.6632

epoch:452/50, training loss:1.1608514785766602
Train Acc 0.6681
 Acc 0.6634
new best val f1: 0.6633504294630167
ogbn-arxiv,dgl,1,451,6.9809,0.6634

epoch:453/50, training loss:1.160189151763916
Train Acc 0.6683
 Acc 0.6634
new best val f1: 0.6634328204494429
ogbn-arxiv,dgl,1,452,6.9933,0.6634

epoch:454/50, training loss:1.159534215927124
Train Acc 0.6685
 Acc 0.6633
ogbn-arxiv,dgl,1,453,7.0057,0.6633

epoch:455/50, training loss:1.1588839292526245
Train Acc 0.6685
 Acc 0.6637
new best val f1: 0.6636799934087211
ogbn-arxiv,dgl,1,454,7.0181,0.6637

epoch:456/50, training loss:1.1582353115081787
Train Acc 0.6688
 Acc 0.6638
new best val f1: 0.6637623843951471
ogbn-arxiv,dgl,1,455,7.0305,0.6638

epoch:457/50, training loss:1.1575921773910522
Train Acc 0.6690
 Acc 0.6641
new best val f1: 0.6640507528476385
ogbn-arxiv,dgl,1,456,7.0429,0.6641

epoch:458/50, training loss:1.1569525003433228
Train Acc 0.6691
 Acc 0.6640
ogbn-arxiv,dgl,1,457,7.0554,0.6640

epoch:459/50, training loss:1.156319260597229
Train Acc 0.6691
 Acc 0.6642
new best val f1: 0.6641743393272775
ogbn-arxiv,dgl,1,458,7.0677,0.6642

epoch:460/50, training loss:1.1556869745254517
Train Acc 0.6692
 Acc 0.6643
new best val f1: 0.6643391213001297
ogbn-arxiv,dgl,1,459,7.0802,0.6643

epoch:461/50, training loss:1.1550580263137817
Train Acc 0.6694
 Acc 0.6644
new best val f1: 0.6644009145399493
ogbn-arxiv,dgl,1,460,7.0927,0.6644

epoch:462/50, training loss:1.154432773590088
Train Acc 0.6695
 Acc 0.6646
new best val f1: 0.6646068920060145
ogbn-arxiv,dgl,1,461,7.1052,0.6646

epoch:463/50, training loss:1.1538127660751343
Train Acc 0.6696
 Acc 0.6649
new best val f1: 0.6648540649652928
ogbn-arxiv,dgl,1,462,7.1175,0.6649

epoch:464/50, training loss:1.1531944274902344
Train Acc 0.6698
 Acc 0.6650
new best val f1: 0.6650394446847515
ogbn-arxiv,dgl,1,463,7.1300,0.6650

epoch:465/50, training loss:1.1525834798812866
Train Acc 0.6700
 Acc 0.6652
new best val f1: 0.6651630311643906
ogbn-arxiv,dgl,1,464,7.1424,0.6652

epoch:466/50, training loss:1.1519711017608643
Train Acc 0.6701
 Acc 0.6652
new best val f1: 0.6652454221508167
ogbn-arxiv,dgl,1,465,7.1550,0.6652

epoch:467/50, training loss:1.151366114616394
Train Acc 0.6702
 Acc 0.6654
new best val f1: 0.6654308018702754
ogbn-arxiv,dgl,1,466,7.1674,0.6654

epoch:468/50, training loss:1.150763750076294
Train Acc 0.6703
 Acc 0.6654
ogbn-arxiv,dgl,1,467,7.1799,0.6654

epoch:469/50, training loss:1.1501656770706177
Train Acc 0.6704
 Acc 0.6656
new best val f1: 0.6655955838431276
ogbn-arxiv,dgl,1,468,7.1923,0.6656

epoch:470/50, training loss:1.1495684385299683
Train Acc 0.6705
 Acc 0.6655
ogbn-arxiv,dgl,1,469,7.2049,0.6655

epoch:471/50, training loss:1.1489770412445068
Train Acc 0.6706
 Acc 0.6656
ogbn-arxiv,dgl,1,470,7.2172,0.6656

epoch:472/50, training loss:1.148389220237732
Train Acc 0.6707
 Acc 0.6657
new best val f1: 0.6656985725761602
ogbn-arxiv,dgl,1,471,7.2298,0.6657

epoch:473/50, training loss:1.1478031873703003
Train Acc 0.6709
 Acc 0.6659
new best val f1: 0.6658839522956188
ogbn-arxiv,dgl,1,472,7.2421,0.6659

epoch:474/50, training loss:1.1472201347351074
Train Acc 0.6710
 Acc 0.6661
new best val f1: 0.6660693320150776
ogbn-arxiv,dgl,1,473,7.2546,0.6661

epoch:475/50, training loss:1.1466457843780518
Train Acc 0.6712
 Acc 0.6662
new best val f1: 0.6662135162413232
ogbn-arxiv,dgl,1,474,7.2669,0.6662

epoch:476/50, training loss:1.1460671424865723
Train Acc 0.6714
 Acc 0.6663
new best val f1: 0.6662959072277492
ogbn-arxiv,dgl,1,475,7.2794,0.6663

epoch:477/50, training loss:1.145495057106018
Train Acc 0.6714
 Acc 0.6663
new best val f1: 0.6663371027209624
ogbn-arxiv,dgl,1,476,7.2917,0.6663

epoch:478/50, training loss:1.1449254751205444
Train Acc 0.6716
 Acc 0.6665
new best val f1: 0.6664606892006014
ogbn-arxiv,dgl,1,477,7.3042,0.6665

epoch:479/50, training loss:1.1443581581115723
Train Acc 0.6717
 Acc 0.6667
new best val f1: 0.6667490576530928
ogbn-arxiv,dgl,1,478,7.3165,0.6667

epoch:480/50, training loss:1.1437971591949463
Train Acc 0.6719
 Acc 0.6668
new best val f1: 0.6667696553996992
ogbn-arxiv,dgl,1,479,7.3290,0.6668

epoch:481/50, training loss:1.143236517906189
Train Acc 0.6720
 Acc 0.6670
new best val f1: 0.667037426105584
ogbn-arxiv,dgl,1,480,7.3414,0.6670

epoch:482/50, training loss:1.1426808834075928
Train Acc 0.6722
 Acc 0.6672
new best val f1: 0.6671610125852232
ogbn-arxiv,dgl,1,481,7.3539,0.6672

epoch:483/50, training loss:1.1421279907226562
Train Acc 0.6724
 Acc 0.6671
ogbn-arxiv,dgl,1,482,7.3663,0.6671

epoch:484/50, training loss:1.141576886177063
Train Acc 0.6724
 Acc 0.6672
new best val f1: 0.6672434035716492
ogbn-arxiv,dgl,1,483,7.3788,0.6672

epoch:485/50, training loss:1.1410295963287354
Train Acc 0.6726
 Acc 0.6674
new best val f1: 0.6673875877978949
ogbn-arxiv,dgl,1,484,7.3912,0.6674

epoch:486/50, training loss:1.1404825448989868
Train Acc 0.6726
 Acc 0.6673
ogbn-arxiv,dgl,1,485,7.4036,0.6673

epoch:487/50, training loss:1.1399424076080322
Train Acc 0.6727
 Acc 0.6675
new best val f1: 0.6675317720241406
ogbn-arxiv,dgl,1,486,7.4160,0.6675

epoch:488/50, training loss:1.139402151107788
Train Acc 0.6729
 Acc 0.6677
new best val f1: 0.6677171517435992
ogbn-arxiv,dgl,1,487,7.4285,0.6677

epoch:489/50, training loss:1.1388664245605469
Train Acc 0.6730
 Acc 0.6676
ogbn-arxiv,dgl,1,488,7.4408,0.6676

epoch:490/50, training loss:1.1383308172225952
Train Acc 0.6731
 Acc 0.6678
new best val f1: 0.6677995427300253
ogbn-arxiv,dgl,1,489,7.4534,0.6678

epoch:491/50, training loss:1.137801170349121
Train Acc 0.6731
 Acc 0.6680
new best val f1: 0.667984922449484
ogbn-arxiv,dgl,1,490,7.4658,0.6680

epoch:492/50, training loss:1.1372716426849365
Train Acc 0.6733
 Acc 0.6681
new best val f1: 0.6681085089291231
ogbn-arxiv,dgl,1,491,7.4782,0.6681

epoch:493/50, training loss:1.1367433071136475
Train Acc 0.6735
 Acc 0.6683
new best val f1: 0.6683350841417949
ogbn-arxiv,dgl,1,492,7.4906,0.6683

epoch:494/50, training loss:1.1362230777740479
Train Acc 0.6737
 Acc 0.6683
ogbn-arxiv,dgl,1,493,7.5032,0.6683

epoch:495/50, training loss:1.135704517364502
Train Acc 0.6738
 Acc 0.6684
new best val f1: 0.6683762796350079
ogbn-arxiv,dgl,1,494,7.5156,0.6684

epoch:496/50, training loss:1.1351854801177979
Train Acc 0.6739
 Acc 0.6684
ogbn-arxiv,dgl,1,495,7.5280,0.6684

epoch:497/50, training loss:1.1346725225448608
Train Acc 0.6740
 Acc 0.6688
new best val f1: 0.6687882345671383
ogbn-arxiv,dgl,1,496,7.5404,0.6688

epoch:498/50, training loss:1.1341583728790283
Train Acc 0.6742
 Acc 0.6687
ogbn-arxiv,dgl,1,497,7.5530,0.6687

epoch:499/50, training loss:1.133649230003357
Train Acc 0.6743
 Acc 0.6687
ogbn-arxiv,dgl,1,498,7.5654,0.6687

epoch:500/50, training loss:1.133142113685608
Train Acc 0.6743
 Acc 0.6688
ogbn-arxiv,dgl,1,499,7.5780,0.6688

epoch:501/50, training loss:1.1326370239257812
Train Acc 0.6745
 Acc 0.6688
new best val f1: 0.6688294300603514
ogbn-arxiv,dgl,1,500,7.5903,0.6688

epoch:502/50, training loss:1.132134199142456
Train Acc 0.6745
 Acc 0.6690
new best val f1: 0.6690148097798101
ogbn-arxiv,dgl,1,501,7.6029,0.6690

epoch:503/50, training loss:1.1316348314285278
Train Acc 0.6746
 Acc 0.6691
new best val f1: 0.6690972007662361
ogbn-arxiv,dgl,1,502,7.6152,0.6691

epoch:504/50, training loss:1.1311379671096802
Train Acc 0.6747
 Acc 0.6691
new best val f1: 0.6691177985128427
ogbn-arxiv,dgl,1,503,7.6277,0.6691

epoch:505/50, training loss:1.1306431293487549
Train Acc 0.6748
 Acc 0.6693
new best val f1: 0.6693031782323013
ogbn-arxiv,dgl,1,504,7.6401,0.6693

epoch:506/50, training loss:1.1301506757736206
Train Acc 0.6749
 Acc 0.6695
new best val f1: 0.6695091556983666
ogbn-arxiv,dgl,1,505,7.6526,0.6695

epoch:507/50, training loss:1.1296617984771729
Train Acc 0.6750
 Acc 0.6696
new best val f1: 0.6696121444313992
ogbn-arxiv,dgl,1,506,7.6650,0.6696

epoch:508/50, training loss:1.1291747093200684
Train Acc 0.6751
 Acc 0.6696
new best val f1: 0.6696327421780057
ogbn-arxiv,dgl,1,507,7.6775,0.6696

epoch:509/50, training loss:1.1286864280700684
Train Acc 0.6752
 Acc 0.6697
new best val f1: 0.6697357309110383
ogbn-arxiv,dgl,1,508,7.6898,0.6697

epoch:510/50, training loss:1.1282055377960205
Train Acc 0.6753
 Acc 0.6698
new best val f1: 0.6698181218974644
ogbn-arxiv,dgl,1,509,7.7023,0.6698

epoch:511/50, training loss:1.1277251243591309
Train Acc 0.6754
 Acc 0.6700
new best val f1: 0.6699829038703166
ogbn-arxiv,dgl,1,510,7.7146,0.6700

epoch:512/50, training loss:1.1272478103637695
Train Acc 0.6755
 Acc 0.6700
new best val f1: 0.6700035016169231
ogbn-arxiv,dgl,1,511,7.7272,0.6700

epoch:513/50, training loss:1.1267720460891724
Train Acc 0.6756
 Acc 0.6701
new best val f1: 0.6700858926033492
ogbn-arxiv,dgl,1,512,7.7396,0.6701

epoch:514/50, training loss:1.1262993812561035
Train Acc 0.6756
 Acc 0.6702
new best val f1: 0.6701888813363818
ogbn-arxiv,dgl,1,513,7.7522,0.6702

epoch:515/50, training loss:1.125823974609375
Train Acc 0.6757
 Acc 0.6704
new best val f1: 0.670353663309234
ogbn-arxiv,dgl,1,514,7.7645,0.6704

epoch:516/50, training loss:1.1253571510314941
Train Acc 0.6758
 Acc 0.6705
new best val f1: 0.6704566520422666
ogbn-arxiv,dgl,1,515,7.7770,0.6705

epoch:517/50, training loss:1.12488853931427
Train Acc 0.6759
 Acc 0.6705
ogbn-arxiv,dgl,1,516,7.7894,0.6705

epoch:518/50, training loss:1.1244248151779175
Train Acc 0.6760
 Acc 0.6705
new best val f1: 0.6705390430286927
ogbn-arxiv,dgl,1,517,7.8019,0.6705

epoch:519/50, training loss:1.1239603757858276
Train Acc 0.6761
 Acc 0.6707
new best val f1: 0.6706626295083318
ogbn-arxiv,dgl,1,518,7.8143,0.6707

epoch:520/50, training loss:1.123500108718872
Train Acc 0.6761
 Acc 0.6708
new best val f1: 0.670786215987971
ogbn-arxiv,dgl,1,519,7.8270,0.6708

epoch:521/50, training loss:1.1230435371398926
Train Acc 0.6762
 Acc 0.6708
new best val f1: 0.6708068137345774
ogbn-arxiv,dgl,1,520,7.8393,0.6708

epoch:522/50, training loss:1.1225866079330444
Train Acc 0.6764
 Acc 0.6708
new best val f1: 0.670827411481184
ogbn-arxiv,dgl,1,521,7.8518,0.6708

epoch:523/50, training loss:1.1221321821212769
Train Acc 0.6765
 Acc 0.6707
ogbn-arxiv,dgl,1,522,7.8642,0.6707

epoch:524/50, training loss:1.1216802597045898
Train Acc 0.6766
 Acc 0.6708
new best val f1: 0.6708480092277905
ogbn-arxiv,dgl,1,523,7.8767,0.6708

epoch:525/50, training loss:1.1212286949157715
Train Acc 0.6767
 Acc 0.6710
new best val f1: 0.6709715957074296
ogbn-arxiv,dgl,1,524,7.8891,0.6710

epoch:526/50, training loss:1.1207823753356934
Train Acc 0.6767
 Acc 0.6711
new best val f1: 0.6711363776802818
ogbn-arxiv,dgl,1,525,7.9016,0.6711

epoch:527/50, training loss:1.1203317642211914
Train Acc 0.6769
 Acc 0.6713
new best val f1: 0.671259964159921
ogbn-arxiv,dgl,1,526,7.9140,0.6713

epoch:528/50, training loss:1.1198874711990356
Train Acc 0.6770
 Acc 0.6715
new best val f1: 0.6715071371191992
ogbn-arxiv,dgl,1,527,7.9265,0.6715

epoch:529/50, training loss:1.1194473505020142
Train Acc 0.6771
 Acc 0.6717
new best val f1: 0.6716925168386578
ogbn-arxiv,dgl,1,528,7.9388,0.6717

epoch:530/50, training loss:1.1190083026885986
Train Acc 0.6773
 Acc 0.6715
ogbn-arxiv,dgl,1,529,7.9513,0.6715

epoch:531/50, training loss:1.1185665130615234
Train Acc 0.6773
 Acc 0.6716
ogbn-arxiv,dgl,1,530,7.9637,0.6716

epoch:532/50, training loss:1.1181292533874512
Train Acc 0.6774
 Acc 0.6715
ogbn-arxiv,dgl,1,531,7.9763,0.6715

epoch:533/50, training loss:1.1176937818527222
Train Acc 0.6775
 Acc 0.6717
new best val f1: 0.6717337123318708
ogbn-arxiv,dgl,1,532,7.9888,0.6717

epoch:534/50, training loss:1.1172633171081543
Train Acc 0.6776
 Acc 0.6719
new best val f1: 0.671898494304723
ogbn-arxiv,dgl,1,533,8.0013,0.6719

epoch:535/50, training loss:1.11683189868927
Train Acc 0.6778
 Acc 0.6719
ogbn-arxiv,dgl,1,534,8.0136,0.6719

epoch:536/50, training loss:1.1164023876190186
Train Acc 0.6779
 Acc 0.6720
new best val f1: 0.6720014830377556
ogbn-arxiv,dgl,1,535,8.0261,0.6720

epoch:537/50, training loss:1.115975260734558
Train Acc 0.6780
 Acc 0.6722
new best val f1: 0.6721662650106078
ogbn-arxiv,dgl,1,536,8.0384,0.6722

epoch:538/50, training loss:1.1155470609664917
Train Acc 0.6781
 Acc 0.6722
new best val f1: 0.6722280582504274
ogbn-arxiv,dgl,1,537,8.0509,0.6722

epoch:539/50, training loss:1.115124225616455
Train Acc 0.6782
 Acc 0.6722
ogbn-arxiv,dgl,1,538,8.0633,0.6722

epoch:540/50, training loss:1.1147011518478394
Train Acc 0.6783
 Acc 0.6724
new best val f1: 0.672372242476673
ogbn-arxiv,dgl,1,539,8.0758,0.6724

epoch:541/50, training loss:1.114280343055725
Train Acc 0.6784
 Acc 0.6726
new best val f1: 0.6726194154359513
ogbn-arxiv,dgl,1,540,8.0881,0.6726

epoch:542/50, training loss:1.1138633489608765
Train Acc 0.6785
 Acc 0.6726
ogbn-arxiv,dgl,1,541,8.1010,0.6726

epoch:543/50, training loss:1.1134462356567383
Train Acc 0.6786
 Acc 0.6727
new best val f1: 0.6727018064223774
ogbn-arxiv,dgl,1,542,8.1136,0.6727

epoch:544/50, training loss:1.1130317449569702
Train Acc 0.6787
 Acc 0.6726
ogbn-arxiv,dgl,1,543,8.1263,0.6726

epoch:545/50, training loss:1.112619161605835
Train Acc 0.6787
 Acc 0.6727
ogbn-arxiv,dgl,1,544,8.1389,0.6727

epoch:546/50, training loss:1.112208366394043
Train Acc 0.6789
 Acc 0.6728
new best val f1: 0.6728253929020165
ogbn-arxiv,dgl,1,545,8.1515,0.6728

epoch:547/50, training loss:1.11179780960083
Train Acc 0.6790
 Acc 0.6728
ogbn-arxiv,dgl,1,546,8.1640,0.6728

epoch:548/50, training loss:1.1113888025283813
Train Acc 0.6790
 Acc 0.6727
ogbn-arxiv,dgl,1,547,8.1767,0.6727

epoch:549/50, training loss:1.110984444618225
Train Acc 0.6791
 Acc 0.6729
new best val f1: 0.6729077838884426
ogbn-arxiv,dgl,1,548,8.1892,0.6729

epoch:550/50, training loss:1.1105769872665405
Train Acc 0.6792
 Acc 0.6730
new best val f1: 0.6730107726214752
ogbn-arxiv,dgl,1,549,8.2019,0.6730

epoch:551/50, training loss:1.1101746559143066
Train Acc 0.6793
 Acc 0.6732
new best val f1: 0.6731961523409339
ogbn-arxiv,dgl,1,550,8.2143,0.6732

epoch:552/50, training loss:1.1097688674926758
Train Acc 0.6794
 Acc 0.6732
new best val f1: 0.6732373478341469
ogbn-arxiv,dgl,1,551,8.2269,0.6732

epoch:553/50, training loss:1.1093660593032837
Train Acc 0.6794
 Acc 0.6731
ogbn-arxiv,dgl,1,552,8.2394,0.6731

epoch:554/50, training loss:1.1089624166488647
Train Acc 0.6794
 Acc 0.6732
ogbn-arxiv,dgl,1,553,8.2521,0.6732

epoch:555/50, training loss:1.1085606813430786
Train Acc 0.6794
 Acc 0.6733
new best val f1: 0.673319738820573
ogbn-arxiv,dgl,1,554,8.2646,0.6733

epoch:556/50, training loss:1.108157992362976
Train Acc 0.6796
 Acc 0.6733
ogbn-arxiv,dgl,1,555,8.2772,0.6733

epoch:557/50, training loss:1.107757568359375
Train Acc 0.6797
 Acc 0.6731
ogbn-arxiv,dgl,1,556,8.2897,0.6731

epoch:558/50, training loss:1.1073569059371948
Train Acc 0.6796
 Acc 0.6734
new best val f1: 0.6734433253002121
ogbn-arxiv,dgl,1,557,8.3024,0.6734

epoch:559/50, training loss:1.106953501701355
Train Acc 0.6798
 Acc 0.6736
new best val f1: 0.6735669117798513
ogbn-arxiv,dgl,1,558,8.3149,0.6736

epoch:560/50, training loss:1.106552004814148
Train Acc 0.6799
 Acc 0.6738
new best val f1: 0.6737728892459165
ogbn-arxiv,dgl,1,559,8.3275,0.6738

epoch:561/50, training loss:1.1061501502990723
Train Acc 0.6800
 Acc 0.6738
new best val f1: 0.6738346824857361
ogbn-arxiv,dgl,1,560,8.3400,0.6738

epoch:562/50, training loss:1.1057488918304443
Train Acc 0.6802
 Acc 0.6740
new best val f1: 0.6739788667119817
ogbn-arxiv,dgl,1,561,8.3525,0.6740

epoch:563/50, training loss:1.1053522825241089
Train Acc 0.6802
 Acc 0.6740
ogbn-arxiv,dgl,1,562,8.3650,0.6740

epoch:564/50, training loss:1.1049505472183228
Train Acc 0.6802
 Acc 0.6739
ogbn-arxiv,dgl,1,563,8.3776,0.6739

epoch:565/50, training loss:1.1045551300048828
Train Acc 0.6803
 Acc 0.6738
ogbn-arxiv,dgl,1,564,8.3901,0.6738

epoch:566/50, training loss:1.1041594743728638
Train Acc 0.6804
 Acc 0.6740
ogbn-arxiv,dgl,1,565,8.4027,0.6740

epoch:567/50, training loss:1.1037623882293701
Train Acc 0.6804
 Acc 0.6742
new best val f1: 0.6742260396712599
ogbn-arxiv,dgl,1,566,8.4152,0.6742

epoch:568/50, training loss:1.1033668518066406
Train Acc 0.6806
 Acc 0.6743
new best val f1: 0.674267235164473
ogbn-arxiv,dgl,1,567,8.4278,0.6743

epoch:569/50, training loss:1.102975845336914
Train Acc 0.6807
 Acc 0.6742
ogbn-arxiv,dgl,1,568,8.4402,0.6742

epoch:570/50, training loss:1.102583646774292
Train Acc 0.6807
 Acc 0.6742
ogbn-arxiv,dgl,1,569,8.4527,0.6742

epoch:571/50, training loss:1.1021922826766968
Train Acc 0.6808
 Acc 0.6744
new best val f1: 0.6743908216441121
ogbn-arxiv,dgl,1,570,8.4652,0.6744

epoch:572/50, training loss:1.101801872253418
Train Acc 0.6809
 Acc 0.6742
ogbn-arxiv,dgl,1,571,8.4794,0.6742

epoch:573/50, training loss:1.1014111042022705
Train Acc 0.6809
 Acc 0.6743
ogbn-arxiv,dgl,1,572,8.4917,0.6743

epoch:574/50, training loss:1.1010234355926514
Train Acc 0.6810
 Acc 0.6743
ogbn-arxiv,dgl,1,573,8.5044,0.6743

epoch:575/50, training loss:1.100633978843689
Train Acc 0.6811
 Acc 0.6743
ogbn-arxiv,dgl,1,574,8.5169,0.6743

epoch:576/50, training loss:1.1002541780471802
Train Acc 0.6811
 Acc 0.6744
ogbn-arxiv,dgl,1,575,8.5294,0.6744

epoch:577/50, training loss:1.0998691320419312
Train Acc 0.6812
 Acc 0.6746
new best val f1: 0.6745762013635708
ogbn-arxiv,dgl,1,576,8.5419,0.6746

epoch:578/50, training loss:1.0994844436645508
Train Acc 0.6814
 Acc 0.6746
new best val f1: 0.6745967991101773
ogbn-arxiv,dgl,1,577,8.5544,0.6746

epoch:579/50, training loss:1.09910249710083
Train Acc 0.6814
 Acc 0.6746
ogbn-arxiv,dgl,1,578,8.5669,0.6746

epoch:580/50, training loss:1.098718285560608
Train Acc 0.6814
 Acc 0.6745
ogbn-arxiv,dgl,1,579,8.5795,0.6745

epoch:581/50, training loss:1.0983428955078125
Train Acc 0.6814
 Acc 0.6746
ogbn-arxiv,dgl,1,580,8.5920,0.6746

epoch:582/50, training loss:1.0979629755020142
Train Acc 0.6815
 Acc 0.6748
new best val f1: 0.6747615810830295
ogbn-arxiv,dgl,1,581,8.6046,0.6748

epoch:583/50, training loss:1.0975852012634277
Train Acc 0.6816
 Acc 0.6750
new best val f1: 0.6749675585490947
ogbn-arxiv,dgl,1,582,8.6170,0.6750

epoch:584/50, training loss:1.09720778465271
Train Acc 0.6818
 Acc 0.6748
ogbn-arxiv,dgl,1,583,8.6296,0.6748

epoch:585/50, training loss:1.0968340635299683
Train Acc 0.6818
 Acc 0.6748
ogbn-arxiv,dgl,1,584,8.6420,0.6748

epoch:586/50, training loss:1.0964611768722534
Train Acc 0.6819
 Acc 0.6747
ogbn-arxiv,dgl,1,585,8.6546,0.6747

epoch:587/50, training loss:1.09608793258667
Train Acc 0.6819
 Acc 0.6749
ogbn-arxiv,dgl,1,586,8.6671,0.6749

epoch:588/50, training loss:1.0957156419754028
Train Acc 0.6821
 Acc 0.6749
ogbn-arxiv,dgl,1,587,8.6797,0.6749

epoch:589/50, training loss:1.0953457355499268
Train Acc 0.6822
 Acc 0.6749
ogbn-arxiv,dgl,1,588,8.6921,0.6749

epoch:590/50, training loss:1.0949757099151611
Train Acc 0.6821
 Acc 0.6749
ogbn-arxiv,dgl,1,589,8.7047,0.6749

epoch:591/50, training loss:1.0946087837219238
Train Acc 0.6821
 Acc 0.6752
new best val f1: 0.675214731508373
ogbn-arxiv,dgl,1,590,8.7172,0.6752

epoch:592/50, training loss:1.094242811203003
Train Acc 0.6823
 Acc 0.6754
new best val f1: 0.6753589157346186
ogbn-arxiv,dgl,1,591,8.7297,0.6754

epoch:593/50, training loss:1.0938786268234253
Train Acc 0.6824
 Acc 0.6753
ogbn-arxiv,dgl,1,592,8.7422,0.6753

epoch:594/50, training loss:1.093514323234558
Train Acc 0.6825
 Acc 0.6753
ogbn-arxiv,dgl,1,593,8.7548,0.6753

epoch:595/50, training loss:1.0931508541107178
Train Acc 0.6826
 Acc 0.6756
new best val f1: 0.6756266864405034
ogbn-arxiv,dgl,1,594,8.7672,0.6756

epoch:596/50, training loss:1.0927894115447998
Train Acc 0.6827
 Acc 0.6756
ogbn-arxiv,dgl,1,595,8.7799,0.6756

epoch:597/50, training loss:1.0924283266067505
Train Acc 0.6828
 Acc 0.6756
new best val f1: 0.67564728418711
ogbn-arxiv,dgl,1,596,8.7924,0.6756

epoch:598/50, training loss:1.0920711755752563
Train Acc 0.6828
 Acc 0.6758
new best val f1: 0.675770870666749
ogbn-arxiv,dgl,1,597,8.8050,0.6758

epoch:599/50, training loss:1.0917108058929443
Train Acc 0.6829
 Acc 0.6757
ogbn-arxiv,dgl,1,598,8.8174,0.6757

epoch:600/50, training loss:1.0913554430007935
Train Acc 0.6830
 Acc 0.6761
new best val f1: 0.6760798368658468
ogbn-arxiv,dgl,1,599,8.8301,0.6761

epoch:601/50, training loss:1.0910003185272217
Train Acc 0.6832
 Acc 0.6758
ogbn-arxiv,dgl,1,600,8.8426,0.6758

epoch:602/50, training loss:1.0906496047973633
Train Acc 0.6831
 Acc 0.6756
ogbn-arxiv,dgl,1,601,8.8552,0.6756

epoch:603/50, training loss:1.0902942419052124
Train Acc 0.6831
 Acc 0.6759
ogbn-arxiv,dgl,1,602,8.8675,0.6759

epoch:604/50, training loss:1.089942216873169
Train Acc 0.6833
 Acc 0.6762
new best val f1: 0.676162227852273
ogbn-arxiv,dgl,1,603,8.8800,0.6762

epoch:605/50, training loss:1.0895934104919434
Train Acc 0.6835
 Acc 0.6760
ogbn-arxiv,dgl,1,604,8.8926,0.6760

epoch:606/50, training loss:1.0892456769943237
Train Acc 0.6835
 Acc 0.6759
ogbn-arxiv,dgl,1,605,8.9051,0.6759

epoch:607/50, training loss:1.0888959169387817
Train Acc 0.6834
 Acc 0.6762
ogbn-arxiv,dgl,1,606,8.9175,0.6762

epoch:608/50, training loss:1.088553547859192
Train Acc 0.6836
 Acc 0.6762
new best val f1: 0.6762240210920926
ogbn-arxiv,dgl,1,607,8.9301,0.6762

epoch:609/50, training loss:1.0882080793380737
Train Acc 0.6837
 Acc 0.6763
new best val f1: 0.676285814331912
ogbn-arxiv,dgl,1,608,8.9426,0.6763

epoch:610/50, training loss:1.087862253189087
Train Acc 0.6838
 Acc 0.6762
ogbn-arxiv,dgl,1,609,8.9551,0.6762

epoch:611/50, training loss:1.0875216722488403
Train Acc 0.6838
 Acc 0.6763
new best val f1: 0.6763270098251252
ogbn-arxiv,dgl,1,610,8.9675,0.6763

epoch:612/50, training loss:1.0871806144714355
Train Acc 0.6839
 Acc 0.6763
ogbn-arxiv,dgl,1,611,8.9802,0.6763

epoch:613/50, training loss:1.0868417024612427
Train Acc 0.6839
 Acc 0.6762
ogbn-arxiv,dgl,1,612,8.9926,0.6762

epoch:614/50, training loss:1.0865025520324707
Train Acc 0.6839
 Acc 0.6764
new best val f1: 0.6763682053183382
ogbn-arxiv,dgl,1,613,9.0052,0.6764

epoch:615/50, training loss:1.0861634016036987
Train Acc 0.6841
 Acc 0.6764
ogbn-arxiv,dgl,1,614,9.0176,0.6764

epoch:616/50, training loss:1.0858278274536133
Train Acc 0.6841
 Acc 0.6762
ogbn-arxiv,dgl,1,615,9.0301,0.6762

epoch:617/50, training loss:1.0854933261871338
Train Acc 0.6841
 Acc 0.6763
ogbn-arxiv,dgl,1,616,9.0443,0.6763

epoch:618/50, training loss:1.0851593017578125
Train Acc 0.6841
 Acc 0.6764
new best val f1: 0.6764094008115512
ogbn-arxiv,dgl,1,617,9.0569,0.6764

epoch:619/50, training loss:1.0848290920257568
Train Acc 0.6842
 Acc 0.6764
ogbn-arxiv,dgl,1,618,9.0693,0.6764

epoch:620/50, training loss:1.0844988822937012
Train Acc 0.6842
 Acc 0.6766
new best val f1: 0.6765741827844034
ogbn-arxiv,dgl,1,619,9.0820,0.6766

epoch:621/50, training loss:1.0841689109802246
Train Acc 0.6843
 Acc 0.6766
new best val f1: 0.6766153782776164
ogbn-arxiv,dgl,1,620,9.0945,0.6766

epoch:622/50, training loss:1.0838403701782227
Train Acc 0.6843
 Acc 0.6768
new best val f1: 0.6768007579970752
ogbn-arxiv,dgl,1,621,9.1070,0.6768

epoch:623/50, training loss:1.0835150480270386
Train Acc 0.6844
 Acc 0.6766
ogbn-arxiv,dgl,1,622,9.1194,0.6766

epoch:624/50, training loss:1.0831912755966187
Train Acc 0.6845
 Acc 0.6767
ogbn-arxiv,dgl,1,623,9.1320,0.6767

epoch:625/50, training loss:1.08286714553833
Train Acc 0.6845
 Acc 0.6768
ogbn-arxiv,dgl,1,624,9.1445,0.6768

epoch:626/50, training loss:1.0825425386428833
Train Acc 0.6846
 Acc 0.6768
ogbn-arxiv,dgl,1,625,9.1571,0.6768

epoch:627/50, training loss:1.0822193622589111
Train Acc 0.6847
 Acc 0.6768
ogbn-arxiv,dgl,1,626,9.1695,0.6768

epoch:628/50, training loss:1.0819005966186523
Train Acc 0.6847
 Acc 0.6768
ogbn-arxiv,dgl,1,627,9.1821,0.6768

epoch:629/50, training loss:1.0815844535827637
Train Acc 0.6848
 Acc 0.6767
ogbn-arxiv,dgl,1,628,9.1946,0.6767

epoch:630/50, training loss:1.0812668800354004
Train Acc 0.6848
 Acc 0.6769
new best val f1: 0.6768831489835012
ogbn-arxiv,dgl,1,629,9.2072,0.6769

epoch:631/50, training loss:1.0809478759765625
Train Acc 0.6849
 Acc 0.6769
new best val f1: 0.6769449422233208
ogbn-arxiv,dgl,1,630,9.2196,0.6769

epoch:632/50, training loss:1.0806324481964111
Train Acc 0.6850
 Acc 0.6772
new best val f1: 0.677233310675812
ogbn-arxiv,dgl,1,631,9.2322,0.6772

epoch:633/50, training loss:1.0803191661834717
Train Acc 0.6851
 Acc 0.6774
new best val f1: 0.6773774949020577
ogbn-arxiv,dgl,1,632,9.2451,0.6774

epoch:634/50, training loss:1.0800068378448486
Train Acc 0.6852
 Acc 0.6772
ogbn-arxiv,dgl,1,633,9.2577,0.6772

epoch:635/50, training loss:1.0796936750411987
Train Acc 0.6852
 Acc 0.6770
ogbn-arxiv,dgl,1,634,9.2701,0.6770

epoch:636/50, training loss:1.07938551902771
Train Acc 0.6852
 Acc 0.6774
ogbn-arxiv,dgl,1,635,9.2827,0.6774

epoch:637/50, training loss:1.079075813293457
Train Acc 0.6854
 Acc 0.6774
new best val f1: 0.6773980926486642
ogbn-arxiv,dgl,1,636,9.2951,0.6774

epoch:638/50, training loss:1.0787690877914429
Train Acc 0.6855
 Acc 0.6772
ogbn-arxiv,dgl,1,637,9.3078,0.6772

epoch:639/50, training loss:1.078460454940796
Train Acc 0.6855
 Acc 0.6774
ogbn-arxiv,dgl,1,638,9.3202,0.6774

epoch:640/50, training loss:1.0781521797180176
Train Acc 0.6856
 Acc 0.6775
new best val f1: 0.6775216791283034
ogbn-arxiv,dgl,1,639,9.3327,0.6775

epoch:641/50, training loss:1.0778480768203735
Train Acc 0.6857
 Acc 0.6776
new best val f1: 0.6775628746215164
ogbn-arxiv,dgl,1,640,9.3452,0.6776

epoch:642/50, training loss:1.07754385471344
Train Acc 0.6857
 Acc 0.6775
ogbn-arxiv,dgl,1,641,9.3578,0.6775

epoch:643/50, training loss:1.0772426128387451
Train Acc 0.6857
 Acc 0.6776
new best val f1: 0.6776040701147295
ogbn-arxiv,dgl,1,642,9.3702,0.6776

epoch:644/50, training loss:1.0769431591033936
Train Acc 0.6858
 Acc 0.6779
new best val f1: 0.6778718408206142
ogbn-arxiv,dgl,1,643,9.3828,0.6779

epoch:645/50, training loss:1.0766453742980957
Train Acc 0.6861
 Acc 0.6776
ogbn-arxiv,dgl,1,644,9.3951,0.6776

epoch:646/50, training loss:1.07634437084198
Train Acc 0.6860
 Acc 0.6777
ogbn-arxiv,dgl,1,645,9.4078,0.6777

epoch:647/50, training loss:1.0760469436645508
Train Acc 0.6860
 Acc 0.6782
new best val f1: 0.6782426002595316
ogbn-arxiv,dgl,1,646,9.4203,0.6782

epoch:648/50, training loss:1.0757492780685425
Train Acc 0.6863
 Acc 0.6779
ogbn-arxiv,dgl,1,647,9.4330,0.6779

epoch:649/50, training loss:1.075453758239746
Train Acc 0.6863
 Acc 0.6779
ogbn-arxiv,dgl,1,648,9.4454,0.6779

epoch:650/50, training loss:1.0751607418060303
Train Acc 0.6863
 Acc 0.6782
ogbn-arxiv,dgl,1,649,9.4579,0.6782

epoch:651/50, training loss:1.0748683214187622
Train Acc 0.6864
 Acc 0.6782
ogbn-arxiv,dgl,1,650,9.4703,0.6782

epoch:652/50, training loss:1.074575662612915
Train Acc 0.6865
 Acc 0.6782
ogbn-arxiv,dgl,1,651,9.4829,0.6782

epoch:653/50, training loss:1.0742841958999634
Train Acc 0.6866
 Acc 0.6782
ogbn-arxiv,dgl,1,652,9.4954,0.6782

epoch:654/50, training loss:1.0739951133728027
Train Acc 0.6867
 Acc 0.6785
new best val f1: 0.6785309687120229
ogbn-arxiv,dgl,1,653,9.5080,0.6785

epoch:655/50, training loss:1.0737051963806152
Train Acc 0.6868
 Acc 0.6786
new best val f1: 0.6785927619518425
ogbn-arxiv,dgl,1,654,9.5205,0.6786

epoch:656/50, training loss:1.0734211206436157
Train Acc 0.6868
 Acc 0.6784
ogbn-arxiv,dgl,1,655,9.5330,0.6784

epoch:657/50, training loss:1.073133945465088
Train Acc 0.6868
 Acc 0.6785
ogbn-arxiv,dgl,1,656,9.5455,0.6785

epoch:658/50, training loss:1.0728495121002197
Train Acc 0.6869
 Acc 0.6790
new best val f1: 0.6790253146305795
ogbn-arxiv,dgl,1,657,9.5581,0.6790

epoch:659/50, training loss:1.0725643634796143
Train Acc 0.6871
 Acc 0.6789
ogbn-arxiv,dgl,1,658,9.5705,0.6789

epoch:660/50, training loss:1.0722789764404297
Train Acc 0.6871
 Acc 0.6788
ogbn-arxiv,dgl,1,659,9.5831,0.6788

epoch:661/50, training loss:1.071996808052063
Train Acc 0.6871
 Acc 0.6789
ogbn-arxiv,dgl,1,660,9.5956,0.6789

epoch:662/50, training loss:1.0717180967330933
Train Acc 0.6872
 Acc 0.6788
ogbn-arxiv,dgl,1,661,9.6082,0.6788

epoch:663/50, training loss:1.0714365243911743
Train Acc 0.6872
 Acc 0.6789
ogbn-arxiv,dgl,1,662,9.6206,0.6789

epoch:664/50, training loss:1.0711588859558105
Train Acc 0.6873
 Acc 0.6790
ogbn-arxiv,dgl,1,663,9.6332,0.6790

epoch:665/50, training loss:1.0708802938461304
Train Acc 0.6874
 Acc 0.6790
new best val f1: 0.6790459123771859
ogbn-arxiv,dgl,1,664,9.6456,0.6790

epoch:666/50, training loss:1.0706034898757935
Train Acc 0.6874
 Acc 0.6792
new best val f1: 0.6791694988568251
ogbn-arxiv,dgl,1,665,9.6582,0.6792

epoch:667/50, training loss:1.0703282356262207
Train Acc 0.6874
 Acc 0.6792
new best val f1: 0.6791900966034315
ogbn-arxiv,dgl,1,666,9.6707,0.6792

epoch:668/50, training loss:1.0700535774230957
Train Acc 0.6875
 Acc 0.6793
new best val f1: 0.6793342808296773
ogbn-arxiv,dgl,1,667,9.6832,0.6793

epoch:669/50, training loss:1.069779396057129
Train Acc 0.6877
 Acc 0.6795
new best val f1: 0.6794784650559229
ogbn-arxiv,dgl,1,668,9.6957,0.6795

epoch:670/50, training loss:1.0695087909698486
Train Acc 0.6878
 Acc 0.6793
ogbn-arxiv,dgl,1,669,9.7083,0.6793

epoch:671/50, training loss:1.069236159324646
Train Acc 0.6877
 Acc 0.6793
ogbn-arxiv,dgl,1,670,9.7207,0.6793

epoch:672/50, training loss:1.068965196609497
Train Acc 0.6877
 Acc 0.6797
new best val f1: 0.6796638447753816
ogbn-arxiv,dgl,1,671,9.7334,0.6797

epoch:673/50, training loss:1.068695068359375
Train Acc 0.6879
 Acc 0.6795
ogbn-arxiv,dgl,1,672,9.7458,0.6795

epoch:674/50, training loss:1.0684269666671753
Train Acc 0.6879
 Acc 0.6794
ogbn-arxiv,dgl,1,673,9.7585,0.6794

epoch:675/50, training loss:1.0681592226028442
Train Acc 0.6878
 Acc 0.6796
ogbn-arxiv,dgl,1,674,9.7709,0.6796

epoch:676/50, training loss:1.067894697189331
Train Acc 0.6880
 Acc 0.6799
new best val f1: 0.6799110177346598
ogbn-arxiv,dgl,1,675,9.7834,0.6799

epoch:677/50, training loss:1.0676296949386597
Train Acc 0.6881
 Acc 0.6796
ogbn-arxiv,dgl,1,676,9.7959,0.6796

epoch:678/50, training loss:1.067365288734436
Train Acc 0.6880
 Acc 0.6798
ogbn-arxiv,dgl,1,677,9.8086,0.6798

epoch:679/50, training loss:1.0671007633209229
Train Acc 0.6881
 Acc 0.6802
new best val f1: 0.6802405816803642
ogbn-arxiv,dgl,1,678,9.8210,0.6802

epoch:680/50, training loss:1.0668410062789917
Train Acc 0.6883
 Acc 0.6798
ogbn-arxiv,dgl,1,679,9.8335,0.6798

epoch:681/50, training loss:1.0665788650512695
Train Acc 0.6882
 Acc 0.6799
ogbn-arxiv,dgl,1,680,9.8460,0.6799

epoch:682/50, training loss:1.0663180351257324
Train Acc 0.6882
 Acc 0.6804
new best val f1: 0.6804465591464294
ogbn-arxiv,dgl,1,681,9.8586,0.6804

epoch:683/50, training loss:1.0660614967346191
Train Acc 0.6885
 Acc 0.6802
ogbn-arxiv,dgl,1,682,9.8711,0.6802

epoch:684/50, training loss:1.0658015012741089
Train Acc 0.6885
 Acc 0.6801
ogbn-arxiv,dgl,1,683,9.8837,0.6801

epoch:685/50, training loss:1.0655434131622314
Train Acc 0.6885
 Acc 0.6803
ogbn-arxiv,dgl,1,684,9.8961,0.6803

epoch:686/50, training loss:1.065288782119751
Train Acc 0.6885
 Acc 0.6807
new best val f1: 0.6806731343591012
ogbn-arxiv,dgl,1,685,9.9087,0.6807

epoch:687/50, training loss:1.065034031867981
Train Acc 0.6887
 Acc 0.6805
ogbn-arxiv,dgl,1,686,9.9211,0.6805

epoch:688/50, training loss:1.064779281616211
Train Acc 0.6887
 Acc 0.6803
ogbn-arxiv,dgl,1,687,9.9337,0.6803

epoch:689/50, training loss:1.064528226852417
Train Acc 0.6886
 Acc 0.6807
new best val f1: 0.6806937321057076
ogbn-arxiv,dgl,1,688,9.9461,0.6807

epoch:690/50, training loss:1.064276099205017
Train Acc 0.6889
 Acc 0.6810
new best val f1: 0.680982100558199
ogbn-arxiv,dgl,1,689,9.9586,0.6810

epoch:691/50, training loss:1.064024567604065
Train Acc 0.6889
 Acc 0.6803
ogbn-arxiv,dgl,1,690,9.9711,0.6803

epoch:692/50, training loss:1.0637757778167725
Train Acc 0.6888
 Acc 0.6808
ogbn-arxiv,dgl,1,691,9.9837,0.6808

epoch:693/50, training loss:1.0635286569595337
Train Acc 0.6890
 Acc 0.6808
ogbn-arxiv,dgl,1,692,9.9961,0.6808

epoch:694/50, training loss:1.063277244567871
Train Acc 0.6890
 Acc 0.6807
ogbn-arxiv,dgl,1,693,10.0087,0.6807

epoch:695/50, training loss:1.0630309581756592
Train Acc 0.6890
 Acc 0.6808
ogbn-arxiv,dgl,1,694,10.0212,0.6808

epoch:696/50, training loss:1.062785267829895
Train Acc 0.6891
 Acc 0.6812
new best val f1: 0.6812292735174772
ogbn-arxiv,dgl,1,695,10.0338,0.6812

epoch:697/50, training loss:1.0625388622283936
Train Acc 0.6893
 Acc 0.6808
ogbn-arxiv,dgl,1,696,10.0463,0.6808

epoch:698/50, training loss:1.0622972249984741
Train Acc 0.6891
 Acc 0.6812
ogbn-arxiv,dgl,1,697,10.0589,0.6812

epoch:699/50, training loss:1.0620499849319458
Train Acc 0.6893
 Acc 0.6812
ogbn-arxiv,dgl,1,698,10.0712,0.6812

epoch:700/50, training loss:1.0618104934692383
Train Acc 0.6894
 Acc 0.6812
ogbn-arxiv,dgl,1,699,10.0838,0.6812

epoch:701/50, training loss:1.061569333076477
Train Acc 0.6895
 Acc 0.6812
new best val f1: 0.6812498712640838
ogbn-arxiv,dgl,1,700,10.0963,0.6812

epoch:702/50, training loss:1.0613281726837158
Train Acc 0.6895
 Acc 0.6815
new best val f1: 0.681455848730149
ogbn-arxiv,dgl,1,701,10.1089,0.6815

epoch:703/50, training loss:1.0610893964767456
Train Acc 0.6896
 Acc 0.6813
ogbn-arxiv,dgl,1,702,10.1214,0.6813

epoch:704/50, training loss:1.060848593711853
Train Acc 0.6896
 Acc 0.6812
ogbn-arxiv,dgl,1,703,10.1340,0.6812

epoch:705/50, training loss:1.060611367225647
Train Acc 0.6896
 Acc 0.6815
new best val f1: 0.681497044223362
ogbn-arxiv,dgl,1,704,10.1465,0.6815

epoch:706/50, training loss:1.0603762865066528
Train Acc 0.6897
 Acc 0.6815
ogbn-arxiv,dgl,1,705,10.1592,0.6815

epoch:707/50, training loss:1.0601365566253662
Train Acc 0.6897
 Acc 0.6815
ogbn-arxiv,dgl,1,706,10.1716,0.6815

epoch:708/50, training loss:1.0599031448364258
Train Acc 0.6898
 Acc 0.6815
ogbn-arxiv,dgl,1,707,10.1842,0.6815

epoch:709/50, training loss:1.059668779373169
Train Acc 0.6898
 Acc 0.6818
new best val f1: 0.6818060104224598
ogbn-arxiv,dgl,1,708,10.1967,0.6818

epoch:710/50, training loss:1.0594351291656494
Train Acc 0.6899
 Acc 0.6816
ogbn-arxiv,dgl,1,709,10.2093,0.6816

epoch:711/50, training loss:1.0592013597488403
Train Acc 0.6898
 Acc 0.6817
ogbn-arxiv,dgl,1,710,10.2218,0.6817

epoch:712/50, training loss:1.058968186378479
Train Acc 0.6899
 Acc 0.6820
new best val f1: 0.681970792395312
ogbn-arxiv,dgl,1,711,10.2344,0.6820

epoch:713/50, training loss:1.0587360858917236
Train Acc 0.6900
 Acc 0.6821
new best val f1: 0.6821355743681641
ogbn-arxiv,dgl,1,712,10.2468,0.6821

epoch:714/50, training loss:1.0585073232650757
Train Acc 0.6900
 Acc 0.6821
ogbn-arxiv,dgl,1,713,10.2594,0.6821

epoch:715/50, training loss:1.0582765340805054
Train Acc 0.6901
 Acc 0.6819
ogbn-arxiv,dgl,1,714,10.2719,0.6819

epoch:716/50, training loss:1.0580466985702515
Train Acc 0.6901
 Acc 0.6824
new best val f1: 0.6824445405672619
ogbn-arxiv,dgl,1,715,10.2856,0.6824

epoch:717/50, training loss:1.0578196048736572
Train Acc 0.6902
 Acc 0.6823
ogbn-arxiv,dgl,1,716,10.2981,0.6823

epoch:718/50, training loss:1.0575915575027466
Train Acc 0.6902
 Acc 0.6823
ogbn-arxiv,dgl,1,717,10.3106,0.6823

epoch:719/50, training loss:1.0573641061782837
Train Acc 0.6903
 Acc 0.6827
new best val f1: 0.6826711157799337
ogbn-arxiv,dgl,1,718,10.3232,0.6827

epoch:720/50, training loss:1.0571393966674805
Train Acc 0.6904
 Acc 0.6827
ogbn-arxiv,dgl,1,719,10.3357,0.6827

epoch:721/50, training loss:1.0569134950637817
Train Acc 0.6905
 Acc 0.6825
ogbn-arxiv,dgl,1,720,10.3481,0.6825

epoch:722/50, training loss:1.056687355041504
Train Acc 0.6905
 Acc 0.6828
new best val f1: 0.6827741045129663
ogbn-arxiv,dgl,1,721,10.3607,0.6828

epoch:723/50, training loss:1.056463599205017
Train Acc 0.6906
 Acc 0.6831
new best val f1: 0.6830624729654576
ogbn-arxiv,dgl,1,722,10.3733,0.6831

epoch:724/50, training loss:1.0562409162521362
Train Acc 0.6908
 Acc 0.6829
ogbn-arxiv,dgl,1,723,10.3859,0.6829

epoch:725/50, training loss:1.056018352508545
Train Acc 0.6907
 Acc 0.6830
ogbn-arxiv,dgl,1,724,10.3983,0.6830

epoch:726/50, training loss:1.0557963848114014
Train Acc 0.6908
 Acc 0.6830
ogbn-arxiv,dgl,1,725,10.4109,0.6830

epoch:727/50, training loss:1.0555751323699951
Train Acc 0.6909
 Acc 0.6831
new best val f1: 0.6831036684586707
ogbn-arxiv,dgl,1,726,10.4234,0.6831

epoch:728/50, training loss:1.055355429649353
Train Acc 0.6910
 Acc 0.6830
ogbn-arxiv,dgl,1,727,10.4359,0.6830

epoch:729/50, training loss:1.055135726928711
Train Acc 0.6910
 Acc 0.6831
new best val f1: 0.6831242662052771
ogbn-arxiv,dgl,1,728,10.4484,0.6831

epoch:730/50, training loss:1.054916501045227
Train Acc 0.6911
 Acc 0.6832
new best val f1: 0.6832478526849163
ogbn-arxiv,dgl,1,729,10.4610,0.6832

epoch:731/50, training loss:1.0546998977661133
Train Acc 0.6913
 Acc 0.6831
ogbn-arxiv,dgl,1,730,10.4735,0.6831

epoch:732/50, training loss:1.0544800758361816
Train Acc 0.6912
 Acc 0.6833
new best val f1: 0.6832684504315228
ogbn-arxiv,dgl,1,731,10.4861,0.6833

epoch:733/50, training loss:1.0542620420455933
Train Acc 0.6913
 Acc 0.6834
new best val f1: 0.6833714391645554
ogbn-arxiv,dgl,1,732,10.4986,0.6834

epoch:734/50, training loss:1.0540488958358765
Train Acc 0.6914
 Acc 0.6833
ogbn-arxiv,dgl,1,733,10.5112,0.6833

epoch:735/50, training loss:1.0538333654403687
Train Acc 0.6914
 Acc 0.6833
ogbn-arxiv,dgl,1,734,10.5236,0.6833

epoch:736/50, training loss:1.0536181926727295
Train Acc 0.6914
 Acc 0.6838
new best val f1: 0.6837627963500793
ogbn-arxiv,dgl,1,735,10.5362,0.6838

epoch:737/50, training loss:1.0534056425094604
Train Acc 0.6916
 Acc 0.6834
ogbn-arxiv,dgl,1,736,10.5487,0.6834

epoch:738/50, training loss:1.053189992904663
Train Acc 0.6915
 Acc 0.6834
ogbn-arxiv,dgl,1,737,10.5612,0.6834

epoch:739/50, training loss:1.0529773235321045
Train Acc 0.6915
 Acc 0.6839
new best val f1: 0.6839069805763249
ogbn-arxiv,dgl,1,738,10.5736,0.6839

epoch:740/50, training loss:1.0527687072753906
Train Acc 0.6918
 Acc 0.6836
ogbn-arxiv,dgl,1,739,10.5866,0.6836

epoch:741/50, training loss:1.052553653717041
Train Acc 0.6916
 Acc 0.6834
ogbn-arxiv,dgl,1,740,10.5992,0.6834

epoch:742/50, training loss:1.0523459911346436
Train Acc 0.6916
 Acc 0.6839
ogbn-arxiv,dgl,1,741,10.6118,0.6839

epoch:743/50, training loss:1.0521361827850342
Train Acc 0.6918
 Acc 0.6842
new best val f1: 0.6841747512822097
ogbn-arxiv,dgl,1,742,10.6243,0.6842

epoch:744/50, training loss:1.0519284009933472
Train Acc 0.6919
 Acc 0.6836
ogbn-arxiv,dgl,1,743,10.6370,0.6836

epoch:745/50, training loss:1.0517181158065796
Train Acc 0.6917
 Acc 0.6837
ogbn-arxiv,dgl,1,744,10.6494,0.6837

epoch:746/50, training loss:1.0515093803405762
Train Acc 0.6918
 Acc 0.6842
ogbn-arxiv,dgl,1,745,10.6622,0.6842

epoch:747/50, training loss:1.0513043403625488
Train Acc 0.6921
 Acc 0.6839
ogbn-arxiv,dgl,1,746,10.6747,0.6839

epoch:748/50, training loss:1.0510945320129395
Train Acc 0.6920
 Acc 0.6838
ogbn-arxiv,dgl,1,747,10.6873,0.6838

epoch:749/50, training loss:1.0508896112442017
Train Acc 0.6920
 Acc 0.6842
ogbn-arxiv,dgl,1,748,10.6999,0.6842

epoch:750/50, training loss:1.050682544708252
Train Acc 0.6921
 Acc 0.6844
new best val f1: 0.6843807287482749
ogbn-arxiv,dgl,1,749,10.7125,0.6844

epoch:751/50, training loss:1.0504789352416992
Train Acc 0.6922
 Acc 0.6839
ogbn-arxiv,dgl,1,750,10.7250,0.6839

epoch:752/50, training loss:1.0502769947052002
Train Acc 0.6921
 Acc 0.6842
ogbn-arxiv,dgl,1,751,10.7377,0.6842

epoch:753/50, training loss:1.0500742197036743
Train Acc 0.6923
 Acc 0.6846
new best val f1: 0.6846279017075532
ogbn-arxiv,dgl,1,752,10.7501,0.6846

epoch:754/50, training loss:1.0498707294464111
Train Acc 0.6924
 Acc 0.6844
ogbn-arxiv,dgl,1,753,10.7627,0.6844

epoch:755/50, training loss:1.049668788909912
Train Acc 0.6924
 Acc 0.6843
ogbn-arxiv,dgl,1,754,10.7752,0.6843

epoch:756/50, training loss:1.0494688749313354
Train Acc 0.6924
 Acc 0.6848
new best val f1: 0.6847720859337988
ogbn-arxiv,dgl,1,755,10.7879,0.6848

epoch:757/50, training loss:1.049265742301941
Train Acc 0.6926
 Acc 0.6848
new best val f1: 0.6848338791736184
ogbn-arxiv,dgl,1,756,10.8004,0.6848

epoch:758/50, training loss:1.049067735671997
Train Acc 0.6926
 Acc 0.6844
ogbn-arxiv,dgl,1,757,10.8130,0.6844

epoch:759/50, training loss:1.0488678216934204
Train Acc 0.6925
 Acc 0.6845
ogbn-arxiv,dgl,1,758,10.8256,0.6845

epoch:760/50, training loss:1.048668384552002
Train Acc 0.6926
 Acc 0.6849
new best val f1: 0.6848544769202249
ogbn-arxiv,dgl,1,759,10.8381,0.6849

epoch:761/50, training loss:1.0484693050384521
Train Acc 0.6926
 Acc 0.6846
ogbn-arxiv,dgl,1,760,10.8506,0.6846

epoch:762/50, training loss:1.0482712984085083
Train Acc 0.6926
 Acc 0.6849
ogbn-arxiv,dgl,1,761,10.8632,0.6849

epoch:763/50, training loss:1.0480738878250122
Train Acc 0.6927
 Acc 0.6849
new best val f1: 0.684895672413438
ogbn-arxiv,dgl,1,762,10.8757,0.6849

epoch:764/50, training loss:1.0478774309158325
Train Acc 0.6928
 Acc 0.6850
new best val f1: 0.6849986611464706
ogbn-arxiv,dgl,1,763,10.8883,0.6850

epoch:765/50, training loss:1.0476815700531006
Train Acc 0.6928
 Acc 0.6849
ogbn-arxiv,dgl,1,764,10.9008,0.6849

epoch:766/50, training loss:1.0474858283996582
Train Acc 0.6928
 Acc 0.6848
ogbn-arxiv,dgl,1,765,10.9134,0.6848

epoch:767/50, training loss:1.0472922325134277
Train Acc 0.6928
 Acc 0.6853
new best val f1: 0.685328225092175
ogbn-arxiv,dgl,1,766,10.9264,0.6853

epoch:768/50, training loss:1.0470972061157227
Train Acc 0.6930
 Acc 0.6852
ogbn-arxiv,dgl,1,767,10.9391,0.6852

epoch:769/50, training loss:1.0469036102294922
Train Acc 0.6930
 Acc 0.6850
ogbn-arxiv,dgl,1,768,10.9516,0.6850

epoch:770/50, training loss:1.0467110872268677
Train Acc 0.6930
 Acc 0.6851
ogbn-arxiv,dgl,1,769,10.9643,0.6851

epoch:771/50, training loss:1.0465158224105835
Train Acc 0.6931
 Acc 0.6855
new best val f1: 0.6854724093184206
ogbn-arxiv,dgl,1,770,10.9768,0.6855

epoch:772/50, training loss:1.0463261604309082
Train Acc 0.6932
 Acc 0.6853
ogbn-arxiv,dgl,1,771,10.9894,0.6853

epoch:773/50, training loss:1.0461349487304688
Train Acc 0.6932
 Acc 0.6853
ogbn-arxiv,dgl,1,772,11.0019,0.6853

epoch:774/50, training loss:1.0459437370300293
Train Acc 0.6933
 Acc 0.6856
new best val f1: 0.6855548003048466
ogbn-arxiv,dgl,1,773,11.0146,0.6856

epoch:775/50, training loss:1.0457545518875122
Train Acc 0.6933
 Acc 0.6857
new best val f1: 0.6856577890378792
ogbn-arxiv,dgl,1,774,11.0271,0.6857

epoch:776/50, training loss:1.0455631017684937
Train Acc 0.6933
 Acc 0.6853
ogbn-arxiv,dgl,1,775,11.0405,0.6853

epoch:777/50, training loss:1.045375108718872
Train Acc 0.6933
 Acc 0.6854
ogbn-arxiv,dgl,1,776,11.0529,0.6854

epoch:778/50, training loss:1.0451867580413818
Train Acc 0.6933
 Acc 0.6858
new best val f1: 0.6857813755175184
ogbn-arxiv,dgl,1,777,11.0656,0.6858

epoch:779/50, training loss:1.0449981689453125
Train Acc 0.6934
 Acc 0.6854
ogbn-arxiv,dgl,1,778,11.0780,0.6854

epoch:780/50, training loss:1.04481041431427
Train Acc 0.6934
 Acc 0.6853
ogbn-arxiv,dgl,1,779,11.0906,0.6853

epoch:781/50, training loss:1.044622778892517
Train Acc 0.6934
 Acc 0.6858
new best val f1: 0.6858225710107314
ogbn-arxiv,dgl,1,780,11.1031,0.6858

epoch:782/50, training loss:1.044438123703003
Train Acc 0.6935
 Acc 0.6856
ogbn-arxiv,dgl,1,781,11.1156,0.6856

epoch:783/50, training loss:1.0442479848861694
Train Acc 0.6935
 Acc 0.6854
ogbn-arxiv,dgl,1,782,11.1281,0.6854

epoch:784/50, training loss:1.0440634489059448
Train Acc 0.6935
 Acc 0.6857
ogbn-arxiv,dgl,1,783,11.1407,0.6857

epoch:785/50, training loss:1.0438780784606934
Train Acc 0.6937
 Acc 0.6857
ogbn-arxiv,dgl,1,784,11.1531,0.6857

epoch:786/50, training loss:1.0436911582946777
Train Acc 0.6937
 Acc 0.6856
ogbn-arxiv,dgl,1,785,11.1657,0.6856

epoch:787/50, training loss:1.0435070991516113
Train Acc 0.6936
 Acc 0.6856
ogbn-arxiv,dgl,1,786,11.1782,0.6856

epoch:788/50, training loss:1.043324589729309
Train Acc 0.6937
 Acc 0.6858
ogbn-arxiv,dgl,1,787,11.1908,0.6858

epoch:789/50, training loss:1.0431416034698486
Train Acc 0.6938
 Acc 0.6857
ogbn-arxiv,dgl,1,788,11.2032,0.6857

epoch:790/50, training loss:1.0429564714431763
Train Acc 0.6938
 Acc 0.6857
ogbn-arxiv,dgl,1,789,11.2159,0.6857

epoch:791/50, training loss:1.042773723602295
Train Acc 0.6938
 Acc 0.6858
ogbn-arxiv,dgl,1,790,11.2283,0.6858

epoch:792/50, training loss:1.0425924062728882
Train Acc 0.6939
 Acc 0.6858
new best val f1: 0.685843168757338
ogbn-arxiv,dgl,1,791,11.2409,0.6858

epoch:793/50, training loss:1.0424107313156128
Train Acc 0.6940
 Acc 0.6858
ogbn-arxiv,dgl,1,792,11.2533,0.6858

epoch:794/50, training loss:1.0422300100326538
Train Acc 0.6939
 Acc 0.6857
ogbn-arxiv,dgl,1,793,11.2659,0.6857

epoch:795/50, training loss:1.042051076889038
Train Acc 0.6939
 Acc 0.6860
new best val f1: 0.685966755236977
ogbn-arxiv,dgl,1,794,11.2783,0.6860

epoch:796/50, training loss:1.0418686866760254
Train Acc 0.6941
 Acc 0.6860
new best val f1: 0.6859873529835836
ogbn-arxiv,dgl,1,795,11.2909,0.6860

epoch:797/50, training loss:1.0416903495788574
Train Acc 0.6941
 Acc 0.6858
ogbn-arxiv,dgl,1,796,11.3033,0.6858

epoch:798/50, training loss:1.0415143966674805
Train Acc 0.6942
 Acc 0.6862
new best val f1: 0.6861727327030422
ogbn-arxiv,dgl,1,797,11.3159,0.6862

epoch:799/50, training loss:1.04132878780365
Train Acc 0.6943
 Acc 0.6863
new best val f1: 0.6863375146758944
ogbn-arxiv,dgl,1,798,11.3283,0.6863

epoch:800/50, training loss:1.041154146194458
Train Acc 0.6944
 Acc 0.6858
ogbn-arxiv,dgl,1,799,11.3410,0.6858

epoch:801/50, training loss:1.0409770011901855
Train Acc 0.6943
 Acc 0.6858
ogbn-arxiv,dgl,1,800,11.3534,0.6858

epoch:802/50, training loss:1.040798306465149
Train Acc 0.6944
 Acc 0.6862
ogbn-arxiv,dgl,1,801,11.3660,0.6862

epoch:803/50, training loss:1.0406228303909302
Train Acc 0.6945
 Acc 0.6862
ogbn-arxiv,dgl,1,802,11.3785,0.6862

epoch:804/50, training loss:1.0404452085494995
Train Acc 0.6945
 Acc 0.6860
ogbn-arxiv,dgl,1,803,11.3911,0.6860

epoch:805/50, training loss:1.040269136428833
Train Acc 0.6945
 Acc 0.6859
ogbn-arxiv,dgl,1,804,11.4035,0.6859

epoch:806/50, training loss:1.0400961637496948
Train Acc 0.6945
 Acc 0.6863
ogbn-arxiv,dgl,1,805,11.4165,0.6863

epoch:807/50, training loss:1.0399219989776611
Train Acc 0.6947
 Acc 0.6859
ogbn-arxiv,dgl,1,806,11.4290,0.6859

epoch:808/50, training loss:1.039743185043335
Train Acc 0.6946
 Acc 0.6859
ogbn-arxiv,dgl,1,807,11.4416,0.6859

epoch:809/50, training loss:1.0395698547363281
Train Acc 0.6947
 Acc 0.6864
new best val f1: 0.686358112422501
ogbn-arxiv,dgl,1,808,11.4541,0.6864

epoch:810/50, training loss:1.039394736289978
Train Acc 0.6947
 Acc 0.6862
ogbn-arxiv,dgl,1,809,11.4685,0.6862

epoch:811/50, training loss:1.0392206907272339
Train Acc 0.6948
 Acc 0.6861
ogbn-arxiv,dgl,1,810,11.4810,0.6861

epoch:812/50, training loss:1.0390466451644897
Train Acc 0.6947
 Acc 0.6862
ogbn-arxiv,dgl,1,811,11.4936,0.6862

epoch:813/50, training loss:1.038877010345459
Train Acc 0.6948
 Acc 0.6863
ogbn-arxiv,dgl,1,812,11.5060,0.6863

epoch:814/50, training loss:1.0387040376663208
Train Acc 0.6948
 Acc 0.6862
ogbn-arxiv,dgl,1,813,11.5186,0.6862

epoch:815/50, training loss:1.0385314226150513
Train Acc 0.6949
 Acc 0.6862
ogbn-arxiv,dgl,1,814,11.5310,0.6862

epoch:816/50, training loss:1.0383597612380981
Train Acc 0.6950
 Acc 0.6865
new best val f1: 0.6864816989021401
ogbn-arxiv,dgl,1,815,11.5436,0.6865

epoch:817/50, training loss:1.0381896495819092
Train Acc 0.6950
 Acc 0.6864
ogbn-arxiv,dgl,1,816,11.5561,0.6864

epoch:818/50, training loss:1.0380185842514038
Train Acc 0.6951
 Acc 0.6860
ogbn-arxiv,dgl,1,817,11.5686,0.6860

epoch:819/50, training loss:1.0378481149673462
Train Acc 0.6950
 Acc 0.6865
new best val f1: 0.6865228943953532
ogbn-arxiv,dgl,1,818,11.5810,0.6865

epoch:820/50, training loss:1.0376790761947632
Train Acc 0.6951
 Acc 0.6865
ogbn-arxiv,dgl,1,819,11.5936,0.6865

epoch:821/50, training loss:1.0375069379806519
Train Acc 0.6952
 Acc 0.6864
ogbn-arxiv,dgl,1,820,11.6060,0.6864

epoch:822/50, training loss:1.037339448928833
Train Acc 0.6952
 Acc 0.6865
ogbn-arxiv,dgl,1,821,11.6186,0.6865

epoch:823/50, training loss:1.0371688604354858
Train Acc 0.6952
 Acc 0.6866
new best val f1: 0.6865846876351727
ogbn-arxiv,dgl,1,822,11.6310,0.6866

epoch:824/50, training loss:1.037001371383667
Train Acc 0.6953
 Acc 0.6866
new best val f1: 0.6866258831283858
ogbn-arxiv,dgl,1,823,11.6437,0.6866

epoch:825/50, training loss:1.036832571029663
Train Acc 0.6954
 Acc 0.6863
ogbn-arxiv,dgl,1,824,11.6561,0.6863

epoch:826/50, training loss:1.036664605140686
Train Acc 0.6952
 Acc 0.6867
new best val f1: 0.6866876763682053
ogbn-arxiv,dgl,1,825,11.6686,0.6867

epoch:827/50, training loss:1.0364975929260254
Train Acc 0.6954
 Acc 0.6866
ogbn-arxiv,dgl,1,826,11.6811,0.6866

epoch:828/50, training loss:1.0363308191299438
Train Acc 0.6954
 Acc 0.6863
ogbn-arxiv,dgl,1,827,11.6937,0.6863

epoch:829/50, training loss:1.0361665487289429
Train Acc 0.6953
 Acc 0.6865
ogbn-arxiv,dgl,1,828,11.7062,0.6865

epoch:830/50, training loss:1.0359982252120972
Train Acc 0.6955
 Acc 0.6866
ogbn-arxiv,dgl,1,829,11.7193,0.6866

epoch:831/50, training loss:1.0358314514160156
Train Acc 0.6954
 Acc 0.6865
ogbn-arxiv,dgl,1,830,11.7319,0.6865

epoch:832/50, training loss:1.0356667041778564
Train Acc 0.6954
 Acc 0.6865
ogbn-arxiv,dgl,1,831,11.7446,0.6865

epoch:833/50, training loss:1.0355008840560913
Train Acc 0.6955
 Acc 0.6866
ogbn-arxiv,dgl,1,832,11.7570,0.6866

epoch:834/50, training loss:1.0353385210037231
Train Acc 0.6955
 Acc 0.6865
ogbn-arxiv,dgl,1,833,11.7697,0.6865

epoch:835/50, training loss:1.0351749658584595
Train Acc 0.6955
 Acc 0.6867
ogbn-arxiv,dgl,1,834,11.7828,0.6867

epoch:836/50, training loss:1.0350091457366943
Train Acc 0.6956
 Acc 0.6866
ogbn-arxiv,dgl,1,835,11.7954,0.6866

epoch:837/50, training loss:1.0348478555679321
Train Acc 0.6956
 Acc 0.6866
ogbn-arxiv,dgl,1,836,11.8079,0.6866

epoch:838/50, training loss:1.034684181213379
Train Acc 0.6956
 Acc 0.6867
ogbn-arxiv,dgl,1,837,11.8205,0.6867

epoch:839/50, training loss:1.0345209836959839
Train Acc 0.6957
 Acc 0.6867
ogbn-arxiv,dgl,1,838,11.8329,0.6867

epoch:840/50, training loss:1.034359335899353
Train Acc 0.6957
 Acc 0.6868
new best val f1: 0.6867906651012379
ogbn-arxiv,dgl,1,839,11.8454,0.6868

epoch:841/50, training loss:1.0341955423355103
Train Acc 0.6957
 Acc 0.6868
ogbn-arxiv,dgl,1,840,11.8579,0.6868

epoch:842/50, training loss:1.034034252166748
Train Acc 0.6958
 Acc 0.6870
new best val f1: 0.6869554470740901
ogbn-arxiv,dgl,1,841,11.8705,0.6870

epoch:843/50, training loss:1.0338729619979858
Train Acc 0.6958
 Acc 0.6867
ogbn-arxiv,dgl,1,842,11.8829,0.6867

epoch:844/50, training loss:1.0337141752243042
Train Acc 0.6958
 Acc 0.6870
ogbn-arxiv,dgl,1,843,11.8955,0.6870

epoch:845/50, training loss:1.0335522890090942
Train Acc 0.6959
 Acc 0.6868
ogbn-arxiv,dgl,1,844,11.9080,0.6868

epoch:846/50, training loss:1.033390760421753
Train Acc 0.6959
 Acc 0.6869
ogbn-arxiv,dgl,1,845,11.9205,0.6869

epoch:847/50, training loss:1.0332316160202026
Train Acc 0.6959
 Acc 0.6868
ogbn-arxiv,dgl,1,846,11.9330,0.6868

epoch:848/50, training loss:1.033073902130127
Train Acc 0.6960
 Acc 0.6869
ogbn-arxiv,dgl,1,847,11.9455,0.6869

epoch:849/50, training loss:1.0329140424728394
Train Acc 0.6961
 Acc 0.6870
new best val f1: 0.6869760448206966
ogbn-arxiv,dgl,1,848,11.9579,0.6870

epoch:850/50, training loss:1.032753348350525
Train Acc 0.6961
 Acc 0.6871
new best val f1: 0.6870790335537292
ogbn-arxiv,dgl,1,849,11.9720,0.6871

epoch:851/50, training loss:1.0325944423675537
Train Acc 0.6961
 Acc 0.6872
new best val f1: 0.6871614245401553
ogbn-arxiv,dgl,1,850,11.9845,0.6872

epoch:852/50, training loss:1.0324372053146362
Train Acc 0.6962
 Acc 0.6872
new best val f1: 0.6872026200333683
ogbn-arxiv,dgl,1,851,11.9971,0.6872

epoch:853/50, training loss:1.0322785377502441
Train Acc 0.6962
 Acc 0.6871
ogbn-arxiv,dgl,1,852,12.0095,0.6871

epoch:854/50, training loss:1.032122015953064
Train Acc 0.6962
 Acc 0.6871
ogbn-arxiv,dgl,1,853,12.0221,0.6871

epoch:855/50, training loss:1.0319634675979614
Train Acc 0.6962
 Acc 0.6872
ogbn-arxiv,dgl,1,854,12.0346,0.6872

epoch:856/50, training loss:1.0318089723587036
Train Acc 0.6963
 Acc 0.6873
new best val f1: 0.687305608766401
ogbn-arxiv,dgl,1,855,12.0471,0.6873

epoch:857/50, training loss:1.0316524505615234
Train Acc 0.6963
 Acc 0.6872
ogbn-arxiv,dgl,1,856,12.0614,0.6872

epoch:858/50, training loss:1.0314944982528687
Train Acc 0.6963
 Acc 0.6871
ogbn-arxiv,dgl,1,857,12.0740,0.6871

epoch:859/50, training loss:1.031339406967163
Train Acc 0.6964
 Acc 0.6873
ogbn-arxiv,dgl,1,858,12.0864,0.6873

epoch:860/50, training loss:1.0311834812164307
Train Acc 0.6964
 Acc 0.6873
ogbn-arxiv,dgl,1,859,12.0990,0.6873

epoch:861/50, training loss:1.0310286283493042
Train Acc 0.6965
 Acc 0.6870
ogbn-arxiv,dgl,1,860,12.1114,0.6870

epoch:862/50, training loss:1.0308737754821777
Train Acc 0.6964
 Acc 0.6874
new best val f1: 0.6873674020062205
ogbn-arxiv,dgl,1,861,12.1240,0.6874

epoch:863/50, training loss:1.0307209491729736
Train Acc 0.6966
 Acc 0.6875
new best val f1: 0.6874909884858597
ogbn-arxiv,dgl,1,862,12.1364,0.6875

epoch:864/50, training loss:1.0305640697479248
Train Acc 0.6966
 Acc 0.6872
ogbn-arxiv,dgl,1,863,12.1490,0.6872

epoch:865/50, training loss:1.0304133892059326
Train Acc 0.6965
 Acc 0.6872
ogbn-arxiv,dgl,1,864,12.1614,0.6872

epoch:866/50, training loss:1.030257225036621
Train Acc 0.6966
 Acc 0.6874
ogbn-arxiv,dgl,1,865,12.1740,0.6874

epoch:867/50, training loss:1.030105471611023
Train Acc 0.6966
 Acc 0.6873
ogbn-arxiv,dgl,1,866,12.1865,0.6873

epoch:868/50, training loss:1.0299530029296875
Train Acc 0.6967
 Acc 0.6873
ogbn-arxiv,dgl,1,867,12.1990,0.6873

epoch:869/50, training loss:1.029800534248352
Train Acc 0.6967
 Acc 0.6877
new best val f1: 0.6876969659519249
ogbn-arxiv,dgl,1,868,12.2115,0.6877

epoch:870/50, training loss:1.0296498537063599
Train Acc 0.6968
 Acc 0.6874
ogbn-arxiv,dgl,1,869,12.2241,0.6874

epoch:871/50, training loss:1.0294952392578125
Train Acc 0.6968
 Acc 0.6874
ogbn-arxiv,dgl,1,870,12.2366,0.6874

epoch:872/50, training loss:1.0293442010879517
Train Acc 0.6968
 Acc 0.6877
ogbn-arxiv,dgl,1,871,12.2492,0.6877

epoch:873/50, training loss:1.029194951057434
Train Acc 0.6970
 Acc 0.6875
ogbn-arxiv,dgl,1,872,12.2616,0.6875

epoch:874/50, training loss:1.029045581817627
Train Acc 0.6970
 Acc 0.6877
new best val f1: 0.6877381614451379
ogbn-arxiv,dgl,1,873,12.2742,0.6877

epoch:875/50, training loss:1.028892993927002
Train Acc 0.6970
 Acc 0.6878
new best val f1: 0.687820552431564
ogbn-arxiv,dgl,1,874,12.2867,0.6878

epoch:876/50, training loss:1.028744101524353
Train Acc 0.6970
 Acc 0.6877
ogbn-arxiv,dgl,1,875,12.2992,0.6877

epoch:877/50, training loss:1.0285942554473877
Train Acc 0.6971
 Acc 0.6876
ogbn-arxiv,dgl,1,876,12.3116,0.6876

epoch:878/50, training loss:1.0284439325332642
Train Acc 0.6972
 Acc 0.6880
new best val f1: 0.6879647366578097
ogbn-arxiv,dgl,1,877,12.3243,0.6880

epoch:879/50, training loss:1.0282970666885376
Train Acc 0.6972
 Acc 0.6877
ogbn-arxiv,dgl,1,878,12.3367,0.6877

epoch:880/50, training loss:1.0281481742858887
Train Acc 0.6972
 Acc 0.6876
ogbn-arxiv,dgl,1,879,12.3493,0.6876

epoch:881/50, training loss:1.0280003547668457
Train Acc 0.6972
 Acc 0.6880
new best val f1: 0.6880265298976292
ogbn-arxiv,dgl,1,880,12.3617,0.6880

epoch:882/50, training loss:1.0278511047363281
Train Acc 0.6973
 Acc 0.6878
ogbn-arxiv,dgl,1,881,12.3743,0.6878

epoch:883/50, training loss:1.0277005434036255
Train Acc 0.6973
 Acc 0.6876
ogbn-arxiv,dgl,1,882,12.3868,0.6876

epoch:884/50, training loss:1.0275558233261108
Train Acc 0.6972
 Acc 0.6879
ogbn-arxiv,dgl,1,883,12.3994,0.6879

epoch:885/50, training loss:1.0274068117141724
Train Acc 0.6973
 Acc 0.6880
ogbn-arxiv,dgl,1,884,12.4119,0.6880

epoch:886/50, training loss:1.0272603034973145
Train Acc 0.6974
 Acc 0.6878
ogbn-arxiv,dgl,1,885,12.4245,0.6878

epoch:887/50, training loss:1.027113676071167
Train Acc 0.6974
 Acc 0.6878
ogbn-arxiv,dgl,1,886,12.4370,0.6878

epoch:888/50, training loss:1.0269675254821777
Train Acc 0.6974
 Acc 0.6880
ogbn-arxiv,dgl,1,887,12.4495,0.6880

epoch:889/50, training loss:1.0268200635910034
Train Acc 0.6975
 Acc 0.6879
ogbn-arxiv,dgl,1,888,12.4619,0.6879

epoch:890/50, training loss:1.0266731977462769
Train Acc 0.6975
 Acc 0.6879
ogbn-arxiv,dgl,1,889,12.4746,0.6879

epoch:891/50, training loss:1.0265297889709473
Train Acc 0.6975
 Acc 0.6880
ogbn-arxiv,dgl,1,890,12.4870,0.6880

epoch:892/50, training loss:1.0263829231262207
Train Acc 0.6975
 Acc 0.6880
ogbn-arxiv,dgl,1,891,12.4996,0.6880

epoch:893/50, training loss:1.0262356996536255
Train Acc 0.6976
 Acc 0.6879
ogbn-arxiv,dgl,1,892,12.5121,0.6879

epoch:894/50, training loss:1.0260937213897705
Train Acc 0.6976
 Acc 0.6881
new best val f1: 0.6880883231374487
ogbn-arxiv,dgl,1,893,12.5247,0.6881

epoch:895/50, training loss:1.0259475708007812
Train Acc 0.6977
 Acc 0.6880
ogbn-arxiv,dgl,1,894,12.5372,0.6880

epoch:896/50, training loss:1.025802731513977
Train Acc 0.6976
 Acc 0.6879
ogbn-arxiv,dgl,1,895,12.5497,0.6879

epoch:897/50, training loss:1.0256608724594116
Train Acc 0.6976
 Acc 0.6880
ogbn-arxiv,dgl,1,896,12.5621,0.6880

epoch:898/50, training loss:1.0255135297775269
Train Acc 0.6977
 Acc 0.6882
new best val f1: 0.6882119096170879
ogbn-arxiv,dgl,1,897,12.5748,0.6882

epoch:899/50, training loss:1.025371789932251
Train Acc 0.6977
 Acc 0.6881
ogbn-arxiv,dgl,1,898,12.5872,0.6881

epoch:900/50, training loss:1.025227427482605
Train Acc 0.6978
 Acc 0.6881
ogbn-arxiv,dgl,1,899,12.5998,0.6881

epoch:901/50, training loss:1.025084376335144
Train Acc 0.6978
 Acc 0.6883
new best val f1: 0.6882737028569075
ogbn-arxiv,dgl,1,900,12.6123,0.6883

epoch:902/50, training loss:1.0249429941177368
Train Acc 0.6978
 Acc 0.6883
new best val f1: 0.6883148983501205
ogbn-arxiv,dgl,1,901,12.6249,0.6883

epoch:903/50, training loss:1.0248008966445923
Train Acc 0.6978
 Acc 0.6882
ogbn-arxiv,dgl,1,902,12.6374,0.6882

epoch:904/50, training loss:1.024660587310791
Train Acc 0.6979
 Acc 0.6885
new best val f1: 0.6885208758161857
ogbn-arxiv,dgl,1,903,12.6500,0.6885

epoch:905/50, training loss:1.0245155096054077
Train Acc 0.6979
 Acc 0.6884
ogbn-arxiv,dgl,1,904,12.6624,0.6884

epoch:906/50, training loss:1.0243730545043945
Train Acc 0.6979
 Acc 0.6883
ogbn-arxiv,dgl,1,905,12.6750,0.6883

epoch:907/50, training loss:1.0242317914962769
Train Acc 0.6980
 Acc 0.6887
new best val f1: 0.6886650600424313
ogbn-arxiv,dgl,1,906,12.6875,0.6887

epoch:908/50, training loss:1.0240905284881592
Train Acc 0.6981
 Acc 0.6886
ogbn-arxiv,dgl,1,907,12.7000,0.6886

epoch:909/50, training loss:1.0239485502243042
Train Acc 0.6980
 Acc 0.6883
ogbn-arxiv,dgl,1,908,12.7125,0.6883

epoch:910/50, training loss:1.0238076448440552
Train Acc 0.6980
 Acc 0.6886
ogbn-arxiv,dgl,1,909,12.7251,0.6886

epoch:911/50, training loss:1.023666501045227
Train Acc 0.6981
 Acc 0.6888
new best val f1: 0.6887680487754639
ogbn-arxiv,dgl,1,910,12.7376,0.6888

epoch:912/50, training loss:1.023526906967163
Train Acc 0.6982
 Acc 0.6884
ogbn-arxiv,dgl,1,911,12.7502,0.6884

epoch:913/50, training loss:1.023384928703308
Train Acc 0.6981
 Acc 0.6888
new best val f1: 0.688809244268677
ogbn-arxiv,dgl,1,912,12.7626,0.6888

epoch:914/50, training loss:1.0232453346252441
Train Acc 0.6982
 Acc 0.6885
ogbn-arxiv,dgl,1,913,12.7752,0.6885

epoch:915/50, training loss:1.023105263710022
Train Acc 0.6981
 Acc 0.6887
ogbn-arxiv,dgl,1,914,12.7877,0.6887

epoch:916/50, training loss:1.0229661464691162
Train Acc 0.6982
 Acc 0.6889
new best val f1: 0.6888504397618901
ogbn-arxiv,dgl,1,915,12.8003,0.6889

epoch:917/50, training loss:1.0228264331817627
Train Acc 0.6983
 Acc 0.6886
ogbn-arxiv,dgl,1,916,12.8127,0.6886

epoch:918/50, training loss:1.0226876735687256
Train Acc 0.6983
 Acc 0.6886
ogbn-arxiv,dgl,1,917,12.8252,0.6886

epoch:919/50, training loss:1.0225492715835571
Train Acc 0.6982
 Acc 0.6889
new best val f1: 0.6889328307483161
ogbn-arxiv,dgl,1,918,12.8377,0.6889

epoch:920/50, training loss:1.022408366203308
Train Acc 0.6983
 Acc 0.6887
ogbn-arxiv,dgl,1,919,12.8503,0.6887

epoch:921/50, training loss:1.0222702026367188
Train Acc 0.6984
 Acc 0.6889
ogbn-arxiv,dgl,1,920,12.8626,0.6889

epoch:922/50, training loss:1.0221316814422607
Train Acc 0.6984
 Acc 0.6891
new best val f1: 0.6890564172279553
ogbn-arxiv,dgl,1,921,12.8754,0.6891

epoch:923/50, training loss:1.0219932794570923
Train Acc 0.6985
 Acc 0.6886
ogbn-arxiv,dgl,1,922,12.8878,0.6886

epoch:924/50, training loss:1.0218545198440552
Train Acc 0.6983
 Acc 0.6887
ogbn-arxiv,dgl,1,923,12.9004,0.6887

epoch:925/50, training loss:1.021716833114624
Train Acc 0.6984
 Acc 0.6891
new best val f1: 0.6891182104677748
ogbn-arxiv,dgl,1,924,12.9128,0.6891

epoch:926/50, training loss:1.0215797424316406
Train Acc 0.6985
 Acc 0.6888
ogbn-arxiv,dgl,1,925,12.9253,0.6888

epoch:927/50, training loss:1.0214426517486572
Train Acc 0.6984
 Acc 0.6889
ogbn-arxiv,dgl,1,926,12.9378,0.6889

epoch:928/50, training loss:1.0213040113449097
Train Acc 0.6985
 Acc 0.6892
new best val f1: 0.6892417969474139
ogbn-arxiv,dgl,1,927,12.9505,0.6892

epoch:929/50, training loss:1.021167516708374
Train Acc 0.6986
 Acc 0.6889
ogbn-arxiv,dgl,1,928,12.9629,0.6889

epoch:930/50, training loss:1.0210304260253906
Train Acc 0.6985
 Acc 0.6887
ogbn-arxiv,dgl,1,929,12.9756,0.6887

epoch:931/50, training loss:1.0208954811096191
Train Acc 0.6985
 Acc 0.6892
ogbn-arxiv,dgl,1,930,12.9880,0.6892

epoch:932/50, training loss:1.0207576751708984
Train Acc 0.6987
 Acc 0.6890
ogbn-arxiv,dgl,1,931,13.0005,0.6890

epoch:933/50, training loss:1.0206217765808105
Train Acc 0.6987
 Acc 0.6889
ogbn-arxiv,dgl,1,932,13.0131,0.6889

epoch:934/50, training loss:1.0204864740371704
Train Acc 0.6986
 Acc 0.6892
ogbn-arxiv,dgl,1,933,13.0257,0.6892

epoch:935/50, training loss:1.0203510522842407
Train Acc 0.6988
 Acc 0.6890
ogbn-arxiv,dgl,1,934,13.0381,0.6890

epoch:936/50, training loss:1.0202155113220215
Train Acc 0.6988
 Acc 0.6892
ogbn-arxiv,dgl,1,935,13.0507,0.6892

epoch:937/50, training loss:1.0200823545455933
Train Acc 0.6989
 Acc 0.6894
new best val f1: 0.6893653834270531
ogbn-arxiv,dgl,1,936,13.0631,0.6894

epoch:938/50, training loss:1.019945502281189
Train Acc 0.6991
 Acc 0.6892
ogbn-arxiv,dgl,1,937,13.0757,0.6892

epoch:939/50, training loss:1.0198109149932861
Train Acc 0.6989
 Acc 0.6892
ogbn-arxiv,dgl,1,938,13.0882,0.6892

epoch:940/50, training loss:1.0196748971939087
Train Acc 0.6989
 Acc 0.6893
ogbn-arxiv,dgl,1,939,13.1007,0.6893

epoch:941/50, training loss:1.0195417404174805
Train Acc 0.6990
 Acc 0.6893
ogbn-arxiv,dgl,1,940,13.1130,0.6893

epoch:942/50, training loss:1.0194075107574463
Train Acc 0.6989
 Acc 0.6893
ogbn-arxiv,dgl,1,941,13.1256,0.6893

epoch:943/50, training loss:1.0192731618881226
Train Acc 0.6991
 Acc 0.6894
new best val f1: 0.6894477744134792
ogbn-arxiv,dgl,1,942,13.1382,0.6894

epoch:944/50, training loss:1.019142508506775
Train Acc 0.6991
 Acc 0.6893
ogbn-arxiv,dgl,1,943,13.1508,0.6893

epoch:945/50, training loss:1.0190081596374512
Train Acc 0.6991
 Acc 0.6893
ogbn-arxiv,dgl,1,944,13.1632,0.6893

epoch:946/50, training loss:1.0188722610473633
Train Acc 0.6991
 Acc 0.6896
new best val f1: 0.6895507631465118
ogbn-arxiv,dgl,1,945,13.1758,0.6896

epoch:947/50, training loss:1.0187404155731201
Train Acc 0.6992
 Acc 0.6894
ogbn-arxiv,dgl,1,946,13.1883,0.6894

epoch:948/50, training loss:1.0186071395874023
Train Acc 0.6991
 Acc 0.6893
ogbn-arxiv,dgl,1,947,13.2009,0.6893

epoch:949/50, training loss:1.0184738636016846
Train Acc 0.6992
 Acc 0.6896
new best val f1: 0.6895713608931183
ogbn-arxiv,dgl,1,948,13.2134,0.6896

epoch:950/50, training loss:1.0183415412902832
Train Acc 0.6993
 Acc 0.6895
ogbn-arxiv,dgl,1,949,13.2260,0.6895

epoch:951/50, training loss:1.0182089805603027
Train Acc 0.6993
 Acc 0.6895
ogbn-arxiv,dgl,1,950,13.2384,0.6895

epoch:952/50, training loss:1.01807701587677
Train Acc 0.6993
 Acc 0.6894
ogbn-arxiv,dgl,1,951,13.2510,0.6894

epoch:953/50, training loss:1.0179452896118164
Train Acc 0.6993
 Acc 0.6896
ogbn-arxiv,dgl,1,952,13.2634,0.6896

epoch:954/50, training loss:1.0178139209747314
Train Acc 0.6993
 Acc 0.6896
ogbn-arxiv,dgl,1,953,13.2765,0.6896

epoch:955/50, training loss:1.017682433128357
Train Acc 0.6994
 Acc 0.6897
new best val f1: 0.689715545119364
ogbn-arxiv,dgl,1,954,13.2890,0.6897

epoch:956/50, training loss:1.0175484418869019
Train Acc 0.6995
 Acc 0.6897
ogbn-arxiv,dgl,1,955,13.3016,0.6897

epoch:957/50, training loss:1.0174177885055542
Train Acc 0.6994
 Acc 0.6896
ogbn-arxiv,dgl,1,956,13.3141,0.6896

epoch:958/50, training loss:1.017289161682129
Train Acc 0.6994
 Acc 0.6896
ogbn-arxiv,dgl,1,957,13.3268,0.6896

epoch:959/50, training loss:1.0171585083007812
Train Acc 0.6994
 Acc 0.6897
ogbn-arxiv,dgl,1,958,13.3393,0.6897

epoch:960/50, training loss:1.0170270204544067
Train Acc 0.6994
 Acc 0.6895
ogbn-arxiv,dgl,1,959,13.3519,0.6895

epoch:961/50, training loss:1.0168970823287964
Train Acc 0.6994
 Acc 0.6896
ogbn-arxiv,dgl,1,960,13.3643,0.6896

epoch:962/50, training loss:1.0167673826217651
Train Acc 0.6994
 Acc 0.6896
ogbn-arxiv,dgl,1,961,13.3769,0.6896

epoch:963/50, training loss:1.0166356563568115
Train Acc 0.6994
 Acc 0.6898
new best val f1: 0.6897773383591835
ogbn-arxiv,dgl,1,962,13.3894,0.6898

epoch:964/50, training loss:1.0165094137191772
Train Acc 0.6995
 Acc 0.6897
ogbn-arxiv,dgl,1,963,13.4020,0.6897

epoch:965/50, training loss:1.0163774490356445
Train Acc 0.6996
 Acc 0.6898
ogbn-arxiv,dgl,1,964,13.4144,0.6898

epoch:966/50, training loss:1.0162473917007446
Train Acc 0.6995
 Acc 0.6898
ogbn-arxiv,dgl,1,965,13.4269,0.6898

epoch:967/50, training loss:1.0161162614822388
Train Acc 0.6996
 Acc 0.6898
ogbn-arxiv,dgl,1,966,13.4394,0.6898

epoch:968/50, training loss:1.0159882307052612
Train Acc 0.6997
 Acc 0.6899
new best val f1: 0.6899421203320357
ogbn-arxiv,dgl,1,967,13.4520,0.6899

epoch:969/50, training loss:1.015859842300415
Train Acc 0.6997
 Acc 0.6900
new best val f1: 0.6899833158252487
ogbn-arxiv,dgl,1,968,13.4644,0.6900

epoch:970/50, training loss:1.0157314538955688
Train Acc 0.6997
 Acc 0.6899
ogbn-arxiv,dgl,1,969,13.4770,0.6899

epoch:971/50, training loss:1.0156000852584839
Train Acc 0.6997
 Acc 0.6900
new best val f1: 0.6900245113184618
ogbn-arxiv,dgl,1,970,13.4894,0.6900

epoch:972/50, training loss:1.0154750347137451
Train Acc 0.6997
 Acc 0.6900
ogbn-arxiv,dgl,1,971,13.5023,0.6900

epoch:973/50, training loss:1.0153446197509766
Train Acc 0.6997
 Acc 0.6898
ogbn-arxiv,dgl,1,972,13.5149,0.6898

epoch:974/50, training loss:1.0152148008346558
Train Acc 0.6997
 Acc 0.6900
ogbn-arxiv,dgl,1,973,13.5276,0.6900

epoch:975/50, training loss:1.0150853395462036
Train Acc 0.6999
 Acc 0.6900
ogbn-arxiv,dgl,1,974,13.5402,0.6900

epoch:976/50, training loss:1.0149565935134888
Train Acc 0.6998
 Acc 0.6899
ogbn-arxiv,dgl,1,975,13.5529,0.6899

epoch:977/50, training loss:1.0148298740386963
Train Acc 0.6998
 Acc 0.6901
new best val f1: 0.6901275000514944
ogbn-arxiv,dgl,1,976,13.5655,0.6901

epoch:978/50, training loss:1.0147019624710083
Train Acc 0.6999
 Acc 0.6900
ogbn-arxiv,dgl,1,977,13.5782,0.6900

epoch:979/50, training loss:1.0145745277404785
Train Acc 0.6999
 Acc 0.6900
ogbn-arxiv,dgl,1,978,13.5907,0.6900

epoch:980/50, training loss:1.0144453048706055
Train Acc 0.6999
 Acc 0.6902
new best val f1: 0.6902098910379204
ogbn-arxiv,dgl,1,979,13.6033,0.6902

epoch:981/50, training loss:1.0143190622329712
Train Acc 0.6999
 Acc 0.6900
ogbn-arxiv,dgl,1,980,13.6158,0.6900

epoch:982/50, training loss:1.014192819595337
Train Acc 0.6999
 Acc 0.6901
ogbn-arxiv,dgl,1,981,13.6284,0.6901

epoch:983/50, training loss:1.0140634775161743
Train Acc 0.7000
 Acc 0.6903
new best val f1: 0.6902922820243466
ogbn-arxiv,dgl,1,982,13.6410,0.6903

epoch:984/50, training loss:1.0139392614364624
Train Acc 0.7000
 Acc 0.6901
ogbn-arxiv,dgl,1,983,13.6537,0.6901

epoch:985/50, training loss:1.0138075351715088
Train Acc 0.7000
 Acc 0.6899
ogbn-arxiv,dgl,1,984,13.6662,0.6899

epoch:986/50, training loss:1.0136862993240356
Train Acc 0.7001
 Acc 0.6900
ogbn-arxiv,dgl,1,985,13.6788,0.6900

epoch:987/50, training loss:1.0135549306869507
Train Acc 0.7001
 Acc 0.6903
new best val f1: 0.690312879770953
ogbn-arxiv,dgl,1,986,13.6914,0.6903

epoch:988/50, training loss:1.0134280920028687
Train Acc 0.7002
 Acc 0.6901
ogbn-arxiv,dgl,1,987,13.7041,0.6901

epoch:989/50, training loss:1.013303279876709
Train Acc 0.7002
 Acc 0.6901
ogbn-arxiv,dgl,1,988,13.7166,0.6901

epoch:990/50, training loss:1.0131778717041016
Train Acc 0.7002
 Acc 0.6902
ogbn-arxiv,dgl,1,989,13.7293,0.6902

epoch:991/50, training loss:1.013048529624939
Train Acc 0.7002
 Acc 0.6902
ogbn-arxiv,dgl,1,990,13.7419,0.6902

epoch:992/50, training loss:1.0129245519638062
Train Acc 0.7002
 Acc 0.6904
new best val f1: 0.6903746730107726
ogbn-arxiv,dgl,1,991,13.7545,0.6904

epoch:993/50, training loss:1.0127952098846436
Train Acc 0.7003
 Acc 0.6902
ogbn-arxiv,dgl,1,992,13.7670,0.6902

epoch:994/50, training loss:1.012669563293457
Train Acc 0.7003
 Acc 0.6900
ogbn-arxiv,dgl,1,993,13.7801,0.6900

epoch:995/50, training loss:1.0125442743301392
Train Acc 0.7002
 Acc 0.6904
new best val f1: 0.6904364662505922
ogbn-arxiv,dgl,1,994,13.7944,0.6904

epoch:996/50, training loss:1.0124201774597168
Train Acc 0.7003
 Acc 0.6903
ogbn-arxiv,dgl,1,995,13.8071,0.6903

epoch:997/50, training loss:1.0122919082641602
Train Acc 0.7003
 Acc 0.6902
ogbn-arxiv,dgl,1,996,13.8278,0.6902

epoch:998/50, training loss:1.012168288230896
Train Acc 0.7004
 Acc 0.6904
ogbn-arxiv,dgl,1,997,13.8479,0.6904

epoch:999/50, training loss:1.0120410919189453
Train Acc 0.7004
 Acc 0.6904
ogbn-arxiv,dgl,1,998,13.8604,0.6904

epoch:1000/50, training loss:1.0119155645370483
Train Acc 0.7005
 Acc 0.6901
ogbn-arxiv,dgl,1,999,13.8731,0.6901

epoch:1001/50, training loss:1.0117887258529663
Train Acc 0.7003
 Acc 0.6904
ogbn-arxiv,dgl,1,1000,13.8857,0.6904

epoch:1002/50, training loss:1.0116688013076782
Train Acc 0.7005
 Acc 0.6904
ogbn-arxiv,dgl,1,1001,13.8982,0.6904

epoch:1003/50, training loss:1.0115416049957275
Train Acc 0.7004
 Acc 0.6900
ogbn-arxiv,dgl,1,1002,13.9106,0.6900

epoch:1004/50, training loss:1.0114188194274902
Train Acc 0.7003
 Acc 0.6903
ogbn-arxiv,dgl,1,1003,13.9232,0.6903

epoch:1005/50, training loss:1.0112916231155396
Train Acc 0.7005
 Acc 0.6904
ogbn-arxiv,dgl,1,1004,13.9356,0.6904

epoch:1006/50, training loss:1.011169195175171
Train Acc 0.7005
 Acc 0.6901
ogbn-arxiv,dgl,1,1005,13.9482,0.6901

epoch:1007/50, training loss:1.0110430717468262
Train Acc 0.7005
 Acc 0.6902
ogbn-arxiv,dgl,1,1006,13.9606,0.6902

epoch:1008/50, training loss:1.0109217166900635
Train Acc 0.7005
 Acc 0.6905
new best val f1: 0.6905188572370182
ogbn-arxiv,dgl,1,1007,13.9732,0.6905

epoch:1009/50, training loss:1.010798692703247
Train Acc 0.7006
 Acc 0.6902
ogbn-arxiv,dgl,1,1008,13.9856,0.6902

epoch:1010/50, training loss:1.0106747150421143
Train Acc 0.7005
 Acc 0.6901
ogbn-arxiv,dgl,1,1009,13.9983,0.6901

epoch:1011/50, training loss:1.0105501413345337
Train Acc 0.7006
 Acc 0.6905
new best val f1: 0.6905394549836248
ogbn-arxiv,dgl,1,1010,14.0107,0.6905

epoch:1012/50, training loss:1.0104278326034546
Train Acc 0.7006
 Acc 0.6903
ogbn-arxiv,dgl,1,1011,14.0233,0.6903

epoch:1013/50, training loss:1.0103031396865845
Train Acc 0.7007
 Acc 0.6903
ogbn-arxiv,dgl,1,1012,14.0357,0.6903

epoch:1014/50, training loss:1.0101802349090576
Train Acc 0.7007
 Acc 0.6906
new best val f1: 0.6906424437166574
ogbn-arxiv,dgl,1,1013,14.0482,0.6906

epoch:1015/50, training loss:1.0100574493408203
Train Acc 0.7007
 Acc 0.6906
ogbn-arxiv,dgl,1,1014,14.0606,0.6906

epoch:1016/50, training loss:1.0099341869354248
Train Acc 0.7008
 Acc 0.6903
ogbn-arxiv,dgl,1,1015,14.0731,0.6903

epoch:1017/50, training loss:1.009810447692871
Train Acc 0.7008
 Acc 0.6905
ogbn-arxiv,dgl,1,1016,14.0856,0.6905

epoch:1018/50, training loss:1.0096896886825562
Train Acc 0.7007
 Acc 0.6904
ogbn-arxiv,dgl,1,1017,14.0982,0.6904

epoch:1019/50, training loss:1.0095635652542114
Train Acc 0.7008
 Acc 0.6906
ogbn-arxiv,dgl,1,1018,14.1106,0.6906

epoch:1020/50, training loss:1.0094431638717651
Train Acc 0.7009
 Acc 0.6906
ogbn-arxiv,dgl,1,1019,14.1232,0.6906

epoch:1021/50, training loss:1.0093212127685547
Train Acc 0.7008
 Acc 0.6907
new best val f1: 0.690663041463264
ogbn-arxiv,dgl,1,1020,14.1356,0.6907

epoch:1022/50, training loss:1.00919771194458
Train Acc 0.7009
 Acc 0.6903
ogbn-arxiv,dgl,1,1021,14.1482,0.6903

epoch:1023/50, training loss:1.0090774297714233
Train Acc 0.7008
 Acc 0.6906
ogbn-arxiv,dgl,1,1022,14.1606,0.6906

epoch:1024/50, training loss:1.0089538097381592
Train Acc 0.7008
 Acc 0.6907
new best val f1: 0.6907248347030834
ogbn-arxiv,dgl,1,1023,14.1731,0.6907

epoch:1025/50, training loss:1.0088322162628174
Train Acc 0.7009
 Acc 0.6905
ogbn-arxiv,dgl,1,1024,14.1855,0.6905

epoch:1026/50, training loss:1.0087108612060547
Train Acc 0.7009
 Acc 0.6904
ogbn-arxiv,dgl,1,1025,14.1981,0.6904

epoch:1027/50, training loss:1.0085887908935547
Train Acc 0.7009
 Acc 0.6907
ogbn-arxiv,dgl,1,1026,14.2105,0.6907

epoch:1028/50, training loss:1.0084683895111084
Train Acc 0.7009
 Acc 0.6904
ogbn-arxiv,dgl,1,1027,14.2231,0.6904

epoch:1029/50, training loss:1.0083446502685547
Train Acc 0.7009
 Acc 0.6905
ogbn-arxiv,dgl,1,1028,14.2356,0.6905

epoch:1030/50, training loss:1.008225917816162
Train Acc 0.7009
 Acc 0.6909
new best val f1: 0.6909102144225422
ogbn-arxiv,dgl,1,1029,14.2481,0.6909

epoch:1031/50, training loss:1.008105993270874
Train Acc 0.7011
 Acc 0.6905
ogbn-arxiv,dgl,1,1030,14.2605,0.6905

epoch:1032/50, training loss:1.0079823732376099
Train Acc 0.7010
 Acc 0.6904
ogbn-arxiv,dgl,1,1031,14.2730,0.6904

epoch:1033/50, training loss:1.0078608989715576
Train Acc 0.7010
 Acc 0.6908
ogbn-arxiv,dgl,1,1032,14.2854,0.6908

epoch:1034/50, training loss:1.0077428817749023
Train Acc 0.7011
 Acc 0.6905
ogbn-arxiv,dgl,1,1033,14.2980,0.6905

epoch:1035/50, training loss:1.0076205730438232
Train Acc 0.7011
 Acc 0.6902
ogbn-arxiv,dgl,1,1034,14.3104,0.6902

epoch:1036/50, training loss:1.0075007677078247
Train Acc 0.7010
 Acc 0.6907
ogbn-arxiv,dgl,1,1035,14.3229,0.6907

epoch:1037/50, training loss:1.007379412651062
Train Acc 0.7011
 Acc 0.6906
ogbn-arxiv,dgl,1,1036,14.3354,0.6906

epoch:1038/50, training loss:1.0072576999664307
Train Acc 0.7011
 Acc 0.6904
ogbn-arxiv,dgl,1,1037,14.3479,0.6904

epoch:1039/50, training loss:1.0071405172348022
Train Acc 0.7011
 Acc 0.6907
ogbn-arxiv,dgl,1,1038,14.3604,0.6907

epoch:1040/50, training loss:1.007017731666565
Train Acc 0.7012
 Acc 0.6905
ogbn-arxiv,dgl,1,1039,14.3729,0.6905

epoch:1041/50, training loss:1.006899356842041
Train Acc 0.7012
 Acc 0.6904
ogbn-arxiv,dgl,1,1040,14.3862,0.6904

epoch:1042/50, training loss:1.0067778825759888
Train Acc 0.7012
 Acc 0.6905
ogbn-arxiv,dgl,1,1041,14.3988,0.6905

epoch:1043/50, training loss:1.0066571235656738
Train Acc 0.7012
 Acc 0.6905
ogbn-arxiv,dgl,1,1042,14.4112,0.6905

epoch:1044/50, training loss:1.0065371990203857
Train Acc 0.7013
 Acc 0.6905
ogbn-arxiv,dgl,1,1043,14.4239,0.6905

epoch:1045/50, training loss:1.0064184665679932
Train Acc 0.7013
 Acc 0.6906
ogbn-arxiv,dgl,1,1044,14.4363,0.6906

epoch:1046/50, training loss:1.006300687789917
Train Acc 0.7014
 Acc 0.6909
ogbn-arxiv,dgl,1,1045,14.4489,0.6909

epoch:1047/50, training loss:1.0061806440353394
Train Acc 0.7014
 Acc 0.6906
ogbn-arxiv,dgl,1,1046,14.4613,0.6906

epoch:1048/50, training loss:1.0060608386993408
Train Acc 0.7014
 Acc 0.6905
ogbn-arxiv,dgl,1,1047,14.4738,0.6905

epoch:1049/50, training loss:1.0059434175491333
Train Acc 0.7014
 Acc 0.6909
ogbn-arxiv,dgl,1,1048,14.4862,0.6909

epoch:1050/50, training loss:1.0058220624923706
Train Acc 0.7015
 Acc 0.6908
ogbn-arxiv,dgl,1,1049,14.4987,0.6908

epoch:1051/50, training loss:1.0057036876678467
Train Acc 0.7015
 Acc 0.6905
ogbn-arxiv,dgl,1,1050,14.5111,0.6905

epoch:1052/50, training loss:1.0055863857269287
Train Acc 0.7014
 Acc 0.6908
ogbn-arxiv,dgl,1,1051,14.5236,0.6908

epoch:1053/50, training loss:1.005463719367981
Train Acc 0.7015
 Acc 0.6909
new best val f1: 0.6909308121691486
ogbn-arxiv,dgl,1,1052,14.5360,0.6909

epoch:1054/50, training loss:1.0053491592407227
Train Acc 0.7016
 Acc 0.6907
ogbn-arxiv,dgl,1,1053,14.5486,0.6907

epoch:1055/50, training loss:1.005228877067566
Train Acc 0.7016
 Acc 0.6906
ogbn-arxiv,dgl,1,1054,14.5609,0.6906

epoch:1056/50, training loss:1.0051096677780151
Train Acc 0.7016
 Acc 0.6911
new best val f1: 0.6910749963953944
ogbn-arxiv,dgl,1,1055,14.5735,0.6911

epoch:1057/50, training loss:1.0049926042556763
Train Acc 0.7017
 Acc 0.6909
ogbn-arxiv,dgl,1,1056,14.5859,0.6909

epoch:1058/50, training loss:1.004869818687439
Train Acc 0.7017
 Acc 0.6908
ogbn-arxiv,dgl,1,1057,14.5984,0.6908

epoch:1059/50, training loss:1.004753828048706
Train Acc 0.7017
 Acc 0.6911
ogbn-arxiv,dgl,1,1058,14.6108,0.6911

epoch:1060/50, training loss:1.0046347379684448
Train Acc 0.7017
 Acc 0.6910
ogbn-arxiv,dgl,1,1059,14.6234,0.6910

epoch:1061/50, training loss:1.0045157670974731
Train Acc 0.7018
 Acc 0.6910
ogbn-arxiv,dgl,1,1060,14.6358,0.6910

epoch:1062/50, training loss:1.0043978691101074
Train Acc 0.7018
 Acc 0.6911
ogbn-arxiv,dgl,1,1061,14.6484,0.6911

epoch:1063/50, training loss:1.0042800903320312
Train Acc 0.7018
 Acc 0.6911
new best val f1: 0.6910955941420008
ogbn-arxiv,dgl,1,1062,14.6608,0.6911

epoch:1064/50, training loss:1.0041613578796387
Train Acc 0.7019
 Acc 0.6909
ogbn-arxiv,dgl,1,1063,14.6733,0.6909

epoch:1065/50, training loss:1.0040448904037476
Train Acc 0.7018
 Acc 0.6910
ogbn-arxiv,dgl,1,1064,14.6856,0.6910

epoch:1066/50, training loss:1.0039255619049072
Train Acc 0.7019
 Acc 0.6912
new best val f1: 0.69121918062164
ogbn-arxiv,dgl,1,1065,14.6982,0.6912

epoch:1067/50, training loss:1.0038100481033325
Train Acc 0.7020
 Acc 0.6911
ogbn-arxiv,dgl,1,1066,14.7106,0.6911

epoch:1068/50, training loss:1.003691554069519
Train Acc 0.7020
 Acc 0.6911
ogbn-arxiv,dgl,1,1067,14.7232,0.6911

epoch:1069/50, training loss:1.0035741329193115
Train Acc 0.7020
 Acc 0.6914
new best val f1: 0.6913633648478856
ogbn-arxiv,dgl,1,1068,14.7355,0.6914

epoch:1070/50, training loss:1.0034594535827637
Train Acc 0.7021
 Acc 0.6912
ogbn-arxiv,dgl,1,1069,14.7481,0.6912

epoch:1071/50, training loss:1.003341555595398
Train Acc 0.7020
 Acc 0.6910
ogbn-arxiv,dgl,1,1070,14.7605,0.6910

epoch:1072/50, training loss:1.0032236576080322
Train Acc 0.7020
 Acc 0.6913
ogbn-arxiv,dgl,1,1071,14.7730,0.6913

epoch:1073/50, training loss:1.0031065940856934
Train Acc 0.7021
 Acc 0.6909
ogbn-arxiv,dgl,1,1072,14.7854,0.6909

epoch:1074/50, training loss:1.002989411354065
Train Acc 0.7020
 Acc 0.6911
ogbn-arxiv,dgl,1,1073,14.7980,0.6911

epoch:1075/50, training loss:1.002872109413147
Train Acc 0.7021
 Acc 0.6911
ogbn-arxiv,dgl,1,1074,14.8103,0.6911

epoch:1076/50, training loss:1.0027568340301514
Train Acc 0.7021
 Acc 0.6911
ogbn-arxiv,dgl,1,1075,14.8228,0.6911

epoch:1077/50, training loss:1.0026377439498901
Train Acc 0.7020
 Acc 0.6912
ogbn-arxiv,dgl,1,1076,14.8352,0.6912

epoch:1078/50, training loss:1.0025193691253662
Train Acc 0.7021
 Acc 0.6911
ogbn-arxiv,dgl,1,1077,14.8478,0.6911

epoch:1079/50, training loss:1.0024057626724243
Train Acc 0.7021
 Acc 0.6913
ogbn-arxiv,dgl,1,1078,14.8602,0.6913

epoch:1080/50, training loss:1.0022878646850586
Train Acc 0.7021
 Acc 0.6912
ogbn-arxiv,dgl,1,1079,14.8727,0.6912

epoch:1081/50, training loss:1.0021721124649048
Train Acc 0.7021
 Acc 0.6910
ogbn-arxiv,dgl,1,1080,14.8852,0.6910

epoch:1082/50, training loss:1.00205659866333
Train Acc 0.7021
 Acc 0.6911
ogbn-arxiv,dgl,1,1081,14.8978,0.6911

epoch:1083/50, training loss:1.0019404888153076
Train Acc 0.7022
 Acc 0.6916
new best val f1: 0.6915899400605574
ogbn-arxiv,dgl,1,1082,14.9102,0.6916

epoch:1084/50, training loss:1.0018259286880493
Train Acc 0.7023
 Acc 0.6910
ogbn-arxiv,dgl,1,1083,14.9227,0.6910

epoch:1085/50, training loss:1.0017077922821045
Train Acc 0.7021
 Acc 0.6913
ogbn-arxiv,dgl,1,1084,14.9351,0.6913

epoch:1086/50, training loss:1.001592755317688
Train Acc 0.7023
 Acc 0.6912
ogbn-arxiv,dgl,1,1085,14.9477,0.6912

epoch:1087/50, training loss:1.0014774799346924
Train Acc 0.7023
 Acc 0.6912
ogbn-arxiv,dgl,1,1086,14.9601,0.6912

epoch:1088/50, training loss:1.0013618469238281
Train Acc 0.7023
 Acc 0.6913
ogbn-arxiv,dgl,1,1087,14.9726,0.6913

epoch:1089/50, training loss:1.0012468099594116
Train Acc 0.7023
 Acc 0.6911
ogbn-arxiv,dgl,1,1088,14.9850,0.6911

epoch:1090/50, training loss:1.0011322498321533
Train Acc 0.7024
 Acc 0.6913
ogbn-arxiv,dgl,1,1089,14.9974,0.6913

epoch:1091/50, training loss:1.0010168552398682
Train Acc 0.7023
 Acc 0.6913
ogbn-arxiv,dgl,1,1090,15.0098,0.6913

epoch:1092/50, training loss:1.000899314880371
Train Acc 0.7024
 Acc 0.6910
ogbn-arxiv,dgl,1,1091,15.0224,0.6910

epoch:1093/50, training loss:1.0007864236831665
Train Acc 0.7023
 Acc 0.6912
ogbn-arxiv,dgl,1,1092,15.0349,0.6912

epoch:1094/50, training loss:1.0006710290908813
Train Acc 0.7024
 Acc 0.6916
ogbn-arxiv,dgl,1,1093,15.0474,0.6916

epoch:1095/50, training loss:1.0005552768707275
Train Acc 0.7025
 Acc 0.6911
ogbn-arxiv,dgl,1,1094,15.0598,0.6911

epoch:1096/50, training loss:1.0004401206970215
Train Acc 0.7024
 Acc 0.6915
ogbn-arxiv,dgl,1,1095,15.0723,0.6915

epoch:1097/50, training loss:1.0003236532211304
Train Acc 0.7025
 Acc 0.6917
new best val f1: 0.691651733300377
ogbn-arxiv,dgl,1,1096,15.0847,0.6917

epoch:1098/50, training loss:1.000207781791687
Train Acc 0.7026
 Acc 0.6914
ogbn-arxiv,dgl,1,1097,15.0972,0.6914

epoch:1099/50, training loss:1.00009024143219
Train Acc 0.7026
 Acc 0.6914
ogbn-arxiv,dgl,1,1098,15.1096,0.6914

epoch:1100/50, training loss:0.999974250793457
Train Acc 0.7026
 Acc 0.6919
new best val f1: 0.6918989062596552
ogbn-arxiv,dgl,1,1099,15.1221,0.6919

epoch:1101/50, training loss:0.9998599886894226
Train Acc 0.7028
 Acc 0.6913
ogbn-arxiv,dgl,1,1100,15.1346,0.6913

epoch:1102/50, training loss:0.9997420310974121
Train Acc 0.7026
 Acc 0.6915
ogbn-arxiv,dgl,1,1101,15.1471,0.6915

epoch:1103/50, training loss:0.9996267557144165
Train Acc 0.7027
 Acc 0.6919
new best val f1: 0.6919195040062617
ogbn-arxiv,dgl,1,1102,15.1595,0.6919

epoch:1104/50, training loss:0.9995129108428955
Train Acc 0.7028
 Acc 0.6917
ogbn-arxiv,dgl,1,1103,15.1719,0.6917

epoch:1105/50, training loss:0.9993980526924133
Train Acc 0.7027
 Acc 0.6915
ogbn-arxiv,dgl,1,1104,15.1844,0.6915

epoch:1106/50, training loss:0.9992827773094177
Train Acc 0.7027
 Acc 0.6919
ogbn-arxiv,dgl,1,1105,15.1969,0.6919

epoch:1107/50, training loss:0.9991654753684998
Train Acc 0.7028
 Acc 0.6918
ogbn-arxiv,dgl,1,1106,15.2093,0.6918

epoch:1108/50, training loss:0.9990503191947937
Train Acc 0.7029
 Acc 0.6916
ogbn-arxiv,dgl,1,1107,15.2233,0.6916

epoch:1109/50, training loss:0.9989340901374817
Train Acc 0.7028
 Acc 0.6918
ogbn-arxiv,dgl,1,1108,15.2357,0.6918

epoch:1110/50, training loss:0.9988173842430115
Train Acc 0.7028
 Acc 0.6919
ogbn-arxiv,dgl,1,1109,15.2482,0.6919

epoch:1111/50, training loss:0.998702347278595
Train Acc 0.7029
 Acc 0.6917
ogbn-arxiv,dgl,1,1110,15.2606,0.6917

epoch:1112/50, training loss:0.9985860586166382
Train Acc 0.7029
 Acc 0.6917
ogbn-arxiv,dgl,1,1111,15.2732,0.6917

epoch:1113/50, training loss:0.9984708428382874
Train Acc 0.7029
 Acc 0.6919
new best val f1: 0.6919401017528682
ogbn-arxiv,dgl,1,1112,15.2857,0.6919

epoch:1114/50, training loss:0.9983546137809753
Train Acc 0.7030
 Acc 0.6918
ogbn-arxiv,dgl,1,1113,15.2983,0.6918

epoch:1115/50, training loss:0.9982360005378723
Train Acc 0.7030
 Acc 0.6917
ogbn-arxiv,dgl,1,1114,15.3106,0.6917

epoch:1116/50, training loss:0.9981226921081543
Train Acc 0.7030
 Acc 0.6918
ogbn-arxiv,dgl,1,1115,15.3232,0.6918

epoch:1117/50, training loss:0.9980083107948303
Train Acc 0.7030
 Acc 0.6918
ogbn-arxiv,dgl,1,1116,15.3355,0.6918

epoch:1118/50, training loss:0.9978897571563721
Train Acc 0.7030
 Acc 0.6922
new best val f1: 0.69216667696554
ogbn-arxiv,dgl,1,1117,15.3481,0.6922

epoch:1119/50, training loss:0.9977765679359436
Train Acc 0.7031
 Acc 0.6920
ogbn-arxiv,dgl,1,1118,15.3605,0.6920

epoch:1120/50, training loss:0.9976595640182495
Train Acc 0.7032
 Acc 0.6916
ogbn-arxiv,dgl,1,1119,15.3730,0.6916

epoch:1121/50, training loss:0.9975458979606628
Train Acc 0.7031
 Acc 0.6922
new best val f1: 0.6921872747121465
ogbn-arxiv,dgl,1,1120,15.3853,0.6922

epoch:1122/50, training loss:0.9974287748336792
Train Acc 0.7032
 Acc 0.6920
ogbn-arxiv,dgl,1,1121,15.3979,0.6920

epoch:1123/50, training loss:0.9973123073577881
Train Acc 0.7032
 Acc 0.6916
ogbn-arxiv,dgl,1,1122,15.4102,0.6916

epoch:1124/50, training loss:0.9971979856491089
Train Acc 0.7032
 Acc 0.6922
ogbn-arxiv,dgl,1,1123,15.4227,0.6922

epoch:1125/50, training loss:0.9970833659172058
Train Acc 0.7033
 Acc 0.6921
ogbn-arxiv,dgl,1,1124,15.4351,0.6921

epoch:1126/50, training loss:0.9969656467437744
Train Acc 0.7033
 Acc 0.6917
ogbn-arxiv,dgl,1,1125,15.4476,0.6917

epoch:1127/50, training loss:0.9968498349189758
Train Acc 0.7033
 Acc 0.6921
ogbn-arxiv,dgl,1,1126,15.4600,0.6921

epoch:1128/50, training loss:0.996737003326416
Train Acc 0.7033
 Acc 0.6920
ogbn-arxiv,dgl,1,1127,15.4725,0.6920

epoch:1129/50, training loss:0.9966216683387756
Train Acc 0.7033
 Acc 0.6918
ogbn-arxiv,dgl,1,1128,15.4849,0.6918

epoch:1130/50, training loss:0.99650639295578
Train Acc 0.7034
 Acc 0.6923
new best val f1: 0.6923108611917856
ogbn-arxiv,dgl,1,1129,15.4974,0.6923

epoch:1131/50, training loss:0.9963889122009277
Train Acc 0.7035
 Acc 0.6920
ogbn-arxiv,dgl,1,1130,15.5098,0.6920

epoch:1132/50, training loss:0.9962741136550903
Train Acc 0.7035
 Acc 0.6921
ogbn-arxiv,dgl,1,1131,15.5222,0.6921

epoch:1133/50, training loss:0.9961588382720947
Train Acc 0.7035
 Acc 0.6920
ogbn-arxiv,dgl,1,1132,15.5346,0.6920

epoch:1134/50, training loss:0.9960446357727051
Train Acc 0.7034
 Acc 0.6923
ogbn-arxiv,dgl,1,1133,15.5471,0.6923

epoch:1135/50, training loss:0.9959287047386169
Train Acc 0.7034
 Acc 0.6920
ogbn-arxiv,dgl,1,1134,15.5594,0.6920

epoch:1136/50, training loss:0.9958155155181885
Train Acc 0.7035
 Acc 0.6923
ogbn-arxiv,dgl,1,1135,15.5720,0.6923

epoch:1137/50, training loss:0.9956978559494019
Train Acc 0.7036
 Acc 0.6922
ogbn-arxiv,dgl,1,1136,15.5843,0.6922

epoch:1138/50, training loss:0.9955847859382629
Train Acc 0.7036
 Acc 0.6920
ogbn-arxiv,dgl,1,1137,15.5968,0.6920

epoch:1139/50, training loss:0.9954692721366882
Train Acc 0.7036
 Acc 0.6924
new best val f1: 0.6923726544316052
ogbn-arxiv,dgl,1,1138,15.6093,0.6924

epoch:1140/50, training loss:0.9953546524047852
Train Acc 0.7036
 Acc 0.6922
ogbn-arxiv,dgl,1,1139,15.6218,0.6922

epoch:1141/50, training loss:0.9952380657196045
Train Acc 0.7037
 Acc 0.6920
ogbn-arxiv,dgl,1,1140,15.6342,0.6920

epoch:1142/50, training loss:0.9951228499412537
Train Acc 0.7036
 Acc 0.6924
ogbn-arxiv,dgl,1,1141,15.6467,0.6924

epoch:1143/50, training loss:0.9950090646743774
Train Acc 0.7037
 Acc 0.6922
ogbn-arxiv,dgl,1,1142,15.6590,0.6922

epoch:1144/50, training loss:0.9948923587799072
Train Acc 0.7037
 Acc 0.6922
ogbn-arxiv,dgl,1,1143,15.6716,0.6922

epoch:1145/50, training loss:0.9947764873504639
Train Acc 0.7037
 Acc 0.6921
ogbn-arxiv,dgl,1,1144,15.6839,0.6921

epoch:1146/50, training loss:0.9946628212928772
Train Acc 0.7037
 Acc 0.6922
ogbn-arxiv,dgl,1,1145,15.6964,0.6922

epoch:1147/50, training loss:0.9945473670959473
Train Acc 0.7037
 Acc 0.6924
ogbn-arxiv,dgl,1,1146,15.7087,0.6924

epoch:1148/50, training loss:0.9944342374801636
Train Acc 0.7038
 Acc 0.6921
ogbn-arxiv,dgl,1,1147,15.7213,0.6921

epoch:1149/50, training loss:0.9943180680274963
Train Acc 0.7037
 Acc 0.6924
new best val f1: 0.6923932521782117
ogbn-arxiv,dgl,1,1148,15.7337,0.6924

epoch:1150/50, training loss:0.9942070841789246
Train Acc 0.7038
 Acc 0.6922
ogbn-arxiv,dgl,1,1149,15.7461,0.6922

epoch:1151/50, training loss:0.9940896034240723
Train Acc 0.7038
 Acc 0.6923
ogbn-arxiv,dgl,1,1150,15.7592,0.6923

epoch:1152/50, training loss:0.9939761757850647
Train Acc 0.7039
 Acc 0.6925
new best val f1: 0.6924962409112443
ogbn-arxiv,dgl,1,1151,15.7718,0.6925

epoch:1153/50, training loss:0.9938589930534363
Train Acc 0.7040
 Acc 0.6923
ogbn-arxiv,dgl,1,1152,15.7842,0.6923

epoch:1154/50, training loss:0.9937461018562317
Train Acc 0.7040
 Acc 0.6927
new best val f1: 0.6927022183773095
ogbn-arxiv,dgl,1,1153,15.7967,0.6927

epoch:1155/50, training loss:0.9936285018920898
Train Acc 0.7041
 Acc 0.6926
ogbn-arxiv,dgl,1,1154,15.8091,0.6926

epoch:1156/50, training loss:0.993513822555542
Train Acc 0.7040
 Acc 0.6925
ogbn-arxiv,dgl,1,1155,15.8216,0.6925

epoch:1157/50, training loss:0.9933985471725464
Train Acc 0.7040
 Acc 0.6927
ogbn-arxiv,dgl,1,1156,15.8340,0.6927

epoch:1158/50, training loss:0.993285059928894
Train Acc 0.7041
 Acc 0.6927
new best val f1: 0.6927434138705225
ogbn-arxiv,dgl,1,1157,15.8465,0.6927

epoch:1159/50, training loss:0.9931706190109253
Train Acc 0.7042
 Acc 0.6924
ogbn-arxiv,dgl,1,1158,15.8589,0.6924

epoch:1160/50, training loss:0.9930558800697327
Train Acc 0.7041
 Acc 0.6926
ogbn-arxiv,dgl,1,1159,15.8714,0.6926

epoch:1161/50, training loss:0.9929406642913818
Train Acc 0.7042
 Acc 0.6928
new best val f1: 0.6927846093637356
ogbn-arxiv,dgl,1,1160,15.8857,0.6928

epoch:1162/50, training loss:0.9928244948387146
Train Acc 0.7042
 Acc 0.6924
ogbn-arxiv,dgl,1,1161,15.8981,0.6924

epoch:1163/50, training loss:0.99271160364151
Train Acc 0.7042
 Acc 0.6929
new best val f1: 0.6928670003501617
ogbn-arxiv,dgl,1,1162,15.9104,0.6929

epoch:1164/50, training loss:0.992595911026001
Train Acc 0.7043
 Acc 0.6925
ogbn-arxiv,dgl,1,1163,15.9230,0.6925

epoch:1165/50, training loss:0.9924814701080322
Train Acc 0.7042
 Acc 0.6928
ogbn-arxiv,dgl,1,1164,15.9354,0.6928

epoch:1166/50, training loss:0.9923669695854187
Train Acc 0.7043
 Acc 0.6927
ogbn-arxiv,dgl,1,1165,15.9479,0.6927

epoch:1167/50, training loss:0.9922523498535156
Train Acc 0.7043
 Acc 0.6924
ogbn-arxiv,dgl,1,1166,15.9602,0.6924

epoch:1168/50, training loss:0.9921361207962036
Train Acc 0.7042
 Acc 0.6928
ogbn-arxiv,dgl,1,1167,15.9727,0.6928

epoch:1169/50, training loss:0.9920253157615662
Train Acc 0.7044
 Acc 0.6925
ogbn-arxiv,dgl,1,1168,15.9850,0.6925

epoch:1170/50, training loss:0.9919100999832153
Train Acc 0.7043
 Acc 0.6926
ogbn-arxiv,dgl,1,1169,15.9975,0.6926

epoch:1171/50, training loss:0.9917930960655212
Train Acc 0.7044
 Acc 0.6930
new best val f1: 0.6930317823230139
ogbn-arxiv,dgl,1,1170,16.0099,0.6930

epoch:1172/50, training loss:0.9916815161705017
Train Acc 0.7045
 Acc 0.6925
ogbn-arxiv,dgl,1,1171,16.0224,0.6925

epoch:1173/50, training loss:0.9915627837181091
Train Acc 0.7044
 Acc 0.6927
ogbn-arxiv,dgl,1,1172,16.0348,0.6927

epoch:1174/50, training loss:0.991447925567627
Train Acc 0.7045
 Acc 0.6928
ogbn-arxiv,dgl,1,1173,16.0473,0.6928

epoch:1175/50, training loss:0.9913344383239746
Train Acc 0.7045
 Acc 0.6924
ogbn-arxiv,dgl,1,1174,16.0597,0.6924

epoch:1176/50, training loss:0.9912207126617432
Train Acc 0.7044
 Acc 0.6929
ogbn-arxiv,dgl,1,1175,16.0723,0.6929

epoch:1177/50, training loss:0.9911028146743774
Train Acc 0.7046
 Acc 0.6931
new best val f1: 0.6930935755628335
ogbn-arxiv,dgl,1,1176,16.0847,0.6931

epoch:1178/50, training loss:0.9909890294075012
Train Acc 0.7047
 Acc 0.6924
ogbn-arxiv,dgl,1,1177,16.0972,0.6924

epoch:1179/50, training loss:0.9908769130706787
Train Acc 0.7045
 Acc 0.6931
new best val f1: 0.6931347710560465
ogbn-arxiv,dgl,1,1178,16.1095,0.6931

epoch:1180/50, training loss:0.9907593131065369
Train Acc 0.7047
 Acc 0.6931
ogbn-arxiv,dgl,1,1179,16.1220,0.6931

epoch:1181/50, training loss:0.9906435012817383
Train Acc 0.7047
 Acc 0.6927
ogbn-arxiv,dgl,1,1180,16.1345,0.6927

epoch:1182/50, training loss:0.9905314445495605
Train Acc 0.7047
 Acc 0.6932
new best val f1: 0.6931553688026529
ogbn-arxiv,dgl,1,1181,16.1470,0.6932

epoch:1183/50, training loss:0.9904152154922485
Train Acc 0.7047
 Acc 0.6928
ogbn-arxiv,dgl,1,1182,16.1610,0.6928

epoch:1184/50, training loss:0.9902995824813843
Train Acc 0.7047
 Acc 0.6926
ogbn-arxiv,dgl,1,1183,16.1735,0.6926

epoch:1185/50, training loss:0.9901849031448364
Train Acc 0.7047
 Acc 0.6930
ogbn-arxiv,dgl,1,1184,16.1859,0.6930

epoch:1186/50, training loss:0.99007248878479
Train Acc 0.7048
 Acc 0.6929
ogbn-arxiv,dgl,1,1185,16.1984,0.6929

epoch:1187/50, training loss:0.9899551868438721
Train Acc 0.7048
 Acc 0.6927
ogbn-arxiv,dgl,1,1186,16.2108,0.6927

epoch:1188/50, training loss:0.9898459911346436
Train Acc 0.7046
 Acc 0.6931
ogbn-arxiv,dgl,1,1187,16.2234,0.6931

epoch:1189/50, training loss:0.9897287487983704
Train Acc 0.7048
 Acc 0.6930
ogbn-arxiv,dgl,1,1188,16.2358,0.6930

epoch:1190/50, training loss:0.989615261554718
Train Acc 0.7048
 Acc 0.6925
ogbn-arxiv,dgl,1,1189,16.2483,0.6925

epoch:1191/50, training loss:0.9895015954971313
Train Acc 0.7047
 Acc 0.6931
ogbn-arxiv,dgl,1,1190,16.2612,0.6931

epoch:1192/50, training loss:0.9893885254859924
Train Acc 0.7049
 Acc 0.6928
ogbn-arxiv,dgl,1,1191,16.2738,0.6928

epoch:1193/50, training loss:0.9892730116844177
Train Acc 0.7048
 Acc 0.6929
ogbn-arxiv,dgl,1,1192,16.2863,0.6929

epoch:1194/50, training loss:0.9891601204872131
Train Acc 0.7048
 Acc 0.6931
ogbn-arxiv,dgl,1,1193,16.2988,0.6931

epoch:1195/50, training loss:0.9890477657318115
Train Acc 0.7049
 Acc 0.6929
ogbn-arxiv,dgl,1,1194,16.3112,0.6929

epoch:1196/50, training loss:0.9889329075813293
Train Acc 0.7049
 Acc 0.6930
ogbn-arxiv,dgl,1,1195,16.3237,0.6930

epoch:1197/50, training loss:0.9888193011283875
Train Acc 0.7049
 Acc 0.6930
ogbn-arxiv,dgl,1,1196,16.3361,0.6930

epoch:1198/50, training loss:0.9887079000473022
Train Acc 0.7050
 Acc 0.6929
ogbn-arxiv,dgl,1,1197,16.3487,0.6929

epoch:1199/50, training loss:0.9885953068733215
Train Acc 0.7050
 Acc 0.6930
ogbn-arxiv,dgl,1,1198,16.3611,0.6930

epoch:1200/50, training loss:0.9884811043739319
Train Acc 0.7050
 Acc 0.6930
ogbn-arxiv,dgl,1,1199,16.3738,0.6930

epoch:1201/50, training loss:0.988366425037384
Train Acc 0.7051
 Acc 0.6931
ogbn-arxiv,dgl,1,1200,16.3863,0.6931

epoch:1202/50, training loss:0.9882556796073914
Train Acc 0.7051
 Acc 0.6930
ogbn-arxiv,dgl,1,1201,16.3989,0.6930

epoch:1203/50, training loss:0.9881406426429749
Train Acc 0.7051
 Acc 0.6931
ogbn-arxiv,dgl,1,1202,16.4113,0.6931

epoch:1204/50, training loss:0.9880271553993225
Train Acc 0.7052
 Acc 0.6931
ogbn-arxiv,dgl,1,1203,16.4239,0.6931

epoch:1205/50, training loss:0.9879144430160522
Train Acc 0.7052
 Acc 0.6931
ogbn-arxiv,dgl,1,1204,16.4364,0.6931

epoch:1206/50, training loss:0.987803041934967
Train Acc 0.7052
 Acc 0.6931
ogbn-arxiv,dgl,1,1205,16.4490,0.6931

epoch:1207/50, training loss:0.9876883625984192
Train Acc 0.7052
 Acc 0.6930
ogbn-arxiv,dgl,1,1206,16.4614,0.6930

epoch:1208/50, training loss:0.9875780344009399
Train Acc 0.7052
 Acc 0.6933
new best val f1: 0.6933407485221117
ogbn-arxiv,dgl,1,1207,16.4740,0.6933

epoch:1209/50, training loss:0.9874642491340637
Train Acc 0.7053
 Acc 0.6932
ogbn-arxiv,dgl,1,1208,16.4865,0.6932

epoch:1210/50, training loss:0.9873525500297546
Train Acc 0.7053
 Acc 0.6929
ogbn-arxiv,dgl,1,1209,16.4990,0.6929

epoch:1211/50, training loss:0.9872391223907471
Train Acc 0.7053
 Acc 0.6933
ogbn-arxiv,dgl,1,1210,16.5115,0.6933

epoch:1212/50, training loss:0.9871286749839783
Train Acc 0.7054
 Acc 0.6930
ogbn-arxiv,dgl,1,1211,16.5240,0.6930

epoch:1213/50, training loss:0.9870151281356812
Train Acc 0.7053
 Acc 0.6931
ogbn-arxiv,dgl,1,1212,16.5364,0.6931

epoch:1214/50, training loss:0.9869034886360168
Train Acc 0.7053
 Acc 0.6931
ogbn-arxiv,dgl,1,1213,16.5491,0.6931

epoch:1215/50, training loss:0.9867908954620361
Train Acc 0.7053
 Acc 0.6933
ogbn-arxiv,dgl,1,1214,16.5615,0.6933

epoch:1216/50, training loss:0.9866778254508972
Train Acc 0.7054
 Acc 0.6932
ogbn-arxiv,dgl,1,1215,16.5739,0.6932

epoch:1217/50, training loss:0.9865656495094299
Train Acc 0.7054
 Acc 0.6928
ogbn-arxiv,dgl,1,1216,16.5864,0.6928

epoch:1218/50, training loss:0.9864556193351746
Train Acc 0.7054
 Acc 0.6932
ogbn-arxiv,dgl,1,1217,16.5990,0.6932

epoch:1219/50, training loss:0.9863429665565491
Train Acc 0.7054
 Acc 0.6931
ogbn-arxiv,dgl,1,1218,16.6114,0.6931

epoch:1220/50, training loss:0.9862306118011475
Train Acc 0.7054
 Acc 0.6931
ogbn-arxiv,dgl,1,1219,16.6239,0.6931

epoch:1221/50, training loss:0.9861207008361816
Train Acc 0.7055
 Acc 0.6932
ogbn-arxiv,dgl,1,1220,16.6364,0.6932

epoch:1222/50, training loss:0.9860085844993591
Train Acc 0.7055
 Acc 0.6932
ogbn-arxiv,dgl,1,1221,16.6490,0.6932

epoch:1223/50, training loss:0.985897421836853
Train Acc 0.7055
 Acc 0.6931
ogbn-arxiv,dgl,1,1222,16.6615,0.6931

epoch:1224/50, training loss:0.9857854843139648
Train Acc 0.7055
 Acc 0.6933
ogbn-arxiv,dgl,1,1223,16.6741,0.6933

epoch:1225/50, training loss:0.9856752753257751
Train Acc 0.7056
 Acc 0.6932
ogbn-arxiv,dgl,1,1224,16.6865,0.6932

epoch:1226/50, training loss:0.9855628609657288
Train Acc 0.7056
 Acc 0.6930
ogbn-arxiv,dgl,1,1225,16.6991,0.6930

epoch:1227/50, training loss:0.9854517579078674
Train Acc 0.7055
 Acc 0.6933
ogbn-arxiv,dgl,1,1226,16.7115,0.6933

epoch:1228/50, training loss:0.9853427410125732
Train Acc 0.7056
 Acc 0.6932
ogbn-arxiv,dgl,1,1227,16.7240,0.6932

epoch:1229/50, training loss:0.9852278232574463
Train Acc 0.7056
 Acc 0.6929
ogbn-arxiv,dgl,1,1228,16.7365,0.6929

epoch:1230/50, training loss:0.985119640827179
Train Acc 0.7056
 Acc 0.6935
new best val f1: 0.6934643350017508
ogbn-arxiv,dgl,1,1229,16.7490,0.6935

epoch:1231/50, training loss:0.9850092530250549
Train Acc 0.7056
 Acc 0.6931
ogbn-arxiv,dgl,1,1230,16.7614,0.6931

epoch:1232/50, training loss:0.984898030757904
Train Acc 0.7057
 Acc 0.6934
ogbn-arxiv,dgl,1,1231,16.7739,0.6934

epoch:1233/50, training loss:0.9847857356071472
Train Acc 0.7057
 Acc 0.6935
new best val f1: 0.6935261282415703
ogbn-arxiv,dgl,1,1232,16.7863,0.6935

epoch:1234/50, training loss:0.9846749305725098
Train Acc 0.7058
 Acc 0.6930
ogbn-arxiv,dgl,1,1233,16.7990,0.6930

epoch:1235/50, training loss:0.9845660328865051
Train Acc 0.7057
 Acc 0.6934
ogbn-arxiv,dgl,1,1234,16.8114,0.6934

epoch:1236/50, training loss:0.9844536185264587
Train Acc 0.7058
 Acc 0.6935
ogbn-arxiv,dgl,1,1235,16.8239,0.6935

epoch:1237/50, training loss:0.9843441843986511
Train Acc 0.7058
 Acc 0.6931
ogbn-arxiv,dgl,1,1236,16.8363,0.6931

epoch:1238/50, training loss:0.9842330813407898
Train Acc 0.7057
 Acc 0.6933
ogbn-arxiv,dgl,1,1237,16.8489,0.6933

epoch:1239/50, training loss:0.9841229319572449
Train Acc 0.7058
 Acc 0.6935
new best val f1: 0.6935467259881769
ogbn-arxiv,dgl,1,1238,16.8614,0.6935

epoch:1240/50, training loss:0.9840125441551208
Train Acc 0.7059
 Acc 0.6931
ogbn-arxiv,dgl,1,1239,16.8740,0.6931

epoch:1241/50, training loss:0.9839038848876953
Train Acc 0.7059
 Acc 0.6939
new best val f1: 0.6938762899338813
ogbn-arxiv,dgl,1,1240,16.8864,0.6939

epoch:1242/50, training loss:0.9837927222251892
Train Acc 0.7061
 Acc 0.6938
ogbn-arxiv,dgl,1,1241,16.8989,0.6938

epoch:1243/50, training loss:0.9836819171905518
Train Acc 0.7060
 Acc 0.6932
ogbn-arxiv,dgl,1,1242,16.9114,0.6932

epoch:1244/50, training loss:0.9835746884346008
Train Acc 0.7060
 Acc 0.6937
ogbn-arxiv,dgl,1,1243,16.9239,0.6937

epoch:1245/50, training loss:0.9834646582603455
Train Acc 0.7061
 Acc 0.6938
ogbn-arxiv,dgl,1,1244,16.9363,0.6938

epoch:1246/50, training loss:0.9833530187606812
Train Acc 0.7061
 Acc 0.6931
ogbn-arxiv,dgl,1,1245,16.9488,0.6931

epoch:1247/50, training loss:0.9832476377487183
Train Acc 0.7060
 Acc 0.6938
ogbn-arxiv,dgl,1,1246,16.9612,0.6938

epoch:1248/50, training loss:0.9831366539001465
Train Acc 0.7062
 Acc 0.6937
ogbn-arxiv,dgl,1,1247,16.9738,0.6937

epoch:1249/50, training loss:0.983025074005127
Train Acc 0.7062
 Acc 0.6932
ogbn-arxiv,dgl,1,1248,16.9862,0.6932

epoch:1250/50, training loss:0.9829189777374268
Train Acc 0.7061
 Acc 0.6938
ogbn-arxiv,dgl,1,1249,16.9988,0.6938

epoch:1251/50, training loss:0.9828088283538818
Train Acc 0.7062
 Acc 0.6938
ogbn-arxiv,dgl,1,1250,17.0112,0.6938

epoch:1252/50, training loss:0.9826992750167847
Train Acc 0.7063
 Acc 0.6933
ogbn-arxiv,dgl,1,1251,17.0238,0.6933

epoch:1253/50, training loss:0.9825921058654785
Train Acc 0.7062
 Acc 0.6939
ogbn-arxiv,dgl,1,1252,17.0363,0.6939

epoch:1254/50, training loss:0.9824830889701843
Train Acc 0.7063
 Acc 0.6938
ogbn-arxiv,dgl,1,1253,17.0489,0.6938

epoch:1255/50, training loss:0.9823731184005737
Train Acc 0.7064
 Acc 0.6934
ogbn-arxiv,dgl,1,1254,17.0613,0.6934

epoch:1256/50, training loss:0.9822647571563721
Train Acc 0.7062
 Acc 0.6940
new best val f1: 0.6939586809203073
ogbn-arxiv,dgl,1,1255,17.0738,0.6940

epoch:1257/50, training loss:0.982154905796051
Train Acc 0.7064
 Acc 0.6937
ogbn-arxiv,dgl,1,1256,17.0862,0.6937

epoch:1258/50, training loss:0.9820438027381897
Train Acc 0.7064
 Acc 0.6938
ogbn-arxiv,dgl,1,1257,17.0988,0.6938

epoch:1259/50, training loss:0.9819393754005432
Train Acc 0.7063
 Acc 0.6938
ogbn-arxiv,dgl,1,1258,17.1112,0.6938

epoch:1260/50, training loss:0.9818277359008789
Train Acc 0.7064
 Acc 0.6938
ogbn-arxiv,dgl,1,1259,17.1238,0.6938

epoch:1261/50, training loss:0.9817207455635071
Train Acc 0.7064
 Acc 0.6936
ogbn-arxiv,dgl,1,1260,17.1362,0.6936

epoch:1262/50, training loss:0.9816129803657532
Train Acc 0.7063
 Acc 0.6939
ogbn-arxiv,dgl,1,1261,17.1488,0.6939

epoch:1263/50, training loss:0.9815062880516052
Train Acc 0.7064
 Acc 0.6939
ogbn-arxiv,dgl,1,1262,17.1612,0.6939

epoch:1264/50, training loss:0.9813969135284424
Train Acc 0.7064
 Acc 0.6936
ogbn-arxiv,dgl,1,1263,17.1738,0.6936

epoch:1265/50, training loss:0.9812890291213989
Train Acc 0.7063
 Acc 0.6939
ogbn-arxiv,dgl,1,1264,17.1862,0.6939

epoch:1266/50, training loss:0.9811822772026062
Train Acc 0.7064
 Acc 0.6938
ogbn-arxiv,dgl,1,1265,17.1987,0.6938

epoch:1267/50, training loss:0.9810751080513
Train Acc 0.7063
 Acc 0.6939
ogbn-arxiv,dgl,1,1266,17.2111,0.6939

epoch:1268/50, training loss:0.9809660911560059
Train Acc 0.7064
 Acc 0.6941
new best val f1: 0.694144060639766
ogbn-arxiv,dgl,1,1267,17.2237,0.6941

epoch:1269/50, training loss:0.9808588027954102
Train Acc 0.7065
 Acc 0.6937
ogbn-arxiv,dgl,1,1268,17.2362,0.6937

epoch:1270/50, training loss:0.9807525277137756
Train Acc 0.7064
 Acc 0.6941
ogbn-arxiv,dgl,1,1269,17.2488,0.6941

epoch:1271/50, training loss:0.9806433320045471
Train Acc 0.7066
 Acc 0.6941
ogbn-arxiv,dgl,1,1270,17.2611,0.6941

epoch:1272/50, training loss:0.980536937713623
Train Acc 0.7065
 Acc 0.6939
ogbn-arxiv,dgl,1,1271,17.2737,0.6939

epoch:1273/50, training loss:0.9804311394691467
Train Acc 0.7065
 Acc 0.6940
ogbn-arxiv,dgl,1,1272,17.2862,0.6940

epoch:1274/50, training loss:0.9803216457366943
Train Acc 0.7066
 Acc 0.6941
ogbn-arxiv,dgl,1,1273,17.2988,0.6941

epoch:1275/50, training loss:0.980217456817627
Train Acc 0.7066
 Acc 0.6938
ogbn-arxiv,dgl,1,1274,17.3112,0.6938

epoch:1276/50, training loss:0.9801096320152283
Train Acc 0.7064
 Acc 0.6940
ogbn-arxiv,dgl,1,1275,17.3238,0.6940

epoch:1277/50, training loss:0.9800043702125549
Train Acc 0.7065
 Acc 0.6939
ogbn-arxiv,dgl,1,1276,17.3362,0.6939

epoch:1278/50, training loss:0.9798957705497742
Train Acc 0.7065
 Acc 0.6940
ogbn-arxiv,dgl,1,1277,17.3488,0.6940

epoch:1279/50, training loss:0.9797898530960083
Train Acc 0.7066
 Acc 0.6940
ogbn-arxiv,dgl,1,1278,17.3612,0.6940

epoch:1280/50, training loss:0.979684054851532
Train Acc 0.7065
 Acc 0.6939
ogbn-arxiv,dgl,1,1279,17.3738,0.6939

epoch:1281/50, training loss:0.9795748591423035
Train Acc 0.7065
 Acc 0.6944
new best val f1: 0.6943500381058312
ogbn-arxiv,dgl,1,1280,17.3863,0.6944

epoch:1282/50, training loss:0.9794725179672241
Train Acc 0.7068
 Acc 0.6938
ogbn-arxiv,dgl,1,1281,17.3989,0.6938

epoch:1283/50, training loss:0.979366660118103
Train Acc 0.7066
 Acc 0.6941
ogbn-arxiv,dgl,1,1282,17.4114,0.6941

epoch:1284/50, training loss:0.9792596697807312
Train Acc 0.7066
 Acc 0.6942
ogbn-arxiv,dgl,1,1283,17.4239,0.6942

epoch:1285/50, training loss:0.9791539907455444
Train Acc 0.7067
 Acc 0.6943
ogbn-arxiv,dgl,1,1284,17.4364,0.6943

epoch:1286/50, training loss:0.9790489077568054
Train Acc 0.7067
 Acc 0.6938
ogbn-arxiv,dgl,1,1285,17.4489,0.6938

epoch:1287/50, training loss:0.9789451360702515
Train Acc 0.7066
 Acc 0.6943
ogbn-arxiv,dgl,1,1286,17.4613,0.6943

epoch:1288/50, training loss:0.9788374900817871
Train Acc 0.7068
 Acc 0.6945
new best val f1: 0.6944942223320769
ogbn-arxiv,dgl,1,1287,17.4738,0.6945

epoch:1289/50, training loss:0.9787325263023376
Train Acc 0.7068
 Acc 0.6938
ogbn-arxiv,dgl,1,1288,17.4862,0.6938

epoch:1290/50, training loss:0.9786291718482971
Train Acc 0.7066
 Acc 0.6945
ogbn-arxiv,dgl,1,1289,17.4988,0.6945

epoch:1291/50, training loss:0.9785242676734924
Train Acc 0.7068
 Acc 0.6943
ogbn-arxiv,dgl,1,1290,17.5111,0.6943

epoch:1292/50, training loss:0.9784173369407654
Train Acc 0.7068
 Acc 0.6942
ogbn-arxiv,dgl,1,1291,17.5237,0.6942

epoch:1293/50, training loss:0.9783135652542114
Train Acc 0.7068
 Acc 0.6943
ogbn-arxiv,dgl,1,1292,17.5366,0.6943

epoch:1294/50, training loss:0.9782097935676575
Train Acc 0.7068
 Acc 0.6944
ogbn-arxiv,dgl,1,1293,17.5506,0.6944

epoch:1295/50, training loss:0.9781031012535095
Train Acc 0.7068
 Acc 0.6942
ogbn-arxiv,dgl,1,1294,17.5631,0.6942

epoch:1296/50, training loss:0.9780002236366272
Train Acc 0.7068
 Acc 0.6943
ogbn-arxiv,dgl,1,1295,17.5756,0.6943

epoch:1297/50, training loss:0.9778954386711121
Train Acc 0.7069
 Acc 0.6944
ogbn-arxiv,dgl,1,1296,17.5881,0.6944

epoch:1298/50, training loss:0.977791428565979
Train Acc 0.7068
 Acc 0.6944
ogbn-arxiv,dgl,1,1297,17.6006,0.6944

epoch:1299/50, training loss:0.9776865839958191
Train Acc 0.7069
 Acc 0.6944
ogbn-arxiv,dgl,1,1298,17.6130,0.6944

epoch:1300/50, training loss:0.9775838255882263
Train Acc 0.7069
 Acc 0.6943
ogbn-arxiv,dgl,1,1299,17.6256,0.6943

epoch:1301/50, training loss:0.9774788022041321
Train Acc 0.7068
 Acc 0.6942
ogbn-arxiv,dgl,1,1300,17.6380,0.6942

epoch:1302/50, training loss:0.9773761034011841
Train Acc 0.7069
 Acc 0.6948
new best val f1: 0.6948237862777812
ogbn-arxiv,dgl,1,1301,17.6506,0.6948

epoch:1303/50, training loss:0.9772740006446838
Train Acc 0.7070
 Acc 0.6945
ogbn-arxiv,dgl,1,1302,17.6630,0.6945

epoch:1304/50, training loss:0.9771679639816284
Train Acc 0.7070
 Acc 0.6942
ogbn-arxiv,dgl,1,1303,17.6755,0.6942

epoch:1305/50, training loss:0.9770669341087341
Train Acc 0.7069
 Acc 0.6950
new best val f1: 0.6949885682506334
ogbn-arxiv,dgl,1,1304,17.6879,0.6950

epoch:1306/50, training loss:0.9769662022590637
Train Acc 0.7071
 Acc 0.6943
ogbn-arxiv,dgl,1,1305,17.7005,0.6943

epoch:1307/50, training loss:0.9768591523170471
Train Acc 0.7070
 Acc 0.6942
ogbn-arxiv,dgl,1,1306,17.7129,0.6942

epoch:1308/50, training loss:0.9767572283744812
Train Acc 0.7070
 Acc 0.6946
ogbn-arxiv,dgl,1,1307,17.7254,0.6946

epoch:1309/50, training loss:0.9766533970832825
Train Acc 0.7070
 Acc 0.6943
ogbn-arxiv,dgl,1,1308,17.7378,0.6943

epoch:1310/50, training loss:0.9765511751174927
Train Acc 0.7070
 Acc 0.6943
ogbn-arxiv,dgl,1,1309,17.7503,0.6943

epoch:1311/50, training loss:0.9764482378959656
Train Acc 0.7070
 Acc 0.6945
ogbn-arxiv,dgl,1,1310,17.7628,0.6945

epoch:1312/50, training loss:0.9763463735580444
Train Acc 0.7071
 Acc 0.6941
ogbn-arxiv,dgl,1,1311,17.7753,0.6941

epoch:1313/50, training loss:0.9762430787086487
Train Acc 0.7070
 Acc 0.6943
ogbn-arxiv,dgl,1,1312,17.7878,0.6943

epoch:1314/50, training loss:0.9761420488357544
Train Acc 0.7070
 Acc 0.6945
ogbn-arxiv,dgl,1,1313,17.8003,0.6945

epoch:1315/50, training loss:0.9760401248931885
Train Acc 0.7072
 Acc 0.6941
ogbn-arxiv,dgl,1,1314,17.8127,0.6941

epoch:1316/50, training loss:0.9759378433227539
Train Acc 0.7070
 Acc 0.6943
ogbn-arxiv,dgl,1,1315,17.8252,0.6943

epoch:1317/50, training loss:0.9758357405662537
Train Acc 0.7071
 Acc 0.6945
ogbn-arxiv,dgl,1,1316,17.8376,0.6945

epoch:1318/50, training loss:0.9757339954376221
Train Acc 0.7072
 Acc 0.6941
ogbn-arxiv,dgl,1,1317,17.8502,0.6941

epoch:1319/50, training loss:0.9756330847740173
Train Acc 0.7069
 Acc 0.6946
ogbn-arxiv,dgl,1,1318,17.8626,0.6946

epoch:1320/50, training loss:0.9755337834358215
Train Acc 0.7072
 Acc 0.6945
ogbn-arxiv,dgl,1,1319,17.8752,0.6945

epoch:1321/50, training loss:0.9754300117492676
Train Acc 0.7072
 Acc 0.6940
ogbn-arxiv,dgl,1,1320,17.8876,0.6940

epoch:1322/50, training loss:0.9753316044807434
Train Acc 0.7070
 Acc 0.6947
ogbn-arxiv,dgl,1,1321,17.9002,0.6947

epoch:1323/50, training loss:0.9752307534217834
Train Acc 0.7072
 Acc 0.6943
ogbn-arxiv,dgl,1,1322,17.9126,0.6943

epoch:1324/50, training loss:0.9751256704330444
Train Acc 0.7071
 Acc 0.6942
ogbn-arxiv,dgl,1,1323,17.9252,0.6942

epoch:1325/50, training loss:0.9750292301177979
Train Acc 0.7071
 Acc 0.6946
ogbn-arxiv,dgl,1,1324,17.9376,0.6946

epoch:1326/50, training loss:0.9749260544776917
Train Acc 0.7073
 Acc 0.6944
ogbn-arxiv,dgl,1,1325,17.9501,0.6944

epoch:1327/50, training loss:0.9748265743255615
Train Acc 0.7073
 Acc 0.6942
ogbn-arxiv,dgl,1,1326,17.9625,0.6942

epoch:1328/50, training loss:0.9747269153594971
Train Acc 0.7071
 Acc 0.6947
ogbn-arxiv,dgl,1,1327,17.9751,0.6947

epoch:1329/50, training loss:0.9746255278587341
Train Acc 0.7074
 Acc 0.6946
ogbn-arxiv,dgl,1,1328,17.9876,0.6946

epoch:1330/50, training loss:0.9745234847068787
Train Acc 0.7073
 Acc 0.6941
ogbn-arxiv,dgl,1,1329,18.0000,0.6941

epoch:1331/50, training loss:0.974424421787262
Train Acc 0.7071
 Acc 0.6947
ogbn-arxiv,dgl,1,1330,18.0125,0.6947

epoch:1332/50, training loss:0.9743245840072632
Train Acc 0.7074
 Acc 0.6944
ogbn-arxiv,dgl,1,1331,18.0251,0.6944

epoch:1333/50, training loss:0.9742234349250793
Train Acc 0.7073
 Acc 0.6944
ogbn-arxiv,dgl,1,1332,18.0375,0.6944

epoch:1334/50, training loss:0.9741232395172119
Train Acc 0.7073
 Acc 0.6946
ogbn-arxiv,dgl,1,1333,18.0501,0.6946

epoch:1335/50, training loss:0.9740266799926758
Train Acc 0.7074
 Acc 0.6945
ogbn-arxiv,dgl,1,1334,18.0625,0.6945

epoch:1336/50, training loss:0.9739250540733337
Train Acc 0.7074
 Acc 0.6945
ogbn-arxiv,dgl,1,1335,18.0752,0.6945

epoch:1337/50, training loss:0.9738283157348633
Train Acc 0.7073
 Acc 0.6944
ogbn-arxiv,dgl,1,1336,18.0876,0.6944

epoch:1338/50, training loss:0.9737244844436646
Train Acc 0.7073
 Acc 0.6947
ogbn-arxiv,dgl,1,1337,18.1001,0.6947

epoch:1339/50, training loss:0.9736279249191284
Train Acc 0.7074
 Acc 0.6946
ogbn-arxiv,dgl,1,1338,18.1125,0.6946

epoch:1340/50, training loss:0.9735276699066162
Train Acc 0.7074
 Acc 0.6948
ogbn-arxiv,dgl,1,1339,18.1250,0.6948

epoch:1341/50, training loss:0.9734290242195129
Train Acc 0.7075
 Acc 0.6945
ogbn-arxiv,dgl,1,1340,18.1374,0.6945

epoch:1342/50, training loss:0.9733284115791321
Train Acc 0.7074
 Acc 0.6945
ogbn-arxiv,dgl,1,1341,18.1499,0.6945

epoch:1343/50, training loss:0.9732303023338318
Train Acc 0.7074
 Acc 0.6946
ogbn-arxiv,dgl,1,1342,18.1624,0.6946

epoch:1344/50, training loss:0.973132312297821
Train Acc 0.7075
 Acc 0.6948
ogbn-arxiv,dgl,1,1343,18.1749,0.6948

epoch:1345/50, training loss:0.9730330109596252
Train Acc 0.7075
 Acc 0.6947
ogbn-arxiv,dgl,1,1344,18.1874,0.6947

epoch:1346/50, training loss:0.972934901714325
Train Acc 0.7075
 Acc 0.6946
ogbn-arxiv,dgl,1,1345,18.1999,0.6946

epoch:1347/50, training loss:0.9728378057479858
Train Acc 0.7075
 Acc 0.6946
ogbn-arxiv,dgl,1,1346,18.2124,0.6946

epoch:1348/50, training loss:0.9727376699447632
Train Acc 0.7075
 Acc 0.6947
ogbn-arxiv,dgl,1,1347,18.2250,0.6947

epoch:1349/50, training loss:0.9726393818855286
Train Acc 0.7076
 Acc 0.6946
ogbn-arxiv,dgl,1,1348,18.2375,0.6946

epoch:1350/50, training loss:0.9725409150123596
Train Acc 0.7075
 Acc 0.6947
ogbn-arxiv,dgl,1,1349,18.2500,0.6947

epoch:1351/50, training loss:0.9724429249763489
Train Acc 0.7076
 Acc 0.6948
ogbn-arxiv,dgl,1,1350,18.2624,0.6948

epoch:1352/50, training loss:0.9723443388938904
Train Acc 0.7076
 Acc 0.6946
ogbn-arxiv,dgl,1,1351,18.2750,0.6946

epoch:1353/50, training loss:0.9722461104393005
Train Acc 0.7075
 Acc 0.6947
ogbn-arxiv,dgl,1,1352,18.2874,0.6947

epoch:1354/50, training loss:0.9721468687057495
Train Acc 0.7076
 Acc 0.6948
ogbn-arxiv,dgl,1,1353,18.2999,0.6948

epoch:1355/50, training loss:0.9720491170883179
Train Acc 0.7076
 Acc 0.6946
ogbn-arxiv,dgl,1,1354,18.3123,0.6946

epoch:1356/50, training loss:0.9719527363777161
Train Acc 0.7076
 Acc 0.6949
ogbn-arxiv,dgl,1,1355,18.3249,0.6949

epoch:1357/50, training loss:0.9718544483184814
Train Acc 0.7076
 Acc 0.6949
ogbn-arxiv,dgl,1,1356,18.3372,0.6949

epoch:1358/50, training loss:0.9717561602592468
Train Acc 0.7077
 Acc 0.6945
ogbn-arxiv,dgl,1,1357,18.3499,0.6945

epoch:1359/50, training loss:0.9716625809669495
Train Acc 0.7075
 Acc 0.6952
new best val f1: 0.6951533502234856
ogbn-arxiv,dgl,1,1358,18.3623,0.6952

epoch:1360/50, training loss:0.971563994884491
Train Acc 0.7078
 Acc 0.6947
ogbn-arxiv,dgl,1,1359,18.3748,0.6947

epoch:1361/50, training loss:0.9714657068252563
Train Acc 0.7076
 Acc 0.6949
ogbn-arxiv,dgl,1,1360,18.3872,0.6949

epoch:1362/50, training loss:0.9713668823242188
Train Acc 0.7077
 Acc 0.6949
ogbn-arxiv,dgl,1,1361,18.3998,0.6949

epoch:1363/50, training loss:0.9712702035903931
Train Acc 0.7077
 Acc 0.6949
ogbn-arxiv,dgl,1,1362,18.4123,0.6949

epoch:1364/50, training loss:0.9711736440658569
Train Acc 0.7077
 Acc 0.6948
ogbn-arxiv,dgl,1,1363,18.4248,0.6948

epoch:1365/50, training loss:0.9710770845413208
Train Acc 0.7077
 Acc 0.6951
ogbn-arxiv,dgl,1,1364,18.4372,0.6951

epoch:1366/50, training loss:0.9709789752960205
Train Acc 0.7078
 Acc 0.6949
ogbn-arxiv,dgl,1,1365,18.4498,0.6949

epoch:1367/50, training loss:0.9708815217018127
Train Acc 0.7078
 Acc 0.6949
ogbn-arxiv,dgl,1,1366,18.4622,0.6949

epoch:1368/50, training loss:0.9707859754562378
Train Acc 0.7077
 Acc 0.6950
ogbn-arxiv,dgl,1,1367,18.4749,0.6950

epoch:1369/50, training loss:0.9706895351409912
Train Acc 0.7078
 Acc 0.6952
ogbn-arxiv,dgl,1,1368,18.4873,0.6952

epoch:1370/50, training loss:0.9705935120582581
Train Acc 0.7079
 Acc 0.6950
ogbn-arxiv,dgl,1,1369,18.4999,0.6950

epoch:1371/50, training loss:0.9704970121383667
Train Acc 0.7078
 Acc 0.6950
ogbn-arxiv,dgl,1,1370,18.5123,0.6950

epoch:1372/50, training loss:0.9704008102416992
Train Acc 0.7078
 Acc 0.6951
ogbn-arxiv,dgl,1,1371,18.5249,0.6951

epoch:1373/50, training loss:0.9703036546707153
Train Acc 0.7079
 Acc 0.6952
new best val f1: 0.695173947970092
ogbn-arxiv,dgl,1,1372,18.5388,0.6952

epoch:1374/50, training loss:0.9702069163322449
Train Acc 0.7079
 Acc 0.6948
ogbn-arxiv,dgl,1,1373,18.5514,0.6948

epoch:1375/50, training loss:0.9701113700866699
Train Acc 0.7078
 Acc 0.6954
new best val f1: 0.6954211209293704
ogbn-arxiv,dgl,1,1374,18.5638,0.6954

epoch:1376/50, training loss:0.9700144529342651
Train Acc 0.7080
 Acc 0.6953
ogbn-arxiv,dgl,1,1375,18.5764,0.6953

epoch:1377/50, training loss:0.9699188470840454
Train Acc 0.7080
 Acc 0.6951
ogbn-arxiv,dgl,1,1376,18.5888,0.6951

epoch:1378/50, training loss:0.9698224663734436
Train Acc 0.7079
 Acc 0.6955
new best val f1: 0.6955447074090094
ogbn-arxiv,dgl,1,1377,18.6014,0.6955

epoch:1379/50, training loss:0.9697281122207642
Train Acc 0.7080
 Acc 0.6953
ogbn-arxiv,dgl,1,1378,18.6138,0.6953

epoch:1380/50, training loss:0.9696322083473206
Train Acc 0.7080
 Acc 0.6950
ogbn-arxiv,dgl,1,1379,18.6264,0.6950

epoch:1381/50, training loss:0.9695360660552979
Train Acc 0.7079
 Acc 0.6953
ogbn-arxiv,dgl,1,1380,18.6387,0.6953

epoch:1382/50, training loss:0.9694395065307617
Train Acc 0.7080
 Acc 0.6954
ogbn-arxiv,dgl,1,1381,18.6513,0.6954

epoch:1383/50, training loss:0.9693455696105957
Train Acc 0.7080
 Acc 0.6952
ogbn-arxiv,dgl,1,1382,18.6638,0.6952

epoch:1384/50, training loss:0.9692519307136536
Train Acc 0.7079
 Acc 0.6955
ogbn-arxiv,dgl,1,1383,18.6763,0.6955

epoch:1385/50, training loss:0.9691581726074219
Train Acc 0.7080
 Acc 0.6953
ogbn-arxiv,dgl,1,1384,18.6887,0.6953

epoch:1386/50, training loss:0.9690614938735962
Train Acc 0.7080
 Acc 0.6953
ogbn-arxiv,dgl,1,1385,18.7012,0.6953

epoch:1387/50, training loss:0.9689672589302063
Train Acc 0.7080
 Acc 0.6957
new best val f1: 0.6957300871284682
ogbn-arxiv,dgl,1,1386,18.7135,0.6957

epoch:1388/50, training loss:0.9688773155212402
Train Acc 0.7082
 Acc 0.6949
ogbn-arxiv,dgl,1,1387,18.7261,0.6949

epoch:1389/50, training loss:0.9687852263450623
Train Acc 0.7079
 Acc 0.6954
ogbn-arxiv,dgl,1,1388,18.7384,0.6954

epoch:1390/50, training loss:0.9686828255653381
Train Acc 0.7081
 Acc 0.6955
ogbn-arxiv,dgl,1,1389,18.7509,0.6955

epoch:1391/50, training loss:0.9685905575752258
Train Acc 0.7082
 Acc 0.6950
ogbn-arxiv,dgl,1,1390,18.7633,0.6950

epoch:1392/50, training loss:0.9684998393058777
Train Acc 0.7079
 Acc 0.6954
ogbn-arxiv,dgl,1,1391,18.7758,0.6954

epoch:1393/50, training loss:0.9684023261070251
Train Acc 0.7081
 Acc 0.6956
ogbn-arxiv,dgl,1,1392,18.7882,0.6956

epoch:1394/50, training loss:0.9683080911636353
Train Acc 0.7083
 Acc 0.6950
ogbn-arxiv,dgl,1,1393,18.8007,0.6950

epoch:1395/50, training loss:0.9682183861732483
Train Acc 0.7080
 Acc 0.6954
ogbn-arxiv,dgl,1,1394,18.8131,0.6954

epoch:1396/50, training loss:0.9681221842765808
Train Acc 0.7081
 Acc 0.6955
ogbn-arxiv,dgl,1,1395,18.8256,0.6955

epoch:1397/50, training loss:0.9680275917053223
Train Acc 0.7082
 Acc 0.6950
ogbn-arxiv,dgl,1,1396,18.8380,0.6950

epoch:1398/50, training loss:0.9679417610168457
Train Acc 0.7080
 Acc 0.6955
ogbn-arxiv,dgl,1,1397,18.8506,0.6955

epoch:1399/50, training loss:0.9678399562835693
Train Acc 0.7082
 Acc 0.6954
ogbn-arxiv,dgl,1,1398,18.8629,0.6954

epoch:1400/50, training loss:0.9677495956420898
Train Acc 0.7082
 Acc 0.6949
ogbn-arxiv,dgl,1,1399,18.8755,0.6949

epoch:1401/50, training loss:0.9676580429077148
Train Acc 0.7079
 Acc 0.6956
ogbn-arxiv,dgl,1,1400,18.8879,0.6956

epoch:1402/50, training loss:0.967566967010498
Train Acc 0.7083
 Acc 0.6954
ogbn-arxiv,dgl,1,1401,18.9005,0.6954

epoch:1403/50, training loss:0.9674685001373291
Train Acc 0.7082
 Acc 0.6952
ogbn-arxiv,dgl,1,1402,18.9128,0.6952

epoch:1404/50, training loss:0.9673782587051392
Train Acc 0.7080
 Acc 0.6955
ogbn-arxiv,dgl,1,1403,18.9253,0.6955

epoch:1405/50, training loss:0.9672868251800537
Train Acc 0.7082
 Acc 0.6954
ogbn-arxiv,dgl,1,1404,18.9378,0.6954

epoch:1406/50, training loss:0.9671913981437683
Train Acc 0.7081
 Acc 0.6949
ogbn-arxiv,dgl,1,1405,18.9504,0.6949

epoch:1407/50, training loss:0.9671027064323425
Train Acc 0.7080
 Acc 0.6957
ogbn-arxiv,dgl,1,1406,18.9642,0.6957

epoch:1408/50, training loss:0.9670121073722839
Train Acc 0.7083
 Acc 0.6953
ogbn-arxiv,dgl,1,1407,18.9767,0.6953

epoch:1409/50, training loss:0.9669176936149597
Train Acc 0.7081
 Acc 0.6954
ogbn-arxiv,dgl,1,1408,18.9897,0.6954

epoch:1410/50, training loss:0.9668231010437012
Train Acc 0.7082
 Acc 0.6954
ogbn-arxiv,dgl,1,1409,19.0022,0.6954

epoch:1411/50, training loss:0.9667384624481201
Train Acc 0.7084
 Acc 0.6950
ogbn-arxiv,dgl,1,1410,19.0146,0.6950

epoch:1412/50, training loss:0.9666428565979004
Train Acc 0.7080
 Acc 0.6953
ogbn-arxiv,dgl,1,1411,19.0271,0.6953

epoch:1413/50, training loss:0.9665480852127075
Train Acc 0.7082
 Acc 0.6955
ogbn-arxiv,dgl,1,1412,19.0395,0.6955

epoch:1414/50, training loss:0.9664599895477295
Train Acc 0.7084
 Acc 0.6952
ogbn-arxiv,dgl,1,1413,19.0521,0.6952

epoch:1415/50, training loss:0.9663681387901306
Train Acc 0.7082
 Acc 0.6954
ogbn-arxiv,dgl,1,1414,19.0645,0.6954

epoch:1416/50, training loss:0.9662739634513855
Train Acc 0.7083
 Acc 0.6955
ogbn-arxiv,dgl,1,1415,19.0770,0.6955

epoch:1417/50, training loss:0.9661858081817627
Train Acc 0.7084
 Acc 0.6953
ogbn-arxiv,dgl,1,1416,19.0893,0.6953

epoch:1418/50, training loss:0.966094970703125
Train Acc 0.7082
 Acc 0.6955
ogbn-arxiv,dgl,1,1417,19.1018,0.6955

epoch:1419/50, training loss:0.966002345085144
Train Acc 0.7084
 Acc 0.6955
ogbn-arxiv,dgl,1,1418,19.1142,0.6955

epoch:1420/50, training loss:0.9659110307693481
Train Acc 0.7084
 Acc 0.6953
ogbn-arxiv,dgl,1,1419,19.1268,0.6953

epoch:1421/50, training loss:0.9658212661743164
Train Acc 0.7083
 Acc 0.6955
ogbn-arxiv,dgl,1,1420,19.1391,0.6955

epoch:1422/50, training loss:0.9657331705093384
Train Acc 0.7084
 Acc 0.6957
ogbn-arxiv,dgl,1,1421,19.1516,0.6957

epoch:1423/50, training loss:0.9656373262405396
Train Acc 0.7085
 Acc 0.6953
ogbn-arxiv,dgl,1,1422,19.1640,0.6953

epoch:1424/50, training loss:0.9655516743659973
Train Acc 0.7083
 Acc 0.6957
ogbn-arxiv,dgl,1,1423,19.1766,0.6957

epoch:1425/50, training loss:0.9654596447944641
Train Acc 0.7086
 Acc 0.6955
ogbn-arxiv,dgl,1,1424,19.1889,0.6955

epoch:1426/50, training loss:0.965369701385498
Train Acc 0.7085
 Acc 0.6954
ogbn-arxiv,dgl,1,1425,19.2014,0.6954

epoch:1427/50, training loss:0.965279757976532
Train Acc 0.7084
 Acc 0.6959
new best val f1: 0.6958742713547138
ogbn-arxiv,dgl,1,1426,19.2138,0.6959

epoch:1428/50, training loss:0.9651963710784912
Train Acc 0.7086
 Acc 0.6955
ogbn-arxiv,dgl,1,1427,19.2264,0.6955

epoch:1429/50, training loss:0.9650969505310059
Train Acc 0.7084
 Acc 0.6950
ogbn-arxiv,dgl,1,1428,19.2387,0.6950

epoch:1430/50, training loss:0.9650145769119263
Train Acc 0.7084
 Acc 0.6959
new best val f1: 0.6958948691013204
ogbn-arxiv,dgl,1,1429,19.2512,0.6959

epoch:1431/50, training loss:0.9649255871772766
Train Acc 0.7086
 Acc 0.6955
ogbn-arxiv,dgl,1,1430,19.2636,0.6955

epoch:1432/50, training loss:0.9648292064666748
Train Acc 0.7086
 Acc 0.6951
ogbn-arxiv,dgl,1,1431,19.2762,0.6951

epoch:1433/50, training loss:0.9647420644760132
Train Acc 0.7085
 Acc 0.6957
ogbn-arxiv,dgl,1,1432,19.2885,0.6957

epoch:1434/50, training loss:0.9646613001823425
Train Acc 0.7088
 Acc 0.6956
ogbn-arxiv,dgl,1,1433,19.3009,0.6956

epoch:1435/50, training loss:0.9645628929138184
Train Acc 0.7087
 Acc 0.6956
ogbn-arxiv,dgl,1,1434,19.3133,0.6956

epoch:1436/50, training loss:0.9644755721092224
Train Acc 0.7086
 Acc 0.6956
ogbn-arxiv,dgl,1,1435,19.3258,0.6956

epoch:1437/50, training loss:0.964388370513916
Train Acc 0.7087
 Acc 0.6954
ogbn-arxiv,dgl,1,1436,19.3382,0.6954

epoch:1438/50, training loss:0.9642948508262634
Train Acc 0.7087
 Acc 0.6956
ogbn-arxiv,dgl,1,1437,19.3507,0.6956

epoch:1439/50, training loss:0.9642065167427063
Train Acc 0.7088
 Acc 0.6958
ogbn-arxiv,dgl,1,1438,19.3632,0.6958

epoch:1440/50, training loss:0.9641194939613342
Train Acc 0.7087
 Acc 0.6955
ogbn-arxiv,dgl,1,1439,19.3757,0.6955

epoch:1441/50, training loss:0.964029848575592
Train Acc 0.7088
 Acc 0.6954
ogbn-arxiv,dgl,1,1440,19.3881,0.6954

epoch:1442/50, training loss:0.9639434814453125
Train Acc 0.7088
 Acc 0.6958
ogbn-arxiv,dgl,1,1441,19.4006,0.6958

epoch:1443/50, training loss:0.9638527631759644
Train Acc 0.7088
 Acc 0.6954
ogbn-arxiv,dgl,1,1442,19.4130,0.6954

epoch:1444/50, training loss:0.9637667536735535
Train Acc 0.7087
 Acc 0.6955
ogbn-arxiv,dgl,1,1443,19.4255,0.6955

epoch:1445/50, training loss:0.9636784791946411
Train Acc 0.7087
 Acc 0.6958
ogbn-arxiv,dgl,1,1444,19.4379,0.6958

epoch:1446/50, training loss:0.9635885953903198
Train Acc 0.7089
 Acc 0.6955
ogbn-arxiv,dgl,1,1445,19.4505,0.6955

epoch:1447/50, training loss:0.9635006189346313
Train Acc 0.7088
 Acc 0.6956
ogbn-arxiv,dgl,1,1446,19.4630,0.6956

epoch:1448/50, training loss:0.9634113907814026
Train Acc 0.7088
 Acc 0.6954
ogbn-arxiv,dgl,1,1447,19.4755,0.6954

epoch:1449/50, training loss:0.9633243680000305
Train Acc 0.7088
 Acc 0.6956
ogbn-arxiv,dgl,1,1448,19.4878,0.6956

epoch:1450/50, training loss:0.963239312171936
Train Acc 0.7088
 Acc 0.6959
ogbn-arxiv,dgl,1,1449,19.5003,0.6959

epoch:1451/50, training loss:0.9631475806236267
Train Acc 0.7089
 Acc 0.6957
ogbn-arxiv,dgl,1,1450,19.5126,0.6957

epoch:1452/50, training loss:0.9630607962608337
Train Acc 0.7089
 Acc 0.6956
ogbn-arxiv,dgl,1,1451,19.5252,0.6956

epoch:1453/50, training loss:0.9629720449447632
Train Acc 0.7090
 Acc 0.6954
ogbn-arxiv,dgl,1,1452,19.5376,0.6954

epoch:1454/50, training loss:0.9628875255584717
Train Acc 0.7089
 Acc 0.6956
ogbn-arxiv,dgl,1,1453,19.5500,0.6956

epoch:1455/50, training loss:0.9628007411956787
Train Acc 0.7090
 Acc 0.6956
ogbn-arxiv,dgl,1,1454,19.5624,0.6956

epoch:1456/50, training loss:0.9627121090888977
Train Acc 0.7089
 Acc 0.6956
ogbn-arxiv,dgl,1,1455,19.5749,0.6956

epoch:1457/50, training loss:0.9626266956329346
Train Acc 0.7090
 Acc 0.6957
ogbn-arxiv,dgl,1,1456,19.5873,0.6957

epoch:1458/50, training loss:0.9625377655029297
Train Acc 0.7091
 Acc 0.6956
ogbn-arxiv,dgl,1,1457,19.5998,0.6956

epoch:1459/50, training loss:0.9624481797218323
Train Acc 0.7090
 Acc 0.6956
ogbn-arxiv,dgl,1,1458,19.6122,0.6956

epoch:1460/50, training loss:0.9623634815216064
Train Acc 0.7090
 Acc 0.6955
ogbn-arxiv,dgl,1,1459,19.6248,0.6955

epoch:1461/50, training loss:0.9622754454612732
Train Acc 0.7090
 Acc 0.6957
ogbn-arxiv,dgl,1,1460,19.6372,0.6957

epoch:1462/50, training loss:0.9621910452842712
Train Acc 0.7092
 Acc 0.6957
ogbn-arxiv,dgl,1,1461,19.6496,0.6957

epoch:1463/50, training loss:0.9621015191078186
Train Acc 0.7091
 Acc 0.6957
ogbn-arxiv,dgl,1,1462,19.6619,0.6957

epoch:1464/50, training loss:0.9620158076286316
Train Acc 0.7092
 Acc 0.6955
ogbn-arxiv,dgl,1,1463,19.6745,0.6955

epoch:1465/50, training loss:0.961929202079773
Train Acc 0.7091
 Acc 0.6959
ogbn-arxiv,dgl,1,1464,19.6868,0.6959

epoch:1466/50, training loss:0.961844265460968
Train Acc 0.7091
 Acc 0.6954
ogbn-arxiv,dgl,1,1465,19.6994,0.6954

epoch:1467/50, training loss:0.9617608189582825
Train Acc 0.7090
 Acc 0.6957
ogbn-arxiv,dgl,1,1466,19.7118,0.6957

epoch:1468/50, training loss:0.9616702198982239
Train Acc 0.7091
 Acc 0.6959
ogbn-arxiv,dgl,1,1467,19.7244,0.6959

epoch:1469/50, training loss:0.9615878462791443
Train Acc 0.7092
 Acc 0.6954
ogbn-arxiv,dgl,1,1468,19.7367,0.6954

epoch:1470/50, training loss:0.9615008234977722
Train Acc 0.7091
 Acc 0.6958
ogbn-arxiv,dgl,1,1469,19.7491,0.6958

epoch:1471/50, training loss:0.9614125490188599
Train Acc 0.7092
 Acc 0.6959
ogbn-arxiv,dgl,1,1470,19.7614,0.6959

epoch:1472/50, training loss:0.9613301157951355
Train Acc 0.7092
 Acc 0.6956
ogbn-arxiv,dgl,1,1471,19.7740,0.6956

epoch:1473/50, training loss:0.9612395763397217
Train Acc 0.7092
 Acc 0.6956
ogbn-arxiv,dgl,1,1472,19.7863,0.6956

epoch:1474/50, training loss:0.9611572623252869
Train Acc 0.7092
 Acc 0.6960
new best val f1: 0.6959566623411398
ogbn-arxiv,dgl,1,1473,19.7989,0.6960

epoch:1475/50, training loss:0.9610702395439148
Train Acc 0.7093
 Acc 0.6959
ogbn-arxiv,dgl,1,1474,19.8113,0.6959

epoch:1476/50, training loss:0.9609825015068054
Train Acc 0.7093
 Acc 0.6956
ogbn-arxiv,dgl,1,1475,19.8238,0.6956

epoch:1477/50, training loss:0.9609022736549377
Train Acc 0.7092
 Acc 0.6960
new best val f1: 0.696039053327566
ogbn-arxiv,dgl,1,1476,19.8362,0.6960

epoch:1478/50, training loss:0.9608121514320374
Train Acc 0.7093
 Acc 0.6959
ogbn-arxiv,dgl,1,1477,19.8487,0.6959

epoch:1479/50, training loss:0.9607235789299011
Train Acc 0.7094
 Acc 0.6956
ogbn-arxiv,dgl,1,1478,19.8611,0.6956

epoch:1480/50, training loss:0.96064293384552
Train Acc 0.7092
 Acc 0.6960
ogbn-arxiv,dgl,1,1479,19.8736,0.6960

epoch:1481/50, training loss:0.9605623483657837
Train Acc 0.7093
 Acc 0.6958
ogbn-arxiv,dgl,1,1480,19.8859,0.6958

epoch:1482/50, training loss:0.960471510887146
Train Acc 0.7094
 Acc 0.6959
ogbn-arxiv,dgl,1,1481,19.8984,0.6959

epoch:1483/50, training loss:0.9603857398033142
Train Acc 0.7094
 Acc 0.6962
new best val f1: 0.6961832375538116
ogbn-arxiv,dgl,1,1482,19.9109,0.6962

epoch:1484/50, training loss:0.9603040218353271
Train Acc 0.7095
 Acc 0.6958
ogbn-arxiv,dgl,1,1483,19.9234,0.6958

epoch:1485/50, training loss:0.9602130651473999
Train Acc 0.7093
 Acc 0.6958
ogbn-arxiv,dgl,1,1484,19.9358,0.6958

epoch:1486/50, training loss:0.9601305723190308
Train Acc 0.7094
 Acc 0.6962
new best val f1: 0.6962450307936312
ogbn-arxiv,dgl,1,1485,19.9483,0.6962

epoch:1487/50, training loss:0.9600507020950317
Train Acc 0.7095
 Acc 0.6960
ogbn-arxiv,dgl,1,1486,19.9607,0.6960

epoch:1488/50, training loss:0.9599571228027344
Train Acc 0.7095
 Acc 0.6959
ogbn-arxiv,dgl,1,1487,19.9732,0.6959

epoch:1489/50, training loss:0.9598774909973145
Train Acc 0.7095
 Acc 0.6962
ogbn-arxiv,dgl,1,1488,19.9856,0.6962

epoch:1490/50, training loss:0.9597904682159424
Train Acc 0.7096
 Acc 0.6961
ogbn-arxiv,dgl,1,1489,19.9984,0.6961

epoch:1491/50, training loss:0.959702730178833
Train Acc 0.7095
 Acc 0.6956
ogbn-arxiv,dgl,1,1490,20.0107,0.6956

epoch:1492/50, training loss:0.9596242308616638
Train Acc 0.7095
 Acc 0.6961
ogbn-arxiv,dgl,1,1491,20.0232,0.6961

epoch:1493/50, training loss:0.9595376253128052
Train Acc 0.7096
 Acc 0.6960
ogbn-arxiv,dgl,1,1492,20.0356,0.6960

epoch:1494/50, training loss:0.9594492316246033
Train Acc 0.7096
 Acc 0.6954
ogbn-arxiv,dgl,1,1493,20.0481,0.6954

epoch:1495/50, training loss:0.9593750238418579
Train Acc 0.7094
 Acc 0.6962
ogbn-arxiv,dgl,1,1494,20.0605,0.6962

epoch:1496/50, training loss:0.9592828154563904
Train Acc 0.7097
 Acc 0.6962
ogbn-arxiv,dgl,1,1495,20.0730,0.6962

epoch:1497/50, training loss:0.9591977000236511
Train Acc 0.7096
 Acc 0.6955
ogbn-arxiv,dgl,1,1496,20.0854,0.6955

epoch:1498/50, training loss:0.9591239094734192
Train Acc 0.7095
 Acc 0.6962
ogbn-arxiv,dgl,1,1497,20.0988,0.6962

epoch:1499/50, training loss:0.9590311050415039
Train Acc 0.7098
 Acc 0.6961
ogbn-arxiv,dgl,1,1498,20.1112,0.6961

epoch:1500/50, training loss:0.9589440226554871
Train Acc 0.7098
 Acc 0.6958
ogbn-arxiv,dgl,1,1499,20.1237,0.6958

epoch:1501/50, training loss:0.9588663578033447
Train Acc 0.7095
 Acc 0.6962
ogbn-arxiv,dgl,1,1500,20.1380,0.6962

epoch:1502/50, training loss:0.958774983882904
Train Acc 0.7098
 Acc 0.6964
new best val f1: 0.6964304105130898
ogbn-arxiv,dgl,1,1501,20.1505,0.6964

epoch:1503/50, training loss:0.9586979746818542
Train Acc 0.7099
 Acc 0.6959
ogbn-arxiv,dgl,1,1502,20.1628,0.6959

epoch:1504/50, training loss:0.9586130380630493
Train Acc 0.7096
 Acc 0.6962
ogbn-arxiv,dgl,1,1503,20.1754,0.6962

epoch:1505/50, training loss:0.9585233330726624
Train Acc 0.7098
 Acc 0.6963
ogbn-arxiv,dgl,1,1504,20.1877,0.6963

epoch:1506/50, training loss:0.9584419131278992
Train Acc 0.7099
 Acc 0.6961
ogbn-arxiv,dgl,1,1505,20.2002,0.6961

epoch:1507/50, training loss:0.9583582878112793
Train Acc 0.7099
 Acc 0.6964
ogbn-arxiv,dgl,1,1506,20.2125,0.6964

epoch:1508/50, training loss:0.9582737684249878
Train Acc 0.7099
 Acc 0.6964
ogbn-arxiv,dgl,1,1507,20.2250,0.6964

epoch:1509/50, training loss:0.9581871628761292
Train Acc 0.7100
 Acc 0.6961
ogbn-arxiv,dgl,1,1508,20.2374,0.6961

epoch:1510/50, training loss:0.9581069350242615
Train Acc 0.7100
 Acc 0.6964
ogbn-arxiv,dgl,1,1509,20.2499,0.6964

epoch:1511/50, training loss:0.9580191373825073
Train Acc 0.7100
 Acc 0.6964
ogbn-arxiv,dgl,1,1510,20.2622,0.6964

epoch:1512/50, training loss:0.9579346179962158
Train Acc 0.7100
 Acc 0.6963
ogbn-arxiv,dgl,1,1511,20.2747,0.6963

epoch:1513/50, training loss:0.9578567147254944
Train Acc 0.7101
 Acc 0.6964
ogbn-arxiv,dgl,1,1512,20.2871,0.6964

epoch:1514/50, training loss:0.9577668309211731
Train Acc 0.7101
 Acc 0.6963
ogbn-arxiv,dgl,1,1513,20.2997,0.6963

epoch:1515/50, training loss:0.9576894044876099
Train Acc 0.7100
 Acc 0.6964
ogbn-arxiv,dgl,1,1514,20.3121,0.6964

epoch:1516/50, training loss:0.9576029777526855
Train Acc 0.7101
 Acc 0.6963
ogbn-arxiv,dgl,1,1515,20.3246,0.6963

epoch:1517/50, training loss:0.9575187563896179
Train Acc 0.7101
 Acc 0.6964
ogbn-arxiv,dgl,1,1516,20.3370,0.6964

epoch:1518/50, training loss:0.9574409127235413
Train Acc 0.7101
 Acc 0.6965
new best val f1: 0.6964510082596964
ogbn-arxiv,dgl,1,1517,20.3494,0.6965

epoch:1519/50, training loss:0.9573512077331543
Train Acc 0.7102
 Acc 0.6965
new best val f1: 0.6964922037529094
ogbn-arxiv,dgl,1,1518,20.3619,0.6965

epoch:1520/50, training loss:0.9572731256484985
Train Acc 0.7103
 Acc 0.6963
ogbn-arxiv,dgl,1,1519,20.3745,0.6963

epoch:1521/50, training loss:0.9571895599365234
Train Acc 0.7101
 Acc 0.6964
ogbn-arxiv,dgl,1,1520,20.3868,0.6964

epoch:1522/50, training loss:0.9571025967597961
Train Acc 0.7102
 Acc 0.6966
new best val f1: 0.696553996992729
ogbn-arxiv,dgl,1,1521,20.3994,0.6966

epoch:1523/50, training loss:0.9570227861404419
Train Acc 0.7103
 Acc 0.6964
ogbn-arxiv,dgl,1,1522,20.4118,0.6964

epoch:1524/50, training loss:0.9569368958473206
Train Acc 0.7103
 Acc 0.6965
ogbn-arxiv,dgl,1,1523,20.4243,0.6965

epoch:1525/50, training loss:0.9568538665771484
Train Acc 0.7103
 Acc 0.6965
ogbn-arxiv,dgl,1,1524,20.4366,0.6965

epoch:1526/50, training loss:0.9567708969116211
Train Acc 0.7103
 Acc 0.6966
ogbn-arxiv,dgl,1,1525,20.4491,0.6966

epoch:1527/50, training loss:0.9566889405250549
Train Acc 0.7103
 Acc 0.6964
ogbn-arxiv,dgl,1,1526,20.4615,0.6964

epoch:1528/50, training loss:0.9566063284873962
Train Acc 0.7103
 Acc 0.6964
ogbn-arxiv,dgl,1,1527,20.4740,0.6964

epoch:1529/50, training loss:0.9565231800079346
Train Acc 0.7103
 Acc 0.6965
ogbn-arxiv,dgl,1,1528,20.4864,0.6965

epoch:1530/50, training loss:0.9564405083656311
Train Acc 0.7104
 Acc 0.6964
ogbn-arxiv,dgl,1,1529,20.4989,0.6964

epoch:1531/50, training loss:0.9563555717468262
Train Acc 0.7103
 Acc 0.6965
ogbn-arxiv,dgl,1,1530,20.5113,0.6965

epoch:1532/50, training loss:0.956275224685669
Train Acc 0.7103
 Acc 0.6963
ogbn-arxiv,dgl,1,1531,20.5238,0.6963

epoch:1533/50, training loss:0.9561901092529297
Train Acc 0.7103
 Acc 0.6963
ogbn-arxiv,dgl,1,1532,20.5361,0.6963

epoch:1534/50, training loss:0.9561101198196411
Train Acc 0.7103
 Acc 0.6966
new best val f1: 0.696595192485942
ogbn-arxiv,dgl,1,1533,20.5497,0.6966

epoch:1535/50, training loss:0.9560235738754272
Train Acc 0.7105
 Acc 0.6964
ogbn-arxiv,dgl,1,1534,20.5620,0.6964

epoch:1536/50, training loss:0.9559413194656372
Train Acc 0.7104
 Acc 0.6963
ogbn-arxiv,dgl,1,1535,20.5745,0.6963

epoch:1537/50, training loss:0.9558591842651367
Train Acc 0.7103
 Acc 0.6965
ogbn-arxiv,dgl,1,1536,20.5868,0.6965

epoch:1538/50, training loss:0.955777645111084
Train Acc 0.7104
 Acc 0.6963
ogbn-arxiv,dgl,1,1537,20.5993,0.6963

epoch:1539/50, training loss:0.9556955099105835
Train Acc 0.7104
 Acc 0.6965
ogbn-arxiv,dgl,1,1538,20.6117,0.6965

epoch:1540/50, training loss:0.9556148648262024
Train Acc 0.7105
 Acc 0.6965
ogbn-arxiv,dgl,1,1539,20.6243,0.6965

epoch:1541/50, training loss:0.9555298686027527
Train Acc 0.7105
 Acc 0.6964
ogbn-arxiv,dgl,1,1540,20.6366,0.6964

epoch:1542/50, training loss:0.9554466605186462
Train Acc 0.7104
 Acc 0.6967
new best val f1: 0.6966569857257616
ogbn-arxiv,dgl,1,1541,20.6491,0.6967

epoch:1543/50, training loss:0.9553629755973816
Train Acc 0.7105
 Acc 0.6963
ogbn-arxiv,dgl,1,1542,20.6614,0.6963

epoch:1544/50, training loss:0.9552817940711975
Train Acc 0.7104
 Acc 0.6966
ogbn-arxiv,dgl,1,1543,20.6738,0.6966

epoch:1545/50, training loss:0.9552003741264343
Train Acc 0.7105
 Acc 0.6965
ogbn-arxiv,dgl,1,1544,20.6862,0.6965

epoch:1546/50, training loss:0.9551206231117249
Train Acc 0.7105
 Acc 0.6965
ogbn-arxiv,dgl,1,1545,20.6987,0.6965

epoch:1547/50, training loss:0.9550372362136841
Train Acc 0.7106
 Acc 0.6964
ogbn-arxiv,dgl,1,1546,20.7110,0.6964

epoch:1548/50, training loss:0.9549528360366821
Train Acc 0.7105
 Acc 0.6966
ogbn-arxiv,dgl,1,1547,20.7235,0.6966

epoch:1549/50, training loss:0.9548735618591309
Train Acc 0.7106
 Acc 0.6966
ogbn-arxiv,dgl,1,1548,20.7358,0.6966

epoch:1550/50, training loss:0.9547902345657349
Train Acc 0.7106
 Acc 0.6963
ogbn-arxiv,dgl,1,1549,20.7485,0.6963

epoch:1551/50, training loss:0.9547109603881836
Train Acc 0.7105
 Acc 0.6964
ogbn-arxiv,dgl,1,1550,20.7609,0.6964

epoch:1552/50, training loss:0.9546288847923279
Train Acc 0.7105
 Acc 0.6966
ogbn-arxiv,dgl,1,1551,20.7734,0.6966

epoch:1553/50, training loss:0.954546332359314
Train Acc 0.7107
 Acc 0.6965
ogbn-arxiv,dgl,1,1552,20.7857,0.6965

epoch:1554/50, training loss:0.9544649124145508
Train Acc 0.7106
 Acc 0.6964
ogbn-arxiv,dgl,1,1553,20.7983,0.6964

epoch:1555/50, training loss:0.954383373260498
Train Acc 0.7106
 Acc 0.6966
ogbn-arxiv,dgl,1,1554,20.8107,0.6966

epoch:1556/50, training loss:0.9543008804321289
Train Acc 0.7106
 Acc 0.6966
ogbn-arxiv,dgl,1,1555,20.8233,0.6966

epoch:1557/50, training loss:0.9542215466499329
Train Acc 0.7106
 Acc 0.6962
ogbn-arxiv,dgl,1,1556,20.8357,0.6962

epoch:1558/50, training loss:0.9541425108909607
Train Acc 0.7106
 Acc 0.6966
ogbn-arxiv,dgl,1,1557,20.8482,0.6966

epoch:1559/50, training loss:0.9540559649467468
Train Acc 0.7107
 Acc 0.6966
ogbn-arxiv,dgl,1,1558,20.8605,0.6966

epoch:1560/50, training loss:0.9539777040481567
Train Acc 0.7106
 Acc 0.6963
ogbn-arxiv,dgl,1,1559,20.8730,0.6963

epoch:1561/50, training loss:0.9538968205451965
Train Acc 0.7106
 Acc 0.6965
ogbn-arxiv,dgl,1,1560,20.8854,0.6965

epoch:1562/50, training loss:0.953816831111908
Train Acc 0.7108
 Acc 0.6967
new best val f1: 0.6966775834723681
ogbn-arxiv,dgl,1,1561,20.8979,0.6967

epoch:1563/50, training loss:0.9537310600280762
Train Acc 0.7108
 Acc 0.6965
ogbn-arxiv,dgl,1,1562,20.9103,0.6965

epoch:1564/50, training loss:0.9536517262458801
Train Acc 0.7107
 Acc 0.6965
ogbn-arxiv,dgl,1,1563,20.9228,0.6965

epoch:1565/50, training loss:0.953571617603302
Train Acc 0.7108
 Acc 0.6965
ogbn-arxiv,dgl,1,1564,20.9352,0.6965

epoch:1566/50, training loss:0.9534907341003418
Train Acc 0.7107
 Acc 0.6967
ogbn-arxiv,dgl,1,1565,20.9477,0.6967

epoch:1567/50, training loss:0.9534115791320801
Train Acc 0.7108
 Acc 0.6966
ogbn-arxiv,dgl,1,1566,20.9600,0.6966

epoch:1568/50, training loss:0.9533301591873169
Train Acc 0.7108
 Acc 0.6965
ogbn-arxiv,dgl,1,1567,20.9725,0.6965

epoch:1569/50, training loss:0.9532521367073059
Train Acc 0.7107
 Acc 0.6964
ogbn-arxiv,dgl,1,1568,20.9849,0.6964

epoch:1570/50, training loss:0.9531673789024353
Train Acc 0.7107
 Acc 0.6967
new best val f1: 0.6967393767121877
ogbn-arxiv,dgl,1,1569,20.9974,0.6967

epoch:1571/50, training loss:0.9530888199806213
Train Acc 0.7109
 Acc 0.6966
ogbn-arxiv,dgl,1,1570,21.0097,0.6966

epoch:1572/50, training loss:0.9530081748962402
Train Acc 0.7108
 Acc 0.6964
ogbn-arxiv,dgl,1,1571,21.0222,0.6964

epoch:1573/50, training loss:0.9529299139976501
Train Acc 0.7107
 Acc 0.6967
ogbn-arxiv,dgl,1,1572,21.0345,0.6967

epoch:1574/50, training loss:0.9528481960296631
Train Acc 0.7108
 Acc 0.6965
ogbn-arxiv,dgl,1,1573,21.0469,0.6965

epoch:1575/50, training loss:0.9527671337127686
Train Acc 0.7108
 Acc 0.6965
ogbn-arxiv,dgl,1,1574,21.0594,0.6965

epoch:1576/50, training loss:0.952688992023468
Train Acc 0.7107
 Acc 0.6966
ogbn-arxiv,dgl,1,1575,21.0733,0.6966

epoch:1577/50, training loss:0.9526069760322571
Train Acc 0.7108
 Acc 0.6968
new best val f1: 0.6967599744587942
ogbn-arxiv,dgl,1,1576,21.0856,0.6968

epoch:1578/50, training loss:0.9525289535522461
Train Acc 0.7109
 Acc 0.6964
ogbn-arxiv,dgl,1,1577,21.0982,0.6964

epoch:1579/50, training loss:0.9524478912353516
Train Acc 0.7108
 Acc 0.6966
ogbn-arxiv,dgl,1,1578,21.1105,0.6966

epoch:1580/50, training loss:0.9523694515228271
Train Acc 0.7108
 Acc 0.6967
ogbn-arxiv,dgl,1,1579,21.1230,0.6967

epoch:1581/50, training loss:0.9522898197174072
Train Acc 0.7110
 Acc 0.6965
ogbn-arxiv,dgl,1,1580,21.1354,0.6965

epoch:1582/50, training loss:0.9522095918655396
Train Acc 0.7109
 Acc 0.6966
ogbn-arxiv,dgl,1,1581,21.1479,0.6966

epoch:1583/50, training loss:0.9521335363388062
Train Acc 0.7109
 Acc 0.6966
ogbn-arxiv,dgl,1,1582,21.1602,0.6966

epoch:1584/50, training loss:0.9520490765571594
Train Acc 0.7109
 Acc 0.6967
ogbn-arxiv,dgl,1,1583,21.1726,0.6967

epoch:1585/50, training loss:0.951972484588623
Train Acc 0.7110
 Acc 0.6964
ogbn-arxiv,dgl,1,1584,21.1850,0.6964

epoch:1586/50, training loss:0.9518954157829285
Train Acc 0.7108
 Acc 0.6962
ogbn-arxiv,dgl,1,1585,21.1975,0.6962

epoch:1587/50, training loss:0.951815664768219
Train Acc 0.7108
 Acc 0.6966
ogbn-arxiv,dgl,1,1586,21.2099,0.6966

epoch:1588/50, training loss:0.9517350792884827
Train Acc 0.7109
 Acc 0.6963
ogbn-arxiv,dgl,1,1587,21.2225,0.6963

epoch:1589/50, training loss:0.9516559839248657
Train Acc 0.7109
 Acc 0.6963
ogbn-arxiv,dgl,1,1588,21.2349,0.6963

epoch:1590/50, training loss:0.9515770077705383
Train Acc 0.7109
 Acc 0.6966
ogbn-arxiv,dgl,1,1589,21.2475,0.6966

epoch:1591/50, training loss:0.9515008330345154
Train Acc 0.7110
 Acc 0.6961
ogbn-arxiv,dgl,1,1590,21.2612,0.6961

epoch:1592/50, training loss:0.951418936252594
Train Acc 0.7108
 Acc 0.6965
ogbn-arxiv,dgl,1,1591,21.2738,0.6965

epoch:1593/50, training loss:0.9513400197029114
Train Acc 0.7109
 Acc 0.6966
ogbn-arxiv,dgl,1,1592,21.2862,0.6966

epoch:1594/50, training loss:0.9512621164321899
Train Acc 0.7110
 Acc 0.6964
ogbn-arxiv,dgl,1,1593,21.2987,0.6964

epoch:1595/50, training loss:0.9511821866035461
Train Acc 0.7110
 Acc 0.6963
ogbn-arxiv,dgl,1,1594,21.3110,0.6963

epoch:1596/50, training loss:0.9511042833328247
Train Acc 0.7109
 Acc 0.6965
ogbn-arxiv,dgl,1,1595,21.3236,0.6965

epoch:1597/50, training loss:0.9510242938995361
Train Acc 0.7110
 Acc 0.6963
ogbn-arxiv,dgl,1,1596,21.3359,0.6963

epoch:1598/50, training loss:0.9509459733963013
Train Acc 0.7109
 Acc 0.6964
ogbn-arxiv,dgl,1,1597,21.3484,0.6964

epoch:1599/50, training loss:0.9508663415908813
Train Acc 0.7110
 Acc 0.6964
ogbn-arxiv,dgl,1,1598,21.3609,0.6964

epoch:1600/50, training loss:0.9507890939712524
Train Acc 0.7110
 Acc 0.6960
ogbn-arxiv,dgl,1,1599,21.3734,0.6960

epoch:1601/50, training loss:0.9507099986076355
Train Acc 0.7109
 Acc 0.6967
ogbn-arxiv,dgl,1,1600,21.3858,0.6967

epoch:1602/50, training loss:0.9506323337554932
Train Acc 0.7111
 Acc 0.6965
ogbn-arxiv,dgl,1,1601,21.3983,0.6965

epoch:1603/50, training loss:0.9505519270896912
Train Acc 0.7110
 Acc 0.6960
ogbn-arxiv,dgl,1,1602,21.4107,0.6960

epoch:1604/50, training loss:0.9504760503768921
Train Acc 0.7109
 Acc 0.6967
ogbn-arxiv,dgl,1,1603,21.4233,0.6967

epoch:1605/50, training loss:0.9503971934318542
Train Acc 0.7112
 Acc 0.6964
ogbn-arxiv,dgl,1,1604,21.4356,0.6964

epoch:1606/50, training loss:0.9503166675567627
Train Acc 0.7110
 Acc 0.6962
ogbn-arxiv,dgl,1,1605,21.4487,0.6962

epoch:1607/50, training loss:0.9502381086349487
Train Acc 0.7109
 Acc 0.6967
ogbn-arxiv,dgl,1,1606,21.4611,0.6967

epoch:1608/50, training loss:0.9501626491546631
Train Acc 0.7111
 Acc 0.6962
ogbn-arxiv,dgl,1,1607,21.4736,0.6962

epoch:1609/50, training loss:0.9500839710235596
Train Acc 0.7110
 Acc 0.6965
ogbn-arxiv,dgl,1,1608,21.4859,0.6965

epoch:1610/50, training loss:0.9500026702880859
Train Acc 0.7111
 Acc 0.6966
ogbn-arxiv,dgl,1,1609,21.4985,0.6966

epoch:1611/50, training loss:0.949926495552063
Train Acc 0.7111
 Acc 0.6963
ogbn-arxiv,dgl,1,1610,21.5109,0.6963

epoch:1612/50, training loss:0.9498445987701416
Train Acc 0.7111
 Acc 0.6967
ogbn-arxiv,dgl,1,1611,21.5234,0.6967

epoch:1613/50, training loss:0.9497674703598022
Train Acc 0.7112
 Acc 0.6965
ogbn-arxiv,dgl,1,1612,21.5357,0.6965

epoch:1614/50, training loss:0.9496892094612122
Train Acc 0.7111
 Acc 0.6966
ogbn-arxiv,dgl,1,1613,21.5482,0.6966

epoch:1615/50, training loss:0.9496109485626221
Train Acc 0.7111
 Acc 0.6964
ogbn-arxiv,dgl,1,1614,21.5606,0.6964

epoch:1616/50, training loss:0.949533998966217
Train Acc 0.7111
 Acc 0.6966
ogbn-arxiv,dgl,1,1615,21.5731,0.6966

epoch:1617/50, training loss:0.9494571089744568
Train Acc 0.7112
 Acc 0.6964
ogbn-arxiv,dgl,1,1616,21.5855,0.6964

epoch:1618/50, training loss:0.9493789076805115
Train Acc 0.7111
 Acc 0.6966
ogbn-arxiv,dgl,1,1617,21.5980,0.6966

epoch:1619/50, training loss:0.9492984414100647
Train Acc 0.7112
 Acc 0.6966
ogbn-arxiv,dgl,1,1618,21.6103,0.6966

epoch:1620/50, training loss:0.9492246508598328
Train Acc 0.7111
 Acc 0.6963
ogbn-arxiv,dgl,1,1619,21.6228,0.6963

epoch:1621/50, training loss:0.9491430521011353
Train Acc 0.7111
 Acc 0.6962
ogbn-arxiv,dgl,1,1620,21.6352,0.6962

epoch:1622/50, training loss:0.9490662813186646
Train Acc 0.7111
 Acc 0.6965
ogbn-arxiv,dgl,1,1621,21.6477,0.6965

epoch:1623/50, training loss:0.948991596698761
Train Acc 0.7112
 Acc 0.6965
ogbn-arxiv,dgl,1,1622,21.6601,0.6965

epoch:1624/50, training loss:0.9489102959632874
Train Acc 0.7112
 Acc 0.6963
ogbn-arxiv,dgl,1,1623,21.6726,0.6963

epoch:1625/50, training loss:0.9488349556922913
Train Acc 0.7111
 Acc 0.6965
ogbn-arxiv,dgl,1,1624,21.6849,0.6965

epoch:1626/50, training loss:0.9487564563751221
Train Acc 0.7112
 Acc 0.6963
ogbn-arxiv,dgl,1,1625,21.6974,0.6963

epoch:1627/50, training loss:0.9486775994300842
Train Acc 0.7111
 Acc 0.6964
ogbn-arxiv,dgl,1,1626,21.7098,0.6964

epoch:1628/50, training loss:0.9486017227172852
Train Acc 0.7112
 Acc 0.6965
ogbn-arxiv,dgl,1,1627,21.7223,0.6965

epoch:1629/50, training loss:0.9485248923301697
Train Acc 0.7111
 Acc 0.6965
ogbn-arxiv,dgl,1,1628,21.7346,0.6965

epoch:1630/50, training loss:0.9484494924545288
Train Acc 0.7112
 Acc 0.6963
ogbn-arxiv,dgl,1,1629,21.7471,0.6963

epoch:1631/50, training loss:0.948369026184082
Train Acc 0.7112
 Acc 0.6963
ogbn-arxiv,dgl,1,1630,21.7594,0.6963

epoch:1632/50, training loss:0.9482923746109009
Train Acc 0.7111
 Acc 0.6966
ogbn-arxiv,dgl,1,1631,21.7720,0.6966

epoch:1633/50, training loss:0.9482169151306152
Train Acc 0.7112
 Acc 0.6963
ogbn-arxiv,dgl,1,1632,21.7844,0.6963

epoch:1634/50, training loss:0.9481410980224609
Train Acc 0.7112
 Acc 0.6964
ogbn-arxiv,dgl,1,1633,21.7968,0.6964

epoch:1635/50, training loss:0.9480615258216858
Train Acc 0.7112
 Acc 0.6963
ogbn-arxiv,dgl,1,1634,21.8092,0.6963

epoch:1636/50, training loss:0.9479871392250061
Train Acc 0.7111
 Acc 0.6965
ogbn-arxiv,dgl,1,1635,21.8217,0.6965

epoch:1637/50, training loss:0.9479077458381653
Train Acc 0.7113
 Acc 0.6963
ogbn-arxiv,dgl,1,1636,21.8341,0.6963

epoch:1638/50, training loss:0.947827160358429
Train Acc 0.7113
 Acc 0.6962
ogbn-arxiv,dgl,1,1637,21.8466,0.6962

epoch:1639/50, training loss:0.9477545022964478
Train Acc 0.7112
 Acc 0.6965
ogbn-arxiv,dgl,1,1638,21.8589,0.6965

epoch:1640/50, training loss:0.9476762413978577
Train Acc 0.7113
 Acc 0.6962
ogbn-arxiv,dgl,1,1639,21.8715,0.6962

epoch:1641/50, training loss:0.9476006627082825
Train Acc 0.7113
 Acc 0.6964
ogbn-arxiv,dgl,1,1640,21.8839,0.6964

epoch:1642/50, training loss:0.9475236535072327
Train Acc 0.7113
 Acc 0.6965
ogbn-arxiv,dgl,1,1641,21.8963,0.6965

epoch:1643/50, training loss:0.9474455714225769
Train Acc 0.7113
 Acc 0.6967
ogbn-arxiv,dgl,1,1642,21.9086,0.6967

epoch:1644/50, training loss:0.9473688006401062
Train Acc 0.7113
 Acc 0.6965
ogbn-arxiv,dgl,1,1643,21.9211,0.6965

epoch:1645/50, training loss:0.9472900032997131
Train Acc 0.7113
 Acc 0.6964
ogbn-arxiv,dgl,1,1644,21.9335,0.6964

epoch:1646/50, training loss:0.9472160339355469
Train Acc 0.7113
 Acc 0.6965
ogbn-arxiv,dgl,1,1645,21.9461,0.6965

epoch:1647/50, training loss:0.9471378326416016
Train Acc 0.7114
 Acc 0.6963
ogbn-arxiv,dgl,1,1646,21.9584,0.6963

epoch:1648/50, training loss:0.9470595121383667
Train Acc 0.7113
 Acc 0.6964
ogbn-arxiv,dgl,1,1647,21.9709,0.6964

epoch:1649/50, training loss:0.9469836354255676
Train Acc 0.7113
 Acc 0.6966
ogbn-arxiv,dgl,1,1648,21.9833,0.6966

epoch:1650/50, training loss:0.9469075202941895
Train Acc 0.7114
 Acc 0.6965
ogbn-arxiv,dgl,1,1649,21.9959,0.6965

epoch:1651/50, training loss:0.9468309283256531
Train Acc 0.7114
 Acc 0.6963
ogbn-arxiv,dgl,1,1650,22.0083,0.6963

epoch:1652/50, training loss:0.9467535018920898
Train Acc 0.7113
 Acc 0.6963
ogbn-arxiv,dgl,1,1651,22.0208,0.6963

epoch:1653/50, training loss:0.9466785788536072
Train Acc 0.7114
 Acc 0.6968
new best val f1: 0.6968217676986138
ogbn-arxiv,dgl,1,1652,22.0331,0.6968

epoch:1654/50, training loss:0.9466040134429932
Train Acc 0.7116
 Acc 0.6962
ogbn-arxiv,dgl,1,1653,22.0457,0.6962

epoch:1655/50, training loss:0.9465254545211792
Train Acc 0.7114
 Acc 0.6963
ogbn-arxiv,dgl,1,1654,22.0580,0.6963

epoch:1656/50, training loss:0.9464486837387085
Train Acc 0.7114
 Acc 0.6966
ogbn-arxiv,dgl,1,1655,22.0705,0.6966

epoch:1657/50, training loss:0.9463745951652527
Train Acc 0.7116
 Acc 0.6963
ogbn-arxiv,dgl,1,1656,22.0829,0.6963

epoch:1658/50, training loss:0.9462972283363342
Train Acc 0.7115
 Acc 0.6963
ogbn-arxiv,dgl,1,1657,22.0953,0.6963

epoch:1659/50, training loss:0.9462196826934814
Train Acc 0.7115
 Acc 0.6966
ogbn-arxiv,dgl,1,1658,22.1077,0.6966

epoch:1660/50, training loss:0.9461432099342346
Train Acc 0.7116
 Acc 0.6965
ogbn-arxiv,dgl,1,1659,22.1203,0.6965

epoch:1661/50, training loss:0.9460686445236206
Train Acc 0.7116
 Acc 0.6962
ogbn-arxiv,dgl,1,1660,22.1327,0.6962

epoch:1662/50, training loss:0.9459941983222961
Train Acc 0.7115
 Acc 0.6966
ogbn-arxiv,dgl,1,1661,22.1452,0.6966

epoch:1663/50, training loss:0.9459134936332703
Train Acc 0.7115
 Acc 0.6965
ogbn-arxiv,dgl,1,1662,22.1575,0.6965

epoch:1664/50, training loss:0.9458432197570801
Train Acc 0.7117
 Acc 0.6962
ogbn-arxiv,dgl,1,1663,22.1701,0.6962

epoch:1665/50, training loss:0.9457631707191467
Train Acc 0.7115
 Acc 0.6964
ogbn-arxiv,dgl,1,1664,22.1825,0.6964

epoch:1666/50, training loss:0.9456870555877686
Train Acc 0.7116
 Acc 0.6967
ogbn-arxiv,dgl,1,1665,22.1949,0.6967

epoch:1667/50, training loss:0.9456143975257874
Train Acc 0.7116
 Acc 0.6966
ogbn-arxiv,dgl,1,1666,22.2073,0.6966

epoch:1668/50, training loss:0.9455344676971436
Train Acc 0.7116
 Acc 0.6965
ogbn-arxiv,dgl,1,1667,22.2197,0.6965

epoch:1669/50, training loss:0.9454589486122131
Train Acc 0.7116
 Acc 0.6962
ogbn-arxiv,dgl,1,1668,22.2321,0.6962

epoch:1670/50, training loss:0.9453814029693604
Train Acc 0.7117
 Acc 0.6966
ogbn-arxiv,dgl,1,1669,22.2447,0.6966

epoch:1671/50, training loss:0.9453070759773254
Train Acc 0.7117
 Acc 0.6962
ogbn-arxiv,dgl,1,1670,22.2570,0.6962

epoch:1672/50, training loss:0.9452311396598816
Train Acc 0.7116
 Acc 0.6963
ogbn-arxiv,dgl,1,1671,22.2694,0.6963

epoch:1673/50, training loss:0.9451549649238586
Train Acc 0.7116
 Acc 0.6965
ogbn-arxiv,dgl,1,1672,22.2818,0.6965

epoch:1674/50, training loss:0.9450814127922058
Train Acc 0.7117
 Acc 0.6963
ogbn-arxiv,dgl,1,1673,22.2944,0.6963

epoch:1675/50, training loss:0.9450032114982605
Train Acc 0.7117
 Acc 0.6961
ogbn-arxiv,dgl,1,1674,22.3067,0.6961

epoch:1676/50, training loss:0.9449291825294495
Train Acc 0.7116
 Acc 0.6967
ogbn-arxiv,dgl,1,1675,22.3193,0.6967

epoch:1677/50, training loss:0.9448554515838623
Train Acc 0.7118
 Acc 0.6961
ogbn-arxiv,dgl,1,1676,22.3335,0.6961

epoch:1678/50, training loss:0.9447777271270752
Train Acc 0.7116
 Acc 0.6963
ogbn-arxiv,dgl,1,1677,22.3461,0.6963

epoch:1679/50, training loss:0.9447004199028015
Train Acc 0.7117
 Acc 0.6965
ogbn-arxiv,dgl,1,1678,22.3584,0.6965

epoch:1680/50, training loss:0.9446264505386353
Train Acc 0.7118
 Acc 0.6960
ogbn-arxiv,dgl,1,1679,22.3709,0.6960

epoch:1681/50, training loss:0.9445533156394958
Train Acc 0.7116
 Acc 0.6966
ogbn-arxiv,dgl,1,1680,22.3833,0.6966

epoch:1682/50, training loss:0.9444751739501953
Train Acc 0.7118
 Acc 0.6964
ogbn-arxiv,dgl,1,1681,22.3959,0.6964

epoch:1683/50, training loss:0.9443989396095276
Train Acc 0.7118
 Acc 0.6961
ogbn-arxiv,dgl,1,1682,22.4082,0.6961

epoch:1684/50, training loss:0.944322943687439
Train Acc 0.7117
 Acc 0.6965
ogbn-arxiv,dgl,1,1683,22.4207,0.6965

epoch:1685/50, training loss:0.9442501664161682
Train Acc 0.7118
 Acc 0.6965
ogbn-arxiv,dgl,1,1684,22.4330,0.6965

epoch:1686/50, training loss:0.9441741704940796
Train Acc 0.7118
 Acc 0.6959
ogbn-arxiv,dgl,1,1685,22.4455,0.6959

epoch:1687/50, training loss:0.9441035389900208
Train Acc 0.7116
 Acc 0.6970
new best val f1: 0.6969659519248594
ogbn-arxiv,dgl,1,1686,22.4579,0.6970

epoch:1688/50, training loss:0.94403076171875
Train Acc 0.7119
 Acc 0.6962
ogbn-arxiv,dgl,1,1687,22.4703,0.6962

epoch:1689/50, training loss:0.9439480900764465
Train Acc 0.7118
 Acc 0.6962
ogbn-arxiv,dgl,1,1688,22.4826,0.6962

epoch:1690/50, training loss:0.9438755512237549
Train Acc 0.7118
 Acc 0.6968
ogbn-arxiv,dgl,1,1689,22.4950,0.6968

epoch:1691/50, training loss:0.9437997341156006
Train Acc 0.7119
 Acc 0.6963
ogbn-arxiv,dgl,1,1690,22.5074,0.6963

epoch:1692/50, training loss:0.9437220692634583
Train Acc 0.7118
 Acc 0.6962
ogbn-arxiv,dgl,1,1691,22.5200,0.6962

epoch:1693/50, training loss:0.9436513185501099
Train Acc 0.7118
 Acc 0.6967
ogbn-arxiv,dgl,1,1692,22.5323,0.6967

epoch:1694/50, training loss:0.9435761570930481
Train Acc 0.7119
 Acc 0.6964
ogbn-arxiv,dgl,1,1693,22.5447,0.6964

epoch:1695/50, training loss:0.9434984922409058
Train Acc 0.7119
 Acc 0.6963
ogbn-arxiv,dgl,1,1694,22.5571,0.6963

epoch:1696/50, training loss:0.9434232115745544
Train Acc 0.7119
 Acc 0.6965
ogbn-arxiv,dgl,1,1695,22.5696,0.6965

epoch:1697/50, training loss:0.9433512091636658
Train Acc 0.7119
 Acc 0.6964
ogbn-arxiv,dgl,1,1696,22.5820,0.6964

epoch:1698/50, training loss:0.9432726502418518
Train Acc 0.7119
 Acc 0.6963
ogbn-arxiv,dgl,1,1697,22.5946,0.6963

epoch:1699/50, training loss:0.9432005286216736
Train Acc 0.7119
 Acc 0.6964
ogbn-arxiv,dgl,1,1698,22.6069,0.6964

epoch:1700/50, training loss:0.94312584400177
Train Acc 0.7119
 Acc 0.6964
ogbn-arxiv,dgl,1,1699,22.6194,0.6964

epoch:1701/50, training loss:0.9430515170097351
Train Acc 0.7118
 Acc 0.6966
ogbn-arxiv,dgl,1,1700,22.6318,0.6966

epoch:1702/50, training loss:0.9429765939712524
Train Acc 0.7120
 Acc 0.6963
ogbn-arxiv,dgl,1,1701,22.6443,0.6963

epoch:1703/50, training loss:0.9429023861885071
Train Acc 0.7119
 Acc 0.6964
ogbn-arxiv,dgl,1,1702,22.6567,0.6964

epoch:1704/50, training loss:0.9428278207778931
Train Acc 0.7120
 Acc 0.6962
ogbn-arxiv,dgl,1,1703,22.6692,0.6962

epoch:1705/50, training loss:0.9427533149719238
Train Acc 0.7120
 Acc 0.6967
ogbn-arxiv,dgl,1,1704,22.6816,0.6967

epoch:1706/50, training loss:0.9426808953285217
Train Acc 0.7120
 Acc 0.6963
ogbn-arxiv,dgl,1,1705,22.6941,0.6963

epoch:1707/50, training loss:0.942607045173645
Train Acc 0.7120
 Acc 0.6962
ogbn-arxiv,dgl,1,1706,22.7065,0.6962

epoch:1708/50, training loss:0.9425298571586609
Train Acc 0.7120
 Acc 0.6970
new best val f1: 0.6969865496714659
ogbn-arxiv,dgl,1,1707,22.7190,0.6970

epoch:1709/50, training loss:0.9424620866775513
Train Acc 0.7121
 Acc 0.6961
ogbn-arxiv,dgl,1,1708,22.7314,0.6961

epoch:1710/50, training loss:0.9423820376396179
Train Acc 0.7119
 Acc 0.6960
ogbn-arxiv,dgl,1,1709,22.7439,0.6960

epoch:1711/50, training loss:0.9423084259033203
Train Acc 0.7120
 Acc 0.6972
new best val f1: 0.6971513316443181
ogbn-arxiv,dgl,1,1710,22.7562,0.6972

epoch:1712/50, training loss:0.9422446489334106
Train Acc 0.7122
 Acc 0.6961
ogbn-arxiv,dgl,1,1711,22.7687,0.6961

epoch:1713/50, training loss:0.9421619176864624
Train Acc 0.7121
 Acc 0.6962
ogbn-arxiv,dgl,1,1712,22.7811,0.6962

epoch:1714/50, training loss:0.9420855641365051
Train Acc 0.7121
 Acc 0.6970
ogbn-arxiv,dgl,1,1713,22.7935,0.6970

epoch:1715/50, training loss:0.9420216679573059
Train Acc 0.7122
 Acc 0.6962
ogbn-arxiv,dgl,1,1714,22.8059,0.6962

epoch:1716/50, training loss:0.9419422745704651
Train Acc 0.7121
 Acc 0.6961
ogbn-arxiv,dgl,1,1715,22.8185,0.6961

epoch:1717/50, training loss:0.9418630599975586
Train Acc 0.7120
 Acc 0.6970
ogbn-arxiv,dgl,1,1716,22.8308,0.6970

epoch:1718/50, training loss:0.941796064376831
Train Acc 0.7122
 Acc 0.6963
ogbn-arxiv,dgl,1,1717,22.8434,0.6963

epoch:1719/50, training loss:0.9417181611061096
Train Acc 0.7121
 Acc 0.6960
ogbn-arxiv,dgl,1,1718,22.8557,0.6960

epoch:1720/50, training loss:0.9416443705558777
Train Acc 0.7121
 Acc 0.6966
ogbn-arxiv,dgl,1,1719,22.8682,0.6966

epoch:1721/50, training loss:0.9415731430053711
Train Acc 0.7121
 Acc 0.6964
ogbn-arxiv,dgl,1,1720,22.8805,0.6964

epoch:1722/50, training loss:0.9415020942687988
Train Acc 0.7121
 Acc 0.6965
ogbn-arxiv,dgl,1,1721,22.8931,0.6965

epoch:1723/50, training loss:0.9414178729057312
Train Acc 0.7120
 Acc 0.6965
ogbn-arxiv,dgl,1,1722,22.9055,0.6965

epoch:1724/50, training loss:0.9413498640060425
Train Acc 0.7120
 Acc 0.6963
ogbn-arxiv,dgl,1,1723,22.9180,0.6963

epoch:1725/50, training loss:0.9412756562232971
Train Acc 0.7122
 Acc 0.6963
ogbn-arxiv,dgl,1,1724,22.9304,0.6963

epoch:1726/50, training loss:0.9411954879760742
Train Acc 0.7121
 Acc 0.6966
ogbn-arxiv,dgl,1,1725,22.9430,0.6966

epoch:1727/50, training loss:0.9411349296569824
Train Acc 0.7122
 Acc 0.6961
ogbn-arxiv,dgl,1,1726,22.9553,0.6961

epoch:1728/50, training loss:0.9410616159439087
Train Acc 0.7122
 Acc 0.6967
ogbn-arxiv,dgl,1,1727,22.9678,0.6967

epoch:1729/50, training loss:0.9409769177436829
Train Acc 0.7122
 Acc 0.6967
ogbn-arxiv,dgl,1,1728,22.9802,0.6967

epoch:1730/50, training loss:0.940909743309021
Train Acc 0.7122
 Acc 0.6959
ogbn-arxiv,dgl,1,1729,22.9926,0.6959

epoch:1731/50, training loss:0.9408371448516846
Train Acc 0.7121
 Acc 0.6966
ogbn-arxiv,dgl,1,1730,23.0049,0.6966

epoch:1732/50, training loss:0.9407591819763184
Train Acc 0.7122
 Acc 0.6967
ogbn-arxiv,dgl,1,1731,23.0175,0.6967

epoch:1733/50, training loss:0.94068843126297
Train Acc 0.7123
 Acc 0.6958
ogbn-arxiv,dgl,1,1732,23.0299,0.6958

epoch:1734/50, training loss:0.9406213760375977
Train Acc 0.7121
 Acc 0.6967
ogbn-arxiv,dgl,1,1733,23.0424,0.6967

epoch:1735/50, training loss:0.940537691116333
Train Acc 0.7123
 Acc 0.6970
ogbn-arxiv,dgl,1,1734,23.0548,0.6970

epoch:1736/50, training loss:0.9404695630073547
Train Acc 0.7124
 Acc 0.6958
ogbn-arxiv,dgl,1,1735,23.0673,0.6958

epoch:1737/50, training loss:0.9403985142707825
Train Acc 0.7121
 Acc 0.6965
ogbn-arxiv,dgl,1,1736,23.0797,0.6965

epoch:1738/50, training loss:0.9403175115585327
Train Acc 0.7123
 Acc 0.6972
ogbn-arxiv,dgl,1,1737,23.0923,0.6972

epoch:1739/50, training loss:0.9402521252632141
Train Acc 0.7124
 Acc 0.6958
ogbn-arxiv,dgl,1,1738,23.1046,0.6958

epoch:1740/50, training loss:0.9401847720146179
Train Acc 0.7122
 Acc 0.6966
ogbn-arxiv,dgl,1,1739,23.1171,0.6966

epoch:1741/50, training loss:0.9400981068611145
Train Acc 0.7124
 Acc 0.6972
new best val f1: 0.6972131248841377
ogbn-arxiv,dgl,1,1740,23.1295,0.6972

epoch:1742/50, training loss:0.9400356411933899
Train Acc 0.7125
 Acc 0.6958
ogbn-arxiv,dgl,1,1741,23.1421,0.6958

epoch:1743/50, training loss:0.9399663805961609
Train Acc 0.7121
 Acc 0.6967
ogbn-arxiv,dgl,1,1742,23.1545,0.6967

epoch:1744/50, training loss:0.9398806691169739
Train Acc 0.7125
 Acc 0.6973
new best val f1: 0.6972543203773507
ogbn-arxiv,dgl,1,1743,23.1670,0.6973

epoch:1745/50, training loss:0.9398161768913269
Train Acc 0.7126
 Acc 0.6961
ogbn-arxiv,dgl,1,1744,23.1793,0.6961

epoch:1746/50, training loss:0.9397379159927368
Train Acc 0.7122
 Acc 0.6965
ogbn-arxiv,dgl,1,1745,23.1918,0.6965

epoch:1747/50, training loss:0.9396637678146362
Train Acc 0.7123
 Acc 0.6971
ogbn-arxiv,dgl,1,1746,23.2042,0.6971

epoch:1748/50, training loss:0.9395930171012878
Train Acc 0.7126
 Acc 0.6963
ogbn-arxiv,dgl,1,1747,23.2167,0.6963

epoch:1749/50, training loss:0.9395187497138977
Train Acc 0.7124
 Acc 0.6966
ogbn-arxiv,dgl,1,1748,23.2290,0.6966

epoch:1750/50, training loss:0.9394434094429016
Train Acc 0.7124
 Acc 0.6970
ogbn-arxiv,dgl,1,1749,23.2415,0.6970

epoch:1751/50, training loss:0.9393734931945801
Train Acc 0.7126
 Acc 0.6963
ogbn-arxiv,dgl,1,1750,23.2539,0.6963

epoch:1752/50, training loss:0.9393026828765869
Train Acc 0.7124
 Acc 0.6967
ogbn-arxiv,dgl,1,1751,23.2664,0.6967

epoch:1753/50, training loss:0.9392262101173401
Train Acc 0.7124
 Acc 0.6970
ogbn-arxiv,dgl,1,1752,23.2788,0.6970

epoch:1754/50, training loss:0.9391555190086365
Train Acc 0.7126
 Acc 0.6964
ogbn-arxiv,dgl,1,1753,23.2912,0.6964

epoch:1755/50, training loss:0.9390845894813538
Train Acc 0.7125
 Acc 0.6967
ogbn-arxiv,dgl,1,1754,23.3035,0.6967

epoch:1756/50, training loss:0.9390066266059875
Train Acc 0.7125
 Acc 0.6972
ogbn-arxiv,dgl,1,1755,23.3161,0.6972

epoch:1757/50, training loss:0.938936173915863
Train Acc 0.7127
 Acc 0.6966
ogbn-arxiv,dgl,1,1756,23.3285,0.6966

epoch:1758/50, training loss:0.9388627409934998
Train Acc 0.7125
 Acc 0.6964
ogbn-arxiv,dgl,1,1757,23.3410,0.6964

epoch:1759/50, training loss:0.9387893676757812
Train Acc 0.7124
 Acc 0.6971
ogbn-arxiv,dgl,1,1758,23.3533,0.6971

epoch:1760/50, training loss:0.938718318939209
Train Acc 0.7126
 Acc 0.6969
ogbn-arxiv,dgl,1,1759,23.3657,0.6969

epoch:1761/50, training loss:0.9386449456214905
Train Acc 0.7126
 Acc 0.6966
ogbn-arxiv,dgl,1,1760,23.3782,0.6966

epoch:1762/50, training loss:0.9385709166526794
Train Acc 0.7125
 Acc 0.6969
ogbn-arxiv,dgl,1,1761,23.3907,0.6969

epoch:1763/50, training loss:0.9385002851486206
Train Acc 0.7126
 Acc 0.6970
ogbn-arxiv,dgl,1,1762,23.4032,0.6970

epoch:1764/50, training loss:0.9384270906448364
Train Acc 0.7127
 Acc 0.6965
ogbn-arxiv,dgl,1,1763,23.4158,0.6965

epoch:1765/50, training loss:0.938355028629303
Train Acc 0.7125
 Acc 0.6968
ogbn-arxiv,dgl,1,1764,23.4282,0.6968

epoch:1766/50, training loss:0.9382808208465576
Train Acc 0.7125
 Acc 0.6972
ogbn-arxiv,dgl,1,1765,23.4408,0.6972

epoch:1767/50, training loss:0.9382097721099854
Train Acc 0.7128
 Acc 0.6965
ogbn-arxiv,dgl,1,1766,23.4532,0.6965

epoch:1768/50, training loss:0.9381367564201355
Train Acc 0.7126
 Acc 0.6968
ogbn-arxiv,dgl,1,1767,23.4659,0.6968

epoch:1769/50, training loss:0.9380641579627991
Train Acc 0.7126
 Acc 0.6972
ogbn-arxiv,dgl,1,1768,23.4783,0.6972

epoch:1770/50, training loss:0.9379928708076477
Train Acc 0.7128
 Acc 0.6966
ogbn-arxiv,dgl,1,1769,23.4908,0.6966

epoch:1771/50, training loss:0.9379222393035889
Train Acc 0.7127
 Acc 0.6970
ogbn-arxiv,dgl,1,1770,23.5032,0.6970

epoch:1772/50, training loss:0.9378453493118286
Train Acc 0.7127
 Acc 0.6971
ogbn-arxiv,dgl,1,1771,23.5157,0.6971

epoch:1773/50, training loss:0.9377785921096802
Train Acc 0.7127
 Acc 0.6968
ogbn-arxiv,dgl,1,1772,23.5282,0.6968

epoch:1774/50, training loss:0.9377018809318542
Train Acc 0.7127
 Acc 0.6966
ogbn-arxiv,dgl,1,1773,23.5408,0.6966

epoch:1775/50, training loss:0.9376331567764282
Train Acc 0.7127
 Acc 0.6973
new best val f1: 0.6973161136171703
ogbn-arxiv,dgl,1,1774,23.5532,0.6973

epoch:1776/50, training loss:0.9375665783882141
Train Acc 0.7128
 Acc 0.6968
ogbn-arxiv,dgl,1,1775,23.5658,0.6968

epoch:1777/50, training loss:0.9374876022338867
Train Acc 0.7127
 Acc 0.6968
ogbn-arxiv,dgl,1,1776,23.5782,0.6968

epoch:1778/50, training loss:0.937419056892395
Train Acc 0.7128
 Acc 0.6973
ogbn-arxiv,dgl,1,1777,23.5908,0.6973

epoch:1779/50, training loss:0.9373461604118347
Train Acc 0.7129
 Acc 0.6967
ogbn-arxiv,dgl,1,1778,23.6032,0.6967

epoch:1780/50, training loss:0.9372705221176147
Train Acc 0.7127
 Acc 0.6967
ogbn-arxiv,dgl,1,1779,23.6158,0.6967

epoch:1781/50, training loss:0.9371992349624634
Train Acc 0.7127
 Acc 0.6972
ogbn-arxiv,dgl,1,1780,23.6283,0.6972

epoch:1782/50, training loss:0.9371278285980225
Train Acc 0.7129
 Acc 0.6970
ogbn-arxiv,dgl,1,1781,23.6408,0.6970

epoch:1783/50, training loss:0.9370547533035278
Train Acc 0.7127
 Acc 0.6967
ogbn-arxiv,dgl,1,1782,23.6532,0.6967

epoch:1784/50, training loss:0.9369896054267883
Train Acc 0.7127
 Acc 0.6971
ogbn-arxiv,dgl,1,1783,23.6658,0.6971

epoch:1785/50, training loss:0.9369118809700012
Train Acc 0.7128
 Acc 0.6972
ogbn-arxiv,dgl,1,1784,23.6782,0.6972

epoch:1786/50, training loss:0.9368396997451782
Train Acc 0.7129
 Acc 0.6968
ogbn-arxiv,dgl,1,1785,23.6907,0.6968

epoch:1787/50, training loss:0.9367676973342896
Train Acc 0.7128
 Acc 0.6971
ogbn-arxiv,dgl,1,1786,23.7031,0.6971

epoch:1788/50, training loss:0.9366945624351501
Train Acc 0.7128
 Acc 0.6971
ogbn-arxiv,dgl,1,1787,23.7156,0.6971

epoch:1789/50, training loss:0.9366236329078674
Train Acc 0.7129
 Acc 0.6969
ogbn-arxiv,dgl,1,1788,23.7280,0.6969

epoch:1790/50, training loss:0.9365517497062683
Train Acc 0.7129
 Acc 0.6970
ogbn-arxiv,dgl,1,1789,23.7406,0.6970

epoch:1791/50, training loss:0.936478316783905
Train Acc 0.7130
 Acc 0.6972
ogbn-arxiv,dgl,1,1790,23.7530,0.6972

epoch:1792/50, training loss:0.9364109635353088
Train Acc 0.7130
 Acc 0.6969
ogbn-arxiv,dgl,1,1791,23.7656,0.6969

epoch:1793/50, training loss:0.9363367557525635
Train Acc 0.7129
 Acc 0.6971
ogbn-arxiv,dgl,1,1792,23.7780,0.6971

epoch:1794/50, training loss:0.9362635612487793
Train Acc 0.7130
 Acc 0.6974
new best val f1: 0.6974191023502029
ogbn-arxiv,dgl,1,1793,23.7906,0.6974

epoch:1795/50, training loss:0.9361943602561951
Train Acc 0.7130
 Acc 0.6972
ogbn-arxiv,dgl,1,1794,23.8030,0.6972

epoch:1796/50, training loss:0.9361221194267273
Train Acc 0.7131
 Acc 0.6971
ogbn-arxiv,dgl,1,1795,23.8155,0.6971

epoch:1797/50, training loss:0.9360454082489014
Train Acc 0.7130
 Acc 0.6971
ogbn-arxiv,dgl,1,1796,23.8280,0.6971

epoch:1798/50, training loss:0.9359769225120544
Train Acc 0.7130
 Acc 0.6971
ogbn-arxiv,dgl,1,1797,23.8407,0.6971

epoch:1799/50, training loss:0.9359037280082703
Train Acc 0.7131
 Acc 0.6972
ogbn-arxiv,dgl,1,1798,23.8531,0.6972

epoch:1800/50, training loss:0.9358326196670532
Train Acc 0.7130
 Acc 0.6975
new best val f1: 0.6974808955900225
ogbn-arxiv,dgl,1,1799,23.8656,0.6975

epoch:1801/50, training loss:0.9357643127441406
Train Acc 0.7131
 Acc 0.6971
ogbn-arxiv,dgl,1,1800,23.8781,0.6971

epoch:1802/50, training loss:0.9356920123100281
Train Acc 0.7130
 Acc 0.6972
ogbn-arxiv,dgl,1,1801,23.8907,0.6972

epoch:1803/50, training loss:0.9356186985969543
Train Acc 0.7131
 Acc 0.6974
ogbn-arxiv,dgl,1,1802,23.9031,0.6974

epoch:1804/50, training loss:0.9355503916740417
Train Acc 0.7131
 Acc 0.6969
ogbn-arxiv,dgl,1,1803,23.9156,0.6969

epoch:1805/50, training loss:0.9354776740074158
Train Acc 0.7131
 Acc 0.6973
ogbn-arxiv,dgl,1,1804,23.9280,0.6973

epoch:1806/50, training loss:0.9354045391082764
Train Acc 0.7132
 Acc 0.6973
ogbn-arxiv,dgl,1,1805,23.9406,0.6973

epoch:1807/50, training loss:0.9353343844413757
Train Acc 0.7131
 Acc 0.6971
ogbn-arxiv,dgl,1,1806,23.9531,0.6971

epoch:1808/50, training loss:0.9352598190307617
Train Acc 0.7131
 Acc 0.6973
ogbn-arxiv,dgl,1,1807,23.9657,0.6973

epoch:1809/50, training loss:0.9351893663406372
Train Acc 0.7131
 Acc 0.6975
new best val f1: 0.697542688829842
ogbn-arxiv,dgl,1,1808,23.9781,0.6975

epoch:1810/50, training loss:0.9351189732551575
Train Acc 0.7132
 Acc 0.6970
ogbn-arxiv,dgl,1,1809,23.9906,0.6970

epoch:1811/50, training loss:0.935046374797821
Train Acc 0.7131
 Acc 0.6973
ogbn-arxiv,dgl,1,1810,24.0031,0.6973

epoch:1812/50, training loss:0.9349744915962219
Train Acc 0.7132
 Acc 0.6975
ogbn-arxiv,dgl,1,1811,24.0157,0.6975

epoch:1813/50, training loss:0.9349052309989929
Train Acc 0.7133
 Acc 0.6973
ogbn-arxiv,dgl,1,1812,24.0281,0.6973

epoch:1814/50, training loss:0.9348313808441162
Train Acc 0.7133
 Acc 0.6973
ogbn-arxiv,dgl,1,1813,24.0406,0.6973

epoch:1815/50, training loss:0.9347595572471619
Train Acc 0.7133
 Acc 0.6974
ogbn-arxiv,dgl,1,1814,24.0531,0.6974

epoch:1816/50, training loss:0.9346888661384583
Train Acc 0.7132
 Acc 0.6974
ogbn-arxiv,dgl,1,1815,24.0657,0.6974

epoch:1817/50, training loss:0.9346174597740173
Train Acc 0.7133
 Acc 0.6973
ogbn-arxiv,dgl,1,1816,24.0781,0.6973

epoch:1818/50, training loss:0.9345418810844421
Train Acc 0.7133
 Acc 0.6973
ogbn-arxiv,dgl,1,1817,24.0906,0.6973

epoch:1819/50, training loss:0.9344742298126221
Train Acc 0.7132
 Acc 0.6974
ogbn-arxiv,dgl,1,1818,24.1030,0.6974

epoch:1820/50, training loss:0.9344025254249573
Train Acc 0.7133
 Acc 0.6974
ogbn-arxiv,dgl,1,1819,24.1156,0.6974

epoch:1821/50, training loss:0.934332013130188
Train Acc 0.7134
 Acc 0.6977
new best val f1: 0.6976868730560877
ogbn-arxiv,dgl,1,1820,24.1281,0.6977

epoch:1822/50, training loss:0.934263288974762
Train Acc 0.7134
 Acc 0.6970
ogbn-arxiv,dgl,1,1821,24.1406,0.6970

epoch:1823/50, training loss:0.934191107749939
Train Acc 0.7133
 Acc 0.6974
ogbn-arxiv,dgl,1,1822,24.1529,0.6974

epoch:1824/50, training loss:0.9341159462928772
Train Acc 0.7134
 Acc 0.6975
ogbn-arxiv,dgl,1,1823,24.1655,0.6975

epoch:1825/50, training loss:0.934048593044281
Train Acc 0.7134
 Acc 0.6972
ogbn-arxiv,dgl,1,1824,24.1780,0.6972

epoch:1826/50, training loss:0.933976948261261
Train Acc 0.7133
 Acc 0.6975
ogbn-arxiv,dgl,1,1825,24.1909,0.6975

epoch:1827/50, training loss:0.9339033961296082
Train Acc 0.7134
 Acc 0.6975
ogbn-arxiv,dgl,1,1826,24.2033,0.6975

epoch:1828/50, training loss:0.9338319897651672
Train Acc 0.7134
 Acc 0.6973
ogbn-arxiv,dgl,1,1827,24.2158,0.6973

epoch:1829/50, training loss:0.9337601065635681
Train Acc 0.7134
 Acc 0.6975
ogbn-arxiv,dgl,1,1828,24.2282,0.6975

epoch:1830/50, training loss:0.9336913824081421
Train Acc 0.7135
 Acc 0.6973
ogbn-arxiv,dgl,1,1829,24.2408,0.6973

epoch:1831/50, training loss:0.9336171746253967
Train Acc 0.7134
 Acc 0.6973
ogbn-arxiv,dgl,1,1830,24.2532,0.6973

epoch:1832/50, training loss:0.9335485100746155
Train Acc 0.7134
 Acc 0.6977
new best val f1: 0.6977280685493007
ogbn-arxiv,dgl,1,1831,24.2657,0.6977

epoch:1833/50, training loss:0.9334771037101746
Train Acc 0.7136
 Acc 0.6973
ogbn-arxiv,dgl,1,1832,24.2780,0.6973

epoch:1834/50, training loss:0.9334028959274292
Train Acc 0.7134
 Acc 0.6972
ogbn-arxiv,dgl,1,1833,24.2906,0.6972

epoch:1835/50, training loss:0.9333327412605286
Train Acc 0.7135
 Acc 0.6975
ogbn-arxiv,dgl,1,1834,24.3030,0.6975

epoch:1836/50, training loss:0.9332621097564697
Train Acc 0.7135
 Acc 0.6976
ogbn-arxiv,dgl,1,1835,24.3156,0.6976

epoch:1837/50, training loss:0.933189868927002
Train Acc 0.7135
 Acc 0.6974
ogbn-arxiv,dgl,1,1836,24.3281,0.6974

epoch:1838/50, training loss:0.9331192374229431
Train Acc 0.7135
 Acc 0.6974
ogbn-arxiv,dgl,1,1837,24.3406,0.6974

epoch:1839/50, training loss:0.9330477714538574
Train Acc 0.7136
 Acc 0.6976
ogbn-arxiv,dgl,1,1838,24.3531,0.6976

epoch:1840/50, training loss:0.9329772591590881
Train Acc 0.7135
 Acc 0.6974
ogbn-arxiv,dgl,1,1839,24.3656,0.6974

epoch:1841/50, training loss:0.9329060912132263
Train Acc 0.7136
 Acc 0.6973
ogbn-arxiv,dgl,1,1840,24.3780,0.6973

epoch:1842/50, training loss:0.9328352212905884
Train Acc 0.7135
 Acc 0.6976
ogbn-arxiv,dgl,1,1841,24.3907,0.6976

epoch:1843/50, training loss:0.9327647089958191
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1842,24.4032,0.6974

epoch:1844/50, training loss:0.9326924681663513
Train Acc 0.7136
 Acc 0.6974
ogbn-arxiv,dgl,1,1843,24.4157,0.6974

epoch:1845/50, training loss:0.9326185584068298
Train Acc 0.7135
 Acc 0.6976
ogbn-arxiv,dgl,1,1844,24.4282,0.6976

epoch:1846/50, training loss:0.932549774646759
Train Acc 0.7136
 Acc 0.6974
ogbn-arxiv,dgl,1,1845,24.4407,0.6974

epoch:1847/50, training loss:0.9324779510498047
Train Acc 0.7136
 Acc 0.6975
ogbn-arxiv,dgl,1,1846,24.4531,0.6975

epoch:1848/50, training loss:0.9324036240577698
Train Acc 0.7136
 Acc 0.6976
ogbn-arxiv,dgl,1,1847,24.4656,0.6976

epoch:1849/50, training loss:0.9323333501815796
Train Acc 0.7136
 Acc 0.6974
ogbn-arxiv,dgl,1,1848,24.4781,0.6974

epoch:1850/50, training loss:0.9322649836540222
Train Acc 0.7137
 Acc 0.6976
ogbn-arxiv,dgl,1,1849,24.4906,0.6976

epoch:1851/50, training loss:0.9321900010108948
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1850,24.5030,0.6974

epoch:1852/50, training loss:0.9321202039718628
Train Acc 0.7136
 Acc 0.6975
ogbn-arxiv,dgl,1,1851,24.5156,0.6975

epoch:1853/50, training loss:0.9320465922355652
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1852,24.5280,0.6974

epoch:1854/50, training loss:0.9319773316383362
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1853,24.5407,0.6974

epoch:1855/50, training loss:0.9319052696228027
Train Acc 0.7136
 Acc 0.6974
ogbn-arxiv,dgl,1,1854,24.5532,0.6974

epoch:1856/50, training loss:0.9318345785140991
Train Acc 0.7136
 Acc 0.6974
ogbn-arxiv,dgl,1,1855,24.5657,0.6974

epoch:1857/50, training loss:0.9317625761032104
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1856,24.5781,0.6974

epoch:1858/50, training loss:0.9316883087158203
Train Acc 0.7136
 Acc 0.6975
ogbn-arxiv,dgl,1,1857,24.5906,0.6975

epoch:1859/50, training loss:0.9316198229789734
Train Acc 0.7137
 Acc 0.6976
ogbn-arxiv,dgl,1,1858,24.6031,0.6976

epoch:1860/50, training loss:0.9315486550331116
Train Acc 0.7138
 Acc 0.6975
ogbn-arxiv,dgl,1,1859,24.6156,0.6975

epoch:1861/50, training loss:0.9314761161804199
Train Acc 0.7137
 Acc 0.6975
ogbn-arxiv,dgl,1,1860,24.6281,0.6975

epoch:1862/50, training loss:0.9314069151878357
Train Acc 0.7137
 Acc 0.6975
ogbn-arxiv,dgl,1,1861,24.6407,0.6975

epoch:1863/50, training loss:0.9313349723815918
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1862,24.6531,0.6974

epoch:1864/50, training loss:0.9312639236450195
Train Acc 0.7138
 Acc 0.6977
ogbn-arxiv,dgl,1,1863,24.6657,0.6977

epoch:1865/50, training loss:0.93119215965271
Train Acc 0.7139
 Acc 0.6975
ogbn-arxiv,dgl,1,1864,24.6782,0.6975

epoch:1866/50, training loss:0.931118369102478
Train Acc 0.7137
 Acc 0.6974
ogbn-arxiv,dgl,1,1865,24.6908,0.6974

epoch:1867/50, training loss:0.9310488104820251
Train Acc 0.7138
 Acc 0.6977
new best val f1: 0.6977486662959073
ogbn-arxiv,dgl,1,1866,24.7033,0.6977

epoch:1868/50, training loss:0.9309777617454529
Train Acc 0.7138
 Acc 0.6976
ogbn-arxiv,dgl,1,1867,24.7158,0.6976

epoch:1869/50, training loss:0.9309055209159851
Train Acc 0.7139
 Acc 0.6975
ogbn-arxiv,dgl,1,1868,24.7281,0.6975

epoch:1870/50, training loss:0.9308347105979919
Train Acc 0.7139
 Acc 0.6979
new best val f1: 0.6978928505221529
ogbn-arxiv,dgl,1,1869,24.7407,0.6979

epoch:1871/50, training loss:0.9307654500007629
Train Acc 0.7139
 Acc 0.6976
ogbn-arxiv,dgl,1,1870,24.7531,0.6976

epoch:1872/50, training loss:0.9306914806365967
Train Acc 0.7138
 Acc 0.6975
ogbn-arxiv,dgl,1,1871,24.7656,0.6975

epoch:1873/50, training loss:0.9306188821792603
Train Acc 0.7138
 Acc 0.6978
ogbn-arxiv,dgl,1,1872,24.7779,0.6978

epoch:1874/50, training loss:0.9305502772331238
Train Acc 0.7139
 Acc 0.6976
ogbn-arxiv,dgl,1,1873,24.7904,0.6976

epoch:1875/50, training loss:0.9304774403572083
Train Acc 0.7139
 Acc 0.6977
ogbn-arxiv,dgl,1,1874,24.8029,0.6977

epoch:1876/50, training loss:0.930403470993042
Train Acc 0.7140
 Acc 0.6979
new best val f1: 0.6979134482687595
ogbn-arxiv,dgl,1,1875,24.8154,0.6979

epoch:1877/50, training loss:0.9303333759307861
Train Acc 0.7140
 Acc 0.6976
ogbn-arxiv,dgl,1,1876,24.8278,0.6976

epoch:1878/50, training loss:0.9302643537521362
Train Acc 0.7138
 Acc 0.6976
ogbn-arxiv,dgl,1,1877,24.8403,0.6976

epoch:1879/50, training loss:0.9301931262016296
Train Acc 0.7140
 Acc 0.6978
ogbn-arxiv,dgl,1,1878,24.8527,0.6978

epoch:1880/50, training loss:0.9301242232322693
Train Acc 0.7140
 Acc 0.6976
ogbn-arxiv,dgl,1,1879,24.8652,0.6976

epoch:1881/50, training loss:0.9300500154495239
Train Acc 0.7140
 Acc 0.6975
ogbn-arxiv,dgl,1,1880,24.8776,0.6975

epoch:1882/50, training loss:0.9299820065498352
Train Acc 0.7139
 Acc 0.6979
ogbn-arxiv,dgl,1,1881,24.8901,0.6979

epoch:1883/50, training loss:0.9299122095108032
Train Acc 0.7140
 Acc 0.6978
ogbn-arxiv,dgl,1,1882,24.9025,0.6978

epoch:1884/50, training loss:0.9298354387283325
Train Acc 0.7141
 Acc 0.6977
ogbn-arxiv,dgl,1,1883,24.9151,0.6977

epoch:1885/50, training loss:0.9297735691070557
Train Acc 0.7140
 Acc 0.6980
new best val f1: 0.6979958392551855
ogbn-arxiv,dgl,1,1884,24.9275,0.6980

epoch:1886/50, training loss:0.9297061562538147
Train Acc 0.7141
 Acc 0.6976
ogbn-arxiv,dgl,1,1885,24.9401,0.6976

epoch:1887/50, training loss:0.9296226501464844
Train Acc 0.7140
 Acc 0.6976
ogbn-arxiv,dgl,1,1886,24.9526,0.6976

epoch:1888/50, training loss:0.9295604228973389
Train Acc 0.7140
 Acc 0.6981
new best val f1: 0.6981194257348247
ogbn-arxiv,dgl,1,1887,24.9652,0.6981

epoch:1889/50, training loss:0.9294957518577576
Train Acc 0.7142
 Acc 0.6977
ogbn-arxiv,dgl,1,1888,24.9776,0.6977

epoch:1890/50, training loss:0.9294108152389526
Train Acc 0.7141
 Acc 0.6976
ogbn-arxiv,dgl,1,1889,24.9901,0.6976

epoch:1891/50, training loss:0.9293515086174011
Train Acc 0.7140
 Acc 0.6980
ogbn-arxiv,dgl,1,1890,25.0025,0.6980

epoch:1892/50, training loss:0.9292816519737244
Train Acc 0.7142
 Acc 0.6977
ogbn-arxiv,dgl,1,1891,25.0150,0.6977

epoch:1893/50, training loss:0.9291972517967224
Train Acc 0.7141
 Acc 0.6975
ogbn-arxiv,dgl,1,1892,25.0274,0.6975

epoch:1894/50, training loss:0.9291408658027649
Train Acc 0.7140
 Acc 0.6980
ogbn-arxiv,dgl,1,1893,25.0399,0.6980

epoch:1895/50, training loss:0.9290674924850464
Train Acc 0.7142
 Acc 0.6975
ogbn-arxiv,dgl,1,1894,25.0522,0.6975

epoch:1896/50, training loss:0.9289851188659668
Train Acc 0.7141
 Acc 0.6976
ogbn-arxiv,dgl,1,1895,25.0647,0.6976

epoch:1897/50, training loss:0.9289225935935974
Train Acc 0.7141
 Acc 0.6979
ogbn-arxiv,dgl,1,1896,25.0771,0.6979

epoch:1898/50, training loss:0.9288455247879028
Train Acc 0.7142
 Acc 0.6979
ogbn-arxiv,dgl,1,1897,25.0896,0.6979

epoch:1899/50, training loss:0.9287738800048828
Train Acc 0.7142
 Acc 0.6975
ogbn-arxiv,dgl,1,1898,25.1019,0.6975

epoch:1900/50, training loss:0.9287089109420776
Train Acc 0.7141
 Acc 0.6981
ogbn-arxiv,dgl,1,1899,25.1144,0.6981

epoch:1901/50, training loss:0.9286333322525024
Train Acc 0.7142
 Acc 0.6978
ogbn-arxiv,dgl,1,1900,25.1268,0.6978

epoch:1902/50, training loss:0.9285598993301392
Train Acc 0.7141
 Acc 0.6976
ogbn-arxiv,dgl,1,1901,25.1394,0.6976

epoch:1903/50, training loss:0.9284964203834534
Train Acc 0.7142
 Acc 0.6983
new best val f1: 0.6982842077076767
ogbn-arxiv,dgl,1,1902,25.1518,0.6983

epoch:1904/50, training loss:0.9284252524375916
Train Acc 0.7143
 Acc 0.6977
ogbn-arxiv,dgl,1,1903,25.1643,0.6977

epoch:1905/50, training loss:0.928345263004303
Train Acc 0.7142
 Acc 0.6975
ogbn-arxiv,dgl,1,1904,25.1767,0.6975

epoch:1906/50, training loss:0.9282821416854858
Train Acc 0.7141
 Acc 0.6981
ogbn-arxiv,dgl,1,1905,25.1892,0.6981

epoch:1907/50, training loss:0.928209662437439
Train Acc 0.7143
 Acc 0.6978
ogbn-arxiv,dgl,1,1906,25.2015,0.6978

epoch:1908/50, training loss:0.9281353950500488
Train Acc 0.7141
 Acc 0.6976
ogbn-arxiv,dgl,1,1907,25.2140,0.6976

epoch:1909/50, training loss:0.928071141242981
Train Acc 0.7143
 Acc 0.6981
ogbn-arxiv,dgl,1,1908,25.2264,0.6981

epoch:1910/50, training loss:0.9280003905296326
Train Acc 0.7143
 Acc 0.6977
ogbn-arxiv,dgl,1,1909,25.2389,0.6977

epoch:1911/50, training loss:0.9279205799102783
Train Acc 0.7142
 Acc 0.6977
ogbn-arxiv,dgl,1,1910,25.2512,0.6977

epoch:1912/50, training loss:0.9278569221496582
Train Acc 0.7143
 Acc 0.6981
ogbn-arxiv,dgl,1,1911,25.2637,0.6981

epoch:1913/50, training loss:0.9277846813201904
Train Acc 0.7143
 Acc 0.6977
ogbn-arxiv,dgl,1,1912,25.2761,0.6977

epoch:1914/50, training loss:0.9277094602584839
Train Acc 0.7142
 Acc 0.6976
ogbn-arxiv,dgl,1,1913,25.2886,0.6976

epoch:1915/50, training loss:0.9276476502418518
Train Acc 0.7143
 Acc 0.6982
ogbn-arxiv,dgl,1,1914,25.3010,0.6982

epoch:1916/50, training loss:0.9275776147842407
Train Acc 0.7143
 Acc 0.6977
ogbn-arxiv,dgl,1,1915,25.3135,0.6977

epoch:1917/50, training loss:0.9274960160255432
Train Acc 0.7143
 Acc 0.6978
ogbn-arxiv,dgl,1,1916,25.3259,0.6978

epoch:1918/50, training loss:0.9274308681488037
Train Acc 0.7144
 Acc 0.6981
ogbn-arxiv,dgl,1,1917,25.3384,0.6981

epoch:1919/50, training loss:0.9273598194122314
Train Acc 0.7143
 Acc 0.6977
ogbn-arxiv,dgl,1,1918,25.3508,0.6977

epoch:1920/50, training loss:0.9272856116294861
Train Acc 0.7143
 Acc 0.6976
ogbn-arxiv,dgl,1,1919,25.3633,0.6976

epoch:1921/50, training loss:0.9272189736366272
Train Acc 0.7143
 Acc 0.6984
new best val f1: 0.6984283919339225
ogbn-arxiv,dgl,1,1920,25.3757,0.6984

epoch:1922/50, training loss:0.9271450042724609
Train Acc 0.7144
 Acc 0.6975
ogbn-arxiv,dgl,1,1921,25.3882,0.6975

epoch:1923/50, training loss:0.927070677280426
Train Acc 0.7142
 Acc 0.6977
ogbn-arxiv,dgl,1,1922,25.4006,0.6977

epoch:1924/50, training loss:0.9270046949386597
Train Acc 0.7143
 Acc 0.6983
ogbn-arxiv,dgl,1,1923,25.4131,0.6983

epoch:1925/50, training loss:0.9269334077835083
Train Acc 0.7145
 Acc 0.6976
ogbn-arxiv,dgl,1,1924,25.4255,0.6976

epoch:1926/50, training loss:0.9268576502799988
Train Acc 0.7143
 Acc 0.6977
ogbn-arxiv,dgl,1,1925,25.4381,0.6977

epoch:1927/50, training loss:0.9267938137054443
Train Acc 0.7144
 Acc 0.6986
new best val f1: 0.6985931739067746
ogbn-arxiv,dgl,1,1926,25.4505,0.6986

epoch:1928/50, training loss:0.9267315864562988
Train Acc 0.7145
 Acc 0.6975
ogbn-arxiv,dgl,1,1927,25.4634,0.6975

epoch:1929/50, training loss:0.9266486167907715
Train Acc 0.7144
 Acc 0.6977
ogbn-arxiv,dgl,1,1928,25.4758,0.6977

epoch:1930/50, training loss:0.926577627658844
Train Acc 0.7144
 Acc 0.6987
new best val f1: 0.6987373581330203
ogbn-arxiv,dgl,1,1929,25.4883,0.6987

epoch:1931/50, training loss:0.9265145063400269
Train Acc 0.7146
 Acc 0.6978
ogbn-arxiv,dgl,1,1930,25.5006,0.6978

epoch:1932/50, training loss:0.9264360070228577
Train Acc 0.7144
 Acc 0.6979
ogbn-arxiv,dgl,1,1931,25.5132,0.6979

epoch:1933/50, training loss:0.9263632297515869
Train Acc 0.7145
 Acc 0.6986
ogbn-arxiv,dgl,1,1932,25.5255,0.6986

epoch:1934/50, training loss:0.9262980818748474
Train Acc 0.7146
 Acc 0.6979
ogbn-arxiv,dgl,1,1933,25.5380,0.6979

epoch:1935/50, training loss:0.9262204766273499
Train Acc 0.7144
 Acc 0.6977
ogbn-arxiv,dgl,1,1934,25.5504,0.6977

epoch:1936/50, training loss:0.9261521697044373
Train Acc 0.7144
 Acc 0.6985
ogbn-arxiv,dgl,1,1935,25.5629,0.6985

epoch:1937/50, training loss:0.9260844588279724
Train Acc 0.7147
 Acc 0.6979
ogbn-arxiv,dgl,1,1936,25.5753,0.6979

epoch:1938/50, training loss:0.9260085821151733
Train Acc 0.7145
 Acc 0.6978
ogbn-arxiv,dgl,1,1937,25.5878,0.6978

epoch:1939/50, training loss:0.9259366989135742
Train Acc 0.7145
 Acc 0.6984
ogbn-arxiv,dgl,1,1938,25.6002,0.6984

epoch:1940/50, training loss:0.9258681535720825
Train Acc 0.7146
 Acc 0.6979
ogbn-arxiv,dgl,1,1939,25.6128,0.6979

epoch:1941/50, training loss:0.9257935881614685
Train Acc 0.7144
 Acc 0.6980
ogbn-arxiv,dgl,1,1940,25.6251,0.6980

epoch:1942/50, training loss:0.925726056098938
Train Acc 0.7145
 Acc 0.6984
ogbn-arxiv,dgl,1,1941,25.6375,0.6984

epoch:1943/50, training loss:0.9256575107574463
Train Acc 0.7146
 Acc 0.6979
ogbn-arxiv,dgl,1,1942,25.6499,0.6979

epoch:1944/50, training loss:0.9255846738815308
Train Acc 0.7145
 Acc 0.6980
ogbn-arxiv,dgl,1,1943,25.6624,0.6980

epoch:1945/50, training loss:0.9255189895629883
Train Acc 0.7145
 Acc 0.6983
ogbn-arxiv,dgl,1,1944,25.6748,0.6983

epoch:1946/50, training loss:0.9254464507102966
Train Acc 0.7146
 Acc 0.6981
ogbn-arxiv,dgl,1,1945,25.6873,0.6981

epoch:1947/50, training loss:0.9253750443458557
Train Acc 0.7146
 Acc 0.6979
ogbn-arxiv,dgl,1,1946,25.6997,0.6979

epoch:1948/50, training loss:0.9253082871437073
Train Acc 0.7146
 Acc 0.6984
ogbn-arxiv,dgl,1,1947,25.7122,0.6984

epoch:1949/50, training loss:0.9252380132675171
Train Acc 0.7147
 Acc 0.6979
ogbn-arxiv,dgl,1,1948,25.7246,0.6979

epoch:1950/50, training loss:0.9251610636711121
Train Acc 0.7145
 Acc 0.6979
ogbn-arxiv,dgl,1,1949,25.7371,0.6979

epoch:1951/50, training loss:0.9250957369804382
Train Acc 0.7145
 Acc 0.6986
ogbn-arxiv,dgl,1,1950,25.7494,0.6986

epoch:1952/50, training loss:0.9250229597091675
Train Acc 0.7148
 Acc 0.6983
ogbn-arxiv,dgl,1,1951,25.7620,0.6983

epoch:1953/50, training loss:0.9249508380889893
Train Acc 0.7148
 Acc 0.6979
ogbn-arxiv,dgl,1,1952,25.7743,0.6979

epoch:1954/50, training loss:0.9248839020729065
Train Acc 0.7146
 Acc 0.6984
ogbn-arxiv,dgl,1,1953,25.7869,0.6984

epoch:1955/50, training loss:0.9248101115226746
Train Acc 0.7147
 Acc 0.6984
ogbn-arxiv,dgl,1,1954,25.7992,0.6984

epoch:1956/50, training loss:0.9247440695762634
Train Acc 0.7147
 Acc 0.6977
ogbn-arxiv,dgl,1,1955,25.8117,0.6977

epoch:1957/50, training loss:0.9246765971183777
Train Acc 0.7146
 Acc 0.6986
ogbn-arxiv,dgl,1,1956,25.8241,0.6986

epoch:1958/50, training loss:0.924602210521698
Train Acc 0.7148
 Acc 0.6984
ogbn-arxiv,dgl,1,1957,25.8366,0.6984

epoch:1959/50, training loss:0.9245284795761108
Train Acc 0.7147
 Acc 0.6978
ogbn-arxiv,dgl,1,1958,25.8490,0.6978

epoch:1960/50, training loss:0.9244624376296997
Train Acc 0.7146
 Acc 0.6984
ogbn-arxiv,dgl,1,1959,25.8615,0.6984

epoch:1961/50, training loss:0.9243876934051514
Train Acc 0.7148
 Acc 0.6984
ogbn-arxiv,dgl,1,1960,25.8738,0.6984

epoch:1962/50, training loss:0.9243171811103821
Train Acc 0.7148
 Acc 0.6980
ogbn-arxiv,dgl,1,1961,25.8864,0.6980

epoch:1963/50, training loss:0.9242492318153381
Train Acc 0.7147
 Acc 0.6983
ogbn-arxiv,dgl,1,1962,25.8988,0.6983

epoch:1964/50, training loss:0.9241765141487122
Train Acc 0.7147
 Acc 0.6984
ogbn-arxiv,dgl,1,1963,25.9113,0.6984

epoch:1965/50, training loss:0.9241086840629578
Train Acc 0.7148
 Acc 0.6978
ogbn-arxiv,dgl,1,1964,25.9237,0.6978

epoch:1966/50, training loss:0.924038290977478
Train Acc 0.7147
 Acc 0.6985
ogbn-arxiv,dgl,1,1965,25.9361,0.6985

epoch:1967/50, training loss:0.9239669442176819
Train Acc 0.7149
 Acc 0.6985
ogbn-arxiv,dgl,1,1966,25.9485,0.6985

epoch:1968/50, training loss:0.923897922039032
Train Acc 0.7148
 Acc 0.6981
ogbn-arxiv,dgl,1,1967,25.9611,0.6981

epoch:1969/50, training loss:0.9238262176513672
Train Acc 0.7148
 Acc 0.6983
ogbn-arxiv,dgl,1,1968,25.9734,0.6983

epoch:1970/50, training loss:0.9237563014030457
Train Acc 0.7147
 Acc 0.6985
ogbn-arxiv,dgl,1,1969,25.9860,0.6985

epoch:1971/50, training loss:0.9236865043640137
Train Acc 0.7148
 Acc 0.6982
ogbn-arxiv,dgl,1,1970,25.9983,0.6982

epoch:1972/50, training loss:0.9236165881156921
Train Acc 0.7148
 Acc 0.6981
ogbn-arxiv,dgl,1,1971,26.0108,0.6981

epoch:1973/50, training loss:0.9235498309135437
Train Acc 0.7148
 Acc 0.6985
ogbn-arxiv,dgl,1,1972,26.0230,0.6985

epoch:1974/50, training loss:0.9234775304794312
Train Acc 0.7147
 Acc 0.6980
ogbn-arxiv,dgl,1,1973,26.0357,0.6980

epoch:1975/50, training loss:0.9234089851379395
Train Acc 0.7147
 Acc 0.6981
ogbn-arxiv,dgl,1,1974,26.0481,0.6981

epoch:1976/50, training loss:0.9233348369598389
Train Acc 0.7148
 Acc 0.6986
ogbn-arxiv,dgl,1,1975,26.0606,0.6986

epoch:1977/50, training loss:0.923267126083374
Train Acc 0.7149
 Acc 0.6982
ogbn-arxiv,dgl,1,1976,26.0730,0.6982

epoch:1978/50, training loss:0.923194169998169
Train Acc 0.7149
 Acc 0.6982
ogbn-arxiv,dgl,1,1977,26.0855,0.6982

epoch:1979/50, training loss:0.9231244325637817
Train Acc 0.7148
 Acc 0.6984
ogbn-arxiv,dgl,1,1978,26.0979,0.6984

epoch:1980/50, training loss:0.9230546951293945
Train Acc 0.7149
 Acc 0.6982
ogbn-arxiv,dgl,1,1979,26.1104,0.6982

epoch:1981/50, training loss:0.9229859709739685
Train Acc 0.7149
 Acc 0.6983
ogbn-arxiv,dgl,1,1980,26.1228,0.6983

epoch:1982/50, training loss:0.9229153990745544
Train Acc 0.7149
 Acc 0.6984
ogbn-arxiv,dgl,1,1981,26.1353,0.6984

epoch:1983/50, training loss:0.9228440523147583
Train Acc 0.7149
 Acc 0.6982
ogbn-arxiv,dgl,1,1982,26.1476,0.6982

epoch:1984/50, training loss:0.9227762818336487
Train Acc 0.7149
 Acc 0.6981
ogbn-arxiv,dgl,1,1983,26.1602,0.6981

epoch:1985/50, training loss:0.9227054715156555
Train Acc 0.7149
 Acc 0.6986
ogbn-arxiv,dgl,1,1984,26.1726,0.6986

epoch:1986/50, training loss:0.9226372838020325
Train Acc 0.7150
 Acc 0.6982
ogbn-arxiv,dgl,1,1985,26.1851,0.6982

epoch:1987/50, training loss:0.9225654602050781
Train Acc 0.7149
 Acc 0.6984
ogbn-arxiv,dgl,1,1986,26.1974,0.6984

epoch:1988/50, training loss:0.9224984049797058
Train Acc 0.7149
 Acc 0.6983
ogbn-arxiv,dgl,1,1987,26.2099,0.6983

epoch:1989/50, training loss:0.9224255084991455
Train Acc 0.7149
 Acc 0.6982
ogbn-arxiv,dgl,1,1988,26.2224,0.6982

epoch:1990/50, training loss:0.9223566651344299
Train Acc 0.7150
 Acc 0.6984
ogbn-arxiv,dgl,1,1989,26.2349,0.6984

epoch:1991/50, training loss:0.9222879409790039
Train Acc 0.7151
 Acc 0.6985
ogbn-arxiv,dgl,1,1990,26.2472,0.6985

epoch:1992/50, training loss:0.922217845916748
Train Acc 0.7150
 Acc 0.6982
ogbn-arxiv,dgl,1,1991,26.2597,0.6982

epoch:1993/50, training loss:0.9221488833427429
Train Acc 0.7149
 Acc 0.6981
ogbn-arxiv,dgl,1,1992,26.2720,0.6981

epoch:1994/50, training loss:0.9220780730247498
Train Acc 0.7150
 Acc 0.6982
ogbn-arxiv,dgl,1,1993,26.2845,0.6982

epoch:1995/50, training loss:0.9220080971717834
Train Acc 0.7151
 Acc 0.6985
ogbn-arxiv,dgl,1,1994,26.2969,0.6985

epoch:1996/50, training loss:0.9219405651092529
Train Acc 0.7149
 Acc 0.6982
ogbn-arxiv,dgl,1,1995,26.3093,0.6982

epoch:1997/50, training loss:0.9218705892562866
Train Acc 0.7149
 Acc 0.6980
ogbn-arxiv,dgl,1,1996,26.3217,0.6980

epoch:1998/50, training loss:0.9217956066131592
Train Acc 0.7149
 Acc 0.6984
ogbn-arxiv,dgl,1,1997,26.3342,0.6984

epoch:1999/50, training loss:0.9217334985733032
Train Acc 0.7150
 Acc 0.6983
ogbn-arxiv,dgl,1,1998,26.3465,0.6983

epoch:2000/50, training loss:0.9216630458831787
Train Acc 0.7150
 Acc 0.6980
ogbn-arxiv,dgl,1,1999,26.3591,0.6980

training using time 110.23049068450928
Traceback (most recent call last):
  File "dgl/train_full_load.py", line 340, in <module>
    main(args)
  File "dgl/train_full_load.py", line 317, in main
    model, g, g.ndata['label'], test_mask, False)
TypeError: cannot unpack non-iterable float object
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2121920) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2121920 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     dgl/train_full_load.py FAILED     
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:05:05
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2121920)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

Namespace(csv='full_new.csv', dataset='reddit', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
Inited proc group
Max label: tensor(40)
----Data statistics------'
    #Nodes 232965
    #Edges 114848857
    #Classes/Labels (multi binary labels) 41
    #Train samples 153431
    #Val samples 0
    #Test samples 55703
Running on: 0
GCN(
  (layers): ModuleList(
    (0): GraphConv(in=602, out=128, normalization=both, activation=<function relu at 0x14c4da048f80>)
    (1): GraphConv(in=128, out=41, normalization=both, activation=None)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
Namespace(csv='full_new.csv', dataset='reddit', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
epoch:1/50, training loss:3.840963840484619
Train Acc 0.0083
 Acc 0.0342
new best val f1: 0.03419308137762877
reddit,dgl,1,0,0.4740,0.0342

epoch:2/50, training loss:3.5887670516967773
Train Acc 0.0330
 Acc 0.2620
new best val f1: 0.2619818652849741
reddit,dgl,1,1,0.5766,0.2620

epoch:3/50, training loss:3.4061176776885986
Train Acc 0.2440
 Acc 0.3737
new best val f1: 0.3736856141420299
reddit,dgl,1,2,0.6794,0.3737

epoch:4/50, training loss:3.2873408794403076
Train Acc 0.3534
 Acc 0.4205
new best val f1: 0.42050822919841513
reddit,dgl,1,3,0.7823,0.4205

epoch:5/50, training loss:3.1926634311676025
Train Acc 0.3929
 Acc 0.4756
new best val f1: 0.47561718988113383
reddit,dgl,1,4,0.8845,0.4756

epoch:6/50, training loss:3.1072542667388916
Train Acc 0.4485
 Acc 0.4924
new best val f1: 0.49239942090825967
reddit,dgl,1,5,0.9870,0.4924

epoch:7/50, training loss:3.0275168418884277
Train Acc 0.4692
 Acc 0.5210
new best val f1: 0.5209730265163061
reddit,dgl,1,6,1.0892,0.5210

epoch:8/50, training loss:2.951523542404175
Train Acc 0.5021
 Acc 0.5338
new best val f1: 0.5337930508991161
reddit,dgl,1,7,1.1920,0.5338

epoch:9/50, training loss:2.877714157104492
Train Acc 0.5176
 Acc 0.5613
new best val f1: 0.5612618104236513
reddit,dgl,1,8,1.2948,0.5613

epoch:10/50, training loss:2.805750608444214
Train Acc 0.5466
 Acc 0.5893
new best val f1: 0.5892829929899421
reddit,dgl,1,9,1.3973,0.5893

epoch:11/50, training loss:2.735628843307495
Train Acc 0.5761
 Acc 0.6002
new best val f1: 0.6002171594026211
reddit,dgl,1,10,1.4994,0.6002

epoch:12/50, training loss:2.667306661605835
Train Acc 0.5864
 Acc 0.6073
new best val f1: 0.6073415117342273
reddit,dgl,1,11,1.6021,0.6073

epoch:13/50, training loss:2.600712299346924
Train Acc 0.5933
 Acc 0.6161
new best val f1: 0.6161231331911002
reddit,dgl,1,12,1.7048,0.6161

epoch:14/50, training loss:2.536006212234497
Train Acc 0.6016
 Acc 0.6313
new best val f1: 0.6313433404449863
reddit,dgl,1,13,1.8072,0.6313

epoch:15/50, training loss:2.4731433391571045
Train Acc 0.6157
 Acc 0.6536
new best val f1: 0.653611703748857
reddit,dgl,1,14,1.9093,0.6536

epoch:16/50, training loss:2.412108898162842
Train Acc 0.6382
 Acc 0.6686
new best val f1: 0.6686033221578787
reddit,dgl,1,15,2.0120,0.6686

epoch:17/50, training loss:2.3528735637664795
Train Acc 0.6527
 Acc 0.6803
new best val f1: 0.6802613532459616
reddit,dgl,1,16,2.1145,0.6803

epoch:18/50, training loss:2.2954514026641846
Train Acc 0.6643
 Acc 0.6924
new best val f1: 0.6923575129533679
reddit,dgl,1,17,2.2166,0.6924

epoch:19/50, training loss:2.2398180961608887
Train Acc 0.6754
 Acc 0.7029
new best val f1: 0.7029106979579397
reddit,dgl,1,18,2.3194,0.7029

epoch:20/50, training loss:2.186004638671875
Train Acc 0.6855
 Acc 0.7108
new best val f1: 0.7107779640353551
reddit,dgl,1,19,2.4214,0.7108

epoch:21/50, training loss:2.1339709758758545
Train Acc 0.6936
 Acc 0.7157
new best val f1: 0.7157307223407497
reddit,dgl,1,20,2.5235,0.7157

epoch:22/50, training loss:2.083714008331299
Train Acc 0.6986
 Acc 0.7197
new best val f1: 0.7197310271258762
reddit,dgl,1,21,2.6261,0.7197

epoch:23/50, training loss:2.035234212875366
Train Acc 0.7019
 Acc 0.7233
new best val f1: 0.7232741542212741
reddit,dgl,1,22,2.7287,0.7233

epoch:24/50, training loss:1.9884779453277588
Train Acc 0.7055
 Acc 0.7255
new best val f1: 0.7254647973178909
reddit,dgl,1,23,2.8311,0.7255

epoch:25/50, training loss:1.9434256553649902
Train Acc 0.7081
 Acc 0.7277
new best val f1: 0.7276554404145078
reddit,dgl,1,24,2.9336,0.7277

epoch:26/50, training loss:1.900012493133545
Train Acc 0.7104
 Acc 0.7310
new best val f1: 0.7310461749466626
reddit,dgl,1,25,3.0365,0.7310

epoch:27/50, training loss:1.858168125152588
Train Acc 0.7141
 Acc 0.7355
new best val f1: 0.7355227064919232
reddit,dgl,1,26,3.1390,0.7355

epoch:28/50, training loss:1.8178026676177979
Train Acc 0.7184
 Acc 0.7421
new best val f1: 0.7421136848521792
reddit,dgl,1,27,3.2412,0.7421

epoch:29/50, training loss:1.7788543701171875
Train Acc 0.7243
 Acc 0.7493
new best val f1: 0.7492761353245961
reddit,dgl,1,28,3.3437,0.7493

epoch:30/50, training loss:1.7411881685256958
Train Acc 0.7312
 Acc 0.7651
new best val f1: 0.7650868637610485
reddit,dgl,1,29,3.4463,0.7651

epoch:31/50, training loss:1.7047350406646729
Train Acc 0.7464
 Acc 0.7768
new best val f1: 0.7767639439195367
reddit,dgl,1,30,3.5488,0.7768

epoch:32/50, training loss:1.6694364547729492
Train Acc 0.7593
 Acc 0.7863
new best val f1: 0.7862884791222189
reddit,dgl,1,31,3.6509,0.7863

epoch:33/50, training loss:1.6351985931396484
Train Acc 0.7688
 Acc 0.7956
new best val f1: 0.7956034745504419
reddit,dgl,1,32,3.7536,0.7956

epoch:34/50, training loss:1.6019490957260132
Train Acc 0.7784
 Acc 0.8031
new best val f1: 0.8030707101493447
reddit,dgl,1,33,3.8557,0.8031

epoch:35/50, training loss:1.569640040397644
Train Acc 0.7865
 Acc 0.8100
new best val f1: 0.8100236208473026
reddit,dgl,1,34,3.9578,0.8100

epoch:36/50, training loss:1.5382393598556519
Train Acc 0.7942
 Acc 0.8158
new best val f1: 0.815795489180128
reddit,dgl,1,35,4.0604,0.8158

epoch:37/50, training loss:1.5077097415924072
Train Acc 0.8002
 Acc 0.8206
new best val f1: 0.820557756781469
reddit,dgl,1,36,4.1631,0.8206

epoch:38/50, training loss:1.4780353307724
Train Acc 0.8058
 Acc 0.8238
new best val f1: 0.8237580006095703
reddit,dgl,1,37,4.2656,0.8238

epoch:39/50, training loss:1.4491996765136719
Train Acc 0.8095
 Acc 0.8276
new best val f1: 0.8275678146906431
reddit,dgl,1,38,4.3678,0.8276

epoch:40/50, training loss:1.4211889505386353
Train Acc 0.8132
 Acc 0.8315
new best val f1: 0.8314538250533374
reddit,dgl,1,39,4.4704,0.8315

epoch:41/50, training loss:1.3939942121505737
Train Acc 0.8175
 Acc 0.8367
new best val f1: 0.8366923194148126
reddit,dgl,1,40,4.5729,0.8367

epoch:42/50, training loss:1.3675899505615234
Train Acc 0.8226
 Acc 0.8429
new best val f1: 0.8429213654373666
reddit,dgl,1,41,4.6750,0.8429

epoch:43/50, training loss:1.341981291770935
Train Acc 0.8296
 Acc 0.8496
new best val f1: 0.8495885400792441
reddit,dgl,1,42,4.7776,0.8496

epoch:44/50, training loss:1.3171162605285645
Train Acc 0.8375
 Acc 0.8547
new best val f1: 0.854655592807071
reddit,dgl,1,43,4.8804,0.8547

epoch:45/50, training loss:1.2929867506027222
Train Acc 0.8434
 Acc 0.8601
new best val f1: 0.8601226760134105
reddit,dgl,1,44,4.9829,0.8601

epoch:46/50, training loss:1.2695930004119873
Train Acc 0.8493
 Acc 0.8651
new best val f1: 0.8650563852483999
reddit,dgl,1,45,5.0851,0.8651

epoch:47/50, training loss:1.2468702793121338
Train Acc 0.8548
 Acc 0.8689
new best val f1: 0.8688661993294727
reddit,dgl,1,46,5.1871,0.8689

epoch:48/50, training loss:1.22481369972229
Train Acc 0.8591
 Acc 0.8720
new best val f1: 0.8720283450167632
reddit,dgl,1,47,5.2897,0.8720

epoch:49/50, training loss:1.2034058570861816
Train Acc 0.8631
 Acc 0.8761
new best val f1: 0.8760667479427003
reddit,dgl,1,48,5.3924,0.8761

epoch:50/50, training loss:1.1826400756835938
Train Acc 0.8677
 Acc 0.8808
new best val f1: 0.8808099664736361
reddit,dgl,1,49,5.4949,0.8808

epoch:51/50, training loss:1.162462592124939
Train Acc 0.8724
 Acc 0.8864
new best val f1: 0.8864103931728131
reddit,dgl,1,50,5.5971,0.8864

epoch:52/50, training loss:1.142897367477417
Train Acc 0.8777
 Acc 0.8923
new best val f1: 0.8922775068576654
reddit,dgl,1,51,5.6996,0.8923

epoch:53/50, training loss:1.1238906383514404
Train Acc 0.8835
 Acc 0.8977
new best val f1: 0.8976874428527888
reddit,dgl,1,52,5.8023,0.8977

epoch:54/50, training loss:1.1054491996765137
Train Acc 0.8891
 Acc 0.9026
new best val f1: 0.9025830539469674
reddit,dgl,1,53,5.9045,0.9026

epoch:55/50, training loss:1.087552785873413
Train Acc 0.8942
 Acc 0.9081
new best val f1: 0.908107284364523
reddit,dgl,1,54,6.0069,0.9081

epoch:56/50, training loss:1.0701597929000854
Train Acc 0.8992
 Acc 0.9135
new best val f1: 0.9135172203596464
reddit,dgl,1,55,6.1094,0.9135

epoch:57/50, training loss:1.0532689094543457
Train Acc 0.9043
 Acc 0.9181
new best val f1: 0.9181461444681499
reddit,dgl,1,56,6.2125,0.9181

epoch:58/50, training loss:1.0368613004684448
Train Acc 0.9085
 Acc 0.9216
new best val f1: 0.9216321243523317
reddit,dgl,1,57,6.3147,0.9216

epoch:59/50, training loss:1.0209044218063354
Train Acc 0.9123
 Acc 0.9246
new best val f1: 0.9246418774763792
reddit,dgl,1,58,6.4167,0.9246

epoch:60/50, training loss:1.0053977966308594
Train Acc 0.9154
 Acc 0.9270
new best val f1: 0.9270230112770497
reddit,dgl,1,59,6.5193,0.9270

epoch:61/50, training loss:0.9903275966644287
Train Acc 0.9179
 Acc 0.9294
new best val f1: 0.9294231941481256
reddit,dgl,1,60,6.6220,0.9294

epoch:62/50, training loss:0.9756742119789124
Train Acc 0.9202
 Acc 0.9316
new best val f1: 0.9316328863151478
reddit,dgl,1,61,6.7241,0.9316

epoch:63/50, training loss:0.9614452123641968
Train Acc 0.9225
 Acc 0.9335
new best val f1: 0.9335377933556842
reddit,dgl,1,62,6.8261,0.9335

epoch:64/50, training loss:0.9476162195205688
Train Acc 0.9248
 Acc 0.9354
new best val f1: 0.9354427003962207
reddit,dgl,1,63,6.9288,0.9354

epoch:65/50, training loss:0.9342057704925537
Train Acc 0.9267
 Acc 0.9371
new best val f1: 0.9370999695214873
reddit,dgl,1,64,7.0315,0.9371

epoch:66/50, training loss:0.9211729168891907
Train Acc 0.9287
 Acc 0.9394
new best val f1: 0.9394239561109418
reddit,dgl,1,65,7.1340,0.9394

epoch:67/50, training loss:0.9085155725479126
Train Acc 0.9309
 Acc 0.9409
new best val f1: 0.9408907345321549
reddit,dgl,1,66,7.2361,0.9409

epoch:68/50, training loss:0.8962301015853882
Train Acc 0.9328
 Acc 0.9426
new best val f1: 0.942624199939043
reddit,dgl,1,67,7.3387,0.9426

epoch:69/50, training loss:0.8843047618865967
Train Acc 0.9346
 Acc 0.9445
new best val f1: 0.9445291069795794
reddit,dgl,1,68,7.4414,0.9445

epoch:70/50, training loss:0.8727062344551086
Train Acc 0.9363
 Acc 0.9468
new best val f1: 0.9467768972874123
reddit,dgl,1,69,7.5439,0.9468

epoch:71/50, training loss:0.8614447116851807
Train Acc 0.9384
 Acc 0.9487
new best val f1: 0.9487008533983542
reddit,dgl,1,70,7.6460,0.9487

epoch:72/50, training loss:0.8505112528800964
Train Acc 0.9403
 Acc 0.9505
new best val f1: 0.9504914660164584
reddit,dgl,1,71,7.7487,0.9505

epoch:73/50, training loss:0.8398706316947937
Train Acc 0.9423
 Acc 0.9523
new best val f1: 0.9523201767753734
reddit,dgl,1,72,7.8514,0.9523

epoch:74/50, training loss:0.8295305371284485
Train Acc 0.9442
 Acc 0.9540
new best val f1: 0.9540345931118561
reddit,dgl,1,73,7.9540,0.9540

epoch:75/50, training loss:0.819473922252655
Train Acc 0.9462
 Acc 0.9563
new best val f1: 0.9563014324900945
reddit,dgl,1,74,8.0565,0.9563

epoch:76/50, training loss:0.8096947073936462
Train Acc 0.9485
 Acc 0.9584
new best val f1: 0.9583777811642792
reddit,dgl,1,75,8.1586,0.9584

epoch:77/50, training loss:0.8001808524131775
Train Acc 0.9510
 Acc 0.9604
new best val f1: 0.960358884486437
reddit,dgl,1,76,8.2613,0.9604

epoch:78/50, training loss:0.7909128069877625
Train Acc 0.9531
 Acc 0.9620
new best val f1: 0.9619971045412984
reddit,dgl,1,77,8.3643,0.9620

epoch:79/50, training loss:0.7819001078605652
Train Acc 0.9550
 Acc 0.9640
new best val f1: 0.9639591587930509
reddit,dgl,1,78,8.4668,0.9640

epoch:80/50, training loss:0.7731254696846008
Train Acc 0.9568
 Acc 0.9654
new best val f1: 0.9654068881438586
reddit,dgl,1,79,8.5692,0.9654

epoch:81/50, training loss:0.764571487903595
Train Acc 0.9582
 Acc 0.9663
new best val f1: 0.9663402925937215
reddit,dgl,1,80,8.6718,0.9663

epoch:82/50, training loss:0.7562385201454163
Train Acc 0.9595
 Acc 0.9675
new best val f1: 0.9675022858884487
reddit,dgl,1,81,8.7745,0.9675

epoch:83/50, training loss:0.7481215596199036
Train Acc 0.9605
 Acc 0.9681
new best val f1: 0.9680547089302042
reddit,dgl,1,82,8.8770,0.9681

epoch:84/50, training loss:0.7401970028877258
Train Acc 0.9614
 Acc 0.9690
new best val f1: 0.9689881133800671
reddit,dgl,1,83,8.9791,0.9690

epoch:85/50, training loss:0.7324829697608948
Train Acc 0.9622
 Acc 0.9697
new best val f1: 0.9696929289850655
reddit,dgl,1,84,9.0817,0.9697

epoch:86/50, training loss:0.7249533534049988
Train Acc 0.9629
 Acc 0.9702
new best val f1: 0.9702072538860104
reddit,dgl,1,85,9.1847,0.9702

epoch:87/50, training loss:0.7176020741462708
Train Acc 0.9636
 Acc 0.9706
new best val f1: 0.9705882352941176
reddit,dgl,1,86,9.2873,0.9706

epoch:88/50, training loss:0.7104378342628479
Train Acc 0.9641
 Acc 0.9710
new best val f1: 0.9709882657726303
reddit,dgl,1,87,9.3898,0.9710

epoch:89/50, training loss:0.7034546136856079
Train Acc 0.9647
 Acc 0.9714
new best val f1: 0.9714263943919537
reddit,dgl,1,88,9.4925,0.9714

epoch:90/50, training loss:0.6966342329978943
Train Acc 0.9651
 Acc 0.9717
new best val f1: 0.9717311795184395
reddit,dgl,1,89,9.5956,0.9717

epoch:91/50, training loss:0.6899859309196472
Train Acc 0.9655
 Acc 0.9722
new best val f1: 0.9721883572081682
reddit,dgl,1,90,9.6981,0.9722

epoch:92/50, training loss:0.6834965944290161
Train Acc 0.9660
 Acc 0.9728
new best val f1: 0.9727598293203291
reddit,dgl,1,91,9.8002,0.9728

epoch:93/50, training loss:0.6771591305732727
Train Acc 0.9665
 Acc 0.9732
new best val f1: 0.9731789088692472
reddit,dgl,1,92,9.9030,0.9732

epoch:94/50, training loss:0.6709778904914856
Train Acc 0.9668
 Acc 0.9737
new best val f1: 0.9736741846997866
reddit,dgl,1,93,10.0062,0.9737

epoch:95/50, training loss:0.6649390459060669
Train Acc 0.9672
 Acc 0.9740
new best val f1: 0.9740170679670832
reddit,dgl,1,94,10.1089,0.9740

epoch:96/50, training loss:0.6590461730957031
Train Acc 0.9676
 Acc 0.9744
new best val f1: 0.9743790003047851
reddit,dgl,1,95,10.2113,0.9744

epoch:97/50, training loss:0.6532900333404541
Train Acc 0.9680
 Acc 0.9747
new best val f1: 0.9747028345016763
reddit,dgl,1,96,10.3141,0.9747

epoch:98/50, training loss:0.6476567387580872
Train Acc 0.9685
 Acc 0.9752
new best val f1: 0.9751981103322158
reddit,dgl,1,97,10.4173,0.9752

epoch:99/50, training loss:0.6421602368354797
Train Acc 0.9690
 Acc 0.9756
new best val f1: 0.9755981408107285
reddit,dgl,1,98,10.5199,0.9756

epoch:100/50, training loss:0.6367863416671753
Train Acc 0.9695
 Acc 0.9760
new best val f1: 0.9759600731484304
reddit,dgl,1,99,10.6226,0.9760

epoch:101/50, training loss:0.6315299272537231
Train Acc 0.9699
 Acc 0.9763
new best val f1: 0.9763410545565376
reddit,dgl,1,100,10.7252,0.9763

epoch:102/50, training loss:0.6263931393623352
Train Acc 0.9704
 Acc 0.9770
new best val f1: 0.97698872295032
reddit,dgl,1,101,10.8283,0.9770

epoch:103/50, training loss:0.6213691830635071
Train Acc 0.9709
 Acc 0.9775
new best val f1: 0.9775030478512649
reddit,dgl,1,102,10.9309,0.9775

epoch:104/50, training loss:0.6164565682411194
Train Acc 0.9713
 Acc 0.9779
new best val f1: 0.9779411764705882
reddit,dgl,1,103,11.0337,0.9779

epoch:105/50, training loss:0.611648678779602
Train Acc 0.9716
 Acc 0.9782
new best val f1: 0.9781888143858579
reddit,dgl,1,104,11.1364,0.9782

epoch:106/50, training loss:0.6069456338882446
Train Acc 0.9720
 Acc 0.9784
new best val f1: 0.9784364523011277
reddit,dgl,1,105,11.2394,0.9784

epoch:107/50, training loss:0.6023445129394531
Train Acc 0.9723
 Acc 0.9789
new best val f1: 0.9788555318500457
reddit,dgl,1,106,11.3421,0.9789

epoch:108/50, training loss:0.5978355407714844
Train Acc 0.9727
 Acc 0.9794
new best val f1: 0.9794270039622066
reddit,dgl,1,107,11.4445,0.9794

epoch:109/50, training loss:0.5934274792671204
Train Acc 0.9731
 Acc 0.9797
new best val f1: 0.979655592807071
reddit,dgl,1,108,11.5471,0.9797

epoch:110/50, training loss:0.5891072154045105
Train Acc 0.9734
 Acc 0.9800
new best val f1: 0.9800365742151783
reddit,dgl,1,109,11.6503,0.9800

epoch:111/50, training loss:0.5848812460899353
Train Acc 0.9737
 Acc 0.9803
new best val f1: 0.980284212130448
reddit,dgl,1,110,11.7529,0.9803

epoch:112/50, training loss:0.5807382464408875
Train Acc 0.9741
 Acc 0.9806
new best val f1: 0.9806461444681499
reddit,dgl,1,111,11.8556,0.9806

epoch:113/50, training loss:0.5766848921775818
Train Acc 0.9744
 Acc 0.9809
new best val f1: 0.9808747333130143
reddit,dgl,1,112,11.9582,0.9809

epoch:114/50, training loss:0.5727115273475647
Train Acc 0.9747
 Acc 0.9811
new best val f1: 0.9811414202986894
reddit,dgl,1,113,12.0614,0.9811

epoch:115/50, training loss:0.5688173770904541
Train Acc 0.9751
 Acc 0.9814
new best val f1: 0.9814271563547698
reddit,dgl,1,114,12.1643,0.9814

epoch:116/50, training loss:0.5649974346160889
Train Acc 0.9753
 Acc 0.9817
new best val f1: 0.9816747942700397
reddit,dgl,1,115,12.2668,0.9817

epoch:117/50, training loss:0.5612577199935913
Train Acc 0.9757
 Acc 0.9819
new best val f1: 0.9818843340444986
reddit,dgl,1,116,12.3694,0.9819

epoch:118/50, training loss:0.5575888752937317
Train Acc 0.9759
 Acc 0.9821
new best val f1: 0.982055775678147
reddit,dgl,1,117,12.4726,0.9821

epoch:119/50, training loss:0.5539937019348145
Train Acc 0.9762
 Acc 0.9822
new best val f1: 0.9821891191709845
reddit,dgl,1,118,12.5752,0.9822

epoch:120/50, training loss:0.5504674315452576
Train Acc 0.9765
 Acc 0.9824
new best val f1: 0.9824367570862542
reddit,dgl,1,119,12.6778,0.9824

epoch:121/50, training loss:0.5470027923583984
Train Acc 0.9768
 Acc 0.9829
new best val f1: 0.982893934775983
reddit,dgl,1,120,12.7804,0.9829

epoch:122/50, training loss:0.5436114072799683
Train Acc 0.9771
 Acc 0.9832
new best val f1: 0.9831796708320634
reddit,dgl,1,121,12.8835,0.9832

epoch:123/50, training loss:0.5402771830558777
Train Acc 0.9774
 Acc 0.9836
new best val f1: 0.9836368485217921
reddit,dgl,1,122,12.9861,0.9836

epoch:124/50, training loss:0.5370121002197266
Train Acc 0.9779
 Acc 0.9839
new best val f1: 0.9838654373666565
reddit,dgl,1,123,13.0887,0.9839

epoch:125/50, training loss:0.533802330493927
Train Acc 0.9782
 Acc 0.9841
new best val f1: 0.9841130752819263
reddit,dgl,1,124,13.1912,0.9841

epoch:126/50, training loss:0.5306524038314819
Train Acc 0.9785
 Acc 0.9845
new best val f1: 0.9845131057604389
reddit,dgl,1,125,13.2943,0.9845

epoch:127/50, training loss:0.52756667137146
Train Acc 0.9788
 Acc 0.9849
new best val f1: 0.9848750380981408
reddit,dgl,1,126,13.3969,0.9849

epoch:128/50, training loss:0.5245319604873657
Train Acc 0.9792
 Acc 0.9853
new best val f1: 0.9852750685766535
reddit,dgl,1,127,13.4997,0.9853

epoch:129/50, training loss:0.521555483341217
Train Acc 0.9796
 Acc 0.9855
new best val f1: 0.9855417555623286
reddit,dgl,1,128,13.6023,0.9855

epoch:130/50, training loss:0.5186344981193542
Train Acc 0.9798
 Acc 0.9858
new best val f1: 0.9857703444071929
reddit,dgl,1,129,13.7054,0.9858

epoch:131/50, training loss:0.5157577395439148
Train Acc 0.9801
 Acc 0.9860
new best val f1: 0.986037031392868
reddit,dgl,1,130,13.8082,0.9860

epoch:132/50, training loss:0.5129414796829224
Train Acc 0.9803
 Acc 0.9862
new best val f1: 0.9862275220969217
reddit,dgl,1,131,13.9109,0.9862

epoch:133/50, training loss:0.5101709961891174
Train Acc 0.9805
 Acc 0.9865
new best val f1: 0.9864751600121914
reddit,dgl,1,132,14.0135,0.9865

epoch:134/50, training loss:0.5074471831321716
Train Acc 0.9808
 Acc 0.9867
new best val f1: 0.9866846997866504
reddit,dgl,1,133,14.1166,0.9867

epoch:135/50, training loss:0.5047814846038818
Train Acc 0.9810
 Acc 0.9869
new best val f1: 0.9869132886315147
reddit,dgl,1,134,14.2196,0.9869

epoch:136/50, training loss:0.502152681350708
Train Acc 0.9813
 Acc 0.9872
new best val f1: 0.9872371228284059
reddit,dgl,1,135,14.3225,0.9872

epoch:137/50, training loss:0.49957355856895447
Train Acc 0.9815
 Acc 0.9875
new best val f1: 0.9874657116732704
reddit,dgl,1,136,14.4249,0.9875

epoch:138/50, training loss:0.4970349967479706
Train Acc 0.9817
 Acc 0.9875
new best val f1: 0.987503809814081
reddit,dgl,1,137,14.5281,0.9875

epoch:139/50, training loss:0.49453866481781006
Train Acc 0.9819
 Acc 0.9877
new best val f1: 0.9877323986589455
reddit,dgl,1,138,14.6311,0.9877

epoch:140/50, training loss:0.4920896291732788
Train Acc 0.9821
 Acc 0.9878
new best val f1: 0.9878276440109722
reddit,dgl,1,139,14.7340,0.9878

epoch:141/50, training loss:0.4896770715713501
Train Acc 0.9823
 Acc 0.9879
new best val f1: 0.9879228893629991
reddit,dgl,1,140,14.8366,0.9879

epoch:142/50, training loss:0.48730653524398804
Train Acc 0.9825
 Acc 0.9880
new best val f1: 0.9879609875038098
reddit,dgl,1,141,14.9394,0.9880

epoch:143/50, training loss:0.4849780797958374
Train Acc 0.9826
 Acc 0.9881
new best val f1: 0.9881133800670527
reddit,dgl,1,142,15.0426,0.9881

epoch:144/50, training loss:0.4826812446117401
Train Acc 0.9828
 Acc 0.9882
new best val f1: 0.9882276744894849
reddit,dgl,1,143,15.1447,0.9882

epoch:145/50, training loss:0.4804244339466095
Train Acc 0.9829
 Acc 0.9883
new best val f1: 0.988284821700701
reddit,dgl,1,144,15.2475,0.9883

epoch:146/50, training loss:0.4782065749168396
Train Acc 0.9830
 Acc 0.9885
new best val f1: 0.9885134105455654
reddit,dgl,1,145,15.3505,0.9885

epoch:147/50, training loss:0.4760185778141022
Train Acc 0.9832
 Acc 0.9887
new best val f1: 0.988703901249619
reddit,dgl,1,146,15.4533,0.9887

epoch:148/50, training loss:0.4738663136959076
Train Acc 0.9834
 Acc 0.9889
new best val f1: 0.9888943919536727
reddit,dgl,1,147,15.5559,0.9889

epoch:149/50, training loss:0.4717491567134857
Train Acc 0.9836
 Acc 0.9890
new best val f1: 0.9890467845169155
reddit,dgl,1,148,15.6585,0.9890

epoch:150/50, training loss:0.46966469287872314
Train Acc 0.9837
 Acc 0.9891
new best val f1: 0.9891039317281317
reddit,dgl,1,149,15.7613,0.9891

epoch:151/50, training loss:0.4676087498664856
Train Acc 0.9838
 Acc 0.9892
new best val f1: 0.9891801280097531
reddit,dgl,1,150,15.8643,0.9892

epoch:152/50, training loss:0.46559226512908936
Train Acc 0.9840
 Acc 0.9893
new best val f1: 0.9892944224321853
reddit,dgl,1,151,15.9674,0.9893

epoch:153/50, training loss:0.4635991156101227
Train Acc 0.9841
 Acc 0.9894
new best val f1: 0.9893515696434014
reddit,dgl,1,152,16.0703,0.9894

epoch:154/50, training loss:0.46163880825042725
Train Acc 0.9843
 Acc 0.9894
new best val f1: 0.9894087168546175
reddit,dgl,1,153,16.1728,0.9894

epoch:155/50, training loss:0.45970919728279114
Train Acc 0.9844
 Acc 0.9896
new best val f1: 0.9896373056994818
reddit,dgl,1,154,16.2755,0.9896

epoch:156/50, training loss:0.45780596137046814
Train Acc 0.9845
 Acc 0.9898
new best val f1: 0.9897706491923194
reddit,dgl,1,155,16.3786,0.9898

epoch:157/50, training loss:0.45593422651290894
Train Acc 0.9846
 Acc 0.9898
new best val f1: 0.9898277964035355
reddit,dgl,1,156,16.4815,0.9898

epoch:158/50, training loss:0.45408716797828674
Train Acc 0.9846
 Acc 0.9900
new best val f1: 0.989961139896373
reddit,dgl,1,157,16.5843,0.9900

epoch:159/50, training loss:0.4522678852081299
Train Acc 0.9847
 Acc 0.9902
new best val f1: 0.9901516306004267
reddit,dgl,1,158,16.6867,0.9902

epoch:160/50, training loss:0.4504707157611847
Train Acc 0.9849
 Acc 0.9902
new best val f1: 0.9902468759524535
reddit,dgl,1,159,16.7896,0.9902

epoch:161/50, training loss:0.44870397448539734
Train Acc 0.9850
 Acc 0.9904
new best val f1: 0.9903802194452911
reddit,dgl,1,160,16.8926,0.9904

epoch:162/50, training loss:0.4469608962535858
Train Acc 0.9851
 Acc 0.9904
new best val f1: 0.9904183175861018
reddit,dgl,1,161,16.9956,0.9904

epoch:163/50, training loss:0.4452478885650635
Train Acc 0.9852
 Acc 0.9905
new best val f1: 0.9904754647973179
reddit,dgl,1,162,17.0981,0.9905

epoch:164/50, training loss:0.4435518980026245
Train Acc 0.9853
 Acc 0.9905
new best val f1: 0.9905326120085339
reddit,dgl,1,163,17.2007,0.9905

epoch:165/50, training loss:0.4418826699256897
Train Acc 0.9854
 Acc 0.9906
new best val f1: 0.9906278573605608
reddit,dgl,1,164,17.3035,0.9906

epoch:166/50, training loss:0.4402387738227844
Train Acc 0.9855
 Acc 0.9908
new best val f1: 0.990799298994209
reddit,dgl,1,165,17.4066,0.9908

epoch:167/50, training loss:0.438614159822464
Train Acc 0.9856
 Acc 0.9909
new best val f1: 0.9908754952758305
reddit,dgl,1,166,17.5097,0.9909

epoch:168/50, training loss:0.4370121359825134
Train Acc 0.9857
 Acc 0.9909
new best val f1: 0.9909135934166413
reddit,dgl,1,167,17.6125,0.9909

epoch:169/50, training loss:0.43543484807014465
Train Acc 0.9857
 Acc 0.9911
new best val f1: 0.9911231331911002
reddit,dgl,1,168,17.7151,0.9911

epoch:170/50, training loss:0.43387651443481445
Train Acc 0.9859
 Acc 0.9912
new best val f1: 0.9912183785431271
reddit,dgl,1,169,17.8178,0.9912

epoch:171/50, training loss:0.4323381781578064
Train Acc 0.9860
 Acc 0.9914
new best val f1: 0.9913898201767753
reddit,dgl,1,170,17.9206,0.9914

epoch:172/50, training loss:0.4308225214481354
Train Acc 0.9861
 Acc 0.9915
new best val f1: 0.9915041145992075
reddit,dgl,1,171,18.0239,0.9915

epoch:173/50, training loss:0.4293329119682312
Train Acc 0.9862
 Acc 0.9916
new best val f1: 0.9916184090216398
reddit,dgl,1,172,18.1267,0.9916

epoch:174/50, training loss:0.4278552532196045
Train Acc 0.9863
 Acc 0.9918
new best val f1: 0.9917708015848826
reddit,dgl,1,173,18.2293,0.9918

epoch:175/50, training loss:0.426399827003479
Train Acc 0.9864
 Acc 0.9919
new best val f1: 0.9918850960073149
reddit,dgl,1,174,18.3319,0.9919

epoch:176/50, training loss:0.4249632954597473
Train Acc 0.9865
 Acc 0.9920
new best val f1: 0.9920184395001523
reddit,dgl,1,175,18.4351,0.9920

epoch:177/50, training loss:0.42354634404182434
Train Acc 0.9866
 Acc 0.9921
new best val f1: 0.9920565376409631
reddit,dgl,1,176,18.5381,0.9921

epoch:178/50, training loss:0.4221450090408325
Train Acc 0.9867
 Acc 0.9922
new best val f1: 0.9921708320633953
reddit,dgl,1,177,18.6409,0.9922

epoch:179/50, training loss:0.42076370120048523
Train Acc 0.9868
 Acc 0.9923
new best val f1: 0.9923232246266382
reddit,dgl,1,178,18.7435,0.9923

epoch:180/50, training loss:0.41939878463745117
Train Acc 0.9869
 Acc 0.9924
new best val f1: 0.9923613227674489
reddit,dgl,1,179,18.8458,0.9924

epoch:181/50, training loss:0.4180539548397064
Train Acc 0.9870
 Acc 0.9925
new best val f1: 0.9924756171898811
reddit,dgl,1,180,18.9483,0.9925

epoch:182/50, training loss:0.4167257249355316
Train Acc 0.9871
 Acc 0.9926
new best val f1: 0.992570862541908
reddit,dgl,1,181,19.0509,0.9926

epoch:183/50, training loss:0.415414035320282
Train Acc 0.9872
 Acc 0.9927
new best val f1: 0.9926851569643401
reddit,dgl,1,182,19.1539,0.9927

epoch:184/50, training loss:0.41411784291267395
Train Acc 0.9873
 Acc 0.9928
new best val f1: 0.9927994513867723
reddit,dgl,1,183,19.2572,0.9928

epoch:185/50, training loss:0.4128391444683075
Train Acc 0.9874
 Acc 0.9930
new best val f1: 0.9929708930204206
reddit,dgl,1,184,19.3600,0.9930

epoch:186/50, training loss:0.41157862544059753
Train Acc 0.9875
 Acc 0.9930
new best val f1: 0.9930470893020421
reddit,dgl,1,185,19.4627,0.9930

epoch:187/50, training loss:0.4103272259235382
Train Acc 0.9876
 Acc 0.9931
new best val f1: 0.9930851874428528
reddit,dgl,1,186,19.5654,0.9931

epoch:188/50, training loss:0.4090951681137085
Train Acc 0.9877
 Acc 0.9931
new best val f1: 0.9931423346540689
reddit,dgl,1,187,19.6685,0.9931

epoch:189/50, training loss:0.40787801146507263
Train Acc 0.9878
 Acc 0.9932
new best val f1: 0.9932185309356903
reddit,dgl,1,188,19.7711,0.9932

epoch:190/50, training loss:0.40667179226875305
Train Acc 0.9879
 Acc 0.9934
new best val f1: 0.9933709234989333
reddit,dgl,1,189,19.8743,0.9934

epoch:191/50, training loss:0.4054875671863556
Train Acc 0.9880
 Acc 0.9935
new best val f1: 0.9934661688509601
reddit,dgl,1,190,19.9773,0.9935

epoch:192/50, training loss:0.40431153774261475
Train Acc 0.9881
 Acc 0.9936
new best val f1: 0.9935804632733922
reddit,dgl,1,191,20.0804,0.9936

epoch:193/50, training loss:0.4031504988670349
Train Acc 0.9882
 Acc 0.9936
new best val f1: 0.993618561414203
reddit,dgl,1,192,20.1831,0.9936

epoch:194/50, training loss:0.4020073413848877
Train Acc 0.9882
 Acc 0.9938
new best val f1: 0.9937519049070406
reddit,dgl,1,193,20.2857,0.9938

epoch:195/50, training loss:0.40087413787841797
Train Acc 0.9883
 Acc 0.9939
new best val f1: 0.9938661993294727
reddit,dgl,1,194,20.3885,0.9939

epoch:196/50, training loss:0.3997531533241272
Train Acc 0.9884
 Acc 0.9940
new best val f1: 0.9940185918927157
reddit,dgl,1,195,20.4917,0.9940

epoch:197/50, training loss:0.39864715933799744
Train Acc 0.9885
 Acc 0.9941
new best val f1: 0.994094788174337
reddit,dgl,1,196,20.5947,0.9941

epoch:198/50, training loss:0.39755183458328247
Train Acc 0.9886
 Acc 0.9941
new best val f1: 0.9941138372447424
reddit,dgl,1,197,20.6975,0.9941

epoch:199/50, training loss:0.39646846055984497
Train Acc 0.9887
 Acc 0.9941
reddit,dgl,1,198,20.8006,0.9941

epoch:200/50, training loss:0.3954036831855774
Train Acc 0.9888
 Acc 0.9942
new best val f1: 0.9942090825967693
reddit,dgl,1,199,20.9038,0.9942

epoch:201/50, training loss:0.39434707164764404
Train Acc 0.9888
 Acc 0.9942
reddit,dgl,1,200,21.0069,0.9942

epoch:202/50, training loss:0.39330053329467773
Train Acc 0.9890
 Acc 0.9942
new best val f1: 0.99424718073758
reddit,dgl,1,201,21.1100,0.9942

epoch:203/50, training loss:0.3922685384750366
Train Acc 0.9890
 Acc 0.9943
new best val f1: 0.9942852788783907
reddit,dgl,1,202,21.2127,0.9943

epoch:204/50, training loss:0.3912466764450073
Train Acc 0.9891
 Acc 0.9944
new best val f1: 0.994399573300823
reddit,dgl,1,203,21.3154,0.9944

epoch:205/50, training loss:0.39023756980895996
Train Acc 0.9892
 Acc 0.9946
new best val f1: 0.9945900640048766
reddit,dgl,1,204,21.4182,0.9946

epoch:206/50, training loss:0.3892359137535095
Train Acc 0.9894
 Acc 0.9947
new best val f1: 0.9947043584273088
reddit,dgl,1,205,21.5213,0.9947

epoch:207/50, training loss:0.38825029134750366
Train Acc 0.9895
 Acc 0.9948
new best val f1: 0.9947615056385248
reddit,dgl,1,206,21.6245,0.9948

epoch:208/50, training loss:0.3872724771499634
Train Acc 0.9896
 Acc 0.9948
new best val f1: 0.9948377019201463
reddit,dgl,1,207,21.7276,0.9948

epoch:209/50, training loss:0.3863059878349304
Train Acc 0.9896
 Acc 0.9949
new best val f1: 0.9948948491313624
reddit,dgl,1,208,21.8303,0.9949

epoch:210/50, training loss:0.38534611463546753
Train Acc 0.9896
 Acc 0.9950
new best val f1: 0.9949710454129839
reddit,dgl,1,209,21.9330,0.9950

epoch:211/50, training loss:0.38440099358558655
Train Acc 0.9897
 Acc 0.9951
new best val f1: 0.9950662907650106
reddit,dgl,1,210,22.0361,0.9951

epoch:212/50, training loss:0.38346514105796814
Train Acc 0.9898
 Acc 0.9951
new best val f1: 0.995085339835416
reddit,dgl,1,211,22.1392,0.9951

epoch:213/50, training loss:0.3825378715991974
Train Acc 0.9898
 Acc 0.9952
new best val f1: 0.9951996342578482
reddit,dgl,1,212,22.2420,0.9952

epoch:214/50, training loss:0.3816218078136444
Train Acc 0.9899
 Acc 0.9954
new best val f1: 0.9953710758914965
reddit,dgl,1,213,22.3448,0.9954

epoch:215/50, training loss:0.38071393966674805
Train Acc 0.9900
 Acc 0.9954
new best val f1: 0.9954091740323072
reddit,dgl,1,214,22.4475,0.9954

epoch:216/50, training loss:0.37981441617012024
Train Acc 0.9901
 Acc 0.9955
new best val f1: 0.9954663212435233
reddit,dgl,1,215,22.5503,0.9955

epoch:217/50, training loss:0.37892666459083557
Train Acc 0.9901
 Acc 0.9955
new best val f1: 0.9954853703139287
reddit,dgl,1,216,22.6534,0.9955

epoch:218/50, training loss:0.3780475854873657
Train Acc 0.9902
 Acc 0.9955
new best val f1: 0.9955044193843341
reddit,dgl,1,217,22.7567,0.9955

epoch:219/50, training loss:0.3771730363368988
Train Acc 0.9902
 Acc 0.9955
reddit,dgl,1,218,22.8594,0.9955

epoch:220/50, training loss:0.37631452083587646
Train Acc 0.9903
 Acc 0.9955
new best val f1: 0.9955425175251448
reddit,dgl,1,219,22.9622,0.9955

epoch:221/50, training loss:0.37545669078826904
Train Acc 0.9903
 Acc 0.9956
new best val f1: 0.9956187138067663
reddit,dgl,1,220,23.0648,0.9956

epoch:222/50, training loss:0.3746148645877838
Train Acc 0.9904
 Acc 0.9956
reddit,dgl,1,221,23.1679,0.9956

epoch:223/50, training loss:0.3737766444683075
Train Acc 0.9905
 Acc 0.9957
new best val f1: 0.9957330082291984
reddit,dgl,1,222,23.2711,0.9957

epoch:224/50, training loss:0.37294885516166687
Train Acc 0.9906
 Acc 0.9958
new best val f1: 0.9957520572996038
reddit,dgl,1,223,23.3740,0.9958

epoch:225/50, training loss:0.37212783098220825
Train Acc 0.9907
 Acc 0.9958
new best val f1: 0.9958473026516306
reddit,dgl,1,224,23.4768,0.9958

epoch:226/50, training loss:0.3713186979293823
Train Acc 0.9908
 Acc 0.9960
new best val f1: 0.9959615970740627
reddit,dgl,1,225,23.5795,0.9960

epoch:227/50, training loss:0.3705125153064728
Train Acc 0.9908
 Acc 0.9961
new best val f1: 0.9960568424260896
reddit,dgl,1,226,23.6826,0.9961

epoch:228/50, training loss:0.3697165250778198
Train Acc 0.9909
 Acc 0.9961
new best val f1: 0.9961139896373057
reddit,dgl,1,227,23.7861,0.9961

epoch:229/50, training loss:0.3689274787902832
Train Acc 0.9910
 Acc 0.9962
new best val f1: 0.9961901859189272
reddit,dgl,1,228,23.8891,0.9962

epoch:230/50, training loss:0.3681481182575226
Train Acc 0.9911
 Acc 0.9962
reddit,dgl,1,229,23.9920,0.9962

epoch:231/50, training loss:0.36737194657325745
Train Acc 0.9911
 Acc 0.9962
reddit,dgl,1,230,24.0946,0.9962

epoch:232/50, training loss:0.3666098415851593
Train Acc 0.9911
 Acc 0.9962
new best val f1: 0.9962282840597378
reddit,dgl,1,231,24.1975,0.9962

epoch:233/50, training loss:0.3658459484577179
Train Acc 0.9912
 Acc 0.9963
new best val f1: 0.9962854312709539
reddit,dgl,1,232,24.3005,0.9963

epoch:234/50, training loss:0.3650965690612793
Train Acc 0.9912
 Acc 0.9963
new best val f1: 0.9963235294117647
reddit,dgl,1,233,24.4035,0.9963

epoch:235/50, training loss:0.36435121297836304
Train Acc 0.9913
 Acc 0.9963
reddit,dgl,1,234,24.5063,0.9963

epoch:236/50, training loss:0.3636101186275482
Train Acc 0.9913
 Acc 0.9964
new best val f1: 0.9963806766229808
reddit,dgl,1,235,24.6091,0.9964

epoch:237/50, training loss:0.3628794252872467
Train Acc 0.9914
 Acc 0.9965
new best val f1: 0.9964568729046023
reddit,dgl,1,236,24.7119,0.9965

epoch:238/50, training loss:0.3621566891670227
Train Acc 0.9914
 Acc 0.9966
new best val f1: 0.9965902163974398
reddit,dgl,1,237,24.8150,0.9966

epoch:239/50, training loss:0.3614363968372345
Train Acc 0.9915
 Acc 0.9966
new best val f1: 0.9966092654678451
reddit,dgl,1,238,24.9184,0.9966

epoch:240/50, training loss:0.3607272803783417
Train Acc 0.9916
 Acc 0.9966
reddit,dgl,1,239,25.0213,0.9966

epoch:241/50, training loss:0.3600195646286011
Train Acc 0.9916
 Acc 0.9966
new best val f1: 0.9966473636086559
reddit,dgl,1,240,25.1240,0.9966

epoch:242/50, training loss:0.3593220114707947
Train Acc 0.9917
 Acc 0.9967
new best val f1: 0.9966854617494666
reddit,dgl,1,241,25.2270,0.9967

epoch:243/50, training loss:0.3586273193359375
Train Acc 0.9917
 Acc 0.9967
new best val f1: 0.9967426089606827
reddit,dgl,1,242,25.3298,0.9967

epoch:244/50, training loss:0.3579413890838623
Train Acc 0.9918
 Acc 0.9968
new best val f1: 0.9967807071014935
reddit,dgl,1,243,25.4329,0.9968

epoch:245/50, training loss:0.3572588860988617
Train Acc 0.9918
 Acc 0.9968
new best val f1: 0.9968188052423042
reddit,dgl,1,244,25.5360,0.9968

epoch:246/50, training loss:0.35658469796180725
Train Acc 0.9919
 Acc 0.9969
new best val f1: 0.9968759524535202
reddit,dgl,1,245,25.6389,0.9969

epoch:247/50, training loss:0.35591572523117065
Train Acc 0.9919
 Acc 0.9969
new best val f1: 0.9968950015239256
reddit,dgl,1,246,25.7416,0.9969

epoch:248/50, training loss:0.35525020956993103
Train Acc 0.9919
 Acc 0.9969
new best val f1: 0.996914050594331
reddit,dgl,1,247,25.8444,0.9969

epoch:249/50, training loss:0.3545931875705719
Train Acc 0.9920
 Acc 0.9971
new best val f1: 0.9970854922279793
reddit,dgl,1,248,25.9475,0.9971

epoch:250/50, training loss:0.3539418876171112
Train Acc 0.9921
 Acc 0.9971
new best val f1: 0.9971426394391953
reddit,dgl,1,249,26.0509,0.9971

epoch:251/50, training loss:0.35329532623291016
Train Acc 0.9922
 Acc 0.9972
new best val f1: 0.9971807375800061
reddit,dgl,1,250,26.1540,0.9972

epoch:252/50, training loss:0.35265499353408813
Train Acc 0.9923
 Acc 0.9973
new best val f1: 0.9972569338616275
reddit,dgl,1,251,26.2567,0.9973

epoch:253/50, training loss:0.35202038288116455
Train Acc 0.9923
 Acc 0.9974
new best val f1: 0.9973521792136544
reddit,dgl,1,252,26.3594,0.9974

epoch:254/50, training loss:0.35139063000679016
Train Acc 0.9924
 Acc 0.9974
new best val f1: 0.9974474245656811
reddit,dgl,1,253,26.4622,0.9974

epoch:255/50, training loss:0.35076555609703064
Train Acc 0.9924
 Acc 0.9974
reddit,dgl,1,254,26.5652,0.9974

epoch:256/50, training loss:0.35014569759368896
Train Acc 0.9925
 Acc 0.9975
new best val f1: 0.9975045717768973
reddit,dgl,1,255,26.6684,0.9975

epoch:257/50, training loss:0.3495316803455353
Train Acc 0.9925
 Acc 0.9975
reddit,dgl,1,256,26.7715,0.9975

epoch:258/50, training loss:0.3489219546318054
Train Acc 0.9925
 Acc 0.9975
new best val f1: 0.997542669917708
reddit,dgl,1,257,26.8743,0.9975

epoch:259/50, training loss:0.3483175039291382
Train Acc 0.9926
 Acc 0.9977
new best val f1: 0.9976569643401402
reddit,dgl,1,258,26.9770,0.9977

epoch:260/50, training loss:0.34771761298179626
Train Acc 0.9926
 Acc 0.9976
reddit,dgl,1,259,27.0798,0.9976

epoch:261/50, training loss:0.34712305665016174
Train Acc 0.9926
 Acc 0.9976
reddit,dgl,1,260,27.1829,0.9976

epoch:262/50, training loss:0.3465343713760376
Train Acc 0.9927
 Acc 0.9977
new best val f1: 0.997695062480951
reddit,dgl,1,261,27.2860,0.9977

epoch:263/50, training loss:0.3459495007991791
Train Acc 0.9927
 Acc 0.9978
new best val f1: 0.997752209692167
reddit,dgl,1,262,27.3888,0.9978

epoch:264/50, training loss:0.3453689515590668
Train Acc 0.9928
 Acc 0.9978
new best val f1: 0.9978093569033831
reddit,dgl,1,263,27.4917,0.9978

epoch:265/50, training loss:0.34479305148124695
Train Acc 0.9929
 Acc 0.9979
new best val f1: 0.9978665041145992
reddit,dgl,1,264,27.5945,0.9979

epoch:266/50, training loss:0.3442200720310211
Train Acc 0.9930
 Acc 0.9979
new best val f1: 0.9978855531850046
reddit,dgl,1,265,27.6976,0.9979

epoch:267/50, training loss:0.3436540365219116
Train Acc 0.9930
 Acc 0.9979
new best val f1: 0.9979427003962207
reddit,dgl,1,266,27.8008,0.9979

epoch:268/50, training loss:0.34309282898902893
Train Acc 0.9930
 Acc 0.9979
reddit,dgl,1,267,27.9039,0.9979

epoch:269/50, training loss:0.342533677816391
Train Acc 0.9931
 Acc 0.9980
new best val f1: 0.9979998476074368
reddit,dgl,1,268,28.0066,0.9980

epoch:270/50, training loss:0.3419809341430664
Train Acc 0.9931
 Acc 0.9981
new best val f1: 0.9980950929594635
reddit,dgl,1,269,28.1093,0.9981

epoch:271/50, training loss:0.34143123030662537
Train Acc 0.9932
 Acc 0.9982
new best val f1: 0.9982093873818958
reddit,dgl,1,270,28.2121,0.9982

epoch:272/50, training loss:0.3408854007720947
Train Acc 0.9933
 Acc 0.9983
new best val f1: 0.9982665345931119
reddit,dgl,1,271,28.3153,0.9983

epoch:273/50, training loss:0.3403450548648834
Train Acc 0.9933
 Acc 0.9983
new best val f1: 0.9983046327339226
reddit,dgl,1,272,28.4185,0.9983

epoch:274/50, training loss:0.3398076891899109
Train Acc 0.9934
 Acc 0.9983
new best val f1: 0.9983427308747334
reddit,dgl,1,273,28.5215,0.9983

epoch:275/50, training loss:0.3392748534679413
Train Acc 0.9934
 Acc 0.9984
new best val f1: 0.998380829015544
reddit,dgl,1,274,28.6245,0.9984

epoch:276/50, training loss:0.33874619007110596
Train Acc 0.9935
 Acc 0.9984
reddit,dgl,1,275,28.7273,0.9984

epoch:277/50, training loss:0.3382222354412079
Train Acc 0.9935
 Acc 0.9984
new best val f1: 0.9984379762267601
reddit,dgl,1,276,28.8303,0.9984

epoch:278/50, training loss:0.33770111203193665
Train Acc 0.9936
 Acc 0.9984
reddit,dgl,1,277,28.9336,0.9984

epoch:279/50, training loss:0.33718305826187134
Train Acc 0.9936
 Acc 0.9985
new best val f1: 0.9984760743675709
reddit,dgl,1,278,29.0367,0.9985

epoch:280/50, training loss:0.33667150139808655
Train Acc 0.9937
 Acc 0.9985
new best val f1: 0.9984951234379762
reddit,dgl,1,279,29.1395,0.9985

epoch:281/50, training loss:0.3361610472202301
Train Acc 0.9937
 Acc 0.9986
new best val f1: 0.9985522706491923
reddit,dgl,1,280,29.2422,0.9986

epoch:282/50, training loss:0.33565473556518555
Train Acc 0.9938
 Acc 0.9986
new best val f1: 0.9985713197195977
reddit,dgl,1,281,29.3453,0.9986

epoch:283/50, training loss:0.33515262603759766
Train Acc 0.9938
 Acc 0.9986
new best val f1: 0.9985903687900031
reddit,dgl,1,282,29.4483,0.9986

epoch:284/50, training loss:0.3346550762653351
Train Acc 0.9939
 Acc 0.9986
new best val f1: 0.9986475160012191
reddit,dgl,1,283,29.5515,0.9986

epoch:285/50, training loss:0.334160178899765
Train Acc 0.9939
 Acc 0.9987
new best val f1: 0.9986665650716245
reddit,dgl,1,284,29.6544,0.9987

epoch:286/50, training loss:0.3336687982082367
Train Acc 0.9940
 Acc 0.9986
reddit,dgl,1,285,29.7574,0.9986

epoch:287/50, training loss:0.3331829905509949
Train Acc 0.9940
 Acc 0.9987
reddit,dgl,1,286,29.8602,0.9987

epoch:288/50, training loss:0.3326977789402008
Train Acc 0.9941
 Acc 0.9986
reddit,dgl,1,287,29.9634,0.9986

epoch:289/50, training loss:0.3322177827358246
Train Acc 0.9941
 Acc 0.9987
new best val f1: 0.9986856141420298
reddit,dgl,1,288,30.0666,0.9987

epoch:290/50, training loss:0.33173906803131104
Train Acc 0.9941
 Acc 0.9987
new best val f1: 0.9987237122828406
reddit,dgl,1,289,30.1698,0.9987

epoch:291/50, training loss:0.3312647044658661
Train Acc 0.9942
 Acc 0.9988
new best val f1: 0.9987618104236513
reddit,dgl,1,290,30.2725,0.9988

epoch:292/50, training loss:0.3307952582836151
Train Acc 0.9942
 Acc 0.9987
reddit,dgl,1,291,30.3752,0.9987

epoch:293/50, training loss:0.3303270936012268
Train Acc 0.9942
 Acc 0.9987
reddit,dgl,1,292,30.4783,0.9987

epoch:294/50, training loss:0.3298618197441101
Train Acc 0.9942
 Acc 0.9987
reddit,dgl,1,293,30.5817,0.9987

epoch:295/50, training loss:0.32940077781677246
Train Acc 0.9943
 Acc 0.9988
new best val f1: 0.9987808594940567
reddit,dgl,1,294,30.6850,0.9988

epoch:296/50, training loss:0.32894381880760193
Train Acc 0.9943
 Acc 0.9987
reddit,dgl,1,295,30.7878,0.9987

epoch:297/50, training loss:0.3284893333911896
Train Acc 0.9943
 Acc 0.9988
new best val f1: 0.9987999085644621
reddit,dgl,1,296,30.8908,0.9988

epoch:298/50, training loss:0.32803672552108765
Train Acc 0.9944
 Acc 0.9989
new best val f1: 0.9988761048460835
reddit,dgl,1,297,30.9938,0.9989

epoch:299/50, training loss:0.3275868594646454
Train Acc 0.9944
 Acc 0.9989
new best val f1: 0.9988951539164889
reddit,dgl,1,298,31.0968,0.9989

epoch:300/50, training loss:0.32714152336120605
Train Acc 0.9945
 Acc 0.9989
new best val f1: 0.9989332520572997
reddit,dgl,1,299,31.2003,0.9989

epoch:301/50, training loss:0.32669901847839355
Train Acc 0.9945
 Acc 0.9989
reddit,dgl,1,300,31.3034,0.9989

epoch:302/50, training loss:0.3262587785720825
Train Acc 0.9945
 Acc 0.9989
reddit,dgl,1,301,31.4062,0.9989

epoch:303/50, training loss:0.3258225917816162
Train Acc 0.9945
 Acc 0.9989
reddit,dgl,1,302,31.5092,0.9989

epoch:304/50, training loss:0.3253880441188812
Train Acc 0.9946
 Acc 0.9990
new best val f1: 0.9989903992685157
reddit,dgl,1,303,31.6123,0.9990

epoch:305/50, training loss:0.3249569833278656
Train Acc 0.9947
 Acc 0.9990
new best val f1: 0.9990284974093264
reddit,dgl,1,304,31.7149,0.9990

epoch:306/50, training loss:0.32452860474586487
Train Acc 0.9947
 Acc 0.9990
new best val f1: 0.9990475464797318
reddit,dgl,1,305,31.8180,0.9990

epoch:307/50, training loss:0.3241024315357208
Train Acc 0.9947
 Acc 0.9991
new best val f1: 0.9991046936909479
reddit,dgl,1,306,31.9212,0.9991

epoch:308/50, training loss:0.32367968559265137
Train Acc 0.9948
 Acc 0.9991
reddit,dgl,1,307,32.0245,0.9991

epoch:309/50, training loss:0.32325878739356995
Train Acc 0.9948
 Acc 0.9991
reddit,dgl,1,308,32.1276,0.9991

epoch:310/50, training loss:0.32284092903137207
Train Acc 0.9948
 Acc 0.9991
reddit,dgl,1,309,32.2303,0.9991

epoch:311/50, training loss:0.32242539525032043
Train Acc 0.9948
 Acc 0.9992
new best val f1: 0.999161840902164
reddit,dgl,1,310,32.3331,0.9992

epoch:312/50, training loss:0.32201316952705383
Train Acc 0.9949
 Acc 0.9992
new best val f1: 0.9992380371837855
reddit,dgl,1,311,32.4362,0.9992

epoch:313/50, training loss:0.32160383462905884
Train Acc 0.9949
 Acc 0.9993
new best val f1: 0.9992951843950015
reddit,dgl,1,312,32.5394,0.9993

epoch:314/50, training loss:0.321196049451828
Train Acc 0.9950
 Acc 0.9994
new best val f1: 0.9993904297470283
reddit,dgl,1,313,32.6425,0.9994

epoch:315/50, training loss:0.32079294323921204
Train Acc 0.9950
 Acc 0.9994
reddit,dgl,1,314,32.7453,0.9994

epoch:316/50, training loss:0.32038891315460205
Train Acc 0.9950
 Acc 0.9995
new best val f1: 0.9994666260286498
reddit,dgl,1,315,32.8481,0.9995

epoch:317/50, training loss:0.3199900686740875
Train Acc 0.9951
 Acc 0.9995
new best val f1: 0.9994856750990552
reddit,dgl,1,316,32.9512,0.9995

epoch:318/50, training loss:0.3195919096469879
Train Acc 0.9951
 Acc 0.9995
new best val f1: 0.9995047241694606
reddit,dgl,1,317,33.0544,0.9995

epoch:319/50, training loss:0.31919610500335693
Train Acc 0.9951
 Acc 0.9995
reddit,dgl,1,318,33.1575,0.9995

epoch:320/50, training loss:0.3188033103942871
Train Acc 0.9952
 Acc 0.9994
reddit,dgl,1,319,33.2603,0.9994

epoch:321/50, training loss:0.31841444969177246
Train Acc 0.9952
 Acc 0.9995
reddit,dgl,1,320,33.3630,0.9995

epoch:322/50, training loss:0.31802693009376526
Train Acc 0.9952
 Acc 0.9995
new best val f1: 0.9995237732398659
reddit,dgl,1,321,33.4660,0.9995

epoch:323/50, training loss:0.3176417648792267
Train Acc 0.9953
 Acc 0.9995
new best val f1: 0.9995428223102713
reddit,dgl,1,322,33.5693,0.9995

epoch:324/50, training loss:0.31725889444351196
Train Acc 0.9954
 Acc 0.9996
new best val f1: 0.9995809204510819
reddit,dgl,1,323,33.6723,0.9996

epoch:325/50, training loss:0.3168775737285614
Train Acc 0.9954
 Acc 0.9996
new best val f1: 0.9996380676622981
reddit,dgl,1,324,33.7752,0.9996

epoch:326/50, training loss:0.31649890542030334
Train Acc 0.9954
 Acc 0.9997
new best val f1: 0.9996571167327034
reddit,dgl,1,325,33.8779,0.9997

epoch:327/50, training loss:0.31612101197242737
Train Acc 0.9955
 Acc 0.9997
new best val f1: 0.9996952148735142
reddit,dgl,1,326,33.9810,0.9997

epoch:328/50, training loss:0.31574687361717224
Train Acc 0.9955
 Acc 0.9997
new best val f1: 0.9997333130143249
reddit,dgl,1,327,34.0844,0.9997

epoch:329/50, training loss:0.3153761327266693
Train Acc 0.9955
 Acc 0.9997
reddit,dgl,1,328,34.1876,0.9997

epoch:330/50, training loss:0.3150060772895813
Train Acc 0.9956
 Acc 0.9998
new best val f1: 0.999790460225541
reddit,dgl,1,329,34.2905,0.9998

epoch:331/50, training loss:0.31463754177093506
Train Acc 0.9956
 Acc 0.9998
reddit,dgl,1,330,34.3934,0.9998

epoch:332/50, training loss:0.3142733573913574
Train Acc 0.9957
 Acc 0.9998
new best val f1: 0.9998285583663518
reddit,dgl,1,331,34.4964,0.9998

epoch:333/50, training loss:0.3139089345932007
Train Acc 0.9957
 Acc 0.9998
new best val f1: 0.999847607436757
reddit,dgl,1,332,34.5996,0.9998

epoch:334/50, training loss:0.3135480582714081
Train Acc 0.9958
 Acc 0.9998
reddit,dgl,1,333,34.7028,0.9998

epoch:335/50, training loss:0.31319040060043335
Train Acc 0.9958
 Acc 0.9998
reddit,dgl,1,334,34.8056,0.9998

epoch:336/50, training loss:0.31282997131347656
Train Acc 0.9958
 Acc 0.9998
reddit,dgl,1,335,34.9087,0.9998

epoch:337/50, training loss:0.31247588992118835
Train Acc 0.9958
 Acc 0.9998
reddit,dgl,1,336,35.0116,0.9998

epoch:338/50, training loss:0.3121216595172882
Train Acc 0.9958
 Acc 0.9999
new best val f1: 0.9998857055775678
reddit,dgl,1,337,35.1147,0.9999

epoch:339/50, training loss:0.3117695748806
Train Acc 0.9959
 Acc 0.9999
reddit,dgl,1,338,35.2181,0.9999

epoch:340/50, training loss:0.3114214837551117
Train Acc 0.9959
 Acc 0.9999
new best val f1: 0.9999047546479731
reddit,dgl,1,339,35.3213,0.9999

epoch:341/50, training loss:0.31107455492019653
Train Acc 0.9960
 Acc 1.0000
new best val f1: 0.9999619018591893
reddit,dgl,1,340,35.4241,1.0000

epoch:342/50, training loss:0.31072723865509033
Train Acc 0.9960
 Acc 0.9999
reddit,dgl,1,341,35.5270,0.9999

epoch:343/50, training loss:0.31038305163383484
Train Acc 0.9960
 Acc 0.9999
reddit,dgl,1,342,35.6300,0.9999

epoch:344/50, training loss:0.31004324555397034
Train Acc 0.9961
 Acc 1.0000
new best val f1: 0.9999809509295946
reddit,dgl,1,343,35.7333,1.0000

epoch:345/50, training loss:0.3097021281719208
Train Acc 0.9961
 Acc 1.0000
new best val f1: 1.0000380981408108
reddit,dgl,1,344,35.8361,1.0000

epoch:346/50, training loss:0.3093641996383667
Train Acc 0.9961
 Acc 1.0001
new best val f1: 1.0000761962816214
reddit,dgl,1,345,35.9388,1.0001

epoch:347/50, training loss:0.3090270757675171
Train Acc 0.9962
 Acc 1.0001
reddit,dgl,1,346,36.0419,1.0001

epoch:348/50, training loss:0.3086925745010376
Train Acc 0.9962
 Acc 1.0001
reddit,dgl,1,347,36.1451,1.0001

epoch:349/50, training loss:0.30835992097854614
Train Acc 0.9962
 Acc 1.0001
new best val f1: 1.0001333434928374
reddit,dgl,1,348,36.2483,1.0001

epoch:350/50, training loss:0.30802878737449646
Train Acc 0.9962
 Acc 1.0002
new best val f1: 1.0001904907040537
reddit,dgl,1,349,36.3512,1.0002

epoch:351/50, training loss:0.3076992928981781
Train Acc 0.9963
 Acc 1.0002
new best val f1: 1.0002285888448643
reddit,dgl,1,350,36.4540,1.0002

epoch:352/50, training loss:0.3073711395263672
Train Acc 0.9963
 Acc 1.0003
new best val f1: 1.0002666869856751
reddit,dgl,1,351,36.5568,1.0003

epoch:353/50, training loss:0.30704396963119507
Train Acc 0.9964
 Acc 1.0003
reddit,dgl,1,352,36.6601,1.0003

epoch:354/50, training loss:0.3067207932472229
Train Acc 0.9964
 Acc 1.0003
reddit,dgl,1,353,36.7634,1.0003

epoch:355/50, training loss:0.30639830231666565
Train Acc 0.9964
 Acc 1.0003
new best val f1: 1.0003238341968912
reddit,dgl,1,354,36.8662,1.0003

epoch:356/50, training loss:0.306075781583786
Train Acc 0.9965
 Acc 1.0004
new best val f1: 1.0003809814081073
reddit,dgl,1,355,36.9690,1.0004

epoch:357/50, training loss:0.3057563602924347
Train Acc 0.9965
 Acc 1.0004
reddit,dgl,1,356,37.0720,1.0004

epoch:358/50, training loss:0.30543842911720276
Train Acc 0.9965
 Acc 1.0004
new best val f1: 1.0004381286193234
reddit,dgl,1,357,37.1751,1.0004

epoch:359/50, training loss:0.3051222860813141
Train Acc 0.9965
 Acc 1.0005
new best val f1: 1.0004952758305394
reddit,dgl,1,358,37.2783,1.0005

epoch:360/50, training loss:0.3048081398010254
Train Acc 0.9966
 Acc 1.0005
new best val f1: 1.0005333739713502
reddit,dgl,1,359,37.3815,1.0005

epoch:361/50, training loss:0.3044953942298889
Train Acc 0.9966
 Acc 1.0006
new best val f1: 1.0005524230417555
reddit,dgl,1,360,37.4846,1.0006

epoch:362/50, training loss:0.30418264865875244
Train Acc 0.9966
 Acc 1.0006
new best val f1: 1.000571472112161
reddit,dgl,1,361,37.5876,1.0006

epoch:363/50, training loss:0.3038733899593353
Train Acc 0.9967
 Acc 1.0006
reddit,dgl,1,362,37.6906,1.0006

epoch:364/50, training loss:0.30356431007385254
Train Acc 0.9967
 Acc 1.0006
new best val f1: 1.0006095702529716
reddit,dgl,1,363,37.7936,1.0006

epoch:365/50, training loss:0.30325746536254883
Train Acc 0.9967
 Acc 1.0006
reddit,dgl,1,364,37.8971,1.0006

epoch:366/50, training loss:0.30295202136039734
Train Acc 0.9967
 Acc 1.0006
new best val f1: 1.0006476683937824
reddit,dgl,1,365,38.0006,1.0006

epoch:367/50, training loss:0.3026466965675354
Train Acc 0.9968
 Acc 1.0007
new best val f1: 1.0006667174641877
reddit,dgl,1,366,38.1034,1.0007

epoch:368/50, training loss:0.30234354734420776
Train Acc 0.9968
 Acc 1.0007
new best val f1: 1.0006857665345932
reddit,dgl,1,367,38.2062,1.0007

epoch:369/50, training loss:0.30204281210899353
Train Acc 0.9968
 Acc 1.0007
reddit,dgl,1,368,38.3090,1.0007

epoch:370/50, training loss:0.30174264311790466
Train Acc 0.9969
 Acc 1.0007
new best val f1: 1.0007429137458093
reddit,dgl,1,369,38.4120,1.0007

epoch:371/50, training loss:0.30144378542900085
Train Acc 0.9969
 Acc 1.0008
new best val f1: 1.0007619628162145
reddit,dgl,1,370,38.5152,1.0008

epoch:372/50, training loss:0.3011479675769806
Train Acc 0.9969
 Acc 1.0008
new best val f1: 1.0007810118866198
reddit,dgl,1,371,38.6182,1.0008

epoch:373/50, training loss:0.30085140466690063
Train Acc 0.9970
 Acc 1.0008
reddit,dgl,1,372,38.7210,1.0008

epoch:374/50, training loss:0.3005566895008087
Train Acc 0.9970
 Acc 1.0008
new best val f1: 1.0008000609570253
reddit,dgl,1,373,38.8239,1.0008

epoch:375/50, training loss:0.30026447772979736
Train Acc 0.9970
 Acc 1.0009
new best val f1: 1.0008572081682414
reddit,dgl,1,374,38.9269,1.0009

epoch:376/50, training loss:0.29997366666793823
Train Acc 0.9970
 Acc 1.0009
reddit,dgl,1,375,39.0304,1.0009

epoch:377/50, training loss:0.29968342185020447
Train Acc 0.9970
 Acc 1.0009
new best val f1: 1.0008953063090522
reddit,dgl,1,376,39.1335,1.0009

epoch:378/50, training loss:0.29939377307891846
Train Acc 0.9971
 Acc 1.0009
new best val f1: 1.0009143553794575
reddit,dgl,1,377,39.2363,1.0009

epoch:379/50, training loss:0.2991061508655548
Train Acc 0.9971
 Acc 1.0009
new best val f1: 1.0009334044498628
reddit,dgl,1,378,39.3390,1.0009

epoch:380/50, training loss:0.2988196909427643
Train Acc 0.9971
 Acc 1.0009
reddit,dgl,1,379,39.4421,1.0009

epoch:381/50, training loss:0.29853498935699463
Train Acc 0.9971
 Acc 1.0009
reddit,dgl,1,380,39.5461,1.0009

epoch:382/50, training loss:0.2982514500617981
Train Acc 0.9972
 Acc 1.0009
reddit,dgl,1,381,39.6495,1.0009

epoch:383/50, training loss:0.2979695200920105
Train Acc 0.9972
 Acc 1.0009
reddit,dgl,1,382,39.7525,1.0009

epoch:384/50, training loss:0.2976889908313751
Train Acc 0.9972
 Acc 1.0009
reddit,dgl,1,383,39.8556,1.0009

epoch:385/50, training loss:0.29740846157073975
Train Acc 0.9972
 Acc 1.0009
reddit,dgl,1,384,39.9586,1.0009

epoch:386/50, training loss:0.29712948203086853
Train Acc 0.9973
 Acc 1.0009
reddit,dgl,1,385,40.0619,1.0009

epoch:387/50, training loss:0.29685449600219727
Train Acc 0.9973
 Acc 1.0009
reddit,dgl,1,386,40.1651,1.0009

epoch:388/50, training loss:0.29657670855522156
Train Acc 0.9973
 Acc 1.0009
reddit,dgl,1,387,40.2682,1.0009

epoch:389/50, training loss:0.2963036894798279
Train Acc 0.9973
 Acc 1.0009
reddit,dgl,1,388,40.3712,1.0009

epoch:390/50, training loss:0.29602906107902527
Train Acc 0.9974
 Acc 1.0010
new best val f1: 1.0009524535202683
reddit,dgl,1,389,40.4740,1.0010

epoch:391/50, training loss:0.29575520753860474
Train Acc 0.9974
 Acc 1.0010
new best val f1: 1.0009905516610789
reddit,dgl,1,390,40.5771,1.0010

epoch:392/50, training loss:0.2954847812652588
Train Acc 0.9975
 Acc 1.0010
reddit,dgl,1,391,40.6803,1.0010

epoch:393/50, training loss:0.2952144145965576
Train Acc 0.9974
 Acc 1.0010
new best val f1: 1.0010286498018897
reddit,dgl,1,392,40.7835,1.0010

epoch:394/50, training loss:0.29494553804397583
Train Acc 0.9975
 Acc 1.0010
new best val f1: 1.001047698872295
reddit,dgl,1,393,40.8864,1.0010

epoch:395/50, training loss:0.2946789860725403
Train Acc 0.9975
 Acc 1.0010
reddit,dgl,1,394,40.9893,1.0010

epoch:396/50, training loss:0.2944115400314331
Train Acc 0.9976
 Acc 1.0010
reddit,dgl,1,395,41.0924,1.0010

epoch:397/50, training loss:0.29414626955986023
Train Acc 0.9976
 Acc 1.0011
new best val f1: 1.0010857970131057
reddit,dgl,1,396,41.1955,1.0011

epoch:398/50, training loss:0.29388174414634705
Train Acc 0.9976
 Acc 1.0011
new best val f1: 1.0011238951539165
reddit,dgl,1,397,41.2988,1.0011

epoch:399/50, training loss:0.2936191260814667
Train Acc 0.9977
 Acc 1.0011
reddit,dgl,1,398,41.4016,1.0011

epoch:400/50, training loss:0.29335635900497437
Train Acc 0.9977
 Acc 1.0011
reddit,dgl,1,399,41.5046,1.0011

epoch:401/50, training loss:0.29309484362602234
Train Acc 0.9977
 Acc 1.0012
new best val f1: 1.001200091435538
reddit,dgl,1,400,41.6075,1.0012

epoch:402/50, training loss:0.29283633828163147
Train Acc 0.9978
 Acc 1.0011
reddit,dgl,1,401,41.7107,1.0011

epoch:403/50, training loss:0.2925776243209839
Train Acc 0.9978
 Acc 1.0012
reddit,dgl,1,402,41.8140,1.0012

epoch:404/50, training loss:0.2923184037208557
Train Acc 0.9978
 Acc 1.0012
reddit,dgl,1,403,41.9171,1.0012

epoch:405/50, training loss:0.2920607924461365
Train Acc 0.9978
 Acc 1.0012
reddit,dgl,1,404,42.0200,1.0012

epoch:406/50, training loss:0.2918056547641754
Train Acc 0.9978
 Acc 1.0012
reddit,dgl,1,405,42.1230,1.0012

epoch:407/50, training loss:0.29155024886131287
Train Acc 0.9978
 Acc 1.0012
new best val f1: 1.0012381895763487
reddit,dgl,1,406,42.2261,1.0012

epoch:408/50, training loss:0.2912976145744324
Train Acc 0.9979
 Acc 1.0013
new best val f1: 1.0012953367875648
reddit,dgl,1,407,42.3293,1.0013

epoch:409/50, training loss:0.29104340076446533
Train Acc 0.9979
 Acc 1.0013
new best val f1: 1.00131438585797
reddit,dgl,1,408,42.4328,1.0013

epoch:410/50, training loss:0.29079198837280273
Train Acc 0.9980
 Acc 1.0013
reddit,dgl,1,409,42.5358,1.0013

epoch:411/50, training loss:0.29054126143455505
Train Acc 0.9980
 Acc 1.0013
new best val f1: 1.0013334349283756
reddit,dgl,1,410,42.6388,1.0013

epoch:412/50, training loss:0.2902914881706238
Train Acc 0.9980
 Acc 1.0013
reddit,dgl,1,411,42.7416,1.0013

epoch:413/50, training loss:0.2900420129299164
Train Acc 0.9980
 Acc 1.0014
new best val f1: 1.0013524839987809
reddit,dgl,1,412,42.8447,1.0014

epoch:414/50, training loss:0.2897937297821045
Train Acc 0.9981
 Acc 1.0014
new best val f1: 1.0013715330691861
reddit,dgl,1,413,42.9479,1.0014

epoch:415/50, training loss:0.28954648971557617
Train Acc 0.9981
 Acc 1.0014
reddit,dgl,1,414,43.0510,1.0014

epoch:416/50, training loss:0.2893007695674896
Train Acc 0.9981
 Acc 1.0014
new best val f1: 1.001409631209997
reddit,dgl,1,415,43.1540,1.0014

epoch:417/50, training loss:0.28905680775642395
Train Acc 0.9982
 Acc 1.0014
new best val f1: 1.0014286802804022
reddit,dgl,1,416,43.2569,1.0014

epoch:418/50, training loss:0.288812518119812
Train Acc 0.9982
 Acc 1.0014
reddit,dgl,1,417,43.3600,1.0014

epoch:419/50, training loss:0.28856807947158813
Train Acc 0.9982
 Acc 1.0014
reddit,dgl,1,418,43.4632,1.0014

epoch:420/50, training loss:0.28832635283470154
Train Acc 0.9982
 Acc 1.0014
new best val f1: 1.0014477293508077
reddit,dgl,1,419,43.5664,1.0014

epoch:421/50, training loss:0.28808557987213135
Train Acc 0.9982
 Acc 1.0015
new best val f1: 1.0014858274916185
reddit,dgl,1,420,43.6695,1.0015

epoch:422/50, training loss:0.28784385323524475
Train Acc 0.9983
 Acc 1.0015
reddit,dgl,1,421,43.7725,1.0015

epoch:423/50, training loss:0.2876049876213074
Train Acc 0.9983
 Acc 1.0015
new best val f1: 1.0015048765620238
reddit,dgl,1,422,43.8755,1.0015

epoch:424/50, training loss:0.28736674785614014
Train Acc 0.9983
 Acc 1.0015
new best val f1: 1.0015429747028346
reddit,dgl,1,423,43.9785,1.0015

epoch:425/50, training loss:0.28712931275367737
Train Acc 0.9983
 Acc 1.0016
new best val f1: 1.0015620237732399
reddit,dgl,1,424,44.0817,1.0016

epoch:426/50, training loss:0.28689122200012207
Train Acc 0.9984
 Acc 1.0016
reddit,dgl,1,425,44.1854,1.0016

epoch:427/50, training loss:0.28665778040885925
Train Acc 0.9984
 Acc 1.0016
new best val f1: 1.0015810728436452
reddit,dgl,1,426,44.2885,1.0016

epoch:428/50, training loss:0.28642216324806213
Train Acc 0.9984
 Acc 1.0016
new best val f1: 1.001619170984456
reddit,dgl,1,427,44.3916,1.0016

epoch:429/50, training loss:0.28618717193603516
Train Acc 0.9985
 Acc 1.0016
reddit,dgl,1,428,44.4944,1.0016

epoch:430/50, training loss:0.2859553396701813
Train Acc 0.9985
 Acc 1.0016
reddit,dgl,1,429,44.5977,1.0016

epoch:431/50, training loss:0.28572118282318115
Train Acc 0.9985
 Acc 1.0016
reddit,dgl,1,430,44.7009,1.0016

epoch:432/50, training loss:0.2854902744293213
Train Acc 0.9985
 Acc 1.0017
new best val f1: 1.0016572691252668
reddit,dgl,1,431,44.8046,1.0017

epoch:433/50, training loss:0.285260945558548
Train Acc 0.9985
 Acc 1.0017
new best val f1: 1.001676318195672
reddit,dgl,1,432,44.9077,1.0017

epoch:434/50, training loss:0.2850300967693329
Train Acc 0.9985
 Acc 1.0017
reddit,dgl,1,433,45.0105,1.0017

epoch:435/50, training loss:0.2848013639450073
Train Acc 0.9986
 Acc 1.0017
reddit,dgl,1,434,45.1134,1.0017

epoch:436/50, training loss:0.28457286953926086
Train Acc 0.9986
 Acc 1.0017
reddit,dgl,1,435,45.2162,1.0017

epoch:437/50, training loss:0.28434544801712036
Train Acc 0.9986
 Acc 1.0017
reddit,dgl,1,436,45.3194,1.0017

epoch:438/50, training loss:0.28412064909935
Train Acc 0.9986
 Acc 1.0017
new best val f1: 1.0017144163364828
reddit,dgl,1,437,45.4230,1.0017

epoch:439/50, training loss:0.28389379382133484
Train Acc 0.9987
 Acc 1.0018
new best val f1: 1.0017525144772934
reddit,dgl,1,438,45.5261,1.0018

epoch:440/50, training loss:0.28366950154304504
Train Acc 0.9987
 Acc 1.0017
reddit,dgl,1,439,45.6289,1.0017

epoch:441/50, training loss:0.28344520926475525
Train Acc 0.9987
 Acc 1.0018
new best val f1: 1.001771563547699
reddit,dgl,1,440,45.7318,1.0018

epoch:442/50, training loss:0.28322306275367737
Train Acc 0.9987
 Acc 1.0018
new best val f1: 1.0017906126181042
reddit,dgl,1,441,45.8352,1.0018

epoch:443/50, training loss:0.28300079703330994
Train Acc 0.9987
 Acc 1.0018
new best val f1: 1.001828710758915
reddit,dgl,1,442,45.9384,1.0018

epoch:444/50, training loss:0.28277912735939026
Train Acc 0.9988
 Acc 1.0018
reddit,dgl,1,443,46.0417,1.0018

epoch:445/50, training loss:0.28255805373191833
Train Acc 0.9988
 Acc 1.0018
reddit,dgl,1,444,46.1446,1.0018

epoch:446/50, training loss:0.28233832120895386
Train Acc 0.9988
 Acc 1.0019
new best val f1: 1.001885857970131
reddit,dgl,1,445,46.2475,1.0019

epoch:447/50, training loss:0.28211963176727295
Train Acc 0.9989
 Acc 1.0019
reddit,dgl,1,446,46.3506,1.0019

epoch:448/50, training loss:0.2819025218486786
Train Acc 0.9989
 Acc 1.0019
new best val f1: 1.0019239561109419
reddit,dgl,1,447,46.4538,1.0019

epoch:449/50, training loss:0.28168371319770813
Train Acc 0.9989
 Acc 1.0020
new best val f1: 1.001981103322158
reddit,dgl,1,448,46.5571,1.0020

epoch:450/50, training loss:0.2814665138721466
Train Acc 0.9990
 Acc 1.0020
reddit,dgl,1,449,46.6600,1.0020

epoch:451/50, training loss:0.2812509536743164
Train Acc 0.9990
 Acc 1.0020
new best val f1: 1.0020001523925632
reddit,dgl,1,450,46.7630,1.0020

epoch:452/50, training loss:0.28103527426719666
Train Acc 0.9990
 Acc 1.0020
reddit,dgl,1,451,46.8659,1.0020

epoch:453/50, training loss:0.2808208167552948
Train Acc 0.9991
 Acc 1.0020
reddit,dgl,1,452,46.9687,1.0020

epoch:454/50, training loss:0.28060683608055115
Train Acc 0.9991
 Acc 1.0019
reddit,dgl,1,453,47.0721,1.0019

epoch:455/50, training loss:0.2803945541381836
Train Acc 0.9991
 Acc 1.0020
reddit,dgl,1,454,47.1758,1.0020

epoch:456/50, training loss:0.2801816165447235
Train Acc 0.9991
 Acc 1.0020
new best val f1: 1.002038250533374
reddit,dgl,1,455,47.2789,1.0020

epoch:457/50, training loss:0.2799696624279022
Train Acc 0.9992
 Acc 1.0021
new best val f1: 1.00209539774459
reddit,dgl,1,456,47.3819,1.0021

epoch:458/50, training loss:0.2797597348690033
Train Acc 0.9992
 Acc 1.0021
new best val f1: 1.0021144468149954
reddit,dgl,1,457,47.4848,1.0021

epoch:459/50, training loss:0.2795487642288208
Train Acc 0.9992
 Acc 1.0022
new best val f1: 1.0021525449558062
reddit,dgl,1,458,47.5875,1.0022

epoch:460/50, training loss:0.27933964133262634
Train Acc 0.9992
 Acc 1.0022
reddit,dgl,1,459,47.6910,1.0022

epoch:461/50, training loss:0.2791305482387543
Train Acc 0.9992
 Acc 1.0022
new best val f1: 1.0022096921670223
reddit,dgl,1,460,47.7942,1.0022

epoch:462/50, training loss:0.27892279624938965
Train Acc 0.9993
 Acc 1.0023
new best val f1: 1.0022858884486436
reddit,dgl,1,461,47.8974,1.0023

epoch:463/50, training loss:0.27871572971343994
Train Acc 0.9993
 Acc 1.0023
new best val f1: 1.0023049375190491
reddit,dgl,1,462,48.0003,1.0023

epoch:464/50, training loss:0.27850982546806335
Train Acc 0.9993
 Acc 1.0023
new best val f1: 1.0023430356598597
reddit,dgl,1,463,48.1030,1.0023

epoch:465/50, training loss:0.278302937746048
Train Acc 0.9994
 Acc 1.0024
new best val f1: 1.0023811338006705
reddit,dgl,1,464,48.2060,1.0024

epoch:466/50, training loss:0.2780989110469818
Train Acc 0.9994
 Acc 1.0024
reddit,dgl,1,465,48.3093,1.0024

epoch:467/50, training loss:0.2778937518596649
Train Acc 0.9994
 Acc 1.0024
new best val f1: 1.0024382810118866
reddit,dgl,1,466,48.4125,1.0024

epoch:468/50, training loss:0.27769026160240173
Train Acc 0.9995
 Acc 1.0025
new best val f1: 1.0024573300822919
reddit,dgl,1,467,48.5158,1.0025

epoch:469/50, training loss:0.27748730778694153
Train Acc 0.9995
 Acc 1.0025
new best val f1: 1.0024763791526974
reddit,dgl,1,468,48.6188,1.0025

epoch:470/50, training loss:0.2772851884365082
Train Acc 0.9995
 Acc 1.0025
reddit,dgl,1,469,48.7218,1.0025

epoch:471/50, training loss:0.27708300948143005
Train Acc 0.9995
 Acc 1.0025
reddit,dgl,1,470,48.8248,1.0025

epoch:472/50, training loss:0.2768827974796295
Train Acc 0.9996
 Acc 1.0025
reddit,dgl,1,471,48.9275,1.0025

epoch:473/50, training loss:0.27668240666389465
Train Acc 0.9996
 Acc 1.0026
new best val f1: 1.0025716245047243
reddit,dgl,1,472,49.0306,1.0026

epoch:474/50, training loss:0.2764824330806732
Train Acc 0.9996
 Acc 1.0026
new best val f1: 1.0025906735751295
reddit,dgl,1,473,49.1338,1.0026

epoch:475/50, training loss:0.2762831747531891
Train Acc 0.9997
 Acc 1.0026
reddit,dgl,1,474,49.2371,1.0026

epoch:476/50, training loss:0.27608421444892883
Train Acc 0.9997
 Acc 1.0026
new best val f1: 1.0026097226455348
reddit,dgl,1,475,49.3401,1.0026

epoch:477/50, training loss:0.27588722109794617
Train Acc 0.9997
 Acc 1.0026
reddit,dgl,1,476,49.4431,1.0026

epoch:478/50, training loss:0.2756895422935486
Train Acc 0.9997
 Acc 1.0026
reddit,dgl,1,477,49.5459,1.0026

epoch:479/50, training loss:0.27549201250076294
Train Acc 0.9998
 Acc 1.0026
reddit,dgl,1,478,49.6491,1.0026

epoch:480/50, training loss:0.2752961218357086
Train Acc 0.9998
 Acc 1.0026
reddit,dgl,1,479,49.7524,1.0026

epoch:481/50, training loss:0.2751010060310364
Train Acc 0.9998
 Acc 1.0026
reddit,dgl,1,480,49.8556,1.0026

epoch:482/50, training loss:0.2749071419239044
Train Acc 0.9998
 Acc 1.0027
new best val f1: 1.002666869856751
reddit,dgl,1,481,49.9586,1.0027

epoch:483/50, training loss:0.2747132480144501
Train Acc 0.9999
 Acc 1.0027
reddit,dgl,1,482,50.0613,1.0027

epoch:484/50, training loss:0.27451860904693604
Train Acc 0.9999
 Acc 1.0027
reddit,dgl,1,483,50.1641,1.0027

epoch:485/50, training loss:0.2743268311023712
Train Acc 0.9999
 Acc 1.0027
reddit,dgl,1,484,50.2672,1.0027

epoch:486/50, training loss:0.27413326501846313
Train Acc 0.9999
 Acc 1.0027
new best val f1: 1.0027049679975617
reddit,dgl,1,485,50.3704,1.0027

epoch:487/50, training loss:0.2739424705505371
Train Acc 1.0000
 Acc 1.0027
reddit,dgl,1,486,50.4735,1.0027

epoch:488/50, training loss:0.27375152707099915
Train Acc 1.0000
 Acc 1.0027
new best val f1: 1.0027430661383725
reddit,dgl,1,487,50.5765,1.0027

epoch:489/50, training loss:0.273561030626297
Train Acc 1.0000
 Acc 1.0027
reddit,dgl,1,488,50.6795,1.0027

epoch:490/50, training loss:0.27337101101875305
Train Acc 1.0000
 Acc 1.0027
reddit,dgl,1,489,50.7824,1.0027

epoch:491/50, training loss:0.2731824815273285
Train Acc 1.0000
 Acc 1.0028
new best val f1: 1.002781164279183
reddit,dgl,1,490,50.8856,1.0028

epoch:492/50, training loss:0.27299243211746216
Train Acc 1.0001
 Acc 1.0029
new best val f1: 1.00287640963121
reddit,dgl,1,491,50.9889,1.0029

epoch:493/50, training loss:0.27280521392822266
Train Acc 1.0001
 Acc 1.0029
new best val f1: 1.0028954587016154
reddit,dgl,1,492,51.0920,1.0029

epoch:494/50, training loss:0.2726174592971802
Train Acc 1.0002
 Acc 1.0029
reddit,dgl,1,493,51.1949,1.0029

epoch:495/50, training loss:0.27243003249168396
Train Acc 1.0002
 Acc 1.0030
new best val f1: 1.0029526059128315
reddit,dgl,1,494,51.2980,1.0030

epoch:496/50, training loss:0.27224451303482056
Train Acc 1.0002
 Acc 1.0029
reddit,dgl,1,495,51.4010,1.0029

epoch:497/50, training loss:0.27205872535705566
Train Acc 1.0003
 Acc 1.0030
new best val f1: 1.002990704053642
reddit,dgl,1,496,51.5041,1.0030

epoch:498/50, training loss:0.2718729078769684
Train Acc 1.0003
 Acc 1.0030
new best val f1: 1.003028802194453
reddit,dgl,1,497,51.6073,1.0030

epoch:499/50, training loss:0.2716878354549408
Train Acc 1.0003
 Acc 1.0030
reddit,dgl,1,498,51.7104,1.0030

epoch:500/50, training loss:0.271504282951355
Train Acc 1.0003
 Acc 1.0030
reddit,dgl,1,499,51.8134,1.0030

epoch:501/50, training loss:0.27132025361061096
Train Acc 1.0003
 Acc 1.0030
reddit,dgl,1,500,51.9161,1.0030

epoch:502/50, training loss:0.27113741636276245
Train Acc 1.0004
 Acc 1.0030
reddit,dgl,1,501,52.0192,1.0030

epoch:503/50, training loss:0.27095404267311096
Train Acc 1.0004
 Acc 1.0030
reddit,dgl,1,502,52.1228,1.0030

epoch:504/50, training loss:0.2707732021808624
Train Acc 1.0004
 Acc 1.0030
reddit,dgl,1,503,52.2267,1.0030

epoch:505/50, training loss:0.27059146761894226
Train Acc 1.0004
 Acc 1.0030
new best val f1: 1.0030478512648582
reddit,dgl,1,504,52.3298,1.0030

epoch:506/50, training loss:0.27041104435920715
Train Acc 1.0004
 Acc 1.0031
new best val f1: 1.003085949405669
reddit,dgl,1,505,52.4327,1.0031

epoch:507/50, training loss:0.2702305316925049
Train Acc 1.0005
 Acc 1.0031
new best val f1: 1.0031049984760743
reddit,dgl,1,506,52.5356,1.0031

epoch:508/50, training loss:0.2700513005256653
Train Acc 1.0005
 Acc 1.0031
new best val f1: 1.0031240475464798
reddit,dgl,1,507,52.6386,1.0031

epoch:509/50, training loss:0.26987165212631226
Train Acc 1.0005
 Acc 1.0032
new best val f1: 1.0031621456872906
reddit,dgl,1,508,52.7419,1.0032

epoch:510/50, training loss:0.26969313621520996
Train Acc 1.0005
 Acc 1.0031
reddit,dgl,1,509,52.8457,1.0031

epoch:511/50, training loss:0.26951491832733154
Train Acc 1.0005
 Acc 1.0032
new best val f1: 1.0031811947576958
reddit,dgl,1,510,52.9488,1.0032

epoch:512/50, training loss:0.2693377435207367
Train Acc 1.0006
 Acc 1.0032
reddit,dgl,1,511,53.0519,1.0032

epoch:513/50, training loss:0.2691601514816284
Train Acc 1.0006
 Acc 1.0032
new best val f1: 1.0032192928985066
reddit,dgl,1,512,53.1548,1.0032

epoch:514/50, training loss:0.26898375153541565
Train Acc 1.0006
 Acc 1.0032
reddit,dgl,1,513,53.2579,1.0032

epoch:515/50, training loss:0.268807590007782
Train Acc 1.0006
 Acc 1.0032
reddit,dgl,1,514,53.3611,1.0032

epoch:516/50, training loss:0.26863184571266174
Train Acc 1.0006
 Acc 1.0032
reddit,dgl,1,515,53.4644,1.0032

epoch:517/50, training loss:0.26845651865005493
Train Acc 1.0006
 Acc 1.0032
reddit,dgl,1,516,53.5673,1.0032

epoch:518/50, training loss:0.26828092336654663
Train Acc 1.0007
 Acc 1.0032
reddit,dgl,1,517,53.6702,1.0032

epoch:519/50, training loss:0.2681081295013428
Train Acc 1.0007
 Acc 1.0032
reddit,dgl,1,518,53.7732,1.0032

epoch:520/50, training loss:0.267934113740921
Train Acc 1.0007
 Acc 1.0032
reddit,dgl,1,519,53.8768,1.0032

epoch:521/50, training loss:0.26776033639907837
Train Acc 1.0007
 Acc 1.0032
reddit,dgl,1,520,53.9805,1.0032

epoch:522/50, training loss:0.267587810754776
Train Acc 1.0007
 Acc 1.0032
reddit,dgl,1,521,54.0836,1.0032

epoch:523/50, training loss:0.2674165666103363
Train Acc 1.0007
 Acc 1.0031
reddit,dgl,1,522,54.1865,1.0031

epoch:524/50, training loss:0.26724424958229065
Train Acc 1.0007
 Acc 1.0031
reddit,dgl,1,523,54.2895,1.0031

epoch:525/50, training loss:0.26707276701927185
Train Acc 1.0007
 Acc 1.0031
reddit,dgl,1,524,54.3925,1.0031

epoch:526/50, training loss:0.26690253615379333
Train Acc 1.0008
 Acc 1.0031
reddit,dgl,1,525,54.4957,1.0031

epoch:527/50, training loss:0.2667318880558014
Train Acc 1.0008
 Acc 1.0031
reddit,dgl,1,526,54.5990,1.0031

epoch:528/50, training loss:0.2665630877017975
Train Acc 1.0008
 Acc 1.0031
reddit,dgl,1,527,54.7022,1.0031

epoch:529/50, training loss:0.2663930654525757
Train Acc 1.0008
 Acc 1.0031
reddit,dgl,1,528,54.8052,1.0031

epoch:530/50, training loss:0.2662239372730255
Train Acc 1.0009
 Acc 1.0032
reddit,dgl,1,529,54.9081,1.0032

epoch:531/50, training loss:0.2660548985004425
Train Acc 1.0009
 Acc 1.0031
reddit,dgl,1,530,55.0112,1.0031

epoch:532/50, training loss:0.2658880054950714
Train Acc 1.0009
 Acc 1.0032
reddit,dgl,1,531,55.1145,1.0032

epoch:533/50, training loss:0.2657202184200287
Train Acc 1.0009
 Acc 1.0032
reddit,dgl,1,532,55.2177,1.0032

epoch:534/50, training loss:0.2655530571937561
Train Acc 1.0010
 Acc 1.0032
reddit,dgl,1,533,55.3207,1.0032

epoch:535/50, training loss:0.2653869390487671
Train Acc 1.0010
 Acc 1.0032
reddit,dgl,1,534,55.4237,1.0032

epoch:536/50, training loss:0.2652221918106079
Train Acc 1.0010
 Acc 1.0032
new best val f1: 1.003238341968912
reddit,dgl,1,535,55.5268,1.0032

epoch:537/50, training loss:0.2650560438632965
Train Acc 1.0010
 Acc 1.0032
reddit,dgl,1,536,55.6298,1.0032

epoch:538/50, training loss:0.2648910880088806
Train Acc 1.0010
 Acc 1.0032
reddit,dgl,1,537,55.7331,1.0032

epoch:539/50, training loss:0.2647268772125244
Train Acc 1.0011
 Acc 1.0033
new best val f1: 1.0032573910393172
reddit,dgl,1,538,55.8362,1.0033

epoch:540/50, training loss:0.26456335186958313
Train Acc 1.0011
 Acc 1.0033
new best val f1: 1.003295489180128
reddit,dgl,1,539,55.9391,1.0033

epoch:541/50, training loss:0.2643991708755493
Train Acc 1.0011
 Acc 1.0033
new best val f1: 1.0033335873209388
reddit,dgl,1,540,56.0420,1.0033

epoch:542/50, training loss:0.2642355263233185
Train Acc 1.0011
 Acc 1.0034
new best val f1: 1.0033716854617494
reddit,dgl,1,541,56.1451,1.0034

epoch:543/50, training loss:0.2640739381313324
Train Acc 1.0011
 Acc 1.0034
new best val f1: 1.0033907345321549
reddit,dgl,1,542,56.2483,1.0034

epoch:544/50, training loss:0.2639109790325165
Train Acc 1.0011
 Acc 1.0035
new best val f1: 1.0034669308137762
reddit,dgl,1,543,56.3516,1.0035

epoch:545/50, training loss:0.26374876499176025
Train Acc 1.0012
 Acc 1.0035
reddit,dgl,1,544,56.4548,1.0035

epoch:546/50, training loss:0.2635883688926697
Train Acc 1.0012
 Acc 1.0035
new best val f1: 1.0034859798841818
reddit,dgl,1,545,56.5578,1.0035

epoch:547/50, training loss:0.2634275257587433
Train Acc 1.0012
 Acc 1.0035
reddit,dgl,1,546,56.6608,1.0035

epoch:548/50, training loss:0.2632659077644348
Train Acc 1.0012
 Acc 1.0035
new best val f1: 1.003505028954587
reddit,dgl,1,547,56.7638,1.0035

epoch:549/50, training loss:0.263107568025589
Train Acc 1.0013
 Acc 1.0035
new best val f1: 1.0035431270953978
reddit,dgl,1,548,56.8670,1.0035

epoch:550/50, training loss:0.26294755935668945
Train Acc 1.0013
 Acc 1.0035
reddit,dgl,1,549,56.9703,1.0035

epoch:551/50, training loss:0.2627876102924347
Train Acc 1.0013
 Acc 1.0035
reddit,dgl,1,550,57.0732,1.0035

epoch:552/50, training loss:0.2626294791698456
Train Acc 1.0013
 Acc 1.0035
reddit,dgl,1,551,57.1763,1.0035

epoch:553/50, training loss:0.2624717056751251
Train Acc 1.0013
 Acc 1.0036
new best val f1: 1.0035621761658031
reddit,dgl,1,552,57.2792,1.0036

epoch:554/50, training loss:0.26231443881988525
Train Acc 1.0013
 Acc 1.0036
new best val f1: 1.0035812252362084
reddit,dgl,1,553,57.3826,1.0036

epoch:555/50, training loss:0.26215630769729614
Train Acc 1.0013
 Acc 1.0036
new best val f1: 1.003600274306614
reddit,dgl,1,554,57.4863,1.0036

epoch:556/50, training loss:0.26199933886528015
Train Acc 1.0014
 Acc 1.0036
new best val f1: 1.0036383724474245
reddit,dgl,1,555,57.5896,1.0036

epoch:557/50, training loss:0.26184332370758057
Train Acc 1.0014
 Acc 1.0036
reddit,dgl,1,556,57.6926,1.0036

epoch:558/50, training loss:0.2616879642009735
Train Acc 1.0014
 Acc 1.0036
reddit,dgl,1,557,57.7955,1.0036

epoch:559/50, training loss:0.26153066754341125
Train Acc 1.0014
 Acc 1.0036
reddit,dgl,1,558,57.8984,1.0036

epoch:560/50, training loss:0.2613757252693176
Train Acc 1.0014
 Acc 1.0037
new best val f1: 1.0036764705882353
reddit,dgl,1,559,58.0016,1.0037

epoch:561/50, training loss:0.2612212300300598
Train Acc 1.0015
 Acc 1.0037
reddit,dgl,1,560,58.1048,1.0037

epoch:562/50, training loss:0.26106733083724976
Train Acc 1.0015
 Acc 1.0037
new best val f1: 1.0036955196586406
reddit,dgl,1,561,58.2085,1.0037

epoch:563/50, training loss:0.26091283559799194
Train Acc 1.0015
 Acc 1.0037
new best val f1: 1.0037336177994514
reddit,dgl,1,562,58.3114,1.0037

epoch:564/50, training loss:0.26075971126556396
Train Acc 1.0016
 Acc 1.0037
reddit,dgl,1,563,58.4145,1.0037

epoch:565/50, training loss:0.26060694456100464
Train Acc 1.0016
 Acc 1.0038
new best val f1: 1.0037526668698566
reddit,dgl,1,564,58.5173,1.0038

epoch:566/50, training loss:0.26045310497283936
Train Acc 1.0016
 Acc 1.0038
new best val f1: 1.0037717159402622
reddit,dgl,1,565,58.6206,1.0038

epoch:567/50, training loss:0.2603010833263397
Train Acc 1.0016
 Acc 1.0038
new best val f1: 1.0037907650106674
reddit,dgl,1,566,58.7237,1.0038

epoch:568/50, training loss:0.2601490616798401
Train Acc 1.0016
 Acc 1.0038
reddit,dgl,1,567,58.8270,1.0038

epoch:569/50, training loss:0.259997695684433
Train Acc 1.0016
 Acc 1.0038
reddit,dgl,1,568,58.9299,1.0038

epoch:570/50, training loss:0.25984543561935425
Train Acc 1.0016
 Acc 1.0038
new best val f1: 1.0038288631514782
reddit,dgl,1,569,59.0329,1.0038

epoch:571/50, training loss:0.25969555974006653
Train Acc 1.0017
 Acc 1.0038
reddit,dgl,1,570,59.1359,1.0038

epoch:572/50, training loss:0.25954577326774597
Train Acc 1.0017
 Acc 1.0038
reddit,dgl,1,571,59.2391,1.0038

epoch:573/50, training loss:0.25939464569091797
Train Acc 1.0017
 Acc 1.0038
new best val f1: 1.0038479122218835
reddit,dgl,1,572,59.3428,1.0038

epoch:574/50, training loss:0.2592458724975586
Train Acc 1.0017
 Acc 1.0038
reddit,dgl,1,573,59.4460,1.0038

epoch:575/50, training loss:0.2590969204902649
Train Acc 1.0017
 Acc 1.0039
new best val f1: 1.0038860103626943
reddit,dgl,1,574,59.5489,1.0039

epoch:576/50, training loss:0.2589472830295563
Train Acc 1.0018
 Acc 1.0039
reddit,dgl,1,575,59.6519,1.0039

epoch:577/50, training loss:0.25879815220832825
Train Acc 1.0018
 Acc 1.0039
reddit,dgl,1,576,59.7549,1.0039

epoch:578/50, training loss:0.2586509883403778
Train Acc 1.0018
 Acc 1.0039
new best val f1: 1.0039050594330996
reddit,dgl,1,577,59.8582,1.0039

epoch:579/50, training loss:0.25850313901901245
Train Acc 1.0018
 Acc 1.0039
reddit,dgl,1,578,59.9618,1.0039

epoch:580/50, training loss:0.25835564732551575
Train Acc 1.0018
 Acc 1.0039
new best val f1: 1.003924108503505
reddit,dgl,1,579,60.0650,1.0039

epoch:581/50, training loss:0.25820884108543396
Train Acc 1.0018
 Acc 1.0040
new best val f1: 1.0039812557147212
reddit,dgl,1,580,60.1681,1.0040

epoch:582/50, training loss:0.2580624222755432
Train Acc 1.0018
 Acc 1.0040
new best val f1: 1.0040003047851265
reddit,dgl,1,581,60.2710,1.0040

epoch:583/50, training loss:0.257915198802948
Train Acc 1.0018
 Acc 1.0040
reddit,dgl,1,582,60.3739,1.0040

epoch:584/50, training loss:0.2577703893184662
Train Acc 1.0018
 Acc 1.0040
reddit,dgl,1,583,60.4770,1.0040

epoch:585/50, training loss:0.2576247453689575
Train Acc 1.0019
 Acc 1.0040
reddit,dgl,1,584,60.5802,1.0040

epoch:586/50, training loss:0.25747939944267273
Train Acc 1.0019
 Acc 1.0040
reddit,dgl,1,585,60.6835,1.0040

epoch:587/50, training loss:0.2573346495628357
Train Acc 1.0019
 Acc 1.0040
reddit,dgl,1,586,60.7867,1.0040

epoch:588/50, training loss:0.25719010829925537
Train Acc 1.0019
 Acc 1.0041
new best val f1: 1.0040765010667478
reddit,dgl,1,587,60.8898,1.0041

epoch:589/50, training loss:0.2570463716983795
Train Acc 1.0019
 Acc 1.0041
new best val f1: 1.0040955501371533
reddit,dgl,1,588,60.9928,1.0041

epoch:590/50, training loss:0.25690215826034546
Train Acc 1.0020
 Acc 1.0041
new best val f1: 1.0041145992075586
reddit,dgl,1,589,61.0961,1.0041

epoch:591/50, training loss:0.25675874948501587
Train Acc 1.0020
 Acc 1.0041
new best val f1: 1.0041336482779641
reddit,dgl,1,590,61.1998,1.0041

epoch:592/50, training loss:0.2566162347793579
Train Acc 1.0020
 Acc 1.0041
reddit,dgl,1,591,61.3029,1.0041

epoch:593/50, training loss:0.2564728558063507
Train Acc 1.0020
 Acc 1.0041
reddit,dgl,1,592,61.4060,1.0041

epoch:594/50, training loss:0.2563313841819763
Train Acc 1.0020
 Acc 1.0041
reddit,dgl,1,593,61.5090,1.0041

epoch:595/50, training loss:0.25618934631347656
Train Acc 1.0020
 Acc 1.0042
new best val f1: 1.0041526973483694
reddit,dgl,1,594,61.6120,1.0042

epoch:596/50, training loss:0.25604748725891113
Train Acc 1.0020
 Acc 1.0042
new best val f1: 1.0041717464187747
reddit,dgl,1,595,61.7153,1.0042

epoch:597/50, training loss:0.255906879901886
Train Acc 1.0021
 Acc 1.0041
reddit,dgl,1,596,61.8185,1.0041

epoch:598/50, training loss:0.2557656764984131
Train Acc 1.0020
 Acc 1.0041
reddit,dgl,1,597,61.9215,1.0041

epoch:599/50, training loss:0.25562477111816406
Train Acc 1.0021
 Acc 1.0041
reddit,dgl,1,598,62.0245,1.0041

epoch:600/50, training loss:0.25548508763313293
Train Acc 1.0021
 Acc 1.0041
reddit,dgl,1,599,62.1275,1.0041

epoch:601/50, training loss:0.25534525513648987
Train Acc 1.0021
 Acc 1.0041
reddit,dgl,1,600,62.2307,1.0041

epoch:602/50, training loss:0.2552051246166229
Train Acc 1.0021
 Acc 1.0041
reddit,dgl,1,601,62.3339,1.0041

epoch:603/50, training loss:0.255066454410553
Train Acc 1.0021
 Acc 1.0041
reddit,dgl,1,602,62.4372,1.0041

epoch:604/50, training loss:0.2549276351928711
Train Acc 1.0022
 Acc 1.0041
reddit,dgl,1,603,62.5401,1.0041

epoch:605/50, training loss:0.2547900676727295
Train Acc 1.0022
 Acc 1.0041
reddit,dgl,1,604,62.6432,1.0041

epoch:606/50, training loss:0.2546515166759491
Train Acc 1.0022
 Acc 1.0042
reddit,dgl,1,605,62.7462,1.0042

epoch:607/50, training loss:0.2545134425163269
Train Acc 1.0022
 Acc 1.0042
reddit,dgl,1,606,62.8497,1.0042

epoch:608/50, training loss:0.25437548756599426
Train Acc 1.0022
 Acc 1.0042
reddit,dgl,1,607,62.9534,1.0042

epoch:609/50, training loss:0.2542387545108795
Train Acc 1.0022
 Acc 1.0042
reddit,dgl,1,608,63.0567,1.0042

epoch:610/50, training loss:0.2541015148162842
Train Acc 1.0022
 Acc 1.0042
new best val f1: 1.0041907954891802
reddit,dgl,1,609,63.1597,1.0042

epoch:611/50, training loss:0.25396624207496643
Train Acc 1.0022
 Acc 1.0042
reddit,dgl,1,610,63.2625,1.0042

epoch:612/50, training loss:0.25382915139198303
Train Acc 1.0022
 Acc 1.0042
new best val f1: 1.0042098445595855
reddit,dgl,1,611,63.3655,1.0042

epoch:613/50, training loss:0.253694087266922
Train Acc 1.0023
 Acc 1.0042
reddit,dgl,1,612,63.4685,1.0042

epoch:614/50, training loss:0.253557950258255
Train Acc 1.0023
 Acc 1.0042
reddit,dgl,1,613,63.5717,1.0042

epoch:615/50, training loss:0.25342288613319397
Train Acc 1.0023
 Acc 1.0042
reddit,dgl,1,614,63.6750,1.0042

epoch:616/50, training loss:0.2532877027988434
Train Acc 1.0023
 Acc 1.0042
new best val f1: 1.0042288936299908
reddit,dgl,1,615,63.7780,1.0042

epoch:617/50, training loss:0.2531529366970062
Train Acc 1.0023
 Acc 1.0043
new best val f1: 1.0042860408412069
reddit,dgl,1,616,63.8807,1.0043

epoch:618/50, training loss:0.2530191242694855
Train Acc 1.0024
 Acc 1.0044
new best val f1: 1.0043622371228285
reddit,dgl,1,617,63.9836,1.0044

epoch:619/50, training loss:0.2528845965862274
Train Acc 1.0024
 Acc 1.0043
reddit,dgl,1,618,64.0868,1.0043

epoch:620/50, training loss:0.2527526021003723
Train Acc 1.0024
 Acc 1.0043
reddit,dgl,1,619,64.1904,1.0043

epoch:621/50, training loss:0.25261810421943665
Train Acc 1.0024
 Acc 1.0043
reddit,dgl,1,620,64.2936,1.0043

epoch:622/50, training loss:0.2524852752685547
Train Acc 1.0024
 Acc 1.0044
reddit,dgl,1,621,64.3966,1.0044

epoch:623/50, training loss:0.2523523271083832
Train Acc 1.0024
 Acc 1.0044
new best val f1: 1.0043812861932337
reddit,dgl,1,622,64.4995,1.0044

epoch:624/50, training loss:0.2522200644016266
Train Acc 1.0025
 Acc 1.0044
new best val f1: 1.004400335263639
reddit,dgl,1,623,64.6027,1.0044

epoch:625/50, training loss:0.2520875632762909
Train Acc 1.0025
 Acc 1.0044
reddit,dgl,1,624,64.7061,1.0044

epoch:626/50, training loss:0.2519562542438507
Train Acc 1.0025
 Acc 1.0044
new best val f1: 1.0044193843340445
reddit,dgl,1,625,64.8098,1.0044

epoch:627/50, training loss:0.2518254220485687
Train Acc 1.0025
 Acc 1.0044
new best val f1: 1.0044384334044498
reddit,dgl,1,626,64.9130,1.0044

epoch:628/50, training loss:0.2516937553882599
Train Acc 1.0025
 Acc 1.0045
new best val f1: 1.0044765315452606
reddit,dgl,1,627,65.0161,1.0045

epoch:629/50, training loss:0.2515626847743988
Train Acc 1.0026
 Acc 1.0045
reddit,dgl,1,628,65.1190,1.0045

epoch:630/50, training loss:0.25143277645111084
Train Acc 1.0025
 Acc 1.0045
reddit,dgl,1,629,65.2221,1.0045

epoch:631/50, training loss:0.25130218267440796
Train Acc 1.0026
 Acc 1.0045
reddit,dgl,1,630,65.3253,1.0045

epoch:632/50, training loss:0.25117236375808716
Train Acc 1.0026
 Acc 1.0045
new best val f1: 1.004495580615666
reddit,dgl,1,631,65.4286,1.0045

epoch:633/50, training loss:0.2510424554347992
Train Acc 1.0026
 Acc 1.0045
new best val f1: 1.0045336787564767
reddit,dgl,1,632,65.5315,1.0045

epoch:634/50, training loss:0.250912606716156
Train Acc 1.0026
 Acc 1.0046
new best val f1: 1.004552727826882
reddit,dgl,1,633,65.6347,1.0046

epoch:635/50, training loss:0.25078436732292175
Train Acc 1.0027
 Acc 1.0046
new best val f1: 1.0045717768972875
reddit,dgl,1,634,65.7380,1.0046

epoch:636/50, training loss:0.2506553530693054
Train Acc 1.0027
 Acc 1.0046
new best val f1: 1.0045908259676928
reddit,dgl,1,635,65.8409,1.0046

epoch:637/50, training loss:0.2505261301994324
Train Acc 1.0027
 Acc 1.0045
reddit,dgl,1,636,65.9440,1.0045

epoch:638/50, training loss:0.2503988742828369
Train Acc 1.0027
 Acc 1.0046
reddit,dgl,1,637,66.0470,1.0046

epoch:639/50, training loss:0.25027045607566833
Train Acc 1.0027
 Acc 1.0045
reddit,dgl,1,638,66.1502,1.0045

epoch:640/50, training loss:0.2501431703567505
Train Acc 1.0027
 Acc 1.0046
reddit,dgl,1,639,66.2535,1.0046

epoch:641/50, training loss:0.25001633167266846
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,640,66.3567,1.0045

epoch:642/50, training loss:0.2498885840177536
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,641,66.4597,1.0045

epoch:643/50, training loss:0.24976210296154022
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,642,66.5628,1.0045

epoch:644/50, training loss:0.24963536858558655
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,643,66.6694,1.0045

epoch:645/50, training loss:0.24950946867465973
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,644,66.7729,1.0045

epoch:646/50, training loss:0.2493831217288971
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,645,66.8766,1.0045

epoch:647/50, training loss:0.24925801157951355
Train Acc 1.0028
 Acc 1.0045
reddit,dgl,1,646,66.9797,1.0045

epoch:648/50, training loss:0.24913246929645538
Train Acc 1.0029
 Acc 1.0046
reddit,dgl,1,647,67.0827,1.0046

epoch:649/50, training loss:0.2490074187517166
Train Acc 1.0029
 Acc 1.0045
reddit,dgl,1,648,67.1856,1.0045

epoch:650/50, training loss:0.24888218939304352
Train Acc 1.0029
 Acc 1.0045
reddit,dgl,1,649,67.2890,1.0045

epoch:651/50, training loss:0.24875761568546295
Train Acc 1.0030
 Acc 1.0045
reddit,dgl,1,650,67.3922,1.0045

epoch:652/50, training loss:0.2486342340707779
Train Acc 1.0030
 Acc 1.0045
reddit,dgl,1,651,67.4954,1.0045

epoch:653/50, training loss:0.2485097199678421
Train Acc 1.0030
 Acc 1.0046
reddit,dgl,1,652,67.5985,1.0046

epoch:654/50, training loss:0.24838490784168243
Train Acc 1.0030
 Acc 1.0046
new best val f1: 1.004609875038098
reddit,dgl,1,653,67.7015,1.0046

epoch:655/50, training loss:0.24826215207576752
Train Acc 1.0030
 Acc 1.0046
new best val f1: 1.0046289241085036
reddit,dgl,1,654,67.8045,1.0046

epoch:656/50, training loss:0.24814002215862274
Train Acc 1.0031
 Acc 1.0047
new best val f1: 1.0046670222493141
reddit,dgl,1,655,67.9075,1.0047

epoch:657/50, training loss:0.24801653623580933
Train Acc 1.0031
 Acc 1.0046
reddit,dgl,1,656,68.0105,1.0046

epoch:658/50, training loss:0.24789422750473022
Train Acc 1.0031
 Acc 1.0046
reddit,dgl,1,657,68.1140,1.0046

epoch:659/50, training loss:0.24777212738990784
Train Acc 1.0031
 Acc 1.0047
new best val f1: 1.0046860713197197
reddit,dgl,1,658,68.2173,1.0047

epoch:660/50, training loss:0.2476499229669571
Train Acc 1.0032
 Acc 1.0047
new best val f1: 1.0047241694605302
reddit,dgl,1,659,68.3205,1.0047

epoch:661/50, training loss:0.2475275993347168
Train Acc 1.0032
 Acc 1.0048
new best val f1: 1.004762267601341
reddit,dgl,1,660,68.4234,1.0048

epoch:662/50, training loss:0.2474069446325302
Train Acc 1.0032
 Acc 1.0047
reddit,dgl,1,661,68.5264,1.0047

epoch:663/50, training loss:0.24728550016880035
Train Acc 1.0032
 Acc 1.0048
new best val f1: 1.0048003657421518
reddit,dgl,1,662,68.6298,1.0048

epoch:664/50, training loss:0.2471643090248108
Train Acc 1.0032
 Acc 1.0048
reddit,dgl,1,663,68.7335,1.0048

epoch:665/50, training loss:0.24704313278198242
Train Acc 1.0033
 Acc 1.0048
new best val f1: 1.0048384638829626
reddit,dgl,1,664,68.8366,1.0048

epoch:666/50, training loss:0.24692323803901672
Train Acc 1.0033
 Acc 1.0048
reddit,dgl,1,665,68.9397,1.0048

epoch:667/50, training loss:0.24680195748806
Train Acc 1.0033
 Acc 1.0048
reddit,dgl,1,666,69.0428,1.0048

epoch:668/50, training loss:0.2466832399368286
Train Acc 1.0033
 Acc 1.0049
new best val f1: 1.004857512953368
reddit,dgl,1,667,69.1461,1.0049

epoch:669/50, training loss:0.24656273424625397
Train Acc 1.0033
 Acc 1.0049
new best val f1: 1.0048765620237732
reddit,dgl,1,668,69.2493,1.0049

epoch:670/50, training loss:0.24644328653812408
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,669,69.3526,1.0049

epoch:671/50, training loss:0.2463238537311554
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,670,69.4558,1.0049

epoch:672/50, training loss:0.24620535969734192
Train Acc 1.0034
 Acc 1.0049
new best val f1: 1.004914660164584
reddit,dgl,1,671,69.5588,1.0049

epoch:673/50, training loss:0.2460862100124359
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,672,69.6618,1.0049

epoch:674/50, training loss:0.24596866965293884
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,673,69.7650,1.0049

epoch:675/50, training loss:0.2458501011133194
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,674,69.8682,1.0049

epoch:676/50, training loss:0.24573206901550293
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,675,69.9714,1.0049

epoch:677/50, training loss:0.2456144541501999
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,676,70.0743,1.0049

epoch:678/50, training loss:0.24549664556980133
Train Acc 1.0034
 Acc 1.0049
new best val f1: 1.0049337092349893
reddit,dgl,1,677,70.1774,1.0049

epoch:679/50, training loss:0.24537961184978485
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,678,70.2806,1.0049

epoch:680/50, training loss:0.24526196718215942
Train Acc 1.0034
 Acc 1.0049
reddit,dgl,1,679,70.3843,1.0049

epoch:681/50, training loss:0.2451462745666504
Train Acc 1.0035
 Acc 1.0049
reddit,dgl,1,680,70.4873,1.0049

epoch:682/50, training loss:0.24502912163734436
Train Acc 1.0035
 Acc 1.0050
new best val f1: 1.0049527583053948
reddit,dgl,1,681,70.5904,1.0050

epoch:683/50, training loss:0.24491307139396667
Train Acc 1.0035
 Acc 1.0050
reddit,dgl,1,682,70.6934,1.0050

epoch:684/50, training loss:0.2447967529296875
Train Acc 1.0035
 Acc 1.0050
new best val f1: 1.0049908564462053
reddit,dgl,1,683,70.7967,1.0050

epoch:685/50, training loss:0.24468103051185608
Train Acc 1.0035
 Acc 1.0050
reddit,dgl,1,684,70.9004,1.0050

epoch:686/50, training loss:0.24456532299518585
Train Acc 1.0035
 Acc 1.0050
new best val f1: 1.0050099055166108
reddit,dgl,1,685,71.0038,1.0050

epoch:687/50, training loss:0.24445007741451263
Train Acc 1.0035
 Acc 1.0050
new best val f1: 1.0050289545870161
reddit,dgl,1,686,71.1067,1.0050

epoch:688/50, training loss:0.24433453381061554
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,687,71.2097,1.0050

epoch:689/50, training loss:0.2442197948694229
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,688,71.3127,1.0050

epoch:690/50, training loss:0.24410589039325714
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,689,71.4160,1.0050

epoch:691/50, training loss:0.2439904510974884
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,690,71.5192,1.0050

epoch:692/50, training loss:0.24387729167938232
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,691,71.6221,1.0050

epoch:693/50, training loss:0.24376235902309418
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,692,71.7251,1.0050

epoch:694/50, training loss:0.24364930391311646
Train Acc 1.0036
 Acc 1.0050
reddit,dgl,1,693,71.8282,1.0050

epoch:695/50, training loss:0.2435353398323059
Train Acc 1.0037
 Acc 1.0050
reddit,dgl,1,694,71.9314,1.0050

epoch:696/50, training loss:0.2434225082397461
Train Acc 1.0037
 Acc 1.0050
new best val f1: 1.0050480036574214
reddit,dgl,1,695,72.0347,1.0050

epoch:697/50, training loss:0.24330897629261017
Train Acc 1.0037
 Acc 1.0050
reddit,dgl,1,696,72.1376,1.0050

epoch:698/50, training loss:0.2431960254907608
Train Acc 1.0037
 Acc 1.0050
reddit,dgl,1,697,72.2406,1.0050

epoch:699/50, training loss:0.24308331310749054
Train Acc 1.0037
 Acc 1.0051
new best val f1: 1.0050861017982322
reddit,dgl,1,698,72.3436,1.0051

epoch:700/50, training loss:0.24297139048576355
Train Acc 1.0037
 Acc 1.0051
new best val f1: 1.0051051508686377
reddit,dgl,1,699,72.4468,1.0051

epoch:701/50, training loss:0.2428584098815918
Train Acc 1.0037
 Acc 1.0051
new best val f1: 1.0051432490094483
reddit,dgl,1,700,72.5500,1.0051

epoch:702/50, training loss:0.24274630844593048
Train Acc 1.0038
 Acc 1.0051
reddit,dgl,1,701,72.6530,1.0051

epoch:703/50, training loss:0.24263495206832886
Train Acc 1.0038
 Acc 1.0052
new best val f1: 1.0051622980798538
reddit,dgl,1,702,72.7558,1.0052

epoch:704/50, training loss:0.2425232082605362
Train Acc 1.0038
 Acc 1.0052
reddit,dgl,1,703,72.8589,1.0052

epoch:705/50, training loss:0.24241220951080322
Train Acc 1.0038
 Acc 1.0052
new best val f1: 1.005181347150259
reddit,dgl,1,704,72.9621,1.0052

epoch:706/50, training loss:0.24230046570301056
Train Acc 1.0038
 Acc 1.0052
new best val f1: 1.0052003962206644
reddit,dgl,1,705,73.0658,1.0052

epoch:707/50, training loss:0.24219034612178802
Train Acc 1.0038
 Acc 1.0052
reddit,dgl,1,706,73.1690,1.0052

epoch:708/50, training loss:0.24208001792430878
Train Acc 1.0038
 Acc 1.0052
new best val f1: 1.0052194452910699
reddit,dgl,1,707,73.2719,1.0052

epoch:709/50, training loss:0.24196943640708923
Train Acc 1.0038
 Acc 1.0052
reddit,dgl,1,708,73.3749,1.0052

epoch:710/50, training loss:0.24185840785503387
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,709,73.4779,1.0052

epoch:711/50, training loss:0.2417483776807785
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,710,73.5812,1.0052

epoch:712/50, training loss:0.2416388988494873
Train Acc 1.0039
 Acc 1.0053
new best val f1: 1.0052575434318805
reddit,dgl,1,711,73.6849,1.0053

epoch:713/50, training loss:0.24152861535549164
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,712,73.7882,1.0052

epoch:714/50, training loss:0.24141988158226013
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,713,73.8912,1.0052

epoch:715/50, training loss:0.2413102686405182
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,714,73.9942,1.0052

epoch:716/50, training loss:0.241201713681221
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,715,74.0977,1.0052

epoch:717/50, training loss:0.24109292030334473
Train Acc 1.0039
 Acc 1.0052
reddit,dgl,1,716,74.2007,1.0052

epoch:718/50, training loss:0.24098306894302368
Train Acc 1.0040
 Acc 1.0052
reddit,dgl,1,717,74.3042,1.0052

epoch:719/50, training loss:0.24087601900100708
Train Acc 1.0040
 Acc 1.0052
reddit,dgl,1,718,74.4081,1.0052

epoch:720/50, training loss:0.24076664447784424
Train Acc 1.0040
 Acc 1.0052
reddit,dgl,1,719,74.5116,1.0052

epoch:721/50, training loss:0.24065910279750824
Train Acc 1.0040
 Acc 1.0052
reddit,dgl,1,720,74.6147,1.0052

epoch:722/50, training loss:0.24055105447769165
Train Acc 1.0040
 Acc 1.0052
reddit,dgl,1,721,74.7180,1.0052

epoch:723/50, training loss:0.2404434084892273
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,722,74.8212,1.0052

epoch:724/50, training loss:0.2403361201286316
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,723,74.9242,1.0052

epoch:725/50, training loss:0.2402287870645523
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,724,75.0287,1.0052

epoch:726/50, training loss:0.24012182652950287
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,725,75.1319,1.0052

epoch:727/50, training loss:0.2400149255990982
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,726,75.2350,1.0052

epoch:728/50, training loss:0.2399076372385025
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,727,75.3379,1.0052

epoch:729/50, training loss:0.2398018091917038
Train Acc 1.0041
 Acc 1.0052
reddit,dgl,1,728,75.4408,1.0052

epoch:730/50, training loss:0.23969510197639465
Train Acc 1.0042
 Acc 1.0052
reddit,dgl,1,729,75.5443,1.0052

epoch:731/50, training loss:0.23958931863307953
Train Acc 1.0042
 Acc 1.0053
new best val f1: 1.005276592502286
reddit,dgl,1,730,75.6480,1.0053

epoch:732/50, training loss:0.23948392271995544
Train Acc 1.0042
 Acc 1.0054
new best val f1: 1.0053527887839073
reddit,dgl,1,731,75.7513,1.0054

epoch:733/50, training loss:0.23937778174877167
Train Acc 1.0042
 Acc 1.0054
new best val f1: 1.0053718378543126
reddit,dgl,1,732,75.8543,1.0054

epoch:734/50, training loss:0.23927249014377594
Train Acc 1.0042
 Acc 1.0054
reddit,dgl,1,733,75.9573,1.0054

epoch:735/50, training loss:0.23916718363761902
Train Acc 1.0043
 Acc 1.0054
reddit,dgl,1,734,76.0607,1.0054

epoch:736/50, training loss:0.23906183242797852
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,735,76.1642,1.0053

epoch:737/50, training loss:0.23895731568336487
Train Acc 1.0043
 Acc 1.0054
reddit,dgl,1,736,76.2679,1.0054

epoch:738/50, training loss:0.23885303735733032
Train Acc 1.0043
 Acc 1.0054
reddit,dgl,1,737,76.3708,1.0054

epoch:739/50, training loss:0.23874720931053162
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,738,76.4738,1.0053

epoch:740/50, training loss:0.23864337801933289
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,739,76.5768,1.0053

epoch:741/50, training loss:0.2385397106409073
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,740,76.6804,1.0053

epoch:742/50, training loss:0.23843517899513245
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,741,76.7841,1.0053

epoch:743/50, training loss:0.23833107948303223
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,742,76.8872,1.0053

epoch:744/50, training loss:0.23822811245918274
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,743,76.9902,1.0053

epoch:745/50, training loss:0.23812472820281982
Train Acc 1.0043
 Acc 1.0053
reddit,dgl,1,744,77.0931,1.0053

epoch:746/50, training loss:0.2380215972661972
Train Acc 1.0043
 Acc 1.0054
reddit,dgl,1,745,77.1964,1.0054

epoch:747/50, training loss:0.2379184514284134
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,746,77.2996,1.0054

epoch:748/50, training loss:0.2378162145614624
Train Acc 1.0044
 Acc 1.0054
new best val f1: 1.0054099359951234
reddit,dgl,1,747,77.4033,1.0054

epoch:749/50, training loss:0.23771332204341888
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,748,77.5064,1.0054

epoch:750/50, training loss:0.23761104047298431
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,749,77.6095,1.0054

epoch:751/50, training loss:0.23750810325145721
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,750,77.7125,1.0054

epoch:752/50, training loss:0.2374061942100525
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,751,77.8155,1.0054

epoch:753/50, training loss:0.23730473220348358
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,752,77.9189,1.0054

epoch:754/50, training loss:0.23720265924930573
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,753,78.0221,1.0054

epoch:755/50, training loss:0.23710161447525024
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,754,78.1253,1.0054

epoch:756/50, training loss:0.23700009286403656
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,755,78.2283,1.0054

epoch:757/50, training loss:0.23689903318881989
Train Acc 1.0044
 Acc 1.0054
reddit,dgl,1,756,78.3312,1.0054

epoch:758/50, training loss:0.23679837584495544
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,757,78.4342,1.0053

epoch:759/50, training loss:0.23669777810573578
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,758,78.5375,1.0053

epoch:760/50, training loss:0.23659737408161163
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,759,78.6408,1.0053

epoch:761/50, training loss:0.23649731278419495
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,760,78.7437,1.0053

epoch:762/50, training loss:0.23639710247516632
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,761,78.8467,1.0053

epoch:763/50, training loss:0.23629705607891083
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,762,78.9497,1.0053

epoch:764/50, training loss:0.23619785904884338
Train Acc 1.0045
 Acc 1.0053
reddit,dgl,1,763,79.0530,1.0053

epoch:765/50, training loss:0.23609688878059387
Train Acc 1.0045
 Acc 1.0054
reddit,dgl,1,764,79.1566,1.0054

epoch:766/50, training loss:0.23599912226200104
Train Acc 1.0045
 Acc 1.0054
reddit,dgl,1,765,79.2599,1.0054

epoch:767/50, training loss:0.23589947819709778
Train Acc 1.0046
 Acc 1.0054
reddit,dgl,1,766,79.3628,1.0054

epoch:768/50, training loss:0.23579975962638855
Train Acc 1.0046
 Acc 1.0054
reddit,dgl,1,767,79.4658,1.0054

epoch:769/50, training loss:0.23570170998573303
Train Acc 1.0046
 Acc 1.0054
reddit,dgl,1,768,79.5689,1.0054

epoch:770/50, training loss:0.23560293018817902
Train Acc 1.0046
 Acc 1.0053
reddit,dgl,1,769,79.6721,1.0053

epoch:771/50, training loss:0.23550355434417725
Train Acc 1.0046
 Acc 1.0054
reddit,dgl,1,770,79.7758,1.0054

epoch:772/50, training loss:0.2354062795639038
Train Acc 1.0046
 Acc 1.0054
reddit,dgl,1,771,79.8790,1.0054

epoch:773/50, training loss:0.23530788719654083
Train Acc 1.0047
 Acc 1.0054
reddit,dgl,1,772,79.9820,1.0054

epoch:774/50, training loss:0.23521052300930023
Train Acc 1.0047
 Acc 1.0054
new best val f1: 1.0054480341359342
reddit,dgl,1,773,80.0850,1.0054

epoch:775/50, training loss:0.2351127564907074
Train Acc 1.0047
 Acc 1.0054
reddit,dgl,1,774,80.1880,1.0054

epoch:776/50, training loss:0.23501469194889069
Train Acc 1.0047
 Acc 1.0054
reddit,dgl,1,775,80.2912,1.0054

epoch:777/50, training loss:0.2349175065755844
Train Acc 1.0047
 Acc 1.0054
reddit,dgl,1,776,80.3945,1.0054

epoch:778/50, training loss:0.23482055962085724
Train Acc 1.0047
 Acc 1.0054
reddit,dgl,1,777,80.4974,1.0054

epoch:779/50, training loss:0.23472397029399872
Train Acc 1.0048
 Acc 1.0054
reddit,dgl,1,778,80.6005,1.0054

epoch:780/50, training loss:0.2346266657114029
Train Acc 1.0048
 Acc 1.0054
reddit,dgl,1,779,80.7035,1.0054

epoch:781/50, training loss:0.23452989757061005
Train Acc 1.0048
 Acc 1.0053
reddit,dgl,1,780,80.8070,1.0053

epoch:782/50, training loss:0.23443324863910675
Train Acc 1.0048
 Acc 1.0054
reddit,dgl,1,781,80.9108,1.0054

epoch:783/50, training loss:0.23433615267276764
Train Acc 1.0048
 Acc 1.0054
reddit,dgl,1,782,81.0139,1.0054

epoch:784/50, training loss:0.23424121737480164
Train Acc 1.0048
 Acc 1.0054
reddit,dgl,1,783,81.1170,1.0054

epoch:785/50, training loss:0.2341446727514267
Train Acc 1.0048
 Acc 1.0054
reddit,dgl,1,784,81.2200,1.0054

epoch:786/50, training loss:0.2340485155582428
Train Acc 1.0048
 Acc 1.0055
new best val f1: 1.0054670832063395
reddit,dgl,1,785,81.3232,1.0055

epoch:787/50, training loss:0.2339535653591156
Train Acc 1.0048
 Acc 1.0055
reddit,dgl,1,786,81.4269,1.0055

epoch:788/50, training loss:0.23385773599147797
Train Acc 1.0048
 Acc 1.0055
new best val f1: 1.0055051813471503
reddit,dgl,1,787,81.5302,1.0055

epoch:789/50, training loss:0.23376336693763733
Train Acc 1.0049
 Acc 1.0055
reddit,dgl,1,788,81.6332,1.0055

epoch:790/50, training loss:0.23366720974445343
Train Acc 1.0049
 Acc 1.0055
reddit,dgl,1,789,81.7362,1.0055

epoch:791/50, training loss:0.23357252776622772
Train Acc 1.0049
 Acc 1.0055
new best val f1: 1.005543279487961
reddit,dgl,1,790,81.8393,1.0055

epoch:792/50, training loss:0.2334776073694229
Train Acc 1.0049
 Acc 1.0055
reddit,dgl,1,791,81.9426,1.0055

epoch:793/50, training loss:0.23338335752487183
Train Acc 1.0049
 Acc 1.0056
new best val f1: 1.0055623285583664
reddit,dgl,1,792,82.0458,1.0056

epoch:794/50, training loss:0.23328882455825806
Train Acc 1.0049
 Acc 1.0055
reddit,dgl,1,793,82.1488,1.0055

epoch:795/50, training loss:0.2331933081150055
Train Acc 1.0049
 Acc 1.0056
reddit,dgl,1,794,82.2517,1.0056

epoch:796/50, training loss:0.23309995234012604
Train Acc 1.0049
 Acc 1.0056
new best val f1: 1.0055813776287716
reddit,dgl,1,795,82.3552,1.0056

epoch:797/50, training loss:0.23300614953041077
Train Acc 1.0049
 Acc 1.0056
new best val f1: 1.0056004266991772
reddit,dgl,1,796,82.4589,1.0056

epoch:798/50, training loss:0.2329118549823761
Train Acc 1.0050
 Acc 1.0056
reddit,dgl,1,797,82.5622,1.0056

epoch:799/50, training loss:0.23281778395175934
Train Acc 1.0050
 Acc 1.0056
reddit,dgl,1,798,82.6651,1.0056

epoch:800/50, training loss:0.23272490501403809
Train Acc 1.0050
 Acc 1.0056
reddit,dgl,1,799,82.7680,1.0056

epoch:801/50, training loss:0.23263156414031982
Train Acc 1.0050
 Acc 1.0056
reddit,dgl,1,800,82.8710,1.0056

epoch:802/50, training loss:0.23253722488880157
Train Acc 1.0050
 Acc 1.0056
reddit,dgl,1,801,82.9745,1.0056

epoch:803/50, training loss:0.232444629073143
Train Acc 1.0050
 Acc 1.0056
reddit,dgl,1,802,83.0781,1.0056

epoch:804/50, training loss:0.2323518544435501
Train Acc 1.0051
 Acc 1.0056
reddit,dgl,1,803,83.1815,1.0056

epoch:805/50, training loss:0.23226004838943481
Train Acc 1.0051
 Acc 1.0056
reddit,dgl,1,804,83.2844,1.0056

epoch:806/50, training loss:0.23216667771339417
Train Acc 1.0051
 Acc 1.0056
reddit,dgl,1,805,83.3873,1.0056

epoch:807/50, training loss:0.2320743203163147
Train Acc 1.0051
 Acc 1.0056
reddit,dgl,1,806,83.4903,1.0056

epoch:808/50, training loss:0.23198233544826508
Train Acc 1.0051
 Acc 1.0056
reddit,dgl,1,807,83.5936,1.0056

epoch:809/50, training loss:0.2318894863128662
Train Acc 1.0052
 Acc 1.0056
reddit,dgl,1,808,83.6969,1.0056

epoch:810/50, training loss:0.2317976951599121
Train Acc 1.0052
 Acc 1.0056
new best val f1: 1.0056194757695824
reddit,dgl,1,809,83.7999,1.0056

epoch:811/50, training loss:0.23170581459999084
Train Acc 1.0052
 Acc 1.0056
reddit,dgl,1,810,83.9029,1.0056

epoch:812/50, training loss:0.23161426186561584
Train Acc 1.0052
 Acc 1.0056
reddit,dgl,1,811,84.0059,1.0056

epoch:813/50, training loss:0.23152270913124084
Train Acc 1.0052
 Acc 1.0057
new best val f1: 1.0056575739103932
reddit,dgl,1,812,84.1091,1.0057

epoch:814/50, training loss:0.231431245803833
Train Acc 1.0052
 Acc 1.0057
new best val f1: 1.0056766229807985
reddit,dgl,1,813,84.2123,1.0057

epoch:815/50, training loss:0.23133999109268188
Train Acc 1.0052
 Acc 1.0057
reddit,dgl,1,814,84.3154,1.0057

epoch:816/50, training loss:0.23124833405017853
Train Acc 1.0052
 Acc 1.0057
reddit,dgl,1,815,84.4186,1.0057

epoch:817/50, training loss:0.2311582714319229
Train Acc 1.0053
 Acc 1.0057
reddit,dgl,1,816,84.5215,1.0057

epoch:818/50, training loss:0.23106791079044342
Train Acc 1.0053
 Acc 1.0057
new best val f1: 1.0057147211216093
reddit,dgl,1,817,84.6249,1.0057

epoch:819/50, training loss:0.23097650706768036
Train Acc 1.0053
 Acc 1.0057
reddit,dgl,1,818,84.7285,1.0057

epoch:820/50, training loss:0.23088563978672028
Train Acc 1.0054
 Acc 1.0057
new best val f1: 1.0057337701920146
reddit,dgl,1,819,84.8317,1.0057

epoch:821/50, training loss:0.23079590499401093
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,820,84.9346,1.0057

epoch:822/50, training loss:0.23070503771305084
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,821,85.0377,1.0057

epoch:823/50, training loss:0.23061585426330566
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,822,85.1407,1.0057

epoch:824/50, training loss:0.2305254489183426
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,823,85.2442,1.0057

epoch:825/50, training loss:0.23043544590473175
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,824,85.3469,1.0057

epoch:826/50, training loss:0.23034529387950897
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,825,85.4502,1.0057

epoch:827/50, training loss:0.2302560657262802
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,826,85.5535,1.0057

epoch:828/50, training loss:0.23016689717769623
Train Acc 1.0054
 Acc 1.0057
reddit,dgl,1,827,85.6564,1.0057

epoch:829/50, training loss:0.2300775647163391
Train Acc 1.0054
 Acc 1.0058
new best val f1: 1.0057528192624199
reddit,dgl,1,828,85.7594,1.0058

epoch:830/50, training loss:0.22998830676078796
Train Acc 1.0055
 Acc 1.0058
reddit,dgl,1,829,85.8625,1.0058

epoch:831/50, training loss:0.22989942133426666
Train Acc 1.0055
 Acc 1.0058
new best val f1: 1.0057909174032307
reddit,dgl,1,830,85.9659,1.0058

epoch:832/50, training loss:0.22981062531471252
Train Acc 1.0055
 Acc 1.0058
reddit,dgl,1,831,86.0692,1.0058

epoch:833/50, training loss:0.22972159087657928
Train Acc 1.0055
 Acc 1.0058
reddit,dgl,1,832,86.1721,1.0058

epoch:834/50, training loss:0.22963304817676544
Train Acc 1.0055
 Acc 1.0058
reddit,dgl,1,833,86.2752,1.0058

epoch:835/50, training loss:0.22954480350017548
Train Acc 1.0055
 Acc 1.0058
new best val f1: 1.0058099664736362
reddit,dgl,1,834,86.3782,1.0058

epoch:836/50, training loss:0.22945673763751984
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,835,86.4817,1.0058

epoch:837/50, training loss:0.22936779260635376
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,836,86.5854,1.0058

epoch:838/50, training loss:0.22927983105182648
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,837,86.6887,1.0058

epoch:839/50, training loss:0.22919338941574097
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,838,86.7916,1.0058

epoch:840/50, training loss:0.22910425066947937
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,839,86.8946,1.0058

epoch:841/50, training loss:0.2290172576904297
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,840,86.9976,1.0058

epoch:842/50, training loss:0.22892925143241882
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,841,87.1009,1.0058

epoch:843/50, training loss:0.2288421094417572
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,842,87.2045,1.0058

epoch:844/50, training loss:0.22875502705574036
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,843,87.3075,1.0058

epoch:845/50, training loss:0.22866740822792053
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,844,87.4105,1.0058

epoch:846/50, training loss:0.2285807877779007
Train Acc 1.0056
 Acc 1.0058
reddit,dgl,1,845,87.5136,1.0058

epoch:847/50, training loss:0.2284945845603943
Train Acc 1.0057
 Acc 1.0058
new best val f1: 1.0058290155440415
reddit,dgl,1,846,87.6168,1.0058

epoch:848/50, training loss:0.2284073829650879
Train Acc 1.0057
 Acc 1.0058
reddit,dgl,1,847,87.7205,1.0058

epoch:849/50, training loss:0.22832053899765015
Train Acc 1.0057
 Acc 1.0058
new best val f1: 1.0058480646144468
reddit,dgl,1,848,87.8237,1.0058

epoch:850/50, training loss:0.2282339334487915
Train Acc 1.0057
 Acc 1.0058
reddit,dgl,1,849,87.9268,1.0058

epoch:851/50, training loss:0.22814823687076569
Train Acc 1.0057
 Acc 1.0058
reddit,dgl,1,850,88.0297,1.0058

epoch:852/50, training loss:0.22806210815906525
Train Acc 1.0057
 Acc 1.0058
reddit,dgl,1,851,88.1332,1.0058

epoch:853/50, training loss:0.22797541320323944
Train Acc 1.0057
 Acc 1.0059
new best val f1: 1.0058861627552576
reddit,dgl,1,852,88.2370,1.0059

epoch:854/50, training loss:0.2278897762298584
Train Acc 1.0057
 Acc 1.0059
reddit,dgl,1,853,88.3402,1.0059

epoch:855/50, training loss:0.22780437767505646
Train Acc 1.0057
 Acc 1.0059
new best val f1: 1.0059052118256628
reddit,dgl,1,854,88.4435,1.0059

epoch:856/50, training loss:0.22771863639354706
Train Acc 1.0058
 Acc 1.0060
new best val f1: 1.005962359036879
reddit,dgl,1,855,88.5465,1.0060

epoch:857/50, training loss:0.22763344645500183
Train Acc 1.0058
 Acc 1.0060
reddit,dgl,1,856,88.6494,1.0060

epoch:858/50, training loss:0.22754749655723572
Train Acc 1.0058
 Acc 1.0060
new best val f1: 1.0059814081072844
reddit,dgl,1,857,88.7524,1.0060

epoch:859/50, training loss:0.22746197879314423
Train Acc 1.0058
 Acc 1.0060
reddit,dgl,1,858,88.8556,1.0060

epoch:860/50, training loss:0.22737610340118408
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,859,88.9593,1.0060

epoch:861/50, training loss:0.2272917777299881
Train Acc 1.0059
 Acc 1.0060
new best val f1: 1.0060004571776897
reddit,dgl,1,860,89.0626,1.0060

epoch:862/50, training loss:0.2272072285413742
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,861,89.1658,1.0060

epoch:863/50, training loss:0.2271222025156021
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,862,89.2688,1.0060

epoch:864/50, training loss:0.22703734040260315
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,863,89.3719,1.0060

epoch:865/50, training loss:0.2269536554813385
Train Acc 1.0059
 Acc 1.0060
new best val f1: 1.006019506248095
reddit,dgl,1,864,89.4753,1.0060

epoch:866/50, training loss:0.22686821222305298
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,865,89.5785,1.0060

epoch:867/50, training loss:0.2267838418483734
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,866,89.6822,1.0060

epoch:868/50, training loss:0.22670011222362518
Train Acc 1.0059
 Acc 1.0060
reddit,dgl,1,867,89.7854,1.0060

epoch:869/50, training loss:0.22661574184894562
Train Acc 1.0060
 Acc 1.0060
reddit,dgl,1,868,89.8883,1.0060

epoch:870/50, training loss:0.2265307754278183
Train Acc 1.0060
 Acc 1.0060
reddit,dgl,1,869,89.9913,1.0060

epoch:871/50, training loss:0.22644838690757751
Train Acc 1.0060
 Acc 1.0060
reddit,dgl,1,870,90.0943,1.0060

epoch:872/50, training loss:0.2263643443584442
Train Acc 1.0060
 Acc 1.0060
new best val f1: 1.0060385553185005
reddit,dgl,1,871,90.1977,1.0060

epoch:873/50, training loss:0.22628100216388702
Train Acc 1.0060
 Acc 1.0060
reddit,dgl,1,872,90.3014,1.0060

epoch:874/50, training loss:0.2261972874403
Train Acc 1.0060
 Acc 1.0061
new best val f1: 1.0060576043889058
reddit,dgl,1,873,90.4045,1.0061

epoch:875/50, training loss:0.2261143922805786
Train Acc 1.0060
 Acc 1.0061
reddit,dgl,1,874,90.5075,1.0061

epoch:876/50, training loss:0.22603096067905426
Train Acc 1.0060
 Acc 1.0061
new best val f1: 1.0060957025297166
reddit,dgl,1,875,90.6105,1.0061

epoch:877/50, training loss:0.2259478121995926
Train Acc 1.0060
 Acc 1.0061
reddit,dgl,1,876,90.7139,1.0061

epoch:878/50, training loss:0.22586563229560852
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,877,90.8172,1.0061

epoch:879/50, training loss:0.22578227519989014
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,878,90.9204,1.0061

epoch:880/50, training loss:0.22569966316223145
Train Acc 1.0061
 Acc 1.0061
new best val f1: 1.0061147516001219
reddit,dgl,1,879,91.0234,1.0061

epoch:881/50, training loss:0.22561681270599365
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,880,91.1263,1.0061

epoch:882/50, training loss:0.22553427517414093
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,881,91.2294,1.0061

epoch:883/50, training loss:0.2254517823457718
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,882,91.3327,1.0061

epoch:884/50, training loss:0.2253698855638504
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,883,91.4360,1.0061

epoch:885/50, training loss:0.22528775036334991
Train Acc 1.0061
 Acc 1.0061
reddit,dgl,1,884,91.5393,1.0061

epoch:886/50, training loss:0.22520607709884644
Train Acc 1.0062
 Acc 1.0061
reddit,dgl,1,885,91.6423,1.0061

epoch:887/50, training loss:0.225124329328537
Train Acc 1.0062
 Acc 1.0061
reddit,dgl,1,886,91.7454,1.0061

epoch:888/50, training loss:0.2250424474477768
Train Acc 1.0062
 Acc 1.0061
new best val f1: 1.0061338006705274
reddit,dgl,1,887,91.8486,1.0061

epoch:889/50, training loss:0.22496092319488525
Train Acc 1.0062
 Acc 1.0062
new best val f1: 1.0061528497409327
reddit,dgl,1,888,91.9522,1.0062

epoch:890/50, training loss:0.22487938404083252
Train Acc 1.0062
 Acc 1.0062
new best val f1: 1.006171898811338
reddit,dgl,1,889,92.0554,1.0062

epoch:891/50, training loss:0.2247975766658783
Train Acc 1.0062
 Acc 1.0062
reddit,dgl,1,890,92.1585,1.0062

epoch:892/50, training loss:0.22471624612808228
Train Acc 1.0063
 Acc 1.0062
new best val f1: 1.0061909478817435
reddit,dgl,1,891,92.2614,1.0062

epoch:893/50, training loss:0.22463548183441162
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,892,92.3645,1.0062

epoch:894/50, training loss:0.22455455362796783
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,893,92.4678,1.0062

epoch:895/50, training loss:0.22447310388088226
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,894,92.5711,1.0062

epoch:896/50, training loss:0.2243930846452713
Train Acc 1.0063
 Acc 1.0062
new best val f1: 1.0062099969521487
reddit,dgl,1,895,92.6741,1.0062

epoch:897/50, training loss:0.2243124097585678
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,896,92.7770,1.0062

epoch:898/50, training loss:0.22423171997070312
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,897,92.8801,1.0062

epoch:899/50, training loss:0.224150612950325
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,898,92.9833,1.0062

epoch:900/50, training loss:0.22407123446464539
Train Acc 1.0063
 Acc 1.0062
reddit,dgl,1,899,93.0866,1.0062

epoch:901/50, training loss:0.22399121522903442
Train Acc 1.0063
 Acc 1.0062
new best val f1: 1.006229046022554
reddit,dgl,1,900,93.1896,1.0062

epoch:902/50, training loss:0.2239105999469757
Train Acc 1.0063
 Acc 1.0062
new best val f1: 1.0062480950929595
reddit,dgl,1,901,93.2927,1.0062

epoch:903/50, training loss:0.22383025288581848
Train Acc 1.0064
 Acc 1.0062
reddit,dgl,1,902,93.3957,1.0062

epoch:904/50, training loss:0.22375062108039856
Train Acc 1.0064
 Acc 1.0062
reddit,dgl,1,903,93.4990,1.0062

epoch:905/50, training loss:0.2236710786819458
Train Acc 1.0064
 Acc 1.0062
reddit,dgl,1,904,93.6026,1.0062

epoch:906/50, training loss:0.22359172999858856
Train Acc 1.0064
 Acc 1.0062
reddit,dgl,1,905,93.7059,1.0062

epoch:907/50, training loss:0.22351261973381042
Train Acc 1.0064
 Acc 1.0062
reddit,dgl,1,906,93.8089,1.0062

epoch:908/50, training loss:0.22343209385871887
Train Acc 1.0064
 Acc 1.0062
reddit,dgl,1,907,93.9119,1.0062

epoch:909/50, training loss:0.22335360944271088
Train Acc 1.0065
 Acc 1.0062
reddit,dgl,1,908,94.0153,1.0062

epoch:910/50, training loss:0.22327496111392975
Train Acc 1.0065
 Acc 1.0062
reddit,dgl,1,909,94.1190,1.0062

epoch:911/50, training loss:0.2231956124305725
Train Acc 1.0065
 Acc 1.0063
new best val f1: 1.0062671441633648
reddit,dgl,1,910,94.2223,1.0063

epoch:912/50, training loss:0.22311681509017944
Train Acc 1.0065
 Acc 1.0063
reddit,dgl,1,911,94.3253,1.0063

epoch:913/50, training loss:0.22303742170333862
Train Acc 1.0065
 Acc 1.0063
reddit,dgl,1,912,94.4280,1.0063

epoch:914/50, training loss:0.22295908629894257
Train Acc 1.0065
 Acc 1.0063
new best val f1: 1.00628619323377
reddit,dgl,1,913,94.5310,1.0063

epoch:915/50, training loss:0.22288061678409576
Train Acc 1.0065
 Acc 1.0063
reddit,dgl,1,914,94.6342,1.0063

epoch:916/50, training loss:0.22280243039131165
Train Acc 1.0065
 Acc 1.0063
new best val f1: 1.006324291374581
reddit,dgl,1,915,94.7379,1.0063

epoch:917/50, training loss:0.2227238416671753
Train Acc 1.0066
 Acc 1.0063
new best val f1: 1.0063433404449862
reddit,dgl,1,916,94.8411,1.0063

epoch:918/50, training loss:0.22264596819877625
Train Acc 1.0066
 Acc 1.0063
reddit,dgl,1,917,94.9442,1.0063

epoch:919/50, training loss:0.22256842255592346
Train Acc 1.0066
 Acc 1.0063
reddit,dgl,1,918,95.0471,1.0063

epoch:920/50, training loss:0.22248974442481995
Train Acc 1.0066
 Acc 1.0064
new best val f1: 1.0063623895153917
reddit,dgl,1,919,95.1506,1.0064

epoch:921/50, training loss:0.22241152822971344
Train Acc 1.0066
 Acc 1.0064
reddit,dgl,1,920,95.2542,1.0064

epoch:922/50, training loss:0.2223341315984726
Train Acc 1.0066
 Acc 1.0064
new best val f1: 1.006381438585797
reddit,dgl,1,921,95.3574,1.0064

epoch:923/50, training loss:0.22225646674633026
Train Acc 1.0066
 Acc 1.0064
reddit,dgl,1,922,95.4604,1.0064

epoch:924/50, training loss:0.22217871248722076
Train Acc 1.0066
 Acc 1.0065
new best val f1: 1.0064576348674186
reddit,dgl,1,923,95.5634,1.0065

epoch:925/50, training loss:0.2221015989780426
Train Acc 1.0067
 Acc 1.0065
reddit,dgl,1,924,95.6668,1.0065

epoch:926/50, training loss:0.22202402353286743
Train Acc 1.0067
 Acc 1.0065
new best val f1: 1.0064766839378239
reddit,dgl,1,925,95.7700,1.0065

epoch:927/50, training loss:0.2219465672969818
Train Acc 1.0067
 Acc 1.0065
reddit,dgl,1,926,95.8737,1.0065

epoch:928/50, training loss:0.22186966240406036
Train Acc 1.0067
 Acc 1.0065
reddit,dgl,1,927,95.9767,1.0065

epoch:929/50, training loss:0.22179263830184937
Train Acc 1.0067
 Acc 1.0065
reddit,dgl,1,928,96.0796,1.0065

epoch:930/50, training loss:0.22171613574028015
Train Acc 1.0067
 Acc 1.0065
new best val f1: 1.0064957330082291
reddit,dgl,1,929,96.1827,1.0065

epoch:931/50, training loss:0.2216382771730423
Train Acc 1.0067
 Acc 1.0065
new best val f1: 1.0065147820786347
reddit,dgl,1,930,96.2859,1.0065

epoch:932/50, training loss:0.221562460064888
Train Acc 1.0067
 Acc 1.0065
reddit,dgl,1,931,96.3896,1.0065

epoch:933/50, training loss:0.22148624062538147
Train Acc 1.0067
 Acc 1.0065
new best val f1: 1.00653383114904
reddit,dgl,1,932,96.4927,1.0065

epoch:934/50, training loss:0.2214093804359436
Train Acc 1.0067
 Acc 1.0065
reddit,dgl,1,933,96.5958,1.0065

epoch:935/50, training loss:0.22133348882198334
Train Acc 1.0068
 Acc 1.0066
new best val f1: 1.0066100274306613
reddit,dgl,1,934,96.6987,1.0066

epoch:936/50, training loss:0.22125652432441711
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,935,96.8020,1.0066

epoch:937/50, training loss:0.22118110954761505
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,936,96.9057,1.0066

epoch:938/50, training loss:0.22110453248023987
Train Acc 1.0068
 Acc 1.0066
new best val f1: 1.006648125571472
reddit,dgl,1,937,97.0090,1.0066

epoch:939/50, training loss:0.22102896869182587
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,938,97.1121,1.0066

epoch:940/50, training loss:0.22095343470573425
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,939,97.2150,1.0066

epoch:941/50, training loss:0.22087720036506653
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,940,97.3184,1.0066

epoch:942/50, training loss:0.22080184519290924
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,941,97.4223,1.0066

epoch:943/50, training loss:0.22072602808475494
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,942,97.5262,1.0066

epoch:944/50, training loss:0.22065140306949615
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,943,97.6294,1.0066

epoch:945/50, training loss:0.22057586908340454
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,944,97.7327,1.0066

epoch:946/50, training loss:0.22050055861473083
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,945,97.8360,1.0066

epoch:947/50, training loss:0.2204257845878601
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,946,97.9394,1.0066

epoch:948/50, training loss:0.2203504741191864
Train Acc 1.0068
 Acc 1.0067
new best val f1: 1.0066671746418774
reddit,dgl,1,947,98.0431,1.0067

epoch:949/50, training loss:0.22027623653411865
Train Acc 1.0068
 Acc 1.0066
reddit,dgl,1,948,98.1463,1.0066

epoch:950/50, training loss:0.2202012538909912
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,949,98.2494,1.0067

epoch:951/50, training loss:0.22012653946876526
Train Acc 1.0069
 Acc 1.0067
new best val f1: 1.006686223712283
reddit,dgl,1,950,98.3523,1.0067

epoch:952/50, training loss:0.22005169093608856
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,951,98.4554,1.0067

epoch:953/50, training loss:0.21997778117656708
Train Acc 1.0069
 Acc 1.0067
new best val f1: 1.0067052727826882
reddit,dgl,1,952,98.5585,1.0067

epoch:954/50, training loss:0.219903364777565
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,953,98.6618,1.0067

epoch:955/50, training loss:0.2198290377855301
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,954,98.7647,1.0067

epoch:956/50, training loss:0.2197551131248474
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,955,98.8677,1.0067

epoch:957/50, training loss:0.21968071162700653
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,956,98.9708,1.0067

epoch:958/50, training loss:0.21960653364658356
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,957,99.0740,1.0067

epoch:959/50, training loss:0.21953290700912476
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,958,99.1773,1.0067

epoch:960/50, training loss:0.2194591760635376
Train Acc 1.0069
 Acc 1.0067
reddit,dgl,1,959,99.2802,1.0067

epoch:961/50, training loss:0.21938492357730865
Train Acc 1.0070
 Acc 1.0067
reddit,dgl,1,960,99.3831,1.0067

epoch:962/50, training loss:0.21931178867816925
Train Acc 1.0070
 Acc 1.0067
reddit,dgl,1,961,99.4866,1.0067

epoch:963/50, training loss:0.2192377746105194
Train Acc 1.0070
 Acc 1.0067
new best val f1: 1.0067243218530935
reddit,dgl,1,962,99.5898,1.0067

epoch:964/50, training loss:0.21916460990905762
Train Acc 1.0070
 Acc 1.0068
new best val f1: 1.0067624199939043
reddit,dgl,1,963,99.6935,1.0068

epoch:965/50, training loss:0.21909110248088837
Train Acc 1.0070
 Acc 1.0068
new best val f1: 1.006800518134715
reddit,dgl,1,964,99.7967,1.0068

epoch:966/50, training loss:0.21901826560497284
Train Acc 1.0070
 Acc 1.0068
reddit,dgl,1,965,99.8999,1.0068

epoch:967/50, training loss:0.21894484758377075
Train Acc 1.0070
 Acc 1.0068
new best val f1: 1.0068195672051203
reddit,dgl,1,966,100.0028,1.0068

epoch:968/50, training loss:0.21887138485908508
Train Acc 1.0070
 Acc 1.0068
reddit,dgl,1,967,100.1059,1.0068

epoch:969/50, training loss:0.21879898011684418
Train Acc 1.0070
 Acc 1.0068
reddit,dgl,1,968,100.2094,1.0068

epoch:970/50, training loss:0.21872632205486298
Train Acc 1.0070
 Acc 1.0068
reddit,dgl,1,969,100.3131,1.0068

epoch:971/50, training loss:0.21865326166152954
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,970,100.4163,1.0068

epoch:972/50, training loss:0.21858066320419312
Train Acc 1.0070
 Acc 1.0068
new best val f1: 1.0068386162755258
reddit,dgl,1,971,100.5194,1.0068

epoch:973/50, training loss:0.21850869059562683
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,972,100.6224,1.0068

epoch:974/50, training loss:0.2184358537197113
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,973,100.7259,1.0068

epoch:975/50, training loss:0.21836350858211517
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,974,100.8291,1.0068

epoch:976/50, training loss:0.21829132735729218
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,975,100.9324,1.0068

epoch:977/50, training loss:0.21821904182434082
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,976,101.0356,1.0068

epoch:978/50, training loss:0.218146413564682
Train Acc 1.0071
 Acc 1.0068
reddit,dgl,1,977,101.1387,1.0068

epoch:979/50, training loss:0.21807530522346497
Train Acc 1.0071
 Acc 1.0069
new best val f1: 1.0068576653459311
reddit,dgl,1,978,101.2418,1.0069

epoch:980/50, training loss:0.2180033028125763
Train Acc 1.0071
 Acc 1.0069
reddit,dgl,1,979,101.3450,1.0069

epoch:981/50, training loss:0.21793107688426971
Train Acc 1.0072
 Acc 1.0069
reddit,dgl,1,980,101.4483,1.0069

epoch:982/50, training loss:0.21785968542099
Train Acc 1.0072
 Acc 1.0068
reddit,dgl,1,981,101.5512,1.0068

epoch:983/50, training loss:0.217787966132164
Train Acc 1.0072
 Acc 1.0068
reddit,dgl,1,982,101.6542,1.0068

epoch:984/50, training loss:0.21771614253520966
Train Acc 1.0072
 Acc 1.0068
reddit,dgl,1,983,101.7573,1.0068

epoch:985/50, training loss:0.217645525932312
Train Acc 1.0072
 Acc 1.0069
reddit,dgl,1,984,101.8604,1.0069

epoch:986/50, training loss:0.21757417917251587
Train Acc 1.0072
 Acc 1.0068
reddit,dgl,1,985,101.9641,1.0068

epoch:987/50, training loss:0.21750237047672272
Train Acc 1.0072
 Acc 1.0068
reddit,dgl,1,986,102.0674,1.0068

epoch:988/50, training loss:0.2174309492111206
Train Acc 1.0072
 Acc 1.0069
reddit,dgl,1,987,102.1705,1.0069

epoch:989/50, training loss:0.21735990047454834
Train Acc 1.0073
 Acc 1.0069
new best val f1: 1.0068767144163364
reddit,dgl,1,988,102.2735,1.0069

epoch:990/50, training loss:0.21728911995887756
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,989,102.3767,1.0069

epoch:991/50, training loss:0.21721859276294708
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,990,102.4804,1.0069

epoch:992/50, training loss:0.2171475887298584
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,991,102.5835,1.0069

epoch:993/50, training loss:0.21707683801651
Train Acc 1.0073
 Acc 1.0068
reddit,dgl,1,992,102.6865,1.0068

epoch:994/50, training loss:0.21700608730316162
Train Acc 1.0073
 Acc 1.0068
reddit,dgl,1,993,102.7897,1.0068

epoch:995/50, training loss:0.21693579852581024
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,994,102.8927,1.0069

epoch:996/50, training loss:0.21686534583568573
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,995,102.9964,1.0069

epoch:997/50, training loss:0.21679523587226868
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,996,103.0996,1.0069

epoch:998/50, training loss:0.21672406792640686
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,997,103.2026,1.0069

epoch:999/50, training loss:0.21665431559085846
Train Acc 1.0073
 Acc 1.0069
reddit,dgl,1,998,103.3055,1.0069

epoch:1000/50, training loss:0.2165844589471817
Train Acc 1.0074
 Acc 1.0069
reddit,dgl,1,999,103.4087,1.0069

epoch:1001/50, training loss:0.21651357412338257
Train Acc 1.0074
 Acc 1.0069
new best val f1: 1.0069148125571472
reddit,dgl,1,1000,103.5119,1.0069

epoch:1002/50, training loss:0.216444194316864
Train Acc 1.0074
 Acc 1.0069
reddit,dgl,1,1001,103.6153,1.0069

epoch:1003/50, training loss:0.21637490391731262
Train Acc 1.0074
 Acc 1.0068
reddit,dgl,1,1002,103.7182,1.0068

epoch:1004/50, training loss:0.2163042426109314
Train Acc 1.0074
 Acc 1.0069
reddit,dgl,1,1003,103.8212,1.0069

epoch:1005/50, training loss:0.2162346988916397
Train Acc 1.0074
 Acc 1.0069
reddit,dgl,1,1004,103.9247,1.0069

epoch:1006/50, training loss:0.21616578102111816
Train Acc 1.0074
 Acc 1.0069
reddit,dgl,1,1005,104.0282,1.0069

epoch:1007/50, training loss:0.21609564125537872
Train Acc 1.0074
 Acc 1.0068
reddit,dgl,1,1006,104.1311,1.0068

epoch:1008/50, training loss:0.21602673828601837
Train Acc 1.0074
 Acc 1.0068
reddit,dgl,1,1007,104.2342,1.0068

epoch:1009/50, training loss:0.21595726907253265
Train Acc 1.0074
 Acc 1.0068
reddit,dgl,1,1008,104.3373,1.0068

epoch:1010/50, training loss:0.2158876359462738
Train Acc 1.0074
 Acc 1.0068
reddit,dgl,1,1009,104.4405,1.0068

epoch:1011/50, training loss:0.21581876277923584
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1010,104.5442,1.0068

epoch:1012/50, training loss:0.21574969589710236
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1011,104.6472,1.0068

epoch:1013/50, training loss:0.2156803160905838
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1012,104.7502,1.0068

epoch:1014/50, training loss:0.2156112939119339
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1013,104.8533,1.0068

epoch:1015/50, training loss:0.21554240584373474
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1014,104.9568,1.0068

epoch:1016/50, training loss:0.2154739499092102
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1015,105.0607,1.0068

epoch:1017/50, training loss:0.21540528535842896
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1016,105.1639,1.0068

epoch:1018/50, training loss:0.21533705294132233
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1017,105.2670,1.0068

epoch:1019/50, training loss:0.21526874601840973
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1018,105.3699,1.0068

epoch:1020/50, training loss:0.2152000516653061
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1019,105.4734,1.0068

epoch:1021/50, training loss:0.2151317447423935
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1020,105.5768,1.0068

epoch:1022/50, training loss:0.2150636911392212
Train Acc 1.0075
 Acc 1.0068
reddit,dgl,1,1021,105.6802,1.0068

epoch:1023/50, training loss:0.214995875954628
Train Acc 1.0075
 Acc 1.0069
reddit,dgl,1,1022,105.7834,1.0069

epoch:1024/50, training loss:0.2149277627468109
Train Acc 1.0075
 Acc 1.0069
reddit,dgl,1,1023,105.8865,1.0069

epoch:1025/50, training loss:0.2148597687482834
Train Acc 1.0076
 Acc 1.0069
reddit,dgl,1,1024,105.9898,1.0069

epoch:1026/50, training loss:0.21479183435440063
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1025,106.0930,1.0068

epoch:1027/50, training loss:0.21472445130348206
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1026,106.1963,1.0068

epoch:1028/50, training loss:0.21465662121772766
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1027,106.2994,1.0068

epoch:1029/50, training loss:0.2145889550447464
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1028,106.4024,1.0068

epoch:1030/50, training loss:0.21452119946479797
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1029,106.5055,1.0068

epoch:1031/50, training loss:0.21445463597774506
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1030,106.6091,1.0068

epoch:1032/50, training loss:0.21438685059547424
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1031,106.7122,1.0068

epoch:1033/50, training loss:0.21431976556777954
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1032,106.8151,1.0068

epoch:1034/50, training loss:0.21425268054008484
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1033,106.9181,1.0068

epoch:1035/50, training loss:0.21418555080890656
Train Acc 1.0076
 Acc 1.0068
reddit,dgl,1,1034,107.0215,1.0068

epoch:1036/50, training loss:0.2141180783510208
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1035,107.1253,1.0068

epoch:1037/50, training loss:0.21405166387557983
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1036,107.2286,1.0068

epoch:1038/50, training loss:0.2139846235513687
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1037,107.3317,1.0068

epoch:1039/50, training loss:0.21391810476779938
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1038,107.4346,1.0068

epoch:1040/50, training loss:0.21385101974010468
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1039,107.5381,1.0068

epoch:1041/50, training loss:0.21378476917743683
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1040,107.6413,1.0068

epoch:1042/50, training loss:0.21371811628341675
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1041,107.7446,1.0068

epoch:1043/50, training loss:0.21365220844745636
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1042,107.8475,1.0068

epoch:1044/50, training loss:0.2135850340127945
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1043,107.9506,1.0068

epoch:1045/50, training loss:0.21351923048496246
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1044,108.0536,1.0068

epoch:1046/50, training loss:0.21345297992229462
Train Acc 1.0077
 Acc 1.0069
reddit,dgl,1,1045,108.1569,1.0069

epoch:1047/50, training loss:0.2133873850107193
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1046,108.2601,1.0069

epoch:1048/50, training loss:0.21332113444805145
Train Acc 1.0077
 Acc 1.0069
reddit,dgl,1,1047,108.3631,1.0069

epoch:1049/50, training loss:0.2132551372051239
Train Acc 1.0077
 Acc 1.0068
reddit,dgl,1,1048,108.4661,1.0068

epoch:1050/50, training loss:0.21318882703781128
Train Acc 1.0077
 Acc 1.0069
reddit,dgl,1,1049,108.5691,1.0069

epoch:1051/50, training loss:0.2131236344575882
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1050,108.6724,1.0069

epoch:1052/50, training loss:0.21305736899375916
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1051,108.7761,1.0069

epoch:1053/50, training loss:0.2129921317100525
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1052,108.8793,1.0069

epoch:1054/50, training loss:0.21292680501937866
Train Acc 1.0078
 Acc 1.0069
new best val f1: 1.0069338616275525
reddit,dgl,1,1053,108.9824,1.0069

epoch:1055/50, training loss:0.21286137402057648
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1054,109.0854,1.0069

epoch:1056/50, training loss:0.21279528737068176
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1055,109.1886,1.0069

epoch:1057/50, training loss:0.21272967755794525
Train Acc 1.0078
 Acc 1.0069
reddit,dgl,1,1056,109.2917,1.0069

epoch:1058/50, training loss:0.21266518533229828
Train Acc 1.0078
 Acc 1.0070
new best val f1: 1.0069910088387686
reddit,dgl,1,1057,109.3946,1.0070

epoch:1059/50, training loss:0.21259978413581848
Train Acc 1.0079
 Acc 1.0070
new best val f1: 1.007010057909174
reddit,dgl,1,1058,109.4975,1.0070

epoch:1060/50, training loss:0.21253453195095062
Train Acc 1.0079
 Acc 1.0070
new best val f1: 1.0070291069795794
reddit,dgl,1,1059,109.6006,1.0070

epoch:1061/50, training loss:0.2124696671962738
Train Acc 1.0079
 Acc 1.0070
reddit,dgl,1,1060,109.7039,1.0070

epoch:1062/50, training loss:0.212404265999794
Train Acc 1.0079
 Acc 1.0070
new best val f1: 1.0070481560499847
reddit,dgl,1,1061,109.8072,1.0070

epoch:1063/50, training loss:0.21233928203582764
Train Acc 1.0080
 Acc 1.0070
reddit,dgl,1,1062,109.9101,1.0070

epoch:1064/50, training loss:0.21227508783340454
Train Acc 1.0080
 Acc 1.0070
reddit,dgl,1,1063,110.0135,1.0070

epoch:1065/50, training loss:0.2122105062007904
Train Acc 1.0080
 Acc 1.0071
new best val f1: 1.0070672051203902
reddit,dgl,1,1064,110.1170,1.0071

epoch:1066/50, training loss:0.21214519441127777
Train Acc 1.0080
 Acc 1.0071
reddit,dgl,1,1065,110.2205,1.0071

epoch:1067/50, training loss:0.21208125352859497
Train Acc 1.0080
 Acc 1.0071
reddit,dgl,1,1066,110.3237,1.0071

epoch:1068/50, training loss:0.2120169848203659
Train Acc 1.0080
 Acc 1.0071
new best val f1: 1.0070862541907954
reddit,dgl,1,1067,110.4268,1.0071

epoch:1069/50, training loss:0.2119516134262085
Train Acc 1.0080
 Acc 1.0071
new best val f1: 1.0071434014020115
reddit,dgl,1,1068,110.5297,1.0071

epoch:1070/50, training loss:0.21188797056674957
Train Acc 1.0080
 Acc 1.0072
new best val f1: 1.0071814995428223
reddit,dgl,1,1069,110.6328,1.0072

epoch:1071/50, training loss:0.21182309091091156
Train Acc 1.0081
 Acc 1.0072
new best val f1: 1.0072005486132276
reddit,dgl,1,1070,110.7360,1.0072

epoch:1072/50, training loss:0.2117595672607422
Train Acc 1.0081
 Acc 1.0072
new best val f1: 1.0072195976836331
reddit,dgl,1,1071,110.8394,1.0072

epoch:1073/50, training loss:0.21169528365135193
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1072,110.9422,1.0072

epoch:1074/50, training loss:0.2116311639547348
Train Acc 1.0081
 Acc 1.0072
new best val f1: 1.0072386467540384
reddit,dgl,1,1073,111.0452,1.0072

epoch:1075/50, training loss:0.21156683564186096
Train Acc 1.0081
 Acc 1.0073
new best val f1: 1.0072576958244437
reddit,dgl,1,1074,111.1487,1.0073

epoch:1076/50, training loss:0.21150341629981995
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1075,111.2524,1.0072

epoch:1077/50, training loss:0.2114398032426834
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1076,111.3556,1.0072

epoch:1078/50, training loss:0.21137593686580658
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1077,111.4587,1.0072

epoch:1079/50, training loss:0.21131202578544617
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1078,111.5615,1.0072

epoch:1080/50, training loss:0.21124857664108276
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1079,111.6648,1.0072

epoch:1081/50, training loss:0.2111845463514328
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1080,111.7679,1.0072

epoch:1082/50, training loss:0.2111217975616455
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1081,111.8708,1.0072

epoch:1083/50, training loss:0.2110583484172821
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1082,111.9738,1.0072

epoch:1084/50, training loss:0.21099497377872467
Train Acc 1.0081
 Acc 1.0072
reddit,dgl,1,1083,112.0766,1.0072

epoch:1085/50, training loss:0.21093130111694336
Train Acc 1.0082
 Acc 1.0073
new best val f1: 1.0072767448948492
reddit,dgl,1,1084,112.1796,1.0073

epoch:1086/50, training loss:0.21086888015270233
Train Acc 1.0082
 Acc 1.0073
reddit,dgl,1,1085,112.2827,1.0073

epoch:1087/50, training loss:0.21080541610717773
Train Acc 1.0082
 Acc 1.0073
reddit,dgl,1,1086,112.3860,1.0073

epoch:1088/50, training loss:0.21074236929416656
Train Acc 1.0082
 Acc 1.0073
reddit,dgl,1,1087,112.4893,1.0073

epoch:1089/50, training loss:0.2106802612543106
Train Acc 1.0082
 Acc 1.0073
reddit,dgl,1,1088,112.5926,1.0073

epoch:1090/50, training loss:0.21061760187149048
Train Acc 1.0082
 Acc 1.0073
reddit,dgl,1,1089,112.6957,1.0073

epoch:1091/50, training loss:0.21055419743061066
Train Acc 1.0082
 Acc 1.0072
reddit,dgl,1,1090,112.7987,1.0072

epoch:1092/50, training loss:0.21049165725708008
Train Acc 1.0082
 Acc 1.0073
new best val f1: 1.0072957939652545
reddit,dgl,1,1091,112.9022,1.0073

epoch:1093/50, training loss:0.2104293555021286
Train Acc 1.0083
 Acc 1.0073
reddit,dgl,1,1092,113.0061,1.0073

epoch:1094/50, training loss:0.21036645770072937
Train Acc 1.0083
 Acc 1.0073
new best val f1: 1.0073338921060653
reddit,dgl,1,1093,113.1092,1.0073

epoch:1095/50, training loss:0.2103036642074585
Train Acc 1.0083
 Acc 1.0073
reddit,dgl,1,1094,113.2121,1.0073

epoch:1096/50, training loss:0.2102418690919876
Train Acc 1.0083
 Acc 1.0074
new best val f1: 1.0073529411764706
reddit,dgl,1,1095,113.3150,1.0074

epoch:1097/50, training loss:0.21017886698246002
Train Acc 1.0083
 Acc 1.0074
new best val f1: 1.0073719902468758
reddit,dgl,1,1096,113.4185,1.0074

epoch:1098/50, training loss:0.21011681854724884
Train Acc 1.0083
 Acc 1.0074
reddit,dgl,1,1097,113.5220,1.0074

epoch:1099/50, training loss:0.21005456149578094
Train Acc 1.0083
 Acc 1.0074
reddit,dgl,1,1098,113.6255,1.0074

epoch:1100/50, training loss:0.20999212563037872
Train Acc 1.0083
 Acc 1.0074
reddit,dgl,1,1099,113.7287,1.0074

epoch:1101/50, training loss:0.20993056893348694
Train Acc 1.0083
 Acc 1.0074
reddit,dgl,1,1100,113.8319,1.0074

epoch:1102/50, training loss:0.209868386387825
Train Acc 1.0084
 Acc 1.0074
reddit,dgl,1,1101,113.9352,1.0074

epoch:1103/50, training loss:0.20980612933635712
Train Acc 1.0084
 Acc 1.0074
new best val f1: 1.0074100883876866
reddit,dgl,1,1102,114.0383,1.0074

epoch:1104/50, training loss:0.20974479615688324
Train Acc 1.0084
 Acc 1.0074
reddit,dgl,1,1103,114.1420,1.0074

epoch:1105/50, training loss:0.20968244969844818
Train Acc 1.0084
 Acc 1.0074
reddit,dgl,1,1104,114.2450,1.0074

epoch:1106/50, training loss:0.2096213847398758
Train Acc 1.0084
 Acc 1.0074
reddit,dgl,1,1105,114.3480,1.0074

epoch:1107/50, training loss:0.2095591127872467
Train Acc 1.0084
 Acc 1.0074
reddit,dgl,1,1106,114.4511,1.0074

epoch:1108/50, training loss:0.20949779450893402
Train Acc 1.0084
 Acc 1.0074
new best val f1: 1.0074291374580921
reddit,dgl,1,1107,114.5543,1.0074

epoch:1109/50, training loss:0.20943553745746613
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1108,114.6576,1.0074

epoch:1110/50, training loss:0.20937485992908478
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1109,114.7609,1.0074

epoch:1111/50, training loss:0.2093135267496109
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1110,114.8639,1.0074

epoch:1112/50, training loss:0.20925141870975494
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1111,114.9670,1.0074

epoch:1113/50, training loss:0.20919065177440643
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1112,115.0704,1.0074

epoch:1114/50, training loss:0.20912925899028778
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1113,115.1741,1.0074

epoch:1115/50, training loss:0.2090681940317154
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1114,115.2773,1.0074

epoch:1116/50, training loss:0.20900700986385345
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1115,115.3804,1.0074

epoch:1117/50, training loss:0.20894567668437958
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1116,115.4834,1.0074

epoch:1118/50, training loss:0.20888499915599823
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1117,115.5867,1.0074

epoch:1119/50, training loss:0.20882444083690643
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1118,115.6904,1.0074

epoch:1120/50, training loss:0.2087637335062027
Train Acc 1.0086
 Acc 1.0074
reddit,dgl,1,1119,115.7934,1.0074

epoch:1121/50, training loss:0.20870281755924225
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1120,115.8964,1.0074

epoch:1122/50, training loss:0.20864209532737732
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1121,115.9995,1.0074

epoch:1123/50, training loss:0.2085815966129303
Train Acc 1.0085
 Acc 1.0074
reddit,dgl,1,1122,116.1027,1.0074

epoch:1124/50, training loss:0.2085212916135788
Train Acc 1.0086
 Acc 1.0074
new best val f1: 1.0074481865284974
reddit,dgl,1,1123,116.2064,1.0074

epoch:1125/50, training loss:0.20846045017242432
Train Acc 1.0086
 Acc 1.0075
new best val f1: 1.0075053337397135
reddit,dgl,1,1124,116.3096,1.0075

epoch:1126/50, training loss:0.20839983224868774
Train Acc 1.0086
 Acc 1.0075
reddit,dgl,1,1125,116.4127,1.0075

epoch:1127/50, training loss:0.20833902060985565
Train Acc 1.0086
 Acc 1.0075
reddit,dgl,1,1126,116.5158,1.0075

epoch:1128/50, training loss:0.2082788348197937
Train Acc 1.0086
 Acc 1.0075
reddit,dgl,1,1127,116.6193,1.0075

epoch:1129/50, training loss:0.2082190364599228
Train Acc 1.0086
 Acc 1.0075
reddit,dgl,1,1128,116.7229,1.0075

epoch:1130/50, training loss:0.2081587165594101
Train Acc 1.0086
 Acc 1.0075
reddit,dgl,1,1129,116.8258,1.0075

epoch:1131/50, training loss:0.20809920132160187
Train Acc 1.0087
 Acc 1.0075
new best val f1: 1.0075243828101188
reddit,dgl,1,1130,116.9289,1.0075

epoch:1132/50, training loss:0.20803828537464142
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1131,117.0320,1.0075

epoch:1133/50, training loss:0.20797844231128693
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1132,117.1352,1.0075

epoch:1134/50, training loss:0.20791907608509064
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1133,117.2384,1.0075

epoch:1135/50, training loss:0.20785841345787048
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1134,117.3413,1.0075

epoch:1136/50, training loss:0.2077990025281906
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1135,117.4443,1.0075

epoch:1137/50, training loss:0.20773938298225403
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1136,117.5473,1.0075

epoch:1138/50, training loss:0.20767924189567566
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1137,117.6506,1.0075

epoch:1139/50, training loss:0.2076198160648346
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1138,117.7537,1.0075

epoch:1140/50, training loss:0.20756039023399353
Train Acc 1.0087
 Acc 1.0075
new best val f1: 1.0075434318805243
reddit,dgl,1,1139,117.8566,1.0075

epoch:1141/50, training loss:0.20750035345554352
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1140,117.9595,1.0075

epoch:1142/50, training loss:0.20744141936302185
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1141,118.0630,1.0075

epoch:1143/50, training loss:0.20738166570663452
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1142,118.1667,1.0075

epoch:1144/50, training loss:0.20732176303863525
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1143,118.2698,1.0075

epoch:1145/50, training loss:0.20726357400417328
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1144,118.3728,1.0075

epoch:1146/50, training loss:0.20720429718494415
Train Acc 1.0087
 Acc 1.0075
reddit,dgl,1,1145,118.4757,1.0075

epoch:1147/50, training loss:0.20714516937732697
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1146,118.5791,1.0075

epoch:1148/50, training loss:0.20708559453487396
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1147,118.6826,1.0075

epoch:1149/50, training loss:0.2070263922214508
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1148,118.7860,1.0075

epoch:1150/50, training loss:0.20696769654750824
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1149,118.8892,1.0075

epoch:1151/50, training loss:0.20690858364105225
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1150,118.9923,1.0075

epoch:1152/50, training loss:0.20684988796710968
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1151,119.0956,1.0075

epoch:1153/50, training loss:0.2067912518978119
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1152,119.1993,1.0075

epoch:1154/50, training loss:0.20673209428787231
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1153,119.3026,1.0075

epoch:1155/50, training loss:0.20667317509651184
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1154,119.4059,1.0075

epoch:1156/50, training loss:0.20661507546901703
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1155,119.5089,1.0075

epoch:1157/50, training loss:0.20655642449855804
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1156,119.6119,1.0075

epoch:1158/50, training loss:0.20649735629558563
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1157,119.7152,1.0075

epoch:1159/50, training loss:0.20643892884254456
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1158,119.8184,1.0075

epoch:1160/50, training loss:0.20638132095336914
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1159,119.9217,1.0075

epoch:1161/50, training loss:0.20632225275039673
Train Acc 1.0088
 Acc 1.0075
reddit,dgl,1,1160,120.0248,1.0075

epoch:1162/50, training loss:0.20626403391361237
Train Acc 1.0088
 Acc 1.0076
new best val f1: 1.0076005790917404
reddit,dgl,1,1161,120.1277,1.0076

epoch:1163/50, training loss:0.2062060683965683
Train Acc 1.0088
 Acc 1.0076
reddit,dgl,1,1162,120.2312,1.0076

epoch:1164/50, training loss:0.20614753663539886
Train Acc 1.0088
 Acc 1.0076
reddit,dgl,1,1163,120.3349,1.0076

epoch:1165/50, training loss:0.20608988404273987
Train Acc 1.0088
 Acc 1.0076
reddit,dgl,1,1164,120.4382,1.0076

epoch:1166/50, training loss:0.20603162050247192
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1165,120.5412,1.0076

epoch:1167/50, training loss:0.20597362518310547
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1166,120.6441,1.0076

epoch:1168/50, training loss:0.205915167927742
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1167,120.7475,1.0076

epoch:1169/50, training loss:0.20585817098617554
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1168,120.8512,1.0076

epoch:1170/50, training loss:0.20580054819583893
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1169,120.9545,1.0076

epoch:1171/50, training loss:0.20574241876602173
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1170,121.0574,1.0076

epoch:1172/50, training loss:0.20568501949310303
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1171,121.1605,1.0076

epoch:1173/50, training loss:0.20562656223773956
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1172,121.2632,1.0076

epoch:1174/50, training loss:0.20556949079036713
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1173,121.3663,1.0076

epoch:1175/50, training loss:0.20551137626171112
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1174,121.4700,1.0076

epoch:1176/50, training loss:0.20545409619808197
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1175,121.5734,1.0076

epoch:1177/50, training loss:0.20539610087871552
Train Acc 1.0089
 Acc 1.0076
reddit,dgl,1,1176,121.6766,1.0076

epoch:1178/50, training loss:0.20533938705921173
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1177,121.7799,1.0076

epoch:1179/50, training loss:0.20528201758861542
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1178,121.8831,1.0076

epoch:1180/50, training loss:0.20522473752498627
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1179,121.9864,1.0076

epoch:1181/50, training loss:0.20516766607761383
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1180,122.0900,1.0076

epoch:1182/50, training loss:0.20511069893836975
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1181,122.1932,1.0076

epoch:1183/50, training loss:0.20505326986312866
Train Acc 1.0090
 Acc 1.0076
new best val f1: 1.0076196281621457
reddit,dgl,1,1182,122.2962,1.0076

epoch:1184/50, training loss:0.20499610900878906
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1183,122.3993,1.0076

epoch:1185/50, training loss:0.20493954420089722
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1184,122.5025,1.0076

epoch:1186/50, training loss:0.20488198101520538
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1185,122.6062,1.0076

epoch:1187/50, training loss:0.20482508838176727
Train Acc 1.0090
 Acc 1.0076
reddit,dgl,1,1186,122.7094,1.0076

epoch:1188/50, training loss:0.20476828515529633
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1187,122.8125,1.0076

epoch:1189/50, training loss:0.2047119438648224
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1188,122.9157,1.0076

epoch:1190/50, training loss:0.20465467870235443
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1189,123.0188,1.0076

epoch:1191/50, training loss:0.20459823310375214
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1190,123.1220,1.0076

epoch:1192/50, training loss:0.20454129576683044
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1191,123.2254,1.0076

epoch:1193/50, training loss:0.20448468625545502
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1192,123.3283,1.0076

epoch:1194/50, training loss:0.20442861318588257
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1193,123.4314,1.0076

epoch:1195/50, training loss:0.20437195897102356
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1194,123.5345,1.0076

epoch:1196/50, training loss:0.2043156623840332
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1195,123.6378,1.0076

epoch:1197/50, training loss:0.2042594999074936
Train Acc 1.0091
 Acc 1.0076
new best val f1: 1.007638677232551
reddit,dgl,1,1196,123.7414,1.0076

epoch:1198/50, training loss:0.20420324802398682
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1197,123.8447,1.0076

epoch:1199/50, training loss:0.20414623618125916
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1198,123.9477,1.0076

epoch:1200/50, training loss:0.20409072935581207
Train Acc 1.0091
 Acc 1.0077
new best val f1: 1.0076577263029565
reddit,dgl,1,1199,124.0509,1.0077

epoch:1201/50, training loss:0.20403386652469635
Train Acc 1.0092
 Acc 1.0076
reddit,dgl,1,1200,124.1541,1.0076

epoch:1202/50, training loss:0.2039775401353836
Train Acc 1.0092
 Acc 1.0076
reddit,dgl,1,1201,124.2577,1.0076

epoch:1203/50, training loss:0.20392177999019623
Train Acc 1.0091
 Acc 1.0076
reddit,dgl,1,1202,124.3608,1.0076

epoch:1204/50, training loss:0.2038658857345581
Train Acc 1.0092
 Acc 1.0076
reddit,dgl,1,1203,124.4638,1.0076

epoch:1205/50, training loss:0.2038101702928543
Train Acc 1.0092
 Acc 1.0077
reddit,dgl,1,1204,124.5667,1.0077

epoch:1206/50, training loss:0.20375414192676544
Train Acc 1.0092
 Acc 1.0077
new best val f1: 1.0076767753733618
reddit,dgl,1,1205,124.6702,1.0077

epoch:1207/50, training loss:0.20369824767112732
Train Acc 1.0092
 Acc 1.0077
new best val f1: 1.0077148735141725
reddit,dgl,1,1206,124.7738,1.0077

epoch:1208/50, training loss:0.20364199578762054
Train Acc 1.0092
 Acc 1.0077
new best val f1: 1.0077339225845778
reddit,dgl,1,1207,124.8771,1.0077

epoch:1209/50, training loss:0.20358678698539734
Train Acc 1.0092
 Acc 1.0077
reddit,dgl,1,1208,124.9801,1.0077

epoch:1210/50, training loss:0.20353105664253235
Train Acc 1.0092
 Acc 1.0077
reddit,dgl,1,1209,125.0830,1.0077

epoch:1211/50, training loss:0.20347554981708527
Train Acc 1.0092
 Acc 1.0078
new best val f1: 1.0077529716549833
reddit,dgl,1,1210,125.1861,1.0078

epoch:1212/50, training loss:0.2034195065498352
Train Acc 1.0093
 Acc 1.0078
new best val f1: 1.007791069795794
reddit,dgl,1,1211,125.2893,1.0078

epoch:1213/50, training loss:0.2033645063638687
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1212,125.3930,1.0078

epoch:1214/50, training loss:0.2033088058233261
Train Acc 1.0093
 Acc 1.0078
new best val f1: 1.0078101188661994
reddit,dgl,1,1213,125.4962,1.0078

epoch:1215/50, training loss:0.2032536417245865
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1214,125.5992,1.0078

epoch:1216/50, training loss:0.20319844782352448
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1215,125.7023,1.0078

epoch:1217/50, training loss:0.2031424194574356
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1216,125.8055,1.0078

epoch:1218/50, training loss:0.20308715105056763
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1217,125.9092,1.0078

epoch:1219/50, training loss:0.2030327171087265
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1218,126.0123,1.0078

epoch:1220/50, training loss:0.20297759771347046
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1219,126.1158,1.0078

epoch:1221/50, training loss:0.20292189717292786
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1220,126.2188,1.0078

epoch:1222/50, training loss:0.20286718010902405
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1221,126.3219,1.0078

epoch:1223/50, training loss:0.2028125822544098
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1222,126.4251,1.0078

epoch:1224/50, training loss:0.2027575522661209
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1223,126.5282,1.0078

epoch:1225/50, training loss:0.20270174741744995
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1224,126.6311,1.0078

epoch:1226/50, training loss:0.2026473879814148
Train Acc 1.0093
 Acc 1.0078
new best val f1: 1.0078291679366047
reddit,dgl,1,1225,126.7341,1.0078

epoch:1227/50, training loss:0.2025928944349289
Train Acc 1.0093
 Acc 1.0078
reddit,dgl,1,1226,126.8371,1.0078

epoch:1228/50, training loss:0.2025379091501236
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1227,126.9404,1.0078

epoch:1229/50, training loss:0.20248305797576904
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1228,127.0437,1.0078

epoch:1230/50, training loss:0.20242851972579956
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1229,127.1469,1.0078

epoch:1231/50, training loss:0.20237384736537933
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1230,127.2500,1.0078

epoch:1232/50, training loss:0.2023192048072815
Train Acc 1.0094
 Acc 1.0078
new best val f1: 1.00784821700701
reddit,dgl,1,1231,127.3531,1.0078

epoch:1233/50, training loss:0.20226530730724335
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1232,127.4563,1.0078

epoch:1234/50, training loss:0.2022111415863037
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1233,127.5596,1.0078

epoch:1235/50, training loss:0.2021562159061432
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1234,127.6625,1.0078

epoch:1236/50, training loss:0.20210210978984833
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1235,127.7654,1.0078

epoch:1237/50, training loss:0.20204825699329376
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1236,127.8684,1.0078

epoch:1238/50, training loss:0.20199406147003174
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1237,127.9716,1.0078

epoch:1239/50, training loss:0.20193959772586823
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1238,128.0753,1.0078

epoch:1240/50, training loss:0.20188568532466888
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1239,128.1785,1.0078

epoch:1241/50, training loss:0.20183147490024567
Train Acc 1.0094
 Acc 1.0078
reddit,dgl,1,1240,128.2815,1.0078

epoch:1242/50, training loss:0.20177777111530304
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1241,128.3845,1.0078

epoch:1243/50, training loss:0.20172353088855743
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1242,128.4875,1.0078

epoch:1244/50, training loss:0.20166975259780884
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1243,128.5908,1.0078

epoch:1245/50, training loss:0.20161554217338562
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1244,128.6939,1.0078

epoch:1246/50, training loss:0.20156194269657135
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1245,128.7969,1.0078

epoch:1247/50, training loss:0.20150820910930634
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1246,128.8999,1.0078

epoch:1248/50, training loss:0.20145460963249207
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1247,129.0029,1.0078

epoch:1249/50, training loss:0.2014005035161972
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1248,129.1062,1.0078

epoch:1250/50, training loss:0.2013465017080307
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1249,129.2095,1.0078

epoch:1251/50, training loss:0.20129352807998657
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1250,129.3126,1.0078

epoch:1252/50, training loss:0.20123988389968872
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1251,129.4156,1.0078

epoch:1253/50, training loss:0.20118610560894012
Train Acc 1.0095
 Acc 1.0078
reddit,dgl,1,1252,129.5185,1.0078

epoch:1254/50, training loss:0.20113292336463928
Train Acc 1.0095
 Acc 1.0079
new best val f1: 1.007905364218226
reddit,dgl,1,1253,129.6217,1.0079

epoch:1255/50, training loss:0.20107921957969666
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1254,129.7249,1.0079

epoch:1256/50, training loss:0.20102563500404358
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1255,129.8281,1.0079

epoch:1257/50, training loss:0.20097216963768005
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1256,129.9310,1.0079

epoch:1258/50, training loss:0.20091885328292847
Train Acc 1.0096
 Acc 1.0078
reddit,dgl,1,1257,130.0340,1.0078

epoch:1259/50, training loss:0.20086559653282166
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1258,130.1370,1.0079

epoch:1260/50, training loss:0.20081274211406708
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1259,130.2405,1.0079

epoch:1261/50, training loss:0.20075926184654236
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1260,130.3442,1.0079

epoch:1262/50, training loss:0.20070640742778778
Train Acc 1.0096
 Acc 1.0080
new best val f1: 1.0079625114294422
reddit,dgl,1,1261,130.4475,1.0080

epoch:1263/50, training loss:0.20065368711948395
Train Acc 1.0096
 Acc 1.0080
reddit,dgl,1,1262,130.5504,1.0080

epoch:1264/50, training loss:0.20059998333454132
Train Acc 1.0096
 Acc 1.0079
reddit,dgl,1,1263,130.6535,1.0079

epoch:1265/50, training loss:0.20054665207862854
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1264,130.7565,1.0079

epoch:1266/50, training loss:0.2004941701889038
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1265,130.8600,1.0079

epoch:1267/50, training loss:0.20044086873531342
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1266,130.9637,1.0079

epoch:1268/50, training loss:0.2003880739212036
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1267,131.0671,1.0079

epoch:1269/50, training loss:0.2003350853919983
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1268,131.1701,1.0079

epoch:1270/50, training loss:0.20028214156627655
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1269,131.2731,1.0079

epoch:1271/50, training loss:0.2002296894788742
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1270,131.3761,1.0079

epoch:1272/50, training loss:0.2001771330833435
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1271,131.4793,1.0079

epoch:1273/50, training loss:0.20012415945529938
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1272,131.5826,1.0079

epoch:1274/50, training loss:0.200071781873703
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1273,131.6858,1.0079

epoch:1275/50, training loss:0.20001891255378723
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1274,131.7888,1.0079

epoch:1276/50, training loss:0.19996649026870728
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1275,131.8919,1.0079

epoch:1277/50, training loss:0.19991464912891388
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1276,131.9953,1.0079

epoch:1278/50, training loss:0.19986137747764587
Train Acc 1.0097
 Acc 1.0080
reddit,dgl,1,1277,132.0989,1.0080

epoch:1279/50, training loss:0.19980911910533905
Train Acc 1.0097
 Acc 1.0079
reddit,dgl,1,1278,132.2022,1.0079

epoch:1280/50, training loss:0.19975689053535461
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1279,132.3052,1.0079

epoch:1281/50, training loss:0.19970490038394928
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1280,132.4081,1.0079

epoch:1282/50, training loss:0.19965200126171112
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1281,132.5112,1.0079

epoch:1283/50, training loss:0.19960007071495056
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1282,132.6146,1.0079

epoch:1284/50, training loss:0.1995476484298706
Train Acc 1.0098
 Acc 1.0080
reddit,dgl,1,1283,132.7196,1.0080

epoch:1285/50, training loss:0.1994961053133011
Train Acc 1.0098
 Acc 1.0080
reddit,dgl,1,1284,132.8229,1.0080

epoch:1286/50, training loss:0.1994439661502838
Train Acc 1.0098
 Acc 1.0080
reddit,dgl,1,1285,132.9261,1.0080

epoch:1287/50, training loss:0.19939206540584564
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1286,133.0291,1.0079

epoch:1288/50, training loss:0.19933971762657166
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1287,133.1322,1.0079

epoch:1289/50, training loss:0.1992878019809723
Train Acc 1.0098
 Acc 1.0080
reddit,dgl,1,1288,133.2356,1.0080

epoch:1290/50, training loss:0.1992362141609192
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1289,133.3393,1.0079

epoch:1291/50, training loss:0.1991841197013855
Train Acc 1.0098
 Acc 1.0079
reddit,dgl,1,1290,133.4424,1.0079

epoch:1292/50, training loss:0.19913220405578613
Train Acc 1.0098
 Acc 1.0080
new best val f1: 1.008000609570253
reddit,dgl,1,1291,133.5454,1.0080

epoch:1293/50, training loss:0.1990806609392166
Train Acc 1.0099
 Acc 1.0080
reddit,dgl,1,1292,133.6484,1.0080

epoch:1294/50, training loss:0.19902926683425903
Train Acc 1.0099
 Acc 1.0080
reddit,dgl,1,1293,133.7515,1.0080

epoch:1295/50, training loss:0.19897708296775818
Train Acc 1.0099
 Acc 1.0080
reddit,dgl,1,1294,133.8549,1.0080

epoch:1296/50, training loss:0.19892536103725433
Train Acc 1.0099
 Acc 1.0080
reddit,dgl,1,1295,133.9586,1.0080

epoch:1297/50, training loss:0.19887366890907288
Train Acc 1.0099
 Acc 1.0079
reddit,dgl,1,1296,134.0618,1.0079

epoch:1298/50, training loss:0.19882193207740784
Train Acc 1.0099
 Acc 1.0080
reddit,dgl,1,1297,134.1649,1.0080

epoch:1299/50, training loss:0.19877050817012787
Train Acc 1.0099
 Acc 1.0080
reddit,dgl,1,1298,134.2678,1.0080

epoch:1300/50, training loss:0.19871921837329865
Train Acc 1.0099
 Acc 1.0079
reddit,dgl,1,1299,134.3713,1.0079

epoch:1301/50, training loss:0.1986677646636963
Train Acc 1.0099
 Acc 1.0079
reddit,dgl,1,1300,134.4745,1.0079

epoch:1302/50, training loss:0.19861632585525513
Train Acc 1.0099
 Acc 1.0079
reddit,dgl,1,1301,134.5777,1.0079

epoch:1303/50, training loss:0.1985652893781662
Train Acc 1.0099
 Acc 1.0079
reddit,dgl,1,1302,134.6809,1.0079

epoch:1304/50, training loss:0.19851388037204742
Train Acc 1.0100
 Acc 1.0079
reddit,dgl,1,1303,134.7840,1.0079

epoch:1305/50, training loss:0.19846245646476746
Train Acc 1.0100
 Acc 1.0079
reddit,dgl,1,1304,134.8870,1.0079

epoch:1306/50, training loss:0.1984119862318039
Train Acc 1.0100
 Acc 1.0079
reddit,dgl,1,1305,134.9903,1.0079

epoch:1307/50, training loss:0.19836072623729706
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1306,135.0940,1.0080

epoch:1308/50, training loss:0.19830948114395142
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1307,135.1972,1.0080

epoch:1309/50, training loss:0.19825828075408936
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1308,135.3002,1.0080

epoch:1310/50, training loss:0.19820751249790192
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1309,135.4035,1.0080

epoch:1311/50, training loss:0.19815656542778015
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1310,135.5064,1.0080

epoch:1312/50, training loss:0.19810539484024048
Train Acc 1.0100
 Acc 1.0080
new best val f1: 1.0080196586406582
reddit,dgl,1,1311,135.6099,1.0080

epoch:1313/50, training loss:0.19805483520030975
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1312,135.7135,1.0080

epoch:1314/50, training loss:0.19800373911857605
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1313,135.8170,1.0080

epoch:1315/50, training loss:0.19795267283916473
Train Acc 1.0100
 Acc 1.0080
reddit,dgl,1,1314,135.9199,1.0080

epoch:1316/50, training loss:0.19790269434452057
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1315,136.0229,1.0080

epoch:1317/50, training loss:0.19785171747207642
Train Acc 1.0101
 Acc 1.0080
new best val f1: 1.0080387077110637
reddit,dgl,1,1316,136.1260,1.0080

epoch:1318/50, training loss:0.1978011280298233
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1317,136.2292,1.0080

epoch:1319/50, training loss:0.19775082170963287
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1318,136.3325,1.0080

epoch:1320/50, training loss:0.19770018756389618
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1319,136.4354,1.0080

epoch:1321/50, training loss:0.19764967262744904
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1320,136.5383,1.0080

epoch:1322/50, training loss:0.19759957492351532
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1321,136.6414,1.0080

epoch:1323/50, training loss:0.19754904508590698
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1322,136.7446,1.0080

epoch:1324/50, training loss:0.19749777019023895
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1323,136.8479,1.0080

epoch:1325/50, training loss:0.19744744896888733
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1324,136.9508,1.0080

epoch:1326/50, training loss:0.1973976045846939
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1325,137.0539,1.0080

epoch:1327/50, training loss:0.19734744727611542
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1326,137.1569,1.0080

epoch:1328/50, training loss:0.19729681313037872
Train Acc 1.0101
 Acc 1.0080
reddit,dgl,1,1327,137.2605,1.0080

epoch:1329/50, training loss:0.19724677503108978
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1328,137.3638,1.0080

epoch:1330/50, training loss:0.1971968710422516
Train Acc 1.0102
 Acc 1.0081
new best val f1: 1.008057756781469
reddit,dgl,1,1329,137.4668,1.0081

epoch:1331/50, training loss:0.197146937251091
Train Acc 1.0102
 Acc 1.0081
reddit,dgl,1,1330,137.5698,1.0081

epoch:1332/50, training loss:0.19709715247154236
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1331,137.6729,1.0080

epoch:1333/50, training loss:0.19704654812812805
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1332,137.7763,1.0080

epoch:1334/50, training loss:0.19699646532535553
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1333,137.8801,1.0080

epoch:1335/50, training loss:0.19694668054580688
Train Acc 1.0102
 Acc 1.0081
reddit,dgl,1,1334,137.9833,1.0081

epoch:1336/50, training loss:0.1968970149755478
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1335,138.0863,1.0080

epoch:1337/50, training loss:0.19684724509716034
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1336,138.1893,1.0080

epoch:1338/50, training loss:0.19679726660251617
Train Acc 1.0102
 Acc 1.0080
reddit,dgl,1,1337,138.2924,1.0080

epoch:1339/50, training loss:0.19674770534038544
Train Acc 1.0102
 Acc 1.0081
reddit,dgl,1,1338,138.3959,1.0081

epoch:1340/50, training loss:0.19669783115386963
Train Acc 1.0102
 Acc 1.0081
reddit,dgl,1,1339,138.4992,1.0081

epoch:1341/50, training loss:0.19664856791496277
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1340,138.6025,1.0080

epoch:1342/50, training loss:0.1965983808040619
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1341,138.7056,1.0080

epoch:1343/50, training loss:0.19654899835586548
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1342,138.8086,1.0080

epoch:1344/50, training loss:0.196499764919281
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1343,138.9121,1.0080

epoch:1345/50, training loss:0.1964501142501831
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1344,139.0154,1.0080

epoch:1346/50, training loss:0.19640043377876282
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1345,139.1183,1.0080

epoch:1347/50, training loss:0.1963515728712082
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1346,139.2212,1.0080

epoch:1348/50, training loss:0.1963018774986267
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1347,139.3243,1.0080

epoch:1349/50, training loss:0.1962519735097885
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1348,139.4275,1.0080

epoch:1350/50, training loss:0.19620302319526672
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1349,139.5308,1.0080

epoch:1351/50, training loss:0.19615405797958374
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1350,139.6340,1.0080

epoch:1352/50, training loss:0.19610488414764404
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1351,139.7371,1.0080

epoch:1353/50, training loss:0.19605512917041779
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1352,139.8401,1.0080

epoch:1354/50, training loss:0.19600610435009003
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1353,139.9433,1.0080

epoch:1355/50, training loss:0.19595715403556824
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1354,140.0465,1.0080

epoch:1356/50, training loss:0.19590827822685242
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1355,140.1494,1.0080

epoch:1357/50, training loss:0.19585910439491272
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1356,140.2525,1.0080

epoch:1358/50, training loss:0.19580985605716705
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1357,140.3556,1.0080

epoch:1359/50, training loss:0.19576138257980347
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1358,140.4590,1.0080

epoch:1360/50, training loss:0.195712611079216
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1359,140.5624,1.0080

epoch:1361/50, training loss:0.19566327333450317
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1360,140.6653,1.0080

epoch:1362/50, training loss:0.19561520218849182
Train Acc 1.0103
 Acc 1.0081
reddit,dgl,1,1361,140.7684,1.0081

epoch:1363/50, training loss:0.19556567072868347
Train Acc 1.0103
 Acc 1.0081
reddit,dgl,1,1362,140.8714,1.0081

epoch:1364/50, training loss:0.1955176144838333
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1363,140.9750,1.0080

epoch:1365/50, training loss:0.19546830654144287
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1364,141.0783,1.0080

epoch:1366/50, training loss:0.19541989266872406
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1365,141.1813,1.0080

epoch:1367/50, training loss:0.19537147879600525
Train Acc 1.0103
 Acc 1.0081
reddit,dgl,1,1366,141.2843,1.0081

epoch:1368/50, training loss:0.1953228861093521
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1367,141.3873,1.0080

epoch:1369/50, training loss:0.19527454674243927
Train Acc 1.0103
 Acc 1.0080
reddit,dgl,1,1368,141.4908,1.0080

epoch:1370/50, training loss:0.19522598385810852
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1369,141.5946,1.0080

epoch:1371/50, training loss:0.19517700374126434
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1370,141.6977,1.0080

epoch:1372/50, training loss:0.19512835144996643
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1371,141.8007,1.0080

epoch:1373/50, training loss:0.1950794905424118
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1372,141.9038,1.0080

epoch:1374/50, training loss:0.1950315237045288
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1373,142.0068,1.0080

epoch:1375/50, training loss:0.19498370587825775
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1374,142.1102,1.0080

epoch:1376/50, training loss:0.19493496417999268
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1375,142.2141,1.0080

epoch:1377/50, training loss:0.19488708674907684
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1376,142.3175,1.0080

epoch:1378/50, training loss:0.1948387622833252
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1377,142.4207,1.0080

epoch:1379/50, training loss:0.1947898268699646
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1378,142.5239,1.0080

epoch:1380/50, training loss:0.1947421133518219
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1379,142.6272,1.0080

epoch:1381/50, training loss:0.1946946233510971
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1380,142.7306,1.0080

epoch:1382/50, training loss:0.19464603066444397
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1381,142.8342,1.0080

epoch:1383/50, training loss:0.19459810853004456
Train Acc 1.0104
 Acc 1.0080
reddit,dgl,1,1382,142.9373,1.0080

epoch:1384/50, training loss:0.19454991817474365
Train Acc 1.0105
 Acc 1.0080
reddit,dgl,1,1383,143.0404,1.0080

epoch:1385/50, training loss:0.194501593708992
Train Acc 1.0105
 Acc 1.0080
reddit,dgl,1,1384,143.1433,1.0080

epoch:1386/50, training loss:0.1944536715745926
Train Acc 1.0105
 Acc 1.0080
reddit,dgl,1,1385,143.2468,1.0080

epoch:1387/50, training loss:0.19440600275993347
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1386,143.3500,1.0081

epoch:1388/50, training loss:0.19435778260231018
Train Acc 1.0105
 Acc 1.0081
new best val f1: 1.0080768058518745
reddit,dgl,1,1387,143.4538,1.0081

epoch:1389/50, training loss:0.19431060552597046
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1388,143.5570,1.0081

epoch:1390/50, training loss:0.19426243007183075
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1389,143.6600,1.0081

epoch:1391/50, training loss:0.19421419501304626
Train Acc 1.0105
 Acc 1.0080
reddit,dgl,1,1390,143.7631,1.0080

epoch:1392/50, training loss:0.1941664069890976
Train Acc 1.0105
 Acc 1.0080
reddit,dgl,1,1391,143.8662,1.0080

epoch:1393/50, training loss:0.19411930441856384
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1392,143.9699,1.0081

epoch:1394/50, training loss:0.19407148659229279
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1393,144.0731,1.0081

epoch:1395/50, training loss:0.1940234750509262
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1394,144.1761,1.0081

epoch:1396/50, training loss:0.1939762383699417
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1395,144.2791,1.0081

epoch:1397/50, training loss:0.19392895698547363
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1396,144.3825,1.0081

epoch:1398/50, training loss:0.19388136267662048
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1397,144.4863,1.0081

epoch:1399/50, training loss:0.19383370876312256
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1398,144.5899,1.0081

epoch:1400/50, training loss:0.1937865912914276
Train Acc 1.0105
 Acc 1.0081
reddit,dgl,1,1399,144.6929,1.0081

epoch:1401/50, training loss:0.1937384158372879
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1400,144.7959,1.0081

epoch:1402/50, training loss:0.193691223859787
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1401,144.8990,1.0081

epoch:1403/50, training loss:0.19364407658576965
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1402,145.0025,1.0081

epoch:1404/50, training loss:0.19359655678272247
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1403,145.1062,1.0081

epoch:1405/50, training loss:0.19354893267154694
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1404,145.2095,1.0081

epoch:1406/50, training loss:0.19350206851959229
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1405,145.3126,1.0080

epoch:1407/50, training loss:0.1934548169374466
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1406,145.4155,1.0080

epoch:1408/50, training loss:0.1934075653553009
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1407,145.5190,1.0081

epoch:1409/50, training loss:0.19336022436618805
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1408,145.6226,1.0081

epoch:1410/50, training loss:0.19331343472003937
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1409,145.7257,1.0081

epoch:1411/50, training loss:0.19326648116111755
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1410,145.8287,1.0080

epoch:1412/50, training loss:0.1932191252708435
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1411,145.9316,1.0080

epoch:1413/50, training loss:0.19317206740379333
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1412,146.0350,1.0080

epoch:1414/50, training loss:0.19312529265880585
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1413,146.1388,1.0080

epoch:1415/50, training loss:0.19307853281497955
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1414,146.2421,1.0080

epoch:1416/50, training loss:0.19303104281425476
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1415,146.3451,1.0080

epoch:1417/50, training loss:0.1929844170808792
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1416,146.4480,1.0080

epoch:1418/50, training loss:0.1929379552602768
Train Acc 1.0106
 Acc 1.0080
reddit,dgl,1,1417,146.5510,1.0080

epoch:1419/50, training loss:0.19289040565490723
Train Acc 1.0107
 Acc 1.0080
reddit,dgl,1,1418,146.6542,1.0080

epoch:1420/50, training loss:0.1928439438343048
Train Acc 1.0106
 Acc 1.0081
reddit,dgl,1,1419,146.7580,1.0081

epoch:1421/50, training loss:0.1927972137928009
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1420,146.8612,1.0081

epoch:1422/50, training loss:0.19275103509426117
Train Acc 1.0107
 Acc 1.0081
new best val f1: 1.0080958549222798
reddit,dgl,1,1421,146.9641,1.0081

epoch:1423/50, training loss:0.19270336627960205
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1422,147.0670,1.0081

epoch:1424/50, training loss:0.19265732169151306
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1423,147.1703,1.0081

epoch:1425/50, training loss:0.19261054694652557
Train Acc 1.0107
 Acc 1.0082
new best val f1: 1.008153002133496
reddit,dgl,1,1424,147.2735,1.0082

epoch:1426/50, training loss:0.19256436824798584
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1425,147.3772,1.0081

epoch:1427/50, training loss:0.1925177425146103
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1426,147.4801,1.0081

epoch:1428/50, training loss:0.1924709528684616
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1427,147.5831,1.0081

epoch:1429/50, training loss:0.19242458045482635
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1428,147.6861,1.0081

epoch:1430/50, training loss:0.19237765669822693
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1429,147.7897,1.0081

epoch:1431/50, training loss:0.19233182072639465
Train Acc 1.0107
 Acc 1.0082
reddit,dgl,1,1430,147.8933,1.0082

epoch:1432/50, training loss:0.19228501617908478
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1431,147.9967,1.0081

epoch:1433/50, training loss:0.19223885238170624
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1432,148.0997,1.0081

epoch:1434/50, training loss:0.19219255447387695
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1433,148.2027,1.0081

epoch:1435/50, training loss:0.1921462118625641
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1434,148.3062,1.0081

epoch:1436/50, training loss:0.19209979474544525
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1435,148.4099,1.0081

epoch:1437/50, training loss:0.1920536458492279
Train Acc 1.0107
 Acc 1.0081
reddit,dgl,1,1436,148.5130,1.0081

epoch:1438/50, training loss:0.192007377743721
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1437,148.6161,1.0081

epoch:1439/50, training loss:0.19196131825447083
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1438,148.7190,1.0080

epoch:1440/50, training loss:0.19191502034664154
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1439,148.8225,1.0080

epoch:1441/50, training loss:0.19186900556087494
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1440,148.9257,1.0080

epoch:1442/50, training loss:0.19182299077510834
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1441,149.0290,1.0080

epoch:1443/50, training loss:0.19177651405334473
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1442,149.1319,1.0080

epoch:1444/50, training loss:0.19173099100589752
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1443,149.2349,1.0081

epoch:1445/50, training loss:0.19168521463871002
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1444,149.3379,1.0080

epoch:1446/50, training loss:0.19163905084133148
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1445,149.4417,1.0080

epoch:1447/50, training loss:0.19159312546253204
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1446,149.5449,1.0080

epoch:1448/50, training loss:0.19154711067676544
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1447,149.6479,1.0080

epoch:1449/50, training loss:0.1915017068386078
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1448,149.7510,1.0080

epoch:1450/50, training loss:0.19145548343658447
Train Acc 1.0108
 Acc 1.0080
reddit,dgl,1,1449,149.8540,1.0080

epoch:1451/50, training loss:0.1914099156856537
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1450,149.9575,1.0081

epoch:1452/50, training loss:0.19136422872543335
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1451,150.0608,1.0081

epoch:1453/50, training loss:0.19131840765476227
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1452,150.1637,1.0081

epoch:1454/50, training loss:0.19127273559570312
Train Acc 1.0108
 Acc 1.0081
reddit,dgl,1,1453,150.2668,1.0081

epoch:1455/50, training loss:0.1912272423505783
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1454,150.3698,1.0081

epoch:1456/50, training loss:0.19118131697177887
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1455,150.4733,1.0081

epoch:1457/50, training loss:0.19113607704639435
Train Acc 1.0109
 Acc 1.0080
reddit,dgl,1,1456,150.5768,1.0080

epoch:1458/50, training loss:0.19109061360359192
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1457,150.6806,1.0081

epoch:1459/50, training loss:0.19104468822479248
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1458,150.7837,1.0081

epoch:1460/50, training loss:0.19099955260753632
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1459,150.8871,1.0081

epoch:1461/50, training loss:0.19095450639724731
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1460,150.9902,1.0081

epoch:1462/50, training loss:0.19090865552425385
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1461,151.0934,1.0081

epoch:1463/50, training loss:0.1908632069826126
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1462,151.1966,1.0081

epoch:1464/50, training loss:0.1908179670572281
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1463,151.2999,1.0081

epoch:1465/50, training loss:0.19077247381210327
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1464,151.4031,1.0081

epoch:1466/50, training loss:0.19072756171226501
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1465,151.5062,1.0081

epoch:1467/50, training loss:0.19068163633346558
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1466,151.6091,1.0081

epoch:1468/50, training loss:0.1906372308731079
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1467,151.7121,1.0081

epoch:1469/50, training loss:0.19059178233146667
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1468,151.8153,1.0081

epoch:1470/50, training loss:0.19054670631885529
Train Acc 1.0109
 Acc 1.0081
reddit,dgl,1,1469,151.9186,1.0081

epoch:1471/50, training loss:0.19050174951553345
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1470,152.0218,1.0081

epoch:1472/50, training loss:0.19045640528202057
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1471,152.1249,1.0081

epoch:1473/50, training loss:0.19041171669960022
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1472,152.2278,1.0081

epoch:1474/50, training loss:0.1903667151927948
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1473,152.3309,1.0081

epoch:1475/50, training loss:0.19032125174999237
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1474,152.4344,1.0081

epoch:1476/50, training loss:0.19027681648731232
Train Acc 1.0110
 Acc 1.0082
reddit,dgl,1,1475,152.5377,1.0082

epoch:1477/50, training loss:0.19023235142230988
Train Acc 1.0110
 Acc 1.0082
new best val f1: 1.0081720512039012
reddit,dgl,1,1476,152.6410,1.0082

epoch:1478/50, training loss:0.1901865005493164
Train Acc 1.0110
 Acc 1.0082
reddit,dgl,1,1477,152.7440,1.0082

epoch:1479/50, training loss:0.19014251232147217
Train Acc 1.0110
 Acc 1.0082
new best val f1: 1.0081911002743067
reddit,dgl,1,1478,152.8469,1.0082

epoch:1480/50, training loss:0.1900971233844757
Train Acc 1.0110
 Acc 1.0082
reddit,dgl,1,1479,152.9500,1.0082

epoch:1481/50, training loss:0.19005264341831207
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1480,153.0535,1.0081

epoch:1482/50, training loss:0.19000796973705292
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1481,153.1567,1.0081

epoch:1483/50, training loss:0.18996375799179077
Train Acc 1.0110
 Acc 1.0081
reddit,dgl,1,1482,153.2603,1.0081

epoch:1484/50, training loss:0.18991845846176147
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1483,153.3635,1.0081

epoch:1485/50, training loss:0.1898737996816635
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1484,153.4665,1.0081

epoch:1486/50, training loss:0.18983009457588196
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1485,153.5695,1.0081

epoch:1487/50, training loss:0.18978483974933624
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1486,153.6725,1.0081

epoch:1488/50, training loss:0.1897401213645935
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1487,153.7756,1.0081

epoch:1489/50, training loss:0.18969620764255524
Train Acc 1.0111
 Acc 1.0082
reddit,dgl,1,1488,153.8793,1.0082

epoch:1490/50, training loss:0.1896512657403946
Train Acc 1.0111
 Acc 1.0082
reddit,dgl,1,1489,153.9825,1.0082

epoch:1491/50, training loss:0.18960705399513245
Train Acc 1.0111
 Acc 1.0082
reddit,dgl,1,1490,154.0855,1.0082

epoch:1492/50, training loss:0.18956251442432404
Train Acc 1.0111
 Acc 1.0082
reddit,dgl,1,1491,154.1885,1.0082

epoch:1493/50, training loss:0.18951807916164398
Train Acc 1.0112
 Acc 1.0081
reddit,dgl,1,1492,154.2916,1.0081

epoch:1494/50, training loss:0.18947413563728333
Train Acc 1.0111
 Acc 1.0081
reddit,dgl,1,1493,154.3948,1.0081

epoch:1495/50, training loss:0.1894294023513794
Train Acc 1.0112
 Acc 1.0081
reddit,dgl,1,1494,154.4981,1.0081

epoch:1496/50, training loss:0.1893853098154068
Train Acc 1.0111
 Acc 1.0082
reddit,dgl,1,1495,154.6011,1.0082

epoch:1497/50, training loss:0.18934084475040436
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1496,154.7041,1.0082

epoch:1498/50, training loss:0.189296692609787
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1497,154.8072,1.0082

epoch:1499/50, training loss:0.18925245106220245
Train Acc 1.0112
 Acc 1.0081
reddit,dgl,1,1498,154.9107,1.0081

epoch:1500/50, training loss:0.18920843303203583
Train Acc 1.0112
 Acc 1.0081
reddit,dgl,1,1499,155.0147,1.0081

epoch:1501/50, training loss:0.18916402757167816
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1500,155.1180,1.0082

epoch:1502/50, training loss:0.18912014365196228
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1501,155.2216,1.0082

epoch:1503/50, training loss:0.18907605111598969
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1502,155.3247,1.0082

epoch:1504/50, training loss:0.18903174996376038
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1503,155.4277,1.0082

epoch:1505/50, training loss:0.18898792564868927
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1504,155.5311,1.0082

epoch:1506/50, training loss:0.18894384801387787
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1505,155.6347,1.0082

epoch:1507/50, training loss:0.1888996660709381
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1506,155.7383,1.0082

epoch:1508/50, training loss:0.18885619938373566
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1507,155.8416,1.0082

epoch:1509/50, training loss:0.1888115257024765
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1508,155.9446,1.0082

epoch:1510/50, training loss:0.18876832723617554
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1509,156.0480,1.0082

epoch:1511/50, training loss:0.18872413039207458
Train Acc 1.0113
 Acc 1.0082
new best val f1: 1.008210149344712
reddit,dgl,1,1510,156.1514,1.0082

epoch:1512/50, training loss:0.18868058919906616
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1511,156.2550,1.0082

epoch:1513/50, training loss:0.1886371225118637
Train Acc 1.0112
 Acc 1.0082
reddit,dgl,1,1512,156.3586,1.0082

epoch:1514/50, training loss:0.18859268724918365
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1513,156.4619,1.0082

epoch:1515/50, training loss:0.1885494589805603
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1514,156.5649,1.0082

epoch:1516/50, training loss:0.18850547075271606
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1515,156.6679,1.0082

epoch:1517/50, training loss:0.1884620189666748
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1516,156.7713,1.0082

epoch:1518/50, training loss:0.18841779232025146
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1517,156.8751,1.0082

epoch:1519/50, training loss:0.18837447464466095
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1518,156.9787,1.0082

epoch:1520/50, training loss:0.1883309930562973
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1519,157.0820,1.0082

epoch:1521/50, training loss:0.18828779458999634
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1520,157.1850,1.0082

epoch:1522/50, training loss:0.18824385106563568
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1521,157.2880,1.0082

epoch:1523/50, training loss:0.1882004588842392
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1522,157.3910,1.0082

epoch:1524/50, training loss:0.18815666437149048
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1523,157.4943,1.0082

epoch:1525/50, training loss:0.18811385333538055
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1524,157.5976,1.0082

epoch:1526/50, training loss:0.1880703568458557
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1525,157.7005,1.0082

epoch:1527/50, training loss:0.188026562333107
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1526,157.8034,1.0082

epoch:1528/50, training loss:0.1879839301109314
Train Acc 1.0113
 Acc 1.0082
new best val f1: 1.0082482474855228
reddit,dgl,1,1527,157.9064,1.0082

epoch:1529/50, training loss:0.1879402995109558
Train Acc 1.0113
 Acc 1.0082
reddit,dgl,1,1528,158.0094,1.0082

epoch:1530/50, training loss:0.1878967136144638
Train Acc 1.0113
 Acc 1.0083
new best val f1: 1.0082863456263333
reddit,dgl,1,1529,158.1126,1.0083

epoch:1531/50, training loss:0.18785355985164642
Train Acc 1.0113
 Acc 1.0083
reddit,dgl,1,1530,158.2163,1.0083

epoch:1532/50, training loss:0.1878102421760559
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1531,158.3197,1.0083

epoch:1533/50, training loss:0.18776710331439972
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1532,158.4227,1.0083

epoch:1534/50, training loss:0.18772409856319427
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1533,158.5257,1.0083

epoch:1535/50, training loss:0.1876809000968933
Train Acc 1.0114
 Acc 1.0082
reddit,dgl,1,1534,158.6288,1.0082

epoch:1536/50, training loss:0.18763720989227295
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1535,158.7323,1.0083

epoch:1537/50, training loss:0.18759466707706451
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1536,158.8359,1.0083

epoch:1538/50, training loss:0.18755167722702026
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1537,158.9391,1.0083

epoch:1539/50, training loss:0.1875085085630417
Train Acc 1.0114
 Acc 1.0083
new best val f1: 1.0083053946967389
reddit,dgl,1,1538,159.0421,1.0083

epoch:1540/50, training loss:0.18746539950370789
Train Acc 1.0114
 Acc 1.0083
reddit,dgl,1,1539,159.1451,1.0083

epoch:1541/50, training loss:0.1874229609966278
Train Acc 1.0114
 Acc 1.0083
new best val f1: 1.0083244437671441
reddit,dgl,1,1540,159.2485,1.0083

epoch:1542/50, training loss:0.18737919628620148
Train Acc 1.0114
 Acc 1.0083
new best val f1: 1.0083434928375494
reddit,dgl,1,1541,159.3523,1.0083

epoch:1543/50, training loss:0.18733657896518707
Train Acc 1.0114
 Acc 1.0084
new best val f1: 1.0083815909783602
reddit,dgl,1,1542,159.4555,1.0084

epoch:1544/50, training loss:0.18729372322559357
Train Acc 1.0115
 Acc 1.0084
new best val f1: 1.0084387381895763
reddit,dgl,1,1543,159.5585,1.0084

epoch:1545/50, training loss:0.18725091218948364
Train Acc 1.0115
 Acc 1.0085
new best val f1: 1.0084577872599818
reddit,dgl,1,1544,159.6615,1.0085

epoch:1546/50, training loss:0.1872081756591797
Train Acc 1.0115
 Acc 1.0084
reddit,dgl,1,1545,159.7646,1.0084

epoch:1547/50, training loss:0.18716523051261902
Train Acc 1.0115
 Acc 1.0084
reddit,dgl,1,1546,159.8677,1.0084

epoch:1548/50, training loss:0.1871221363544464
Train Acc 1.0115
 Acc 1.0084
reddit,dgl,1,1547,159.9714,1.0084

epoch:1549/50, training loss:0.18707962334156036
Train Acc 1.0115
 Acc 1.0084
reddit,dgl,1,1548,160.0747,1.0084

epoch:1550/50, training loss:0.18703670799732208
Train Acc 1.0115
 Acc 1.0085
reddit,dgl,1,1549,160.1777,1.0085

epoch:1551/50, training loss:0.18699368834495544
Train Acc 1.0115
 Acc 1.0085
reddit,dgl,1,1550,160.2807,1.0085

epoch:1552/50, training loss:0.18695129454135895
Train Acc 1.0115
 Acc 1.0085
reddit,dgl,1,1551,160.3838,1.0085

epoch:1553/50, training loss:0.1869087517261505
Train Acc 1.0115
 Acc 1.0085
reddit,dgl,1,1552,160.4872,1.0085

epoch:1554/50, training loss:0.18686605989933014
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1553,160.5915,1.0084

epoch:1555/50, training loss:0.18682262301445007
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1554,160.6948,1.0084

epoch:1556/50, training loss:0.18678107857704163
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1555,160.7977,1.0084

epoch:1557/50, training loss:0.1867380291223526
Train Acc 1.0116
 Acc 1.0085
reddit,dgl,1,1556,160.9007,1.0085

epoch:1558/50, training loss:0.1866949051618576
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1557,161.0038,1.0084

epoch:1559/50, training loss:0.18665289878845215
Train Acc 1.0116
 Acc 1.0085
reddit,dgl,1,1558,161.1069,1.0085

epoch:1560/50, training loss:0.1866101622581482
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1559,161.2106,1.0084

epoch:1561/50, training loss:0.18656805157661438
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1560,161.3138,1.0084

epoch:1562/50, training loss:0.1865251362323761
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1561,161.4169,1.0084

epoch:1563/50, training loss:0.18648286163806915
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1562,161.5198,1.0084

epoch:1564/50, training loss:0.18644040822982788
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1563,161.6233,1.0084

epoch:1565/50, training loss:0.1863984316587448
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1564,161.7265,1.0084

epoch:1566/50, training loss:0.18635563552379608
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1565,161.8302,1.0084

epoch:1567/50, training loss:0.18631316721439362
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1566,161.9331,1.0084

epoch:1568/50, training loss:0.1862715631723404
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1567,162.0362,1.0084

epoch:1569/50, training loss:0.18622854351997375
Train Acc 1.0116
 Acc 1.0085
reddit,dgl,1,1568,162.1391,1.0085

epoch:1570/50, training loss:0.18618656694889069
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1569,162.2425,1.0084

epoch:1571/50, training loss:0.18614454567432404
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1570,162.3457,1.0084

epoch:1572/50, training loss:0.18610231578350067
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1571,162.4491,1.0084

epoch:1573/50, training loss:0.18605992197990417
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1572,162.5519,1.0084

epoch:1574/50, training loss:0.1860179305076599
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1573,162.6549,1.0084

epoch:1575/50, training loss:0.18597590923309326
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1574,162.7578,1.0084

epoch:1576/50, training loss:0.1859339475631714
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1575,162.8610,1.0084

epoch:1577/50, training loss:0.18589217960834503
Train Acc 1.0116
 Acc 1.0084
reddit,dgl,1,1576,162.9642,1.0084

epoch:1578/50, training loss:0.18584942817687988
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1577,163.0679,1.0084

epoch:1579/50, training loss:0.1858077049255371
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1578,163.1710,1.0084

epoch:1580/50, training loss:0.18576548993587494
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1579,163.2742,1.0084

epoch:1581/50, training loss:0.18572399020195007
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1580,163.3773,1.0084

epoch:1582/50, training loss:0.18568168580532074
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1581,163.4808,1.0084

epoch:1583/50, training loss:0.18563950061798096
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1582,163.5840,1.0084

epoch:1584/50, training loss:0.1855984330177307
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1583,163.6877,1.0084

epoch:1585/50, training loss:0.18555618822574615
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1584,163.7906,1.0084

epoch:1586/50, training loss:0.18551455438137054
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1585,163.8937,1.0084

epoch:1587/50, training loss:0.1854722797870636
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1586,163.9966,1.0084

epoch:1588/50, training loss:0.18543073534965515
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1587,164.1000,1.0084

epoch:1589/50, training loss:0.18538951873779297
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1588,164.2037,1.0084

epoch:1590/50, training loss:0.18534688651561737
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1589,164.3073,1.0084

epoch:1591/50, training loss:0.18530569970607758
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1590,164.4105,1.0084

epoch:1592/50, training loss:0.1852640062570572
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1591,164.5136,1.0084

epoch:1593/50, training loss:0.18522235751152039
Train Acc 1.0117
 Acc 1.0084
reddit,dgl,1,1592,164.6165,1.0084

epoch:1594/50, training loss:0.18518096208572388
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1593,164.7200,1.0084

epoch:1595/50, training loss:0.18513911962509155
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1594,164.8232,1.0084

epoch:1596/50, training loss:0.18509770929813385
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1595,164.9271,1.0084

epoch:1597/50, training loss:0.18505583703517914
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1596,165.0303,1.0084

epoch:1598/50, training loss:0.18501441180706024
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1597,165.1334,1.0084

epoch:1599/50, training loss:0.18497344851493835
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1598,165.2363,1.0084

epoch:1600/50, training loss:0.184932142496109
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1599,165.3399,1.0084

epoch:1601/50, training loss:0.1848905235528946
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1600,165.4431,1.0084

epoch:1602/50, training loss:0.18484938144683838
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1601,165.5468,1.0084

epoch:1603/50, training loss:0.18480829894542694
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1602,165.6499,1.0084

epoch:1604/50, training loss:0.1847669631242752
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1603,165.7531,1.0084

epoch:1605/50, training loss:0.18472515046596527
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1604,165.8560,1.0084

epoch:1606/50, training loss:0.18468411266803741
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1605,165.9595,1.0084

epoch:1607/50, training loss:0.18464289605617523
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1606,166.0631,1.0084

epoch:1608/50, training loss:0.18460167944431305
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1607,166.1664,1.0084

epoch:1609/50, training loss:0.18456055223941803
Train Acc 1.0118
 Acc 1.0084
reddit,dgl,1,1608,166.2693,1.0084

epoch:1610/50, training loss:0.18451960384845734
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1609,166.3724,1.0084

epoch:1611/50, training loss:0.18447820842266083
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1610,166.4755,1.0084

epoch:1612/50, training loss:0.18443726003170013
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1611,166.5789,1.0083

epoch:1613/50, training loss:0.18439653515815735
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1612,166.6826,1.0084

epoch:1614/50, training loss:0.1843552440404892
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1613,166.7859,1.0084

epoch:1615/50, training loss:0.1843138486146927
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1614,166.8889,1.0084

epoch:1616/50, training loss:0.18427348136901855
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1615,166.9920,1.0084

epoch:1617/50, training loss:0.18423213064670563
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1616,167.0950,1.0084

epoch:1618/50, training loss:0.18419091403484344
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1617,167.1984,1.0084

epoch:1619/50, training loss:0.18415102362632751
Train Acc 1.0119
 Acc 1.0084
reddit,dgl,1,1618,167.3019,1.0084

epoch:1620/50, training loss:0.18410950899124146
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1619,167.4057,1.0083

epoch:1621/50, training loss:0.18406879901885986
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1620,167.5089,1.0083

epoch:1622/50, training loss:0.18402783572673798
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1621,167.6120,1.0083

epoch:1623/50, training loss:0.18398697674274445
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1622,167.7149,1.0083

epoch:1624/50, training loss:0.18394622206687927
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1623,167.8180,1.0082

epoch:1625/50, training loss:0.18390582501888275
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1624,167.9214,1.0083

epoch:1626/50, training loss:0.18386514484882355
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1625,168.0247,1.0083

epoch:1627/50, training loss:0.18382437527179718
Train Acc 1.0119
 Acc 1.0083
reddit,dgl,1,1626,168.1279,1.0083

epoch:1628/50, training loss:0.1837838590145111
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1627,168.2310,1.0082

epoch:1629/50, training loss:0.1837434321641922
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1628,168.3341,1.0082

epoch:1630/50, training loss:0.18370190262794495
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1629,168.4376,1.0082

epoch:1631/50, training loss:0.18366186320781708
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1630,168.5409,1.0082

epoch:1632/50, training loss:0.1836216300725937
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1631,168.6449,1.0082

epoch:1633/50, training loss:0.18358100950717926
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1632,168.7482,1.0082

epoch:1634/50, training loss:0.18354032933712006
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1633,168.8515,1.0082

epoch:1635/50, training loss:0.18350006639957428
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1634,168.9546,1.0082

epoch:1636/50, training loss:0.18345941603183746
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1635,169.0577,1.0082

epoch:1637/50, training loss:0.18341891467571259
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1636,169.1609,1.0082

epoch:1638/50, training loss:0.18337887525558472
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1637,169.2642,1.0082

epoch:1639/50, training loss:0.18333861231803894
Train Acc 1.0119
 Acc 1.0082
reddit,dgl,1,1638,169.3671,1.0082

epoch:1640/50, training loss:0.18329793214797974
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1639,169.4701,1.0082

epoch:1641/50, training loss:0.18325786292552948
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1640,169.5731,1.0082

epoch:1642/50, training loss:0.18321727216243744
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1641,169.6766,1.0082

epoch:1643/50, training loss:0.18317736685276031
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1642,169.7802,1.0082

epoch:1644/50, training loss:0.18313708901405334
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1643,169.8835,1.0082

epoch:1645/50, training loss:0.18309694528579712
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1644,169.9864,1.0082

epoch:1646/50, training loss:0.18305690586566925
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1645,170.0894,1.0082

epoch:1647/50, training loss:0.18301638960838318
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1646,170.1924,1.0082

epoch:1648/50, training loss:0.18297626078128815
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1647,170.2957,1.0082

epoch:1649/50, training loss:0.1829364150762558
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1648,170.3989,1.0082

epoch:1650/50, training loss:0.18289628624916077
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1649,170.5019,1.0082

epoch:1651/50, training loss:0.18285606801509857
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1650,170.6051,1.0082

epoch:1652/50, training loss:0.18281713128089905
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1651,170.7080,1.0082

epoch:1653/50, training loss:0.18277615308761597
Train Acc 1.0120
 Acc 1.0082
reddit,dgl,1,1652,170.8114,1.0082

epoch:1654/50, training loss:0.18273688852787018
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1653,170.9152,1.0082

epoch:1655/50, training loss:0.1826968491077423
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1654,171.0188,1.0082

epoch:1656/50, training loss:0.182656928896904
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1655,171.1218,1.0082

epoch:1657/50, training loss:0.18261700868606567
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1656,171.2248,1.0082

epoch:1658/50, training loss:0.18257716298103333
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1657,171.3278,1.0082

epoch:1659/50, training loss:0.182537242770195
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1658,171.4312,1.0082

epoch:1660/50, training loss:0.18249762058258057
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1659,171.5346,1.0082

epoch:1661/50, training loss:0.18245761096477509
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1660,171.6380,1.0082

epoch:1662/50, training loss:0.18241795897483826
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1661,171.7412,1.0082

epoch:1663/50, training loss:0.18237818777561188
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1662,171.8446,1.0082

epoch:1664/50, training loss:0.18233855068683624
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1663,171.9477,1.0082

epoch:1665/50, training loss:0.18229883909225464
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1664,172.0509,1.0082

epoch:1666/50, training loss:0.18225938081741333
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1665,172.1546,1.0082

epoch:1667/50, training loss:0.18222011625766754
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1666,172.2580,1.0082

epoch:1668/50, training loss:0.18217970430850983
Train Acc 1.0121
 Acc 1.0082
reddit,dgl,1,1667,172.3616,1.0082

epoch:1669/50, training loss:0.18214085698127747
Train Acc 1.0122
 Acc 1.0082
reddit,dgl,1,1668,172.4645,1.0082

epoch:1670/50, training loss:0.1821012645959854
Train Acc 1.0122
 Acc 1.0082
reddit,dgl,1,1669,172.5676,1.0082

epoch:1671/50, training loss:0.1820620596408844
Train Acc 1.0122
 Acc 1.0082
reddit,dgl,1,1670,172.6709,1.0082

epoch:1672/50, training loss:0.18202151358127594
Train Acc 1.0122
 Acc 1.0082
reddit,dgl,1,1671,172.7745,1.0082

epoch:1673/50, training loss:0.18198262155056
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1672,172.8776,1.0083

epoch:1674/50, training loss:0.18194344639778137
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1673,172.9806,1.0083

epoch:1675/50, training loss:0.18190331757068634
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1674,173.0834,1.0083

epoch:1676/50, training loss:0.1818644255399704
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1675,173.1869,1.0083

epoch:1677/50, training loss:0.18182507157325745
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1676,173.2901,1.0083

epoch:1678/50, training loss:0.18178553879261017
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1677,173.3934,1.0083

epoch:1679/50, training loss:0.1817464828491211
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1678,173.4964,1.0083

epoch:1680/50, training loss:0.18170708417892456
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1679,173.5994,1.0083

epoch:1681/50, training loss:0.18166758120059967
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1680,173.7025,1.0083

epoch:1682/50, training loss:0.18162867426872253
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1681,173.8057,1.0083

epoch:1683/50, training loss:0.181589275598526
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1682,173.9095,1.0083

epoch:1684/50, training loss:0.18154989182949066
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1683,174.0130,1.0083

epoch:1685/50, training loss:0.18151120841503143
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1684,174.1161,1.0083

epoch:1686/50, training loss:0.18147140741348267
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1685,174.2193,1.0083

epoch:1687/50, training loss:0.18143226206302643
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1686,174.3223,1.0083

epoch:1688/50, training loss:0.18139326572418213
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1687,174.4256,1.0083

epoch:1689/50, training loss:0.18135394155979156
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1688,174.5293,1.0083

epoch:1690/50, training loss:0.18131496012210846
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1689,174.6323,1.0083

epoch:1691/50, training loss:0.18127621710300446
Train Acc 1.0122
 Acc 1.0083
reddit,dgl,1,1690,174.7353,1.0083

epoch:1692/50, training loss:0.18123656511306763
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1691,174.8384,1.0083

epoch:1693/50, training loss:0.18119733035564423
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1692,174.9419,1.0083

epoch:1694/50, training loss:0.18115900456905365
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1693,175.0456,1.0083

epoch:1695/50, training loss:0.18111997842788696
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1694,175.1490,1.0083

epoch:1696/50, training loss:0.1810811161994934
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1695,175.2521,1.0083

epoch:1697/50, training loss:0.18104198575019836
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1696,175.3550,1.0083

epoch:1698/50, training loss:0.18100328743457794
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1697,175.4581,1.0083

epoch:1699/50, training loss:0.180963933467865
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1698,175.5613,1.0083

epoch:1700/50, training loss:0.18092559278011322
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1699,175.6651,1.0083

epoch:1701/50, training loss:0.18088674545288086
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1700,175.7682,1.0083

epoch:1702/50, training loss:0.18084782361984253
Train Acc 1.0123
 Acc 1.0083
reddit,dgl,1,1701,175.8712,1.0083

epoch:1703/50, training loss:0.1808086484670639
Train Acc 1.0123
 Acc 1.0084
reddit,dgl,1,1702,175.9742,1.0084

epoch:1704/50, training loss:0.18077003955841064
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1703,176.0777,1.0083

epoch:1705/50, training loss:0.18073174357414246
Train Acc 1.0124
 Acc 1.0084
reddit,dgl,1,1704,176.1809,1.0084

epoch:1706/50, training loss:0.18069246411323547
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1705,176.2846,1.0083

epoch:1707/50, training loss:0.18065369129180908
Train Acc 1.0124
 Acc 1.0084
reddit,dgl,1,1706,176.3875,1.0084

epoch:1708/50, training loss:0.18061505258083344
Train Acc 1.0124
 Acc 1.0084
reddit,dgl,1,1707,176.4905,1.0084

epoch:1709/50, training loss:0.18057680130004883
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1708,176.5935,1.0083

epoch:1710/50, training loss:0.18053753674030304
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1709,176.6967,1.0083

epoch:1711/50, training loss:0.18049947917461395
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1710,176.8006,1.0083

epoch:1712/50, training loss:0.18046052753925323
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1711,176.9041,1.0083

epoch:1713/50, training loss:0.18042223155498505
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1712,177.0072,1.0083

epoch:1714/50, training loss:0.18038353323936462
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1713,177.1106,1.0083

epoch:1715/50, training loss:0.18034522235393524
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1714,177.2136,1.0083

epoch:1716/50, training loss:0.18030618131160736
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1715,177.3171,1.0083

epoch:1717/50, training loss:0.18026825785636902
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1716,177.4208,1.0083

epoch:1718/50, training loss:0.1802295595407486
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1717,177.5241,1.0083

epoch:1719/50, training loss:0.18019115924835205
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1718,177.6272,1.0083

epoch:1720/50, training loss:0.18015287816524506
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1719,177.7302,1.0083

epoch:1721/50, training loss:0.1801147311925888
Train Acc 1.0124
 Acc 1.0083
reddit,dgl,1,1720,177.8337,1.0083

epoch:1722/50, training loss:0.1800759881734848
Train Acc 1.0124
 Acc 1.0084
reddit,dgl,1,1721,177.9369,1.0084

epoch:1723/50, training loss:0.18003754317760468
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1722,178.0402,1.0084

epoch:1724/50, training loss:0.17999954521656036
Train Acc 1.0125
 Acc 1.0083
reddit,dgl,1,1723,178.1435,1.0083

epoch:1725/50, training loss:0.17996075749397278
Train Acc 1.0125
 Acc 1.0083
reddit,dgl,1,1724,178.2464,1.0083

epoch:1726/50, training loss:0.17992277443408966
Train Acc 1.0124
 Acc 1.0084
reddit,dgl,1,1725,178.3494,1.0084

epoch:1727/50, training loss:0.17988471686840057
Train Acc 1.0125
 Acc 1.0083
reddit,dgl,1,1726,178.4528,1.0083

epoch:1728/50, training loss:0.17984595894813538
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1727,178.5563,1.0084

epoch:1729/50, training loss:0.17980799078941345
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1728,178.6599,1.0084

epoch:1730/50, training loss:0.17976997792720795
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1729,178.7628,1.0084

epoch:1731/50, training loss:0.17973141372203827
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1730,178.8659,1.0084

epoch:1732/50, training loss:0.17969346046447754
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1731,178.9690,1.0084

epoch:1733/50, training loss:0.17965570092201233
Train Acc 1.0125
 Acc 1.0085
reddit,dgl,1,1732,179.0724,1.0085

epoch:1734/50, training loss:0.17961730062961578
Train Acc 1.0125
 Acc 1.0085
reddit,dgl,1,1733,179.1757,1.0085

epoch:1735/50, training loss:0.17957927286624908
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1734,179.2793,1.0084

epoch:1736/50, training loss:0.17954114079475403
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1735,179.3826,1.0084

epoch:1737/50, training loss:0.17950332164764404
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1736,179.4856,1.0084

epoch:1738/50, training loss:0.17946499586105347
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1737,179.5885,1.0084

epoch:1739/50, training loss:0.17942696809768677
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1738,179.6920,1.0084

epoch:1740/50, training loss:0.17938940227031708
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1739,179.7951,1.0084

epoch:1741/50, training loss:0.17935128509998322
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1740,179.8983,1.0084

epoch:1742/50, training loss:0.17931316792964935
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1741,180.0012,1.0084

epoch:1743/50, training loss:0.17927570641040802
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1742,180.1042,1.0084

epoch:1744/50, training loss:0.17923754453659058
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1743,180.2073,1.0084

epoch:1745/50, training loss:0.1791994869709015
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1744,180.3106,1.0084

epoch:1746/50, training loss:0.1791619211435318
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1745,180.4144,1.0084

epoch:1747/50, training loss:0.17912425100803375
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1746,180.5176,1.0084

epoch:1748/50, training loss:0.17908644676208496
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1747,180.6207,1.0084

epoch:1749/50, training loss:0.17904867231845856
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1748,180.7235,1.0084

epoch:1750/50, training loss:0.17901107668876648
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1749,180.8270,1.0084

epoch:1751/50, training loss:0.17897340655326843
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1750,180.9305,1.0084

epoch:1752/50, training loss:0.17893558740615845
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1751,181.0338,1.0084

epoch:1753/50, training loss:0.17889782786369324
Train Acc 1.0126
 Acc 1.0083
reddit,dgl,1,1752,181.1367,1.0083

epoch:1754/50, training loss:0.1788601130247116
Train Acc 1.0125
 Acc 1.0084
reddit,dgl,1,1753,181.2396,1.0084

epoch:1755/50, training loss:0.17882230877876282
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1754,181.3426,1.0084

epoch:1756/50, training loss:0.17878513038158417
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1755,181.4461,1.0084

epoch:1757/50, training loss:0.17874787747859955
Train Acc 1.0126
 Acc 1.0083
reddit,dgl,1,1756,181.5498,1.0083

epoch:1758/50, training loss:0.1787099987268448
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1757,181.6530,1.0084

epoch:1759/50, training loss:0.17867235839366913
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1758,181.7560,1.0084

epoch:1760/50, training loss:0.1786346584558487
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1759,181.8590,1.0084

epoch:1761/50, training loss:0.17859767377376556
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1760,181.9625,1.0084

epoch:1762/50, training loss:0.17856010794639587
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1761,182.0662,1.0084

epoch:1763/50, training loss:0.1785222589969635
Train Acc 1.0127
 Acc 1.0084
reddit,dgl,1,1762,182.1695,1.0084

epoch:1764/50, training loss:0.1784852147102356
Train Acc 1.0126
 Acc 1.0084
reddit,dgl,1,1763,182.2724,1.0084

epoch:1765/50, training loss:0.17844781279563904
Train Acc 1.0126
 Acc 1.0085
reddit,dgl,1,1764,182.3755,1.0085

epoch:1766/50, training loss:0.17841073870658875
Train Acc 1.0127
 Acc 1.0084
reddit,dgl,1,1765,182.4790,1.0084

epoch:1767/50, training loss:0.17837311327457428
Train Acc 1.0127
 Acc 1.0084
reddit,dgl,1,1766,182.5824,1.0084

epoch:1768/50, training loss:0.17833589017391205
Train Acc 1.0127
 Acc 1.0085
new best val f1: 1.0084958854007924
reddit,dgl,1,1767,182.6855,1.0085

epoch:1769/50, training loss:0.17829827964305878
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1768,182.7887,1.0085

epoch:1770/50, training loss:0.17826077342033386
Train Acc 1.0127
 Acc 1.0085
new best val f1: 1.0085149344711979
reddit,dgl,1,1769,182.8917,1.0085

epoch:1771/50, training loss:0.17822353541851044
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1770,182.9947,1.0085

epoch:1772/50, training loss:0.17818643152713776
Train Acc 1.0127
 Acc 1.0085
new best val f1: 1.0085339835416032
reddit,dgl,1,1771,183.0978,1.0085

epoch:1773/50, training loss:0.1781492531299591
Train Acc 1.0127
 Acc 1.0086
new best val f1: 1.0085530326120085
reddit,dgl,1,1772,183.2012,1.0086

epoch:1774/50, training loss:0.17811176180839539
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1773,183.3045,1.0085

epoch:1775/50, training loss:0.17807500064373016
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1774,183.4078,1.0085

epoch:1776/50, training loss:0.17803780734539032
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1775,183.5107,1.0085

epoch:1777/50, training loss:0.17800061404705048
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1776,183.6136,1.0085

epoch:1778/50, training loss:0.17796358466148376
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1777,183.7168,1.0085

epoch:1779/50, training loss:0.17792655527591705
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1778,183.8199,1.0085

epoch:1780/50, training loss:0.17788979411125183
Train Acc 1.0127
 Acc 1.0085
reddit,dgl,1,1779,183.9236,1.0085

epoch:1781/50, training loss:0.17785213887691498
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1780,184.0269,1.0086

epoch:1782/50, training loss:0.17781542241573334
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1781,184.1298,1.0085

epoch:1783/50, training loss:0.1777781844139099
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1782,184.2328,1.0086

epoch:1784/50, training loss:0.1777414083480835
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1783,184.3359,1.0086

epoch:1785/50, training loss:0.17770402133464813
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1784,184.4393,1.0086

epoch:1786/50, training loss:0.17766712605953217
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1785,184.5430,1.0085

epoch:1787/50, training loss:0.17763060331344604
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1786,184.6462,1.0085

epoch:1788/50, training loss:0.17759358882904053
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1787,184.7493,1.0085

epoch:1789/50, training loss:0.17755648493766785
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1788,184.8522,1.0086

epoch:1790/50, training loss:0.17751961946487427
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1789,184.9553,1.0086

epoch:1791/50, training loss:0.17748276889324188
Train Acc 1.0128
 Acc 1.0086
reddit,dgl,1,1790,185.0585,1.0086

epoch:1792/50, training loss:0.17744562029838562
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1791,185.1622,1.0085

epoch:1793/50, training loss:0.1774088740348816
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1792,185.2654,1.0085

epoch:1794/50, training loss:0.1773722916841507
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1793,185.3685,1.0085

epoch:1795/50, training loss:0.17733529210090637
Train Acc 1.0128
 Acc 1.0085
reddit,dgl,1,1794,185.4714,1.0085

epoch:1796/50, training loss:0.17729854583740234
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1795,185.5747,1.0085

epoch:1797/50, training loss:0.17726215720176697
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1796,185.6779,1.0085

epoch:1798/50, training loss:0.17722539603710175
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1797,185.7816,1.0085

epoch:1799/50, training loss:0.17718809843063354
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1798,185.8848,1.0085

epoch:1800/50, training loss:0.17715181410312653
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1799,185.9879,1.0085

epoch:1801/50, training loss:0.17711524665355682
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1800,186.0910,1.0085

epoch:1802/50, training loss:0.1770780086517334
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1801,186.1941,1.0085

epoch:1803/50, training loss:0.1770421266555786
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1802,186.2977,1.0085

epoch:1804/50, training loss:0.17700502276420593
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1803,186.4010,1.0085

epoch:1805/50, training loss:0.17696832120418549
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1804,186.5040,1.0085

epoch:1806/50, training loss:0.17693215608596802
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1805,186.6069,1.0085

epoch:1807/50, training loss:0.17689567804336548
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1806,186.7099,1.0085

epoch:1808/50, training loss:0.17685896158218384
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1807,186.8132,1.0085

epoch:1809/50, training loss:0.17682242393493652
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1808,186.9164,1.0085

epoch:1810/50, training loss:0.17678558826446533
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1809,187.0194,1.0085

epoch:1811/50, training loss:0.17674919962882996
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1810,187.1225,1.0085

epoch:1812/50, training loss:0.17671288549900055
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1811,187.2255,1.0085

epoch:1813/50, training loss:0.17667683959007263
Train Acc 1.0129
 Acc 1.0085
reddit,dgl,1,1812,187.3287,1.0085

epoch:1814/50, training loss:0.17664018273353577
Train Acc 1.0129
 Acc 1.0086
new best val f1: 1.0085911307528193
reddit,dgl,1,1813,187.4324,1.0086

epoch:1815/50, training loss:0.1766032874584198
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1814,187.5356,1.0086

epoch:1816/50, training loss:0.17656712234020233
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1815,187.6387,1.0086

epoch:1817/50, training loss:0.17653116583824158
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1816,187.7416,1.0086

epoch:1818/50, training loss:0.17649409174919128
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1817,187.8451,1.0086

epoch:1819/50, training loss:0.17645849287509918
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1818,187.9488,1.0086

epoch:1820/50, training loss:0.17642199993133545
Train Acc 1.0130
 Acc 1.0086
new best val f1: 1.0086101798232245
reddit,dgl,1,1819,188.0522,1.0086

epoch:1821/50, training loss:0.17638500034809113
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1820,188.1551,1.0086

epoch:1822/50, training loss:0.17634932696819305
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1821,188.2582,1.0086

epoch:1823/50, training loss:0.17631269991397858
Train Acc 1.0130
 Acc 1.0085
reddit,dgl,1,1822,188.3612,1.0085

epoch:1824/50, training loss:0.17627684772014618
Train Acc 1.0130
 Acc 1.0085
reddit,dgl,1,1823,188.4647,1.0085

epoch:1825/50, training loss:0.17624042928218842
Train Acc 1.0130
 Acc 1.0085
reddit,dgl,1,1824,188.5686,1.0085

epoch:1826/50, training loss:0.1762039065361023
Train Acc 1.0130
 Acc 1.0085
reddit,dgl,1,1825,188.6720,1.0085

epoch:1827/50, training loss:0.1761673092842102
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1826,188.7752,1.0086

epoch:1828/50, training loss:0.17613168060779572
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1827,188.8781,1.0086

epoch:1829/50, training loss:0.17609532177448273
Train Acc 1.0130
 Acc 1.0085
reddit,dgl,1,1828,188.9815,1.0085

epoch:1830/50, training loss:0.17605920135974884
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1829,189.0851,1.0086

epoch:1831/50, training loss:0.1760231852531433
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1830,189.1888,1.0086

epoch:1832/50, training loss:0.17598721385002136
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1831,189.2918,1.0086

epoch:1833/50, training loss:0.17595092952251434
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1832,189.3948,1.0086

epoch:1834/50, training loss:0.17591442167758942
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1833,189.4981,1.0086

epoch:1835/50, training loss:0.17587842047214508
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1834,189.6015,1.0086

epoch:1836/50, training loss:0.17584246397018433
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1835,189.7055,1.0086

epoch:1837/50, training loss:0.17580628395080566
Train Acc 1.0130
 Acc 1.0086
reddit,dgl,1,1836,189.8088,1.0086

epoch:1838/50, training loss:0.17577052116394043
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1837,189.9125,1.0086

epoch:1839/50, training loss:0.17573432624340057
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1838,190.0158,1.0086

epoch:1840/50, training loss:0.17569856345653534
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1839,190.1188,1.0086

epoch:1841/50, training loss:0.17566242814064026
Train Acc 1.0131
 Acc 1.0086
new best val f1: 1.0086482779640353
reddit,dgl,1,1840,190.2218,1.0086

epoch:1842/50, training loss:0.1756266951560974
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1841,190.3249,1.0086

epoch:1843/50, training loss:0.17559030652046204
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1842,190.4283,1.0086

epoch:1844/50, training loss:0.1755547821521759
Train Acc 1.0131
 Acc 1.0086
reddit,dgl,1,1843,190.5320,1.0086

epoch:1845/50, training loss:0.1755191832780838
Train Acc 1.0131
 Acc 1.0087
new best val f1: 1.0086863761048461
reddit,dgl,1,1844,190.6354,1.0087

epoch:1846/50, training loss:0.175482839345932
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1845,190.7386,1.0087

epoch:1847/50, training loss:0.17544683814048767
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1846,190.8419,1.0087

epoch:1848/50, training loss:0.17541073262691498
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1847,190.9453,1.0086

epoch:1849/50, training loss:0.17537567019462585
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1848,191.0485,1.0087

epoch:1850/50, training loss:0.1753397285938263
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1849,191.1522,1.0086

epoch:1851/50, training loss:0.1753036379814148
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1850,191.2554,1.0086

epoch:1852/50, training loss:0.17526783049106598
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1851,191.3585,1.0086

epoch:1853/50, training loss:0.1752321869134903
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1852,191.4615,1.0087

epoch:1854/50, training loss:0.1751967966556549
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1853,191.5646,1.0087

epoch:1855/50, training loss:0.17516064643859863
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1854,191.6680,1.0086

epoch:1856/50, training loss:0.17512531578540802
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1855,191.7716,1.0087

epoch:1857/50, training loss:0.17508958280086517
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1856,191.8752,1.0087

epoch:1858/50, training loss:0.17505396902561188
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1857,191.9786,1.0087

epoch:1859/50, training loss:0.1750182956457138
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1858,192.0816,1.0087

epoch:1860/50, training loss:0.1749826818704605
Train Acc 1.0132
 Acc 1.0087
new best val f1: 1.008724474245657
reddit,dgl,1,1859,192.1850,1.0087

epoch:1861/50, training loss:0.17494697868824005
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1860,192.2884,1.0087

epoch:1862/50, training loss:0.17491111159324646
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1861,192.3919,1.0086

epoch:1863/50, training loss:0.17487601935863495
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1862,192.4956,1.0086

epoch:1864/50, training loss:0.1748400628566742
Train Acc 1.0132
 Acc 1.0087
reddit,dgl,1,1863,192.5987,1.0087

epoch:1865/50, training loss:0.1748039871454239
Train Acc 1.0133
 Acc 1.0087
reddit,dgl,1,1864,192.7018,1.0087

epoch:1866/50, training loss:0.17476944625377655
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1865,192.8048,1.0086

epoch:1867/50, training loss:0.17473351955413818
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1866,192.9078,1.0086

epoch:1868/50, training loss:0.17469759285449982
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1867,193.0116,1.0086

epoch:1869/50, training loss:0.17466239631175995
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1868,193.1153,1.0086

epoch:1870/50, training loss:0.17462684214115143
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1869,193.2185,1.0086

epoch:1871/50, training loss:0.17459145188331604
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1870,193.3217,1.0086

epoch:1872/50, training loss:0.17455580830574036
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1871,193.4248,1.0086

epoch:1873/50, training loss:0.17452073097229004
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1872,193.5277,1.0086

epoch:1874/50, training loss:0.17448513209819794
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1873,193.6312,1.0086

epoch:1875/50, training loss:0.17444950342178345
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1874,193.7348,1.0086

epoch:1876/50, training loss:0.1744145154953003
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1875,193.8382,1.0086

epoch:1877/50, training loss:0.17437876760959625
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1876,193.9411,1.0086

epoch:1878/50, training loss:0.17434383928775787
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1877,194.0441,1.0086

epoch:1879/50, training loss:0.17430830001831055
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1878,194.1472,1.0086

epoch:1880/50, training loss:0.17427317798137665
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1879,194.2504,1.0086

epoch:1881/50, training loss:0.17423810064792633
Train Acc 1.0132
 Acc 1.0086
reddit,dgl,1,1880,194.3541,1.0086

epoch:1882/50, training loss:0.17420242726802826
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1881,194.4574,1.0086

epoch:1883/50, training loss:0.17416729032993317
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1882,194.5603,1.0086

epoch:1884/50, training loss:0.17413190007209778
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1883,194.6633,1.0086

epoch:1885/50, training loss:0.1740962564945221
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1884,194.7663,1.0086

epoch:1886/50, training loss:0.17406152188777924
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1885,194.8698,1.0086

epoch:1887/50, training loss:0.17402589321136475
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1886,194.9735,1.0086

epoch:1888/50, training loss:0.1739910989999771
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1887,195.0768,1.0086

epoch:1889/50, training loss:0.17395570874214172
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1888,195.1797,1.0086

epoch:1890/50, training loss:0.17392049729824066
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1889,195.2828,1.0086

epoch:1891/50, training loss:0.17388500273227692
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1890,195.3858,1.0086

epoch:1892/50, training loss:0.17385008931159973
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1891,195.4889,1.0086

epoch:1893/50, training loss:0.17381493747234344
Train Acc 1.0133
 Acc 1.0085
reddit,dgl,1,1892,195.5921,1.0085

epoch:1894/50, training loss:0.1737794280052185
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1893,195.6958,1.0086

epoch:1895/50, training loss:0.1737450361251831
Train Acc 1.0133
 Acc 1.0085
reddit,dgl,1,1894,195.7987,1.0085

epoch:1896/50, training loss:0.17370977997779846
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1895,195.9018,1.0086

epoch:1897/50, training loss:0.17367425560951233
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1896,196.0049,1.0086

epoch:1898/50, training loss:0.17363899946212769
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1897,196.1083,1.0086

epoch:1899/50, training loss:0.1736045777797699
Train Acc 1.0133
 Acc 1.0086
reddit,dgl,1,1898,196.2120,1.0086

epoch:1900/50, training loss:0.17356908321380615
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1899,196.3152,1.0086

epoch:1901/50, training loss:0.17353413999080658
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1900,196.4181,1.0086

epoch:1902/50, training loss:0.17349909245967865
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1901,196.5212,1.0086

epoch:1903/50, training loss:0.17346397042274475
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1902,196.6242,1.0085

epoch:1904/50, training loss:0.17342878878116608
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1903,196.7275,1.0085

epoch:1905/50, training loss:0.1733945608139038
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1904,196.8311,1.0086

epoch:1906/50, training loss:0.1733594834804535
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1905,196.9345,1.0085

epoch:1907/50, training loss:0.17332370579242706
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1906,197.0374,1.0085

epoch:1908/50, training loss:0.1732892543077469
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1907,197.1405,1.0085

epoch:1909/50, training loss:0.17325450479984283
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1908,197.2433,1.0085

epoch:1910/50, training loss:0.1732192039489746
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1909,197.3464,1.0085

epoch:1911/50, training loss:0.17318421602249146
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1910,197.4494,1.0085

epoch:1912/50, training loss:0.17314910888671875
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1911,197.5526,1.0085

epoch:1913/50, training loss:0.17311449348926544
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1912,197.6562,1.0085

epoch:1914/50, training loss:0.17307958006858826
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1913,197.7594,1.0085

epoch:1915/50, training loss:0.17304471135139465
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1914,197.8623,1.0085

epoch:1916/50, training loss:0.1730102151632309
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1915,197.9653,1.0085

epoch:1917/50, training loss:0.17297515273094177
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1916,198.0684,1.0085

epoch:1918/50, training loss:0.17294034361839294
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1917,198.1719,1.0085

epoch:1919/50, training loss:0.17290547490119934
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1918,198.2755,1.0086

epoch:1920/50, training loss:0.17287050187587738
Train Acc 1.0134
 Acc 1.0085
reddit,dgl,1,1919,198.3788,1.0085

epoch:1921/50, training loss:0.1728357970714569
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1920,198.4821,1.0086

epoch:1922/50, training loss:0.17280149459838867
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1921,198.5850,1.0086

epoch:1923/50, training loss:0.17276622354984283
Train Acc 1.0134
 Acc 1.0086
reddit,dgl,1,1922,198.6881,1.0086

epoch:1924/50, training loss:0.17273110151290894
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1923,198.7915,1.0086

epoch:1925/50, training loss:0.1726963371038437
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1924,198.8952,1.0086

epoch:1926/50, training loss:0.17266196012496948
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1925,198.9984,1.0086

epoch:1927/50, training loss:0.172627255320549
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1926,199.1014,1.0086

epoch:1928/50, training loss:0.1725921779870987
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1927,199.2044,1.0086

epoch:1929/50, training loss:0.1725575029850006
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1928,199.3074,1.0086

epoch:1930/50, training loss:0.17252320051193237
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1929,199.4106,1.0086

epoch:1931/50, training loss:0.1724880486726761
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1930,199.5143,1.0086

epoch:1932/50, training loss:0.1724536418914795
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1931,199.6175,1.0086

epoch:1933/50, training loss:0.17241904139518738
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1932,199.7204,1.0086

epoch:1934/50, training loss:0.17238444089889526
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1933,199.8234,1.0086

epoch:1935/50, training loss:0.17234985530376434
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1934,199.9264,1.0086

epoch:1936/50, training loss:0.1723150908946991
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1935,200.0299,1.0086

epoch:1937/50, training loss:0.17228072881698608
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1936,200.1335,1.0086

epoch:1938/50, training loss:0.17224650084972382
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1937,200.2367,1.0086

epoch:1939/50, training loss:0.1722116619348526
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1938,200.3397,1.0086

epoch:1940/50, training loss:0.17217722535133362
Train Acc 1.0135
 Acc 1.0085
reddit,dgl,1,1939,200.4428,1.0085

epoch:1941/50, training loss:0.17214235663414001
Train Acc 1.0135
 Acc 1.0085
reddit,dgl,1,1940,200.5459,1.0085

epoch:1942/50, training loss:0.1721077710390091
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1941,200.6494,1.0086

epoch:1943/50, training loss:0.1720733642578125
Train Acc 1.0135
 Acc 1.0085
reddit,dgl,1,1942,200.7531,1.0085

epoch:1944/50, training loss:0.17203910648822784
Train Acc 1.0135
 Acc 1.0086
reddit,dgl,1,1943,200.8563,1.0086

epoch:1945/50, training loss:0.17200487852096558
Train Acc 1.0135
 Acc 1.0085
reddit,dgl,1,1944,200.9592,1.0085

epoch:1946/50, training loss:0.1719702035188675
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1945,201.0622,1.0085

epoch:1947/50, training loss:0.1719355434179306
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1946,201.1652,1.0086

epoch:1948/50, training loss:0.17190107703208923
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1947,201.2688,1.0086

epoch:1949/50, training loss:0.1718662828207016
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1948,201.3724,1.0086

epoch:1950/50, training loss:0.17183232307434082
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1949,201.4757,1.0085

epoch:1951/50, training loss:0.17179825901985168
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1950,201.5786,1.0086

epoch:1952/50, training loss:0.17176327109336853
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1951,201.6818,1.0086

epoch:1953/50, training loss:0.17172899842262268
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1952,201.7847,1.0086

epoch:1954/50, training loss:0.17169445753097534
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1953,201.8882,1.0085

epoch:1955/50, training loss:0.17165987193584442
Train Acc 1.0136
 Acc 1.0086
reddit,dgl,1,1954,201.9914,1.0086

epoch:1956/50, training loss:0.17162542045116425
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1955,202.0951,1.0085

epoch:1957/50, training loss:0.1715914011001587
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1956,202.1980,1.0085

epoch:1958/50, training loss:0.17155708372592926
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1957,202.3011,1.0085

epoch:1959/50, training loss:0.17152242362499237
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1958,202.4040,1.0085

epoch:1960/50, training loss:0.171488419175148
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1959,202.5075,1.0085

epoch:1961/50, training loss:0.17145447432994843
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1960,202.6109,1.0085

epoch:1962/50, training loss:0.17141935229301453
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1961,202.7148,1.0085

epoch:1963/50, training loss:0.17138564586639404
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1962,202.8183,1.0085

epoch:1964/50, training loss:0.1713511049747467
Train Acc 1.0136
 Acc 1.0085
reddit,dgl,1,1963,202.9217,1.0085

epoch:1965/50, training loss:0.1713167130947113
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1964,203.0247,1.0085

epoch:1966/50, training loss:0.17128236591815948
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1965,203.1278,1.0085

epoch:1967/50, training loss:0.1712484210729599
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1966,203.2312,1.0085

epoch:1968/50, training loss:0.17121443152427673
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1967,203.3349,1.0085

epoch:1969/50, training loss:0.17117981612682343
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1968,203.4385,1.0085

epoch:1970/50, training loss:0.17114540934562683
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1969,203.5415,1.0085

epoch:1971/50, training loss:0.17111146450042725
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1970,203.6444,1.0085

epoch:1972/50, training loss:0.17107699811458588
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1971,203.7473,1.0085

epoch:1973/50, training loss:0.17104318737983704
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1972,203.8508,1.0085

epoch:1974/50, training loss:0.17100869119167328
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1973,203.9540,1.0085

epoch:1975/50, training loss:0.17097458243370056
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1974,204.0572,1.0085

epoch:1976/50, training loss:0.17094066739082336
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1975,204.1601,1.0085

epoch:1977/50, training loss:0.1709059625864029
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1976,204.2631,1.0085

epoch:1978/50, training loss:0.17087271809577942
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1977,204.3661,1.0085

epoch:1979/50, training loss:0.17083778977394104
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1978,204.4695,1.0085

epoch:1980/50, training loss:0.1708042025566101
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1979,204.5728,1.0085

epoch:1981/50, training loss:0.1707698553800583
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1980,204.6764,1.0085

epoch:1982/50, training loss:0.1707359105348587
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1981,204.7797,1.0085

epoch:1983/50, training loss:0.17070232331752777
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1982,204.8827,1.0085

epoch:1984/50, training loss:0.17066754400730133
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1983,204.9856,1.0085

epoch:1985/50, training loss:0.17063379287719727
Train Acc 1.0137
 Acc 1.0085
reddit,dgl,1,1984,205.0886,1.0085

epoch:1986/50, training loss:0.17059968411922455
Train Acc 1.0138
 Acc 1.0086
reddit,dgl,1,1985,205.1921,1.0086

epoch:1987/50, training loss:0.17056578397750854
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1986,205.2956,1.0085

epoch:1988/50, training loss:0.1705312281847
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1987,205.3989,1.0085

epoch:1989/50, training loss:0.17049744725227356
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1988,205.5018,1.0085

epoch:1990/50, training loss:0.17046324908733368
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1989,205.6049,1.0085

epoch:1991/50, training loss:0.17042940855026245
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1990,205.7080,1.0085

epoch:1992/50, training loss:0.17039529979228973
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1991,205.8114,1.0085

epoch:1993/50, training loss:0.17036141455173492
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1992,205.9149,1.0085

epoch:1994/50, training loss:0.170327290892601
Train Acc 1.0138
 Acc 1.0086
reddit,dgl,1,1993,206.0188,1.0086

epoch:1995/50, training loss:0.1702936589717865
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1994,206.1223,1.0085

epoch:1996/50, training loss:0.17025959491729736
Train Acc 1.0138
 Acc 1.0086
reddit,dgl,1,1995,206.2255,1.0086

epoch:1997/50, training loss:0.1702253222465515
Train Acc 1.0138
 Acc 1.0085
reddit,dgl,1,1996,206.3288,1.0085

epoch:1998/50, training loss:0.17019157111644745
Train Acc 1.0138
 Acc 1.0086
reddit,dgl,1,1997,206.4318,1.0086

epoch:1999/50, training loss:0.17015761137008667
Train Acc 1.0139
 Acc 1.0086
reddit,dgl,1,1998,206.5352,1.0086

epoch:2000/50, training loss:0.17012369632720947
Train Acc 1.0138
 Acc 1.0086
reddit,dgl,1,1999,206.6387,1.0086

training using time 398.40216994285583
Traceback (most recent call last):
  File "dgl/train_full_load.py", line 340, in <module>
    main(args)
  File "dgl/train_full_load.py", line 317, in main
    model, g, g.ndata['label'], test_mask, False)
TypeError: cannot unpack non-iterable float object
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2121965) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2121965 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     dgl/train_full_load.py FAILED     
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:12:02
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2121965)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

Namespace(csv='full_new.csv', dataset='meta', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
Inited proc group
Max label: tensor(24.)
----Data statistics------'
    #Nodes 500036
    #Edges 53859150
    #Classes/Labels (multi binary labels) 25
    #Train samples 166678
    #Val samples 0
    #Test samples 166680
Running on: 0
GCN(
  (layers): ModuleList(
    (0): GraphConv(in=64, out=128, normalization=both, activation=<function relu at 0x15034b788f80>)
    (1): GraphConv(in=128, out=25, normalization=both, activation=None)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
Namespace(csv='full_new.csv', dataset='meta', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
epoch:1/50, training loss:3.211090087890625
Train Acc 0.0870
 Acc 0.2002
new best val f1: 0.20022128264906944
meta,dgl,1,0,0.3171,0.2002

epoch:2/50, training loss:3.1976208686828613
Train Acc 0.2006
 Acc 0.2017
new best val f1: 0.20174260086142173
meta,dgl,1,1,0.3511,0.2017

epoch:3/50, training loss:3.1841742992401123
Train Acc 0.2023
 Acc 0.2018
new best val f1: 0.2018084587926924
meta,dgl,1,2,0.3849,0.2018

epoch:4/50, training loss:3.170836925506592
Train Acc 0.2024
 Acc 0.2020
new best val f1: 0.20195334624148786
meta,dgl,1,3,0.4185,0.2020

epoch:5/50, training loss:3.157810688018799
Train Acc 0.2025
 Acc 0.2021
new best val f1: 0.20208506210402918
meta,dgl,1,4,0.4533,0.2021

epoch:6/50, training loss:3.145073175430298
Train Acc 0.2026
 Acc 0.2021
new best val f1: 0.20211799106966452
meta,dgl,1,5,0.4881,0.2021

epoch:7/50, training loss:3.1325247287750244
Train Acc 0.2026
 Acc 0.2022
new best val f1: 0.20217067741468106
meta,dgl,1,6,0.5220,0.2022

epoch:8/50, training loss:3.12011981010437
Train Acc 0.2027
 Acc 0.2022
new best val f1: 0.2021970205871893
meta,dgl,1,7,0.5557,0.2022

epoch:9/50, training loss:3.107762336730957
Train Acc 0.2027
 Acc 0.2022
meta,dgl,1,8,0.5894,0.2022

epoch:10/50, training loss:3.09548020362854
Train Acc 0.2026
 Acc 0.2022
meta,dgl,1,9,0.6231,0.2022

epoch:11/50, training loss:3.0832266807556152
Train Acc 0.2026
 Acc 0.2022
meta,dgl,1,10,0.6567,0.2022

epoch:12/50, training loss:3.0709989070892334
Train Acc 0.2026
 Acc 0.2022
meta,dgl,1,11,0.6914,0.2022

epoch:13/50, training loss:3.0587193965911865
Train Acc 0.2026
 Acc 0.2022
meta,dgl,1,12,0.7250,0.2022

epoch:14/50, training loss:3.046358346939087
Train Acc 0.2026
 Acc 0.2021
meta,dgl,1,13,0.7586,0.2021

epoch:15/50, training loss:3.0339195728302
Train Acc 0.2026
 Acc 0.2021
meta,dgl,1,14,0.7921,0.2021

epoch:16/50, training loss:3.021279811859131
Train Acc 0.2026
 Acc 0.2022
new best val f1: 0.20220360638031637
meta,dgl,1,15,0.8258,0.2022

epoch:17/50, training loss:3.008434534072876
Train Acc 0.2027
 Acc 0.2024
new best val f1: 0.20244069493289077
meta,dgl,1,16,0.8593,0.2024

epoch:18/50, training loss:2.9953808784484863
Train Acc 0.2029
 Acc 0.2027
new best val f1: 0.2026711976923381
meta,dgl,1,17,0.8929,0.2027

epoch:19/50, training loss:2.982189178466797
Train Acc 0.2031
 Acc 0.2032
new best val f1: 0.20320464693563045
meta,dgl,1,18,0.9263,0.2032

epoch:20/50, training loss:2.9689245223999023
Train Acc 0.2037
 Acc 0.2038
new best val f1: 0.20384346886895588
meta,dgl,1,19,0.9599,0.2038

epoch:21/50, training loss:2.9555823802948
Train Acc 0.2044
 Acc 0.2046
new best val f1: 0.20458107769918732
meta,dgl,1,20,0.9934,0.2046

epoch:22/50, training loss:2.942159414291382
Train Acc 0.2050
 Acc 0.2057
new best val f1: 0.2057204199101698
meta,dgl,1,21,1.0280,0.2057

epoch:23/50, training loss:2.928673028945923
Train Acc 0.2063
 Acc 0.2080
new best val f1: 0.20801227591838886
meta,dgl,1,22,1.0626,0.2080

epoch:24/50, training loss:2.9151318073272705
Train Acc 0.2088
 Acc 0.2126
new best val f1: 0.21260915952108111
meta,dgl,1,23,1.0963,0.2126

epoch:25/50, training loss:2.9015440940856934
Train Acc 0.2133
 Acc 0.2184
new best val f1: 0.21842441485228067
meta,dgl,1,24,1.1299,0.2184

epoch:26/50, training loss:2.8879270553588867
Train Acc 0.2191
 Acc 0.2283
new best val f1: 0.22827017557724477
meta,dgl,1,25,1.1635,0.2283

epoch:27/50, training loss:2.874284267425537
Train Acc 0.2290
 Acc 0.2439
new best val f1: 0.2438785052883919
meta,dgl,1,26,1.1972,0.2439

epoch:28/50, training loss:2.860635995864868
Train Acc 0.2446
 Acc 0.2669
new best val f1: 0.26688268068123444
meta,dgl,1,27,1.2307,0.2669

epoch:29/50, training loss:2.8470232486724854
Train Acc 0.2671
 Acc 0.2858
new best val f1: 0.2857904927490418
meta,dgl,1,28,1.2652,0.2858

epoch:30/50, training loss:2.833451271057129
Train Acc 0.2862
 Acc 0.3021
new best val f1: 0.30207057335914966
meta,dgl,1,29,1.2988,0.3021

epoch:31/50, training loss:2.8199667930603027
Train Acc 0.3026
 Acc 0.3078
new best val f1: 0.307767284414062
meta,dgl,1,30,1.3325,0.3078

epoch:32/50, training loss:2.8065855503082275
Train Acc 0.3081
 Acc 0.3057
meta,dgl,1,31,1.3662,0.3057

epoch:33/50, training loss:2.7933199405670166
Train Acc 0.3057
 Acc 0.3065
meta,dgl,1,32,1.4006,0.3065

epoch:34/50, training loss:2.7801995277404785
Train Acc 0.3057
 Acc 0.3087
new best val f1: 0.30866953807247005
meta,dgl,1,33,1.4343,0.3087

epoch:35/50, training loss:2.767225742340088
Train Acc 0.3078
 Acc 0.3135
new best val f1: 0.31351009602086377
meta,dgl,1,34,1.4678,0.3135

epoch:36/50, training loss:2.7544028759002686
Train Acc 0.3125
 Acc 0.3193
new best val f1: 0.3193121797658092
meta,dgl,1,35,1.5015,0.3193

epoch:37/50, training loss:2.7417685985565186
Train Acc 0.3182
 Acc 0.3267
new best val f1: 0.3267343686200129
meta,dgl,1,36,1.5351,0.3267

epoch:38/50, training loss:2.7293131351470947
Train Acc 0.3257
 Acc 0.3347
new best val f1: 0.3347097640968902
meta,dgl,1,37,1.5686,0.3347

epoch:39/50, training loss:2.7170510292053223
Train Acc 0.3337
 Acc 0.3424
new best val f1: 0.34236245571054125
meta,dgl,1,38,1.6021,0.3424

epoch:40/50, training loss:2.7049901485443115
Train Acc 0.3414
 Acc 0.3493
new best val f1: 0.34934998221835856
meta,dgl,1,39,1.6358,0.3493

epoch:41/50, training loss:2.693136692047119
Train Acc 0.3484
 Acc 0.3569
new best val f1: 0.3569104727282307
meta,dgl,1,40,1.6693,0.3569

epoch:42/50, training loss:2.6814818382263184
Train Acc 0.3558
 Acc 0.3641
new best val f1: 0.3641482593748765
meta,dgl,1,41,1.7037,0.3641

epoch:43/50, training loss:2.670037031173706
Train Acc 0.3627
 Acc 0.3699
new best val f1: 0.3699174141541866
meta,dgl,1,42,1.7373,0.3699

epoch:44/50, training loss:2.658798933029175
Train Acc 0.3685
 Acc 0.3756
new best val f1: 0.37556802465720945
meta,dgl,1,43,1.7708,0.3756

epoch:45/50, training loss:2.6477818489074707
Train Acc 0.3742
 Acc 0.3789
new best val f1: 0.3789465365313945
meta,dgl,1,44,1.8044,0.3789

epoch:46/50, training loss:2.636979579925537
Train Acc 0.3777
 Acc 0.3814
new best val f1: 0.3814227947471714
meta,dgl,1,45,1.8379,0.3814

epoch:47/50, training loss:2.6263790130615234
Train Acc 0.3802
 Acc 0.3830
new best val f1: 0.3829506987526508
meta,dgl,1,46,1.8715,0.3830

epoch:48/50, training loss:2.6159727573394775
Train Acc 0.3818
 Acc 0.3838
new best val f1: 0.3838134376522965
meta,dgl,1,47,1.9050,0.3838

epoch:49/50, training loss:2.605756998062134
Train Acc 0.3826
 Acc 0.3842
new best val f1: 0.3841822420674122
meta,dgl,1,48,1.9394,0.3842

epoch:50/50, training loss:2.595720052719116
Train Acc 0.3830
 Acc 0.3842
new best val f1: 0.38419541365366633
meta,dgl,1,49,1.9730,0.3842

epoch:51/50, training loss:2.5858635902404785
Train Acc 0.3830
 Acc 0.3842
meta,dgl,1,50,2.0066,0.3842

epoch:52/50, training loss:2.5761797428131104
Train Acc 0.3830
 Acc 0.3842
meta,dgl,1,51,2.0401,0.3842

epoch:53/50, training loss:2.566657781600952
Train Acc 0.3830
 Acc 0.3842
meta,dgl,1,52,2.0737,0.3842

epoch:54/50, training loss:2.5573019981384277
Train Acc 0.3830
 Acc 0.3841
meta,dgl,1,53,2.1073,0.3841

epoch:55/50, training loss:2.5480854511260986
Train Acc 0.3829
 Acc 0.3840
meta,dgl,1,54,2.1409,0.3840

epoch:56/50, training loss:2.5390217304229736
Train Acc 0.3828
 Acc 0.3839
meta,dgl,1,55,2.1756,0.3839

epoch:57/50, training loss:2.5300891399383545
Train Acc 0.3827
 Acc 0.3838
meta,dgl,1,56,2.2094,0.3838

epoch:58/50, training loss:2.5212783813476562
Train Acc 0.3826
 Acc 0.3837
meta,dgl,1,57,2.2439,0.3837

epoch:59/50, training loss:2.5125927925109863
Train Acc 0.3825
 Acc 0.3837
meta,dgl,1,58,2.2783,0.3837

epoch:60/50, training loss:2.504023551940918
Train Acc 0.3825
 Acc 0.3836
meta,dgl,1,59,2.3119,0.3836

epoch:61/50, training loss:2.4955646991729736
Train Acc 0.3824
 Acc 0.3835
meta,dgl,1,60,2.3463,0.3835

epoch:62/50, training loss:2.487212657928467
Train Acc 0.3823
 Acc 0.3835
meta,dgl,1,61,2.3798,0.3835

epoch:63/50, training loss:2.478968620300293
Train Acc 0.3823
 Acc 0.3834
meta,dgl,1,62,2.4134,0.3834

epoch:64/50, training loss:2.4708237648010254
Train Acc 0.3822
 Acc 0.3833
meta,dgl,1,63,2.4471,0.3833

epoch:65/50, training loss:2.4627835750579834
Train Acc 0.3822
 Acc 0.3833
meta,dgl,1,64,2.4807,0.3833

epoch:66/50, training loss:2.4548370838165283
Train Acc 0.3821
 Acc 0.3834
meta,dgl,1,65,2.5143,0.3834

epoch:67/50, training loss:2.4469897747039795
Train Acc 0.3822
 Acc 0.3864
new best val f1: 0.386447754903123
meta,dgl,1,66,2.5479,0.3864

epoch:68/50, training loss:2.4392318725585938
Train Acc 0.3854
 Acc 0.3927
new best val f1: 0.39271742996009007
meta,dgl,1,67,2.5825,0.3927

epoch:69/50, training loss:2.431572198867798
Train Acc 0.3919
 Acc 0.3995
new best val f1: 0.39954031163973075
meta,dgl,1,68,2.6161,0.3995

epoch:70/50, training loss:2.4240036010742188
Train Acc 0.3984
 Acc 0.4065
new best val f1: 0.40654100973380225
meta,dgl,1,69,2.6507,0.4065

epoch:71/50, training loss:2.4165241718292236
Train Acc 0.4052
 Acc 0.4138
new best val f1: 0.41381831113921047
meta,dgl,1,70,2.6843,0.4138

epoch:72/50, training loss:2.4091286659240723
Train Acc 0.4125
 Acc 0.4206
new best val f1: 0.4206477786119782
meta,dgl,1,71,2.7179,0.4206

epoch:73/50, training loss:2.4018235206604004
Train Acc 0.4192
 Acc 0.4282
new best val f1: 0.4281555827768338
meta,dgl,1,72,2.7514,0.4282

epoch:74/50, training loss:2.394594192504883
Train Acc 0.4268
 Acc 0.4349
new best val f1: 0.43491919231833087
meta,dgl,1,73,2.7851,0.4349

epoch:75/50, training loss:2.387446880340576
Train Acc 0.4336
 Acc 0.4408
new best val f1: 0.44082664875330935
meta,dgl,1,74,2.8186,0.4408

epoch:76/50, training loss:2.3803768157958984
Train Acc 0.4395
 Acc 0.4453
new best val f1: 0.44529840228658735
meta,dgl,1,75,2.8533,0.4453

epoch:77/50, training loss:2.3733863830566406
Train Acc 0.4438
 Acc 0.4495
new best val f1: 0.4494935525085286
meta,dgl,1,76,2.8878,0.4495

epoch:78/50, training loss:2.3664710521698
Train Acc 0.4479
 Acc 0.4527
new best val f1: 0.45269424796828284
meta,dgl,1,77,2.9213,0.4527

epoch:79/50, training loss:2.3596277236938477
Train Acc 0.4512
 Acc 0.4553
new best val f1: 0.45526929308096575
meta,dgl,1,78,2.9548,0.4553

epoch:80/50, training loss:2.352861166000366
Train Acc 0.4538
 Acc 0.4569
new best val f1: 0.456882812397097
meta,dgl,1,79,2.9884,0.4569

epoch:81/50, training loss:2.346163272857666
Train Acc 0.4556
 Acc 0.4583
new best val f1: 0.458272414746908
meta,dgl,1,80,3.0219,0.4583

epoch:82/50, training loss:2.339531421661377
Train Acc 0.4571
 Acc 0.4594
new best val f1: 0.4594446859235258
meta,dgl,1,81,3.0554,0.4594

epoch:83/50, training loss:2.3329782485961914
Train Acc 0.4583
 Acc 0.4605
new best val f1: 0.46046548385822106
meta,dgl,1,82,3.0898,0.4605

epoch:84/50, training loss:2.3264920711517334
Train Acc 0.4596
 Acc 0.4617
new best val f1: 0.4617101987592366
meta,dgl,1,83,3.1234,0.4617

epoch:85/50, training loss:2.3200719356536865
Train Acc 0.4607
 Acc 0.4628
new best val f1: 0.4627639256595672
meta,dgl,1,84,3.1580,0.4628

epoch:86/50, training loss:2.3137218952178955
Train Acc 0.4617
 Acc 0.4640
new best val f1: 0.4639691258018203
meta,dgl,1,85,3.1917,0.4640

epoch:87/50, training loss:2.3074395656585693
Train Acc 0.4629
 Acc 0.4646
new best val f1: 0.4646211193213999
meta,dgl,1,86,3.2255,0.4646

epoch:88/50, training loss:2.3012208938598633
Train Acc 0.4636
 Acc 0.4656
new best val f1: 0.46558264511795155
meta,dgl,1,87,3.2593,0.4656

epoch:89/50, training loss:2.2950685024261475
Train Acc 0.4646
 Acc 0.4662
new best val f1: 0.4661753664993875
meta,dgl,1,88,3.2939,0.4662

epoch:90/50, training loss:2.28898286819458
Train Acc 0.4652
 Acc 0.4671
new best val f1: 0.4671105491234309
meta,dgl,1,89,3.3275,0.4671

epoch:91/50, training loss:2.282963275909424
Train Acc 0.4661
 Acc 0.4678
new best val f1: 0.46776912843613755
meta,dgl,1,90,3.3611,0.4678

epoch:92/50, training loss:2.277001142501831
Train Acc 0.4669
 Acc 0.4689
new best val f1: 0.4688952990608659
meta,dgl,1,91,3.3957,0.4689

epoch:93/50, training loss:2.2711055278778076
Train Acc 0.4678
 Acc 0.4699
new best val f1: 0.46990292540930706
meta,dgl,1,92,3.4303,0.4699

epoch:94/50, training loss:2.265265703201294
Train Acc 0.4687
 Acc 0.4709
new best val f1: 0.4709171375508753
meta,dgl,1,93,3.4639,0.4709

epoch:95/50, training loss:2.2594876289367676
Train Acc 0.4696
 Acc 0.4717
new best val f1: 0.4716679179673608
meta,dgl,1,94,3.4975,0.4717

epoch:96/50, training loss:2.2537624835968018
Train Acc 0.4705
 Acc 0.4724
new best val f1: 0.47243845576322757
meta,dgl,1,95,3.5311,0.4724

epoch:97/50, training loss:2.2480883598327637
Train Acc 0.4712
 Acc 0.4730
new best val f1: 0.4730377629377906
meta,dgl,1,96,3.5657,0.4730

epoch:98/50, training loss:2.242469310760498
Train Acc 0.4717
 Acc 0.4736
new best val f1: 0.47355145480170174
meta,dgl,1,97,3.5994,0.4736

epoch:99/50, training loss:2.2369070053100586
Train Acc 0.4722
 Acc 0.4741
new best val f1: 0.4740980756312483
meta,dgl,1,98,3.6339,0.4741

epoch:100/50, training loss:2.2313947677612305
Train Acc 0.4727
 Acc 0.4744
new best val f1: 0.47437467894258506
meta,dgl,1,99,3.6675,0.4744

epoch:101/50, training loss:2.2259366512298584
Train Acc 0.4731
 Acc 0.4748
new best val f1: 0.47475006915082785
meta,dgl,1,100,3.7012,0.4748

epoch:102/50, training loss:2.220527410507202
Train Acc 0.4735
 Acc 0.4751
new best val f1: 0.47513863094532477
meta,dgl,1,101,3.7356,0.4751

epoch:103/50, training loss:2.215169668197632
Train Acc 0.4738
 Acc 0.4756
new best val f1: 0.47560622225734644
meta,dgl,1,102,3.7692,0.4756

epoch:104/50, training loss:2.2098605632781982
Train Acc 0.4742
 Acc 0.4760
new best val f1: 0.47596185508620803
meta,dgl,1,103,3.8036,0.4760

epoch:105/50, training loss:2.2046027183532715
Train Acc 0.4745
 Acc 0.4764
new best val f1: 0.4763965174325944
meta,dgl,1,104,3.8372,0.4764

epoch:106/50, training loss:2.199397563934326
Train Acc 0.4749
 Acc 0.4768
new best val f1: 0.4768443513652349
meta,dgl,1,105,3.8715,0.4768

epoch:107/50, training loss:2.194241523742676
Train Acc 0.4753
 Acc 0.4771
new best val f1: 0.4771077830903176
meta,dgl,1,106,3.9050,0.4771

epoch:108/50, training loss:2.1891355514526367
Train Acc 0.4756
 Acc 0.4774
new best val f1: 0.47743707274667085
meta,dgl,1,107,3.9386,0.4774

epoch:109/50, training loss:2.1840803623199463
Train Acc 0.4759
 Acc 0.4776
new best val f1: 0.477647818126737
meta,dgl,1,108,3.9722,0.4776

epoch:110/50, training loss:2.1790730953216553
Train Acc 0.4762
 Acc 0.4778
new best val f1: 0.47776636240302417
meta,dgl,1,109,4.0066,0.4778

epoch:111/50, training loss:2.1741178035736084
Train Acc 0.4763
 Acc 0.4779
new best val f1: 0.47793100723120086
meta,dgl,1,110,4.0402,0.4779

epoch:112/50, training loss:2.16921067237854
Train Acc 0.4765
 Acc 0.4782
new best val f1: 0.4781680957837752
meta,dgl,1,111,4.0739,0.4782

epoch:113/50, training loss:2.1643590927124023
Train Acc 0.4767
 Acc 0.4783
new best val f1: 0.4782932258531895
meta,dgl,1,112,4.1074,0.4783

epoch:114/50, training loss:2.159550905227661
Train Acc 0.4768
 Acc 0.4783
new best val f1: 0.47834591219820605
meta,dgl,1,113,4.1410,0.4783

epoch:115/50, training loss:2.1547951698303223
Train Acc 0.4769
 Acc 0.4785
new best val f1: 0.4785237286126368
meta,dgl,1,114,4.1744,0.4785

epoch:116/50, training loss:2.1500813961029053
Train Acc 0.4771
 Acc 0.4786
new best val f1: 0.4785632433713992
meta,dgl,1,115,4.2080,0.4786

epoch:117/50, training loss:2.1454200744628906
Train Acc 0.4771
 Acc 0.4786
new best val f1: 0.4785895865439075
meta,dgl,1,116,4.2415,0.4786

epoch:118/50, training loss:2.140807628631592
Train Acc 0.4772
 Acc 0.4788
new best val f1: 0.47882667509648186
meta,dgl,1,117,4.2760,0.4788

epoch:119/50, training loss:2.1362407207489014
Train Acc 0.4774
 Acc 0.4789
new best val f1: 0.47888594723462546
meta,dgl,1,118,4.3095,0.4789

epoch:120/50, training loss:2.131718873977661
Train Acc 0.4774
 Acc 0.4789
meta,dgl,1,119,4.3431,0.4789

epoch:121/50, training loss:2.1272430419921875
Train Acc 0.4775
 Acc 0.4789
new best val f1: 0.47891229040713373
meta,dgl,1,120,4.3767,0.4789

epoch:122/50, training loss:2.122816562652588
Train Acc 0.4776
 Acc 0.4789
meta,dgl,1,121,4.4103,0.4789

epoch:123/50, training loss:2.118427038192749
Train Acc 0.4776
 Acc 0.4790
new best val f1: 0.4790242488902939
meta,dgl,1,122,4.4440,0.4790

epoch:124/50, training loss:2.1140849590301514
Train Acc 0.4777
 Acc 0.4791
new best val f1: 0.47914279316658104
meta,dgl,1,123,4.4785,0.4791

epoch:125/50, training loss:2.1097841262817383
Train Acc 0.4779
 Acc 0.4793
new best val f1: 0.4793140237878848
meta,dgl,1,124,4.5120,0.4793

epoch:126/50, training loss:2.1055245399475098
Train Acc 0.4779
 Acc 0.4793
meta,dgl,1,125,4.5457,0.4793

epoch:127/50, training loss:2.1013078689575195
Train Acc 0.4780
 Acc 0.4794
new best val f1: 0.4793535385466472
meta,dgl,1,126,4.5793,0.4794

epoch:128/50, training loss:2.0971360206604004
Train Acc 0.4780
 Acc 0.4794
new best val f1: 0.47937988171915547
meta,dgl,1,127,4.6132,0.4794

epoch:129/50, training loss:2.0929956436157227
Train Acc 0.4781
 Acc 0.4794
new best val f1: 0.4794062248916637
meta,dgl,1,128,4.6468,0.4794

epoch:130/50, training loss:2.088900566101074
Train Acc 0.4781
 Acc 0.4794
new best val f1: 0.4794457396504261
meta,dgl,1,129,4.6812,0.4794

epoch:131/50, training loss:2.0848453044891357
Train Acc 0.4781
 Acc 0.4795
new best val f1: 0.4795181833748238
meta,dgl,1,130,4.7149,0.4795

epoch:132/50, training loss:2.080824136734009
Train Acc 0.4781
 Acc 0.4795
meta,dgl,1,131,4.7485,0.4795

epoch:133/50, training loss:2.076838731765747
Train Acc 0.4781
 Acc 0.4795
meta,dgl,1,132,4.7822,0.4795

epoch:134/50, training loss:2.072861433029175
Train Acc 0.4781
 Acc 0.4796
new best val f1: 0.4796235560648569
meta,dgl,1,133,4.8158,0.4796

epoch:135/50, training loss:2.06891131401062
Train Acc 0.4782
 Acc 0.4795
meta,dgl,1,134,4.8503,0.4795

epoch:136/50, training loss:2.065042018890381
Train Acc 0.4782
 Acc 0.4796
new best val f1: 0.47963014185798397
meta,dgl,1,135,4.8848,0.4796

epoch:137/50, training loss:2.0611915588378906
Train Acc 0.4783
 Acc 0.4796
meta,dgl,1,136,4.9183,0.4796

epoch:138/50, training loss:2.0573644638061523
Train Acc 0.4784
 Acc 0.4797
new best val f1: 0.4796630708236193
meta,dgl,1,137,4.9519,0.4797

epoch:139/50, training loss:2.0535614490509033
Train Acc 0.4784
 Acc 0.4796
meta,dgl,1,138,4.9855,0.4796

epoch:140/50, training loss:2.049809455871582
Train Acc 0.4784
 Acc 0.4797
new best val f1: 0.4797289287548899
meta,dgl,1,139,5.0199,0.4797

epoch:141/50, training loss:2.0460872650146484
Train Acc 0.4784
 Acc 0.4799
new best val f1: 0.4799001593761937
meta,dgl,1,140,5.0535,0.4799

epoch:142/50, training loss:2.042393445968628
Train Acc 0.4785
 Acc 0.4799
new best val f1: 0.479933088341829
meta,dgl,1,141,5.0871,0.4799

epoch:143/50, training loss:2.0387229919433594
Train Acc 0.4786
 Acc 0.4800
new best val f1: 0.48003187523873503
meta,dgl,1,142,5.1207,0.4800

epoch:144/50, training loss:2.035085916519165
Train Acc 0.4787
 Acc 0.4801
new best val f1: 0.4800911473768786
meta,dgl,1,143,5.1552,0.4801

epoch:145/50, training loss:2.0314841270446777
Train Acc 0.4787
 Acc 0.4802
new best val f1: 0.4801635911012763
meta,dgl,1,144,5.1888,0.4802

epoch:146/50, training loss:2.027907609939575
Train Acc 0.4788
 Acc 0.4802
new best val f1: 0.4802492064119282
meta,dgl,1,145,5.2224,0.4802

epoch:147/50, training loss:2.024367570877075
Train Acc 0.4789
 Acc 0.4804
new best val f1: 0.4803545791019613
meta,dgl,1,146,5.2559,0.4804

epoch:148/50, training loss:2.020843029022217
Train Acc 0.4790
 Acc 0.4805
new best val f1: 0.48046653758512137
meta,dgl,1,147,5.2896,0.4805

epoch:149/50, training loss:2.0173521041870117
Train Acc 0.4790
 Acc 0.4805
new best val f1: 0.48047312337824843
meta,dgl,1,148,5.3231,0.4805

epoch:150/50, training loss:2.0138914585113525
Train Acc 0.4791
 Acc 0.4806
new best val f1: 0.48055215289577324
meta,dgl,1,149,5.3568,0.4806

epoch:151/50, training loss:2.0104591846466064
Train Acc 0.4791
 Acc 0.4807
new best val f1: 0.4807102119308228
meta,dgl,1,150,5.3912,0.4807

epoch:152/50, training loss:2.007056713104248
Train Acc 0.4792
 Acc 0.4807
new best val f1: 0.48074314089645814
meta,dgl,1,151,5.4248,0.4807

epoch:153/50, training loss:2.0036773681640625
Train Acc 0.4793
 Acc 0.4808
new best val f1: 0.4808485135864912
meta,dgl,1,152,5.4584,0.4808

epoch:154/50, training loss:2.000323534011841
Train Acc 0.4794
 Acc 0.4810
new best val f1: 0.48101315841466785
meta,dgl,1,153,5.4921,0.4810

epoch:155/50, training loss:1.9969996213912964
Train Acc 0.4795
 Acc 0.4811
new best val f1: 0.48113828848408213
meta,dgl,1,154,5.5265,0.4811

epoch:156/50, training loss:1.9936999082565308
Train Acc 0.4797
 Acc 0.4812
new best val f1: 0.48123707538098814
meta,dgl,1,155,5.5601,0.4812

epoch:157/50, training loss:1.9904271364212036
Train Acc 0.4798
 Acc 0.4814
new best val f1: 0.481421477588546
meta,dgl,1,156,5.5937,0.4814

epoch:158/50, training loss:1.9871832132339478
Train Acc 0.4800
 Acc 0.4818
new best val f1: 0.48179686779678876
meta,dgl,1,157,5.6276,0.4818

epoch:159/50, training loss:1.9839625358581543
Train Acc 0.4802
 Acc 0.4821
new best val f1: 0.4820602995218714
meta,dgl,1,158,5.6613,0.4821

epoch:160/50, training loss:1.9807684421539307
Train Acc 0.4805
 Acc 0.4822
new best val f1: 0.48223153014317516
meta,dgl,1,159,5.6961,0.4822

epoch:161/50, training loss:1.9775954484939575
Train Acc 0.4807
 Acc 0.4823
new best val f1: 0.4823369028332082
meta,dgl,1,160,5.7308,0.4823

epoch:162/50, training loss:1.9744466543197632
Train Acc 0.4808
 Acc 0.4824
new best val f1: 0.48240276076447886
meta,dgl,1,161,5.7645,0.4824

epoch:163/50, training loss:1.9713300466537476
Train Acc 0.4809
 Acc 0.4824
new best val f1: 0.4824356897301142
meta,dgl,1,162,5.7982,0.4824

epoch:164/50, training loss:1.9682291746139526
Train Acc 0.4810
 Acc 0.4826
new best val f1: 0.48263326352392616
meta,dgl,1,163,5.8327,0.4826

epoch:165/50, training loss:1.9651508331298828
Train Acc 0.4812
 Acc 0.4828
new best val f1: 0.48283742311086525
meta,dgl,1,164,5.8663,0.4828

epoch:166/50, training loss:1.9621000289916992
Train Acc 0.4814
 Acc 0.4831
new best val f1: 0.4831008548359479
meta,dgl,1,165,5.9000,0.4831

epoch:167/50, training loss:1.9590702056884766
Train Acc 0.4816
 Acc 0.4833
new best val f1: 0.4832654996641246
meta,dgl,1,166,5.9348,0.4833

epoch:168/50, training loss:1.9560621976852417
Train Acc 0.4818
 Acc 0.4835
new best val f1: 0.4835486887685884
meta,dgl,1,167,5.9725,0.4835

epoch:169/50, training loss:1.9530798196792603
Train Acc 0.4820
 Acc 0.4838
new best val f1: 0.4837660199417816
meta,dgl,1,168,6.0061,0.4838

epoch:170/50, training loss:1.9501160383224487
Train Acc 0.4822
 Acc 0.4841
new best val f1: 0.4840953095981349
meta,dgl,1,169,6.0405,0.4841

epoch:171/50, training loss:1.9471784830093384
Train Acc 0.4825
 Acc 0.4844
new best val f1: 0.48437849870259875
meta,dgl,1,170,6.0741,0.4844

epoch:172/50, training loss:1.9442564249038696
Train Acc 0.4828
 Acc 0.4847
new best val f1: 0.4846551020139355
meta,dgl,1,171,6.1084,0.4847

epoch:173/50, training loss:1.9413557052612305
Train Acc 0.4830
 Acc 0.4849
new best val f1: 0.4849119479458911
meta,dgl,1,172,6.1429,0.4849

epoch:174/50, training loss:1.9384809732437134
Train Acc 0.4833
 Acc 0.4851
new best val f1: 0.48506342118781365
meta,dgl,1,173,6.1772,0.4851

epoch:175/50, training loss:1.935619831085205
Train Acc 0.4834
 Acc 0.4853
new best val f1: 0.485300509740388
meta,dgl,1,174,6.2108,0.4853

epoch:176/50, training loss:1.9327871799468994
Train Acc 0.4836
 Acc 0.4855
new best val f1: 0.485491497741073
meta,dgl,1,175,6.2444,0.4855

epoch:177/50, training loss:1.92997407913208
Train Acc 0.4838
 Acc 0.4856
new best val f1: 0.48562979939674134
meta,dgl,1,176,6.2779,0.4856

epoch:178/50, training loss:1.9271745681762695
Train Acc 0.4839
 Acc 0.4858
new best val f1: 0.4858207873974263
meta,dgl,1,177,6.3116,0.4858

epoch:179/50, training loss:1.924399495124817
Train Acc 0.4842
 Acc 0.4860
new best val f1: 0.4860315327774924
meta,dgl,1,178,6.3452,0.4860

epoch:180/50, training loss:1.921642541885376
Train Acc 0.4844
 Acc 0.4862
new best val f1: 0.486189591812542
meta,dgl,1,179,6.3787,0.4862

epoch:181/50, training loss:1.918908953666687
Train Acc 0.4845
 Acc 0.4866
new best val f1: 0.4865781536070389
meta,dgl,1,180,6.4124,0.4866

epoch:182/50, training loss:1.91619074344635
Train Acc 0.4848
 Acc 0.4868
new best val f1: 0.48680207057335917
meta,dgl,1,181,6.4460,0.4868

epoch:183/50, training loss:1.9134941101074219
Train Acc 0.4851
 Acc 0.4871
new best val f1: 0.48705891650531474
meta,dgl,1,182,6.4796,0.4871

epoch:184/50, training loss:1.910813570022583
Train Acc 0.4853
 Acc 0.4874
new best val f1: 0.4873750345754139
meta,dgl,1,183,6.5132,0.4874

epoch:185/50, training loss:1.9081554412841797
Train Acc 0.4856
 Acc 0.4876
new best val f1: 0.48757919416235296
meta,dgl,1,184,6.5468,0.4876

epoch:186/50, training loss:1.9055116176605225
Train Acc 0.4858
 Acc 0.4878
new best val f1: 0.48778335374929205
meta,dgl,1,185,6.5805,0.4878

epoch:187/50, training loss:1.9028904438018799
Train Acc 0.4861
 Acc 0.4880
new best val f1: 0.4879611701637228
meta,dgl,1,186,6.6151,0.4880

epoch:188/50, training loss:1.900286078453064
Train Acc 0.4862
 Acc 0.4880
new best val f1: 0.48801385650873935
meta,dgl,1,187,6.6487,0.4880

epoch:189/50, training loss:1.8976991176605225
Train Acc 0.4864
 Acc 0.4882
new best val f1: 0.4881719155437889
meta,dgl,1,188,6.6831,0.4882

epoch:190/50, training loss:1.895131230354309
Train Acc 0.4866
 Acc 0.4884
new best val f1: 0.48836948933760094
meta,dgl,1,189,6.7167,0.4884

epoch:191/50, training loss:1.8925840854644775
Train Acc 0.4868
 Acc 0.4887
new best val f1: 0.4886658500283189
meta,dgl,1,190,6.7513,0.4887

epoch:192/50, training loss:1.8900489807128906
Train Acc 0.4871
 Acc 0.4890
new best val f1: 0.4889622107190369
meta,dgl,1,191,6.7848,0.4890

epoch:193/50, training loss:1.8875361680984497
Train Acc 0.4873
 Acc 0.4892
new best val f1: 0.4892124708578654
meta,dgl,1,192,6.8184,0.4892

epoch:194/50, training loss:1.8850369453430176
Train Acc 0.4876
 Acc 0.4894
new best val f1: 0.4894363878241857
meta,dgl,1,193,6.8522,0.4894

epoch:195/50, training loss:1.8825569152832031
Train Acc 0.4879
 Acc 0.4900
new best val f1: 0.4900225234124946
meta,dgl,1,194,6.8856,0.4900

epoch:196/50, training loss:1.8800914287567139
Train Acc 0.4884
 Acc 0.4904
new best val f1: 0.49035839886197496
meta,dgl,1,195,6.9201,0.4904

epoch:197/50, training loss:1.8776476383209229
Train Acc 0.4887
 Acc 0.4907
new best val f1: 0.4906811027252012
meta,dgl,1,196,6.9538,0.4907

epoch:198/50, training loss:1.8752150535583496
Train Acc 0.4891
 Acc 0.4910
new best val f1: 0.49104990714031693
meta,dgl,1,197,6.9882,0.4910

epoch:199/50, training loss:1.8728018999099731
Train Acc 0.4894
 Acc 0.4913
new best val f1: 0.4912869956928913
meta,dgl,1,198,7.0218,0.4913

epoch:200/50, training loss:1.8704034090042114
Train Acc 0.4897
 Acc 0.4916
new best val f1: 0.49162945693549875
meta,dgl,1,199,7.0554,0.4916

epoch:201/50, training loss:1.8680217266082764
Train Acc 0.4900
 Acc 0.4920
new best val f1: 0.4919916755574874
meta,dgl,1,200,7.0898,0.4920

epoch:202/50, training loss:1.8656560182571411
Train Acc 0.4903
 Acc 0.4923
new best val f1: 0.4922814504550783
meta,dgl,1,201,7.1236,0.4923

epoch:203/50, training loss:1.863305687904358
Train Acc 0.4906
 Acc 0.4927
new best val f1: 0.49268976962895644
meta,dgl,1,202,7.1573,0.4927

epoch:204/50, training loss:1.8609710931777954
Train Acc 0.4910
 Acc 0.4929
new best val f1: 0.49290051500902254
meta,dgl,1,203,7.1909,0.4929

epoch:205/50, training loss:1.8586503267288208
Train Acc 0.4912
 Acc 0.4932
new best val f1: 0.4932232188722488
meta,dgl,1,204,7.2245,0.4932

epoch:206/50, training loss:1.85634446144104
Train Acc 0.4915
 Acc 0.4935
new best val f1: 0.49352616535609384
meta,dgl,1,205,7.2591,0.4935

epoch:207/50, training loss:1.8540548086166382
Train Acc 0.4918
 Acc 0.4938
new best val f1: 0.4937500823224141
meta,dgl,1,206,7.2937,0.4938

epoch:208/50, training loss:1.8517781496047974
Train Acc 0.4921
 Acc 0.4941
new best val f1: 0.4940727861856403
meta,dgl,1,207,7.3324,0.4941

epoch:209/50, training loss:1.8495187759399414
Train Acc 0.4925
 Acc 0.4944
new best val f1: 0.49436256108323123
meta,dgl,1,208,7.3659,0.4944

epoch:210/50, training loss:1.8472683429718018
Train Acc 0.4928
 Acc 0.4948
new best val f1: 0.4947774660502364
meta,dgl,1,209,7.3993,0.4948

epoch:211/50, training loss:1.845035433769226
Train Acc 0.4931
 Acc 0.4950
new best val f1: 0.4950145546028108
meta,dgl,1,210,7.4328,0.4950

epoch:212/50, training loss:1.842819094657898
Train Acc 0.4934
 Acc 0.4953
new best val f1: 0.49533067267291
meta,dgl,1,211,7.4664,0.4953

epoch:213/50, training loss:1.8406131267547607
Train Acc 0.4937
 Acc 0.4955
new best val f1: 0.49554141805297613
meta,dgl,1,212,7.4999,0.4955

epoch:214/50, training loss:1.8384195566177368
Train Acc 0.4940
 Acc 0.4958
new best val f1: 0.4957982639849317
meta,dgl,1,213,7.5335,0.4958

epoch:215/50, training loss:1.836238145828247
Train Acc 0.4943
 Acc 0.4960
new best val f1: 0.49600242357187074
meta,dgl,1,214,7.5670,0.4960

epoch:216/50, training loss:1.8340741395950317
Train Acc 0.4945
 Acc 0.4963
new best val f1: 0.49633171322822406
meta,dgl,1,215,7.6004,0.4963

epoch:217/50, training loss:1.8319215774536133
Train Acc 0.4948
 Acc 0.4966
new best val f1: 0.4965688017807985
meta,dgl,1,216,7.6340,0.4966

epoch:218/50, training loss:1.8297839164733887
Train Acc 0.4952
 Acc 0.4968
new best val f1: 0.49680589033337286
meta,dgl,1,217,7.6676,0.4968

epoch:219/50, training loss:1.8276561498641968
Train Acc 0.4955
 Acc 0.4971
new best val f1: 0.4971088368172179
meta,dgl,1,218,7.7011,0.4971

epoch:220/50, training loss:1.8255423307418823
Train Acc 0.4958
 Acc 0.4973
new best val f1: 0.49730641061102987
meta,dgl,1,219,7.7356,0.4973

epoch:221/50, training loss:1.823441743850708
Train Acc 0.4961
 Acc 0.4976
new best val f1: 0.4975895997154937
meta,dgl,1,220,7.7692,0.4976

epoch:222/50, training loss:1.8213520050048828
Train Acc 0.4963
 Acc 0.4978
new best val f1: 0.4977740019230516
meta,dgl,1,221,7.8037,0.4978

epoch:223/50, training loss:1.8192750215530396
Train Acc 0.4966
 Acc 0.4982
new best val f1: 0.4981823210969297
meta,dgl,1,222,7.8427,0.4982

epoch:224/50, training loss:1.8172111511230469
Train Acc 0.4969
 Acc 0.4985
new best val f1: 0.49848526758077477
meta,dgl,1,223,7.8762,0.4985

epoch:225/50, training loss:1.8151606321334839
Train Acc 0.4972
 Acc 0.4987
new best val f1: 0.4987486993058574
meta,dgl,1,224,7.9097,0.4987

epoch:226/50, training loss:1.8131190538406372
Train Acc 0.4976
 Acc 0.4991
new best val f1: 0.4990648173759566
meta,dgl,1,225,7.9432,0.4991

epoch:227/50, training loss:1.8110934495925903
Train Acc 0.4979
 Acc 0.4994
new best val f1: 0.4994138644116911
meta,dgl,1,226,7.9778,0.4994

epoch:228/50, training loss:1.8090744018554688
Train Acc 0.4983
 Acc 0.4997
new best val f1: 0.49971681089553616
meta,dgl,1,227,8.0113,0.4997

epoch:229/50, training loss:1.8070697784423828
Train Acc 0.4986
 Acc 0.5001
new best val f1: 0.5001185442762872
meta,dgl,1,228,8.0448,0.5001

epoch:230/50, training loss:1.8050777912139893
Train Acc 0.4990
 Acc 0.5004
new best val f1: 0.5004214907601322
meta,dgl,1,229,8.0783,0.5004

epoch:231/50, training loss:1.8030942678451538
Train Acc 0.4993
 Acc 0.5008
new best val f1: 0.5007507804164856
meta,dgl,1,230,8.1122,0.5008

epoch:232/50, training loss:1.8011242151260376
Train Acc 0.4996
 Acc 0.5011
new best val f1: 0.5011459280041095
meta,dgl,1,231,8.1457,0.5011

epoch:233/50, training loss:1.7991666793823242
Train Acc 0.5000
 Acc 0.5015
new best val f1: 0.5015015608329711
meta,dgl,1,232,8.1793,0.5015

epoch:234/50, training loss:1.7972140312194824
Train Acc 0.5003
 Acc 0.5019
new best val f1: 0.5018769510412139
meta,dgl,1,233,8.2137,0.5019

epoch:235/50, training loss:1.7952815294265747
Train Acc 0.5007
 Acc 0.5022
new best val f1: 0.5022325838700755
meta,dgl,1,234,8.2470,0.5022

epoch:236/50, training loss:1.7933518886566162
Train Acc 0.5011
 Acc 0.5026
new best val f1: 0.5026013882851912
meta,dgl,1,235,8.2809,0.5026

epoch:237/50, training loss:1.7914372682571411
Train Acc 0.5014
 Acc 0.5031
new best val f1: 0.5031216659422294
meta,dgl,1,236,8.3155,0.5031

epoch:238/50, training loss:1.7895317077636719
Train Acc 0.5018
 Acc 0.5034
new best val f1: 0.5034311982192016
meta,dgl,1,237,8.3493,0.5034

epoch:239/50, training loss:1.78763747215271
Train Acc 0.5022
 Acc 0.5039
new best val f1: 0.5038856179449691
meta,dgl,1,238,8.3830,0.5039

epoch:240/50, training loss:1.7857543230056763
Train Acc 0.5026
 Acc 0.5043
new best val f1: 0.5043400376707367
meta,dgl,1,239,8.4166,0.5043

epoch:241/50, training loss:1.7838796377182007
Train Acc 0.5029
 Acc 0.5048
new best val f1: 0.5048010431896314
meta,dgl,1,240,8.4503,0.5048

epoch:242/50, training loss:1.782016634941101
Train Acc 0.5033
 Acc 0.5052
new best val f1: 0.50516326181162
meta,dgl,1,241,8.4847,0.5052

epoch:243/50, training loss:1.7801645994186401
Train Acc 0.5037
 Acc 0.5055
new best val f1: 0.5054662082954651
meta,dgl,1,242,8.5184,0.5055

epoch:244/50, training loss:1.778320550918579
Train Acc 0.5040
 Acc 0.5058
new best val f1: 0.5057889121586913
meta,dgl,1,243,8.5520,0.5058

epoch:245/50, training loss:1.7764898538589478
Train Acc 0.5044
 Acc 0.5062
new best val f1: 0.5062433318844588
meta,dgl,1,244,8.5856,0.5062

epoch:246/50, training loss:1.774667501449585
Train Acc 0.5048
 Acc 0.5067
new best val f1: 0.5066911658170994
meta,dgl,1,245,8.6200,0.5067

epoch:247/50, training loss:1.7728530168533325
Train Acc 0.5052
 Acc 0.5071
new best val f1: 0.5070797276115963
meta,dgl,1,246,8.6539,0.5071

epoch:248/50, training loss:1.7710533142089844
Train Acc 0.5056
 Acc 0.5073
new best val f1: 0.507343159336679
meta,dgl,1,247,8.6879,0.5073

epoch:249/50, training loss:1.76925790309906
Train Acc 0.5059
 Acc 0.5078
new best val f1: 0.5078107506487006
meta,dgl,1,248,8.7217,0.5078

epoch:250/50, training loss:1.7674760818481445
Train Acc 0.5063
 Acc 0.5081
new best val f1: 0.5081334545119268
meta,dgl,1,249,8.7555,0.5081

epoch:251/50, training loss:1.7656992673873901
Train Acc 0.5066
 Acc 0.5085
new best val f1: 0.5084825015476614
meta,dgl,1,250,8.7892,0.5085

epoch:252/50, training loss:1.7639336585998535
Train Acc 0.5069
 Acc 0.5087
new best val f1: 0.5087064185139817
meta,dgl,1,251,8.8228,0.5087

epoch:253/50, training loss:1.7621800899505615
Train Acc 0.5072
 Acc 0.5090
new best val f1: 0.5090291223772079
meta,dgl,1,252,8.8565,0.5090

epoch:254/50, training loss:1.760435938835144
Train Acc 0.5075
 Acc 0.5093
new best val f1: 0.5092793825160364
meta,dgl,1,253,8.8908,0.5093

epoch:255/50, training loss:1.7586958408355713
Train Acc 0.5078
 Acc 0.5096
new best val f1: 0.5095823289998814
meta,dgl,1,254,8.9248,0.5096

epoch:256/50, training loss:1.756967544555664
Train Acc 0.5081
 Acc 0.5098
new best val f1: 0.5098062459662017
meta,dgl,1,255,8.9586,0.5098

epoch:257/50, training loss:1.755250334739685
Train Acc 0.5083
 Acc 0.5101
new best val f1: 0.5100696776912843
meta,dgl,1,256,8.9932,0.5101

epoch:258/50, training loss:1.7535408735275269
Train Acc 0.5086
 Acc 0.5103
new best val f1: 0.5102870088644775
meta,dgl,1,257,9.0269,0.5103

epoch:259/50, training loss:1.751840591430664
Train Acc 0.5089
 Acc 0.5106
new best val f1: 0.5105833695551956
meta,dgl,1,258,9.0605,0.5106

epoch:260/50, training loss:1.7501498460769653
Train Acc 0.5092
 Acc 0.5110
new best val f1: 0.5109982745222007
meta,dgl,1,259,9.0950,0.5110

epoch:261/50, training loss:1.7484647035598755
Train Acc 0.5095
 Acc 0.5113
new best val f1: 0.5112880494197917
meta,dgl,1,260,9.1286,0.5113

epoch:262/50, training loss:1.7467877864837646
Train Acc 0.5098
 Acc 0.5117
new best val f1: 0.5116634396280344
meta,dgl,1,261,9.1623,0.5117

epoch:263/50, training loss:1.745121955871582
Train Acc 0.5101
 Acc 0.5120
new best val f1: 0.5119598003187524
meta,dgl,1,262,9.1959,0.5120

epoch:264/50, training loss:1.7434643507003784
Train Acc 0.5104
 Acc 0.5122
new best val f1: 0.5121837172850726
meta,dgl,1,263,9.2294,0.5122

epoch:265/50, training loss:1.7418129444122314
Train Acc 0.5106
 Acc 0.5125
new best val f1: 0.5124537348032824
meta,dgl,1,264,9.2638,0.5125

epoch:266/50, training loss:1.740173578262329
Train Acc 0.5109
 Acc 0.5127
new best val f1: 0.5126644801833484
meta,dgl,1,265,9.2975,0.5127

epoch:267/50, training loss:1.7385400533676147
Train Acc 0.5111
 Acc 0.5128
new best val f1: 0.5128291250115251
meta,dgl,1,266,9.3312,0.5128

epoch:268/50, training loss:1.736914873123169
Train Acc 0.5114
 Acc 0.5131
new best val f1: 0.5131386572884973
meta,dgl,1,267,9.3647,0.5131

epoch:269/50, training loss:1.7352927923202515
Train Acc 0.5117
 Acc 0.5134
new best val f1: 0.5133955032204528
meta,dgl,1,268,9.3984,0.5134

epoch:270/50, training loss:1.7336852550506592
Train Acc 0.5119
 Acc 0.5137
new best val f1: 0.5136721065317896
meta,dgl,1,269,9.4321,0.5137

epoch:271/50, training loss:1.732081413269043
Train Acc 0.5122
 Acc 0.5139
new best val f1: 0.5138630945324746
meta,dgl,1,270,9.4658,0.5139

epoch:272/50, training loss:1.7304872274398804
Train Acc 0.5124
 Acc 0.5141
new best val f1: 0.514113354671303
meta,dgl,1,271,9.4994,0.5141

epoch:273/50, training loss:1.7289009094238281
Train Acc 0.5127
 Acc 0.5144
new best val f1: 0.5144228869482752
meta,dgl,1,272,9.5331,0.5144

epoch:274/50, training loss:1.7273211479187012
Train Acc 0.5129
 Acc 0.5147
new best val f1: 0.5146665612939766
meta,dgl,1,273,9.5675,0.5147

epoch:275/50, training loss:1.7257522344589233
Train Acc 0.5132
 Acc 0.5148
new best val f1: 0.5148377919152803
meta,dgl,1,274,9.6014,0.5148

epoch:276/50, training loss:1.724188208580017
Train Acc 0.5134
 Acc 0.5151
new best val f1: 0.515101223640363
meta,dgl,1,275,9.6351,0.5151

epoch:277/50, training loss:1.7226296663284302
Train Acc 0.5137
 Acc 0.5154
new best val f1: 0.5153646553654456
meta,dgl,1,276,9.6687,0.5154

epoch:278/50, training loss:1.721081256866455
Train Acc 0.5139
 Acc 0.5157
new best val f1: 0.515693945021799
meta,dgl,1,277,9.7024,0.5157

epoch:279/50, training loss:1.719538927078247
Train Acc 0.5142
 Acc 0.5160
new best val f1: 0.515996891505644
meta,dgl,1,278,9.7369,0.5160

epoch:280/50, training loss:1.7180047035217285
Train Acc 0.5145
 Acc 0.5163
new best val f1: 0.5163261811619974
meta,dgl,1,279,9.7715,0.5163

epoch:281/50, training loss:1.7164779901504517
Train Acc 0.5148
 Acc 0.5166
new best val f1: 0.5166159560595882
meta,dgl,1,280,9.8059,0.5166

epoch:282/50, training loss:1.7149550914764404
Train Acc 0.5151
 Acc 0.5169
new best val f1: 0.516892559370925
meta,dgl,1,281,9.8396,0.5169

epoch:283/50, training loss:1.7134428024291992
Train Acc 0.5153
 Acc 0.5171
new best val f1: 0.5171033047509912
meta,dgl,1,282,9.8733,0.5171

epoch:284/50, training loss:1.7119380235671997
Train Acc 0.5156
 Acc 0.5173
new best val f1: 0.5173469790966926
meta,dgl,1,283,9.9068,0.5173

epoch:285/50, training loss:1.7104368209838867
Train Acc 0.5159
 Acc 0.5176
new best val f1: 0.5176301682011565
meta,dgl,1,284,9.9412,0.5176

epoch:286/50, training loss:1.7089431285858154
Train Acc 0.5161
 Acc 0.5178
new best val f1: 0.5178409135812226
meta,dgl,1,285,9.9758,0.5178

epoch:287/50, training loss:1.7074567079544067
Train Acc 0.5164
 Acc 0.5181
new best val f1: 0.5180648305475428
meta,dgl,1,286,10.0095,0.5181

epoch:288/50, training loss:1.705977201461792
Train Acc 0.5166
 Acc 0.5184
new best val f1: 0.5183611912382609
meta,dgl,1,287,10.0431,0.5184

epoch:289/50, training loss:1.7045047283172607
Train Acc 0.5169
 Acc 0.5186
new best val f1: 0.518578522411454
meta,dgl,1,288,10.0768,0.5186

epoch:290/50, training loss:1.7030404806137085
Train Acc 0.5171
 Acc 0.5188
new best val f1: 0.5188090251709013
meta,dgl,1,289,10.1113,0.5188

epoch:291/50, training loss:1.701581358909607
Train Acc 0.5173
 Acc 0.5190
new best val f1: 0.5190263563440946
meta,dgl,1,290,10.1448,0.5190

epoch:292/50, training loss:1.7001261711120605
Train Acc 0.5176
 Acc 0.5192
new best val f1: 0.5192107585516523
meta,dgl,1,291,10.1784,0.5192

epoch:293/50, training loss:1.6986820697784424
Train Acc 0.5178
 Acc 0.5194
new best val f1: 0.5194346755179726
meta,dgl,1,292,10.2119,0.5194

epoch:294/50, training loss:1.6972416639328003
Train Acc 0.5181
 Acc 0.5196
new best val f1: 0.5195795629667681
meta,dgl,1,293,10.2464,0.5196

epoch:295/50, training loss:1.695807695388794
Train Acc 0.5183
 Acc 0.5198
new best val f1: 0.5198034799330884
meta,dgl,1,294,10.2809,0.5198

epoch:296/50, training loss:1.6943788528442383
Train Acc 0.5185
 Acc 0.5200
new best val f1: 0.5199878821406462
meta,dgl,1,295,10.3145,0.5200

epoch:297/50, training loss:1.6929607391357422
Train Acc 0.5187
 Acc 0.5202
new best val f1: 0.5202183849000935
meta,dgl,1,296,10.3481,0.5202

epoch:298/50, training loss:1.6915442943572998
Train Acc 0.5190
 Acc 0.5205
new best val f1: 0.520468645038922
meta,dgl,1,297,10.3818,0.5205

epoch:299/50, training loss:1.6901353597640991
Train Acc 0.5192
 Acc 0.5207
new best val f1: 0.5207057335914964
meta,dgl,1,298,10.4154,0.5207

epoch:300/50, training loss:1.6887321472167969
Train Acc 0.5195
 Acc 0.5209
new best val f1: 0.5208703784196731
meta,dgl,1,299,10.4491,0.5209

epoch:301/50, training loss:1.6873379945755005
Train Acc 0.5197
 Acc 0.5210
new best val f1: 0.5210481948341039
meta,dgl,1,300,10.4836,0.5210

epoch:302/50, training loss:1.685948133468628
Train Acc 0.5199
 Acc 0.5214
new best val f1: 0.521357727111076
meta,dgl,1,301,10.5172,0.5214

epoch:303/50, training loss:1.6845622062683105
Train Acc 0.5202
 Acc 0.5216
new best val f1: 0.5215816440773963
meta,dgl,1,302,10.5508,0.5216

epoch:304/50, training loss:1.6831834316253662
Train Acc 0.5204
 Acc 0.5219
new best val f1: 0.521858247388733
meta,dgl,1,303,10.5844,0.5219

epoch:305/50, training loss:1.681810736656189
Train Acc 0.5206
 Acc 0.5220
new best val f1: 0.5220426495962909
meta,dgl,1,304,10.6179,0.5220

epoch:306/50, training loss:1.6804450750350952
Train Acc 0.5208
 Acc 0.5222
new best val f1: 0.5222270518038488
meta,dgl,1,305,10.6516,0.5222

epoch:307/50, training loss:1.6790827512741089
Train Acc 0.5210
 Acc 0.5224
new best val f1: 0.5224377971839148
meta,dgl,1,306,10.6860,0.5224

epoch:308/50, training loss:1.6777292490005493
Train Acc 0.5213
 Acc 0.5227
new best val f1: 0.5226748857364892
meta,dgl,1,307,10.7197,0.5227

epoch:309/50, training loss:1.676374912261963
Train Acc 0.5216
 Acc 0.5229
new best val f1: 0.5229317316684449
meta,dgl,1,308,10.7533,0.5229

epoch:310/50, training loss:1.6750332117080688
Train Acc 0.5218
 Acc 0.5232
new best val f1: 0.5231819918072733
meta,dgl,1,309,10.7870,0.5232

epoch:311/50, training loss:1.6736949682235718
Train Acc 0.5220
 Acc 0.5234
new best val f1: 0.5233861513942124
meta,dgl,1,310,10.8206,0.5234

epoch:312/50, training loss:1.6723626852035522
Train Acc 0.5222
 Acc 0.5237
new best val f1: 0.5236759262918034
meta,dgl,1,311,10.8543,0.5237

epoch:313/50, training loss:1.6710360050201416
Train Acc 0.5225
 Acc 0.5239
new best val f1: 0.5238537427062341
meta,dgl,1,312,10.8878,0.5239

epoch:314/50, training loss:1.6697138547897339
Train Acc 0.5227
 Acc 0.5241
new best val f1: 0.5240644880863002
meta,dgl,1,313,10.9215,0.5241

epoch:315/50, training loss:1.6683980226516724
Train Acc 0.5230
 Acc 0.5244
new best val f1: 0.5243674345701452
meta,dgl,1,314,10.9551,0.5244

epoch:316/50, training loss:1.6670852899551392
Train Acc 0.5233
 Acc 0.5246
new best val f1: 0.5246242805021009
meta,dgl,1,315,10.9888,0.5246

epoch:317/50, training loss:1.6657776832580566
Train Acc 0.5235
 Acc 0.5247
new best val f1: 0.524736238985261
meta,dgl,1,316,11.0233,0.5247

epoch:318/50, training loss:1.6644786596298218
Train Acc 0.5238
 Acc 0.5249
new best val f1: 0.5249403985722001
meta,dgl,1,317,11.0580,0.5249

epoch:319/50, training loss:1.663185715675354
Train Acc 0.5240
 Acc 0.5251
new best val f1: 0.5250852860209955
meta,dgl,1,318,11.0916,0.5251

epoch:320/50, training loss:1.6618940830230713
Train Acc 0.5242
 Acc 0.5254
new best val f1: 0.5253882325048406
meta,dgl,1,319,11.1252,0.5254

epoch:321/50, training loss:1.6606082916259766
Train Acc 0.5244
 Acc 0.5255
new best val f1: 0.5255199483673819
meta,dgl,1,320,11.1589,0.5255

epoch:322/50, training loss:1.6593300104141235
Train Acc 0.5246
 Acc 0.5257
new best val f1: 0.5257241079543209
meta,dgl,1,321,11.1926,0.5257

epoch:323/50, training loss:1.6580543518066406
Train Acc 0.5249
 Acc 0.5260
new best val f1: 0.5259875396794036
meta,dgl,1,322,11.2262,0.5260

epoch:324/50, training loss:1.6567867994308472
Train Acc 0.5252
 Acc 0.5262
new best val f1: 0.5262377998182322
meta,dgl,1,323,11.2598,0.5262

epoch:325/50, training loss:1.6555209159851074
Train Acc 0.5254
 Acc 0.5265
new best val f1: 0.526527574715823
meta,dgl,1,324,11.2935,0.5265

epoch:326/50, training loss:1.654258370399475
Train Acc 0.5257
 Acc 0.5267
new best val f1: 0.5267317343027621
meta,dgl,1,325,11.3280,0.5267

epoch:327/50, training loss:1.653007984161377
Train Acc 0.5259
 Acc 0.5270
new best val f1: 0.5269951660278447
meta,dgl,1,326,11.3615,0.5270

epoch:328/50, training loss:1.6517581939697266
Train Acc 0.5261
 Acc 0.5272
new best val f1: 0.5272124972010379
meta,dgl,1,327,11.3960,0.5272

epoch:329/50, training loss:1.6505117416381836
Train Acc 0.5264
 Acc 0.5275
new best val f1: 0.5274956863055018
meta,dgl,1,328,11.4296,0.5275

epoch:330/50, training loss:1.64927339553833
Train Acc 0.5266
 Acc 0.5277
new best val f1: 0.5276800885130596
meta,dgl,1,329,11.4632,0.5277

epoch:331/50, training loss:1.6480367183685303
Train Acc 0.5268
 Acc 0.5279
new best val f1: 0.5278974196862528
meta,dgl,1,330,11.4968,0.5279

epoch:332/50, training loss:1.6468061208724976
Train Acc 0.5271
 Acc 0.5281
new best val f1: 0.5280884076869378
meta,dgl,1,331,11.5305,0.5281

epoch:333/50, training loss:1.6455806493759155
Train Acc 0.5273
 Acc 0.5283
new best val f1: 0.5283452536188933
meta,dgl,1,332,11.5650,0.5283

epoch:334/50, training loss:1.6443599462509155
Train Acc 0.5276
 Acc 0.5285
new best val f1: 0.5285494132058324
meta,dgl,1,333,11.5986,0.5285

epoch:335/50, training loss:1.6431423425674438
Train Acc 0.5278
 Acc 0.5288
new best val f1: 0.5287865017584068
meta,dgl,1,334,11.6329,0.5288

epoch:336/50, training loss:1.6419346332550049
Train Acc 0.5281
 Acc 0.5290
new best val f1: 0.529017004517854
meta,dgl,1,335,11.6665,0.5290

epoch:337/50, training loss:1.6407268047332764
Train Acc 0.5283
 Acc 0.5294
new best val f1: 0.5293528799673345
meta,dgl,1,336,11.7010,0.5294

epoch:338/50, training loss:1.6395227909088135
Train Acc 0.5286
 Acc 0.5296
new best val f1: 0.5296360690717983
meta,dgl,1,337,11.7354,0.5296

epoch:339/50, training loss:1.6383253335952759
Train Acc 0.5289
 Acc 0.5299
new best val f1: 0.5298863292106268
meta,dgl,1,338,11.7691,0.5299

epoch:340/50, training loss:1.6371328830718994
Train Acc 0.5292
 Acc 0.5302
new best val f1: 0.5301761041082178
meta,dgl,1,339,11.8027,0.5302

epoch:341/50, training loss:1.6359453201293945
Train Acc 0.5294
 Acc 0.5303
new best val f1: 0.5303473347295214
meta,dgl,1,340,11.8363,0.5303

epoch:342/50, training loss:1.6347581148147583
Train Acc 0.5297
 Acc 0.5306
new best val f1: 0.5305580801095876
meta,dgl,1,341,11.8700,0.5306

epoch:343/50, training loss:1.6335790157318115
Train Acc 0.5299
 Acc 0.5309
new best val f1: 0.5308807839728138
meta,dgl,1,342,11.9035,0.5309

epoch:344/50, training loss:1.6324021816253662
Train Acc 0.5302
 Acc 0.5312
new best val f1: 0.5312100736291672
meta,dgl,1,343,11.9382,0.5312

epoch:345/50, training loss:1.6312335729599
Train Acc 0.5305
 Acc 0.5315
new best val f1: 0.5314669195611228
meta,dgl,1,344,11.9728,0.5315

epoch:346/50, training loss:1.6300643682479858
Train Acc 0.5308
 Acc 0.5318
new best val f1: 0.5317566944587137
meta,dgl,1,345,12.0064,0.5318

epoch:347/50, training loss:1.6289005279541016
Train Acc 0.5311
 Acc 0.5320
new best val f1: 0.5320135403906693
meta,dgl,1,346,12.0401,0.5320

epoch:348/50, training loss:1.6277424097061157
Train Acc 0.5314
 Acc 0.5323
new best val f1: 0.5323099010813872
meta,dgl,1,347,12.0737,0.5323

epoch:349/50, training loss:1.6265888214111328
Train Acc 0.5317
 Acc 0.5326
new best val f1: 0.5325535754270887
meta,dgl,1,348,12.1074,0.5326

epoch:350/50, training loss:1.6254395246505737
Train Acc 0.5320
 Acc 0.5328
new best val f1: 0.5328038355659171
meta,dgl,1,349,12.1410,0.5328

epoch:351/50, training loss:1.6242918968200684
Train Acc 0.5322
 Acc 0.5331
new best val f1: 0.5330738530841269
meta,dgl,1,350,12.1756,0.5331

epoch:352/50, training loss:1.6231509447097778
Train Acc 0.5325
 Acc 0.5334
new best val f1: 0.5334031427404803
meta,dgl,1,351,12.2095,0.5334

epoch:353/50, training loss:1.6220120191574097
Train Acc 0.5328
 Acc 0.5337
new best val f1: 0.5336731602586899
meta,dgl,1,352,12.2441,0.5337

epoch:354/50, training loss:1.62087881565094
Train Acc 0.5331
 Acc 0.5339
new best val f1: 0.5339168346043914
meta,dgl,1,353,12.2777,0.5339

epoch:355/50, training loss:1.6197516918182373
Train Acc 0.5333
 Acc 0.5342
new best val f1: 0.5341868521226011
meta,dgl,1,354,12.3113,0.5342

epoch:356/50, training loss:1.61862313747406
Train Acc 0.5336
 Acc 0.5344
new best val f1: 0.5344107690889214
meta,dgl,1,355,12.3450,0.5344

epoch:357/50, training loss:1.6175031661987305
Train Acc 0.5339
 Acc 0.5346
new best val f1: 0.5346412718483687
meta,dgl,1,356,12.3794,0.5346

epoch:358/50, training loss:1.6163849830627441
Train Acc 0.5341
 Acc 0.5349
new best val f1: 0.5349112893665784
meta,dgl,1,357,12.4130,0.5349

epoch:359/50, training loss:1.6152713298797607
Train Acc 0.5344
 Acc 0.5352
new best val f1: 0.5352076500572964
meta,dgl,1,358,12.4466,0.5352

epoch:360/50, training loss:1.6141624450683594
Train Acc 0.5347
 Acc 0.5355
new best val f1: 0.535471081782379
meta,dgl,1,359,12.4802,0.5355

epoch:361/50, training loss:1.613054871559143
Train Acc 0.5350
 Acc 0.5356
new best val f1: 0.5356291408174286
meta,dgl,1,360,12.5137,0.5356

epoch:362/50, training loss:1.611952781677246
Train Acc 0.5352
 Acc 0.5358
new best val f1: 0.5358333004043677
meta,dgl,1,361,12.5474,0.5358

epoch:363/50, training loss:1.610854983329773
Train Acc 0.5355
 Acc 0.5361
new best val f1: 0.5360506315775608
meta,dgl,1,362,12.5818,0.5361

epoch:364/50, training loss:1.6097595691680908
Train Acc 0.5358
 Acc 0.5363
new best val f1: 0.5363338206820247
meta,dgl,1,363,12.6155,0.5363

epoch:365/50, training loss:1.6086698770523071
Train Acc 0.5361
 Acc 0.5366
new best val f1: 0.5366235955796157
meta,dgl,1,364,12.6500,0.5366

epoch:366/50, training loss:1.6075822114944458
Train Acc 0.5364
 Acc 0.5369
new best val f1: 0.5369199562703336
meta,dgl,1,365,12.6837,0.5369

epoch:367/50, training loss:1.6064974069595337
Train Acc 0.5367
 Acc 0.5371
new best val f1: 0.5371175300641456
meta,dgl,1,366,12.7173,0.5371

epoch:368/50, training loss:1.605423092842102
Train Acc 0.5369
 Acc 0.5374
new best val f1: 0.5373677902029741
meta,dgl,1,367,12.7509,0.5374

epoch:369/50, training loss:1.6043449640274048
Train Acc 0.5372
 Acc 0.5376
new best val f1: 0.5375982929624215
meta,dgl,1,368,12.7855,0.5376

epoch:370/50, training loss:1.6032706499099731
Train Acc 0.5374
 Acc 0.5378
new best val f1: 0.5378222099287417
meta,dgl,1,369,12.8191,0.5378

epoch:371/50, training loss:1.6022030115127563
Train Acc 0.5377
 Acc 0.5381
new best val f1: 0.5381185706194597
meta,dgl,1,370,12.8529,0.5381

epoch:372/50, training loss:1.6011381149291992
Train Acc 0.5380
 Acc 0.5384
new best val f1: 0.5383688307582882
meta,dgl,1,371,12.8865,0.5384

epoch:373/50, training loss:1.6000771522521973
Train Acc 0.5382
 Acc 0.5386
new best val f1: 0.5386256766902439
meta,dgl,1,372,12.9202,0.5386

epoch:374/50, training loss:1.5990196466445923
Train Acc 0.5385
 Acc 0.5389
new best val f1: 0.5388627652428182
meta,dgl,1,373,12.9539,0.5389

epoch:375/50, training loss:1.597965121269226
Train Acc 0.5387
 Acc 0.5391
new best val f1: 0.5390669248297573
meta,dgl,1,374,12.9884,0.5391

epoch:376/50, training loss:1.5969159603118896
Train Acc 0.5390
 Acc 0.5393
new best val f1: 0.5393171849685857
meta,dgl,1,375,13.0221,0.5393

epoch:377/50, training loss:1.5958672761917114
Train Acc 0.5393
 Acc 0.5396
new best val f1: 0.539633303038685
meta,dgl,1,376,13.0557,0.5396

epoch:378/50, training loss:1.5948224067687988
Train Acc 0.5396
 Acc 0.5399
new best val f1: 0.5398572200050052
meta,dgl,1,377,13.0893,0.5399

epoch:379/50, training loss:1.5937836170196533
Train Acc 0.5399
 Acc 0.5401
new best val f1: 0.5400811369713254
meta,dgl,1,378,13.1237,0.5401

epoch:380/50, training loss:1.5927460193634033
Train Acc 0.5401
 Acc 0.5403
new best val f1: 0.5403445686964081
meta,dgl,1,379,13.1573,0.5403

epoch:381/50, training loss:1.591715931892395
Train Acc 0.5404
 Acc 0.5406
new best val f1: 0.5405684856627284
meta,dgl,1,380,13.1909,0.5406

epoch:382/50, training loss:1.5906834602355957
Train Acc 0.5406
 Acc 0.5408
new best val f1: 0.5408055742153027
meta,dgl,1,381,13.2245,0.5408

epoch:383/50, training loss:1.5896568298339844
Train Acc 0.5408
 Acc 0.5410
new best val f1: 0.5409636332503523
meta,dgl,1,382,13.2582,0.5410

epoch:384/50, training loss:1.58863365650177
Train Acc 0.5410
 Acc 0.5412
new best val f1: 0.5411612070441644
meta,dgl,1,383,13.2918,0.5412

epoch:385/50, training loss:1.5876127481460571
Train Acc 0.5412
 Acc 0.5414
new best val f1: 0.5414180529761199
meta,dgl,1,384,13.3262,0.5414

epoch:386/50, training loss:1.5865973234176636
Train Acc 0.5415
 Acc 0.5417
new best val f1: 0.541720999459965
meta,dgl,1,385,13.3599,0.5417

epoch:387/50, training loss:1.5855815410614014
Train Acc 0.5418
 Acc 0.5419
new best val f1: 0.5418724727018875
meta,dgl,1,386,13.3934,0.5419

epoch:388/50, training loss:1.5845751762390137
Train Acc 0.5420
 Acc 0.5421
new best val f1: 0.5421424902200972
meta,dgl,1,387,13.4270,0.5421

epoch:389/50, training loss:1.5835671424865723
Train Acc 0.5422
 Acc 0.5423
new best val f1: 0.5423071350482739
meta,dgl,1,388,13.4615,0.5423

epoch:390/50, training loss:1.5825624465942383
Train Acc 0.5424
 Acc 0.5425
new best val f1: 0.5424783656695776
meta,dgl,1,389,13.4951,0.5425

epoch:391/50, training loss:1.5815620422363281
Train Acc 0.5426
 Acc 0.5427
new best val f1: 0.5426627678771354
meta,dgl,1,390,13.5288,0.5427

epoch:392/50, training loss:1.5805660486221313
Train Acc 0.5428
 Acc 0.5429
new best val f1: 0.5428735132572016
meta,dgl,1,391,13.5625,0.5429

epoch:393/50, training loss:1.5795714855194092
Train Acc 0.5431
 Acc 0.5432
new best val f1: 0.5431764597410467
meta,dgl,1,392,13.5961,0.5432

epoch:394/50, training loss:1.578580379486084
Train Acc 0.5433
 Acc 0.5434
new best val f1: 0.5433542761554774
meta,dgl,1,393,13.6298,0.5434

epoch:395/50, training loss:1.5775951147079468
Train Acc 0.5435
 Acc 0.5435
new best val f1: 0.5435255067767811
meta,dgl,1,394,13.6643,0.5435

epoch:396/50, training loss:1.5766090154647827
Train Acc 0.5437
 Acc 0.5438
new best val f1: 0.5437691811224826
meta,dgl,1,395,13.6988,0.5438

epoch:397/50, training loss:1.5756254196166992
Train Acc 0.5439
 Acc 0.5440
new best val f1: 0.5439601691231675
meta,dgl,1,396,13.7324,0.5440

epoch:398/50, training loss:1.5746480226516724
Train Acc 0.5441
 Acc 0.5442
new best val f1: 0.5442170150551231
meta,dgl,1,397,13.7663,0.5442

epoch:399/50, training loss:1.5736714601516724
Train Acc 0.5444
 Acc 0.5444
new best val f1: 0.5444277604351893
meta,dgl,1,398,13.7999,0.5444

epoch:400/50, training loss:1.572699785232544
Train Acc 0.5446
 Acc 0.5447
new best val f1: 0.5447372927121613
meta,dgl,1,399,13.8343,0.5447

epoch:401/50, training loss:1.571731686592102
Train Acc 0.5449
 Acc 0.5449
new best val f1: 0.5449480380922275
meta,dgl,1,400,13.8680,0.5449

epoch:402/50, training loss:1.5707652568817139
Train Acc 0.5451
 Acc 0.5452
new best val f1: 0.5452312271966913
meta,dgl,1,401,13.9016,0.5452

epoch:403/50, training loss:1.5698024034500122
Train Acc 0.5453
 Acc 0.5455
new best val f1: 0.545501244714901
meta,dgl,1,402,13.9352,0.5455

epoch:404/50, training loss:1.5688409805297852
Train Acc 0.5456
 Acc 0.5458
new best val f1: 0.5457910196124919
meta,dgl,1,403,13.9688,0.5458

epoch:405/50, training loss:1.5678861141204834
Train Acc 0.5458
 Acc 0.5460
new best val f1: 0.5459622502337956
meta,dgl,1,404,14.0025,0.5460

epoch:406/50, training loss:1.5669286251068115
Train Acc 0.5460
 Acc 0.5462
new best val f1: 0.5462125103726242
meta,dgl,1,405,14.0369,0.5462

epoch:407/50, training loss:1.5659773349761963
Train Acc 0.5462
 Acc 0.5464
new best val f1: 0.5464166699595633
meta,dgl,1,406,14.0705,0.5464

epoch:408/50, training loss:1.5650297403335571
Train Acc 0.5464
 Acc 0.5467
new best val f1: 0.5466603443052647
meta,dgl,1,407,14.1041,0.5467

epoch:409/50, training loss:1.5640857219696045
Train Acc 0.5466
 Acc 0.5468
new best val f1: 0.5468118175471872
meta,dgl,1,408,14.1377,0.5468

epoch:410/50, training loss:1.56314218044281
Train Acc 0.5468
 Acc 0.5470
new best val f1: 0.5470225629272534
meta,dgl,1,409,14.1722,0.5470

epoch:411/50, training loss:1.5622026920318604
Train Acc 0.5471
 Acc 0.5473
new best val f1: 0.547285994652336
meta,dgl,1,410,14.2058,0.5473

epoch:412/50, training loss:1.5612664222717285
Train Acc 0.5473
 Acc 0.5475
new best val f1: 0.547483568446148
meta,dgl,1,411,14.2395,0.5475

epoch:413/50, training loss:1.5603333711624146
Train Acc 0.5475
 Acc 0.5479
new best val f1: 0.5478721302406449
meta,dgl,1,412,14.2732,0.5479

epoch:414/50, training loss:1.5594007968902588
Train Acc 0.5478
 Acc 0.5481
new best val f1: 0.5480631182413298
meta,dgl,1,413,14.3076,0.5481

epoch:415/50, training loss:1.5584713220596313
Train Acc 0.5480
 Acc 0.5482
new best val f1: 0.5482145914832524
meta,dgl,1,414,14.3411,0.5482

epoch:416/50, training loss:1.5575487613677979
Train Acc 0.5482
 Acc 0.5484
new best val f1: 0.5484385084495725
meta,dgl,1,415,14.3748,0.5484

epoch:417/50, training loss:1.556624412536621
Train Acc 0.5484
 Acc 0.5486
new best val f1: 0.5486163248640034
meta,dgl,1,416,14.4083,0.5486

epoch:418/50, training loss:1.5557053089141846
Train Acc 0.5486
 Acc 0.5488
new best val f1: 0.5488336560371966
meta,dgl,1,417,14.4419,0.5488

epoch:419/50, training loss:1.5547884702682495
Train Acc 0.5488
 Acc 0.5490
new best val f1: 0.5490048866585003
meta,dgl,1,418,14.4755,0.5490

epoch:420/50, training loss:1.5538736581802368
Train Acc 0.5489
 Acc 0.5492
new best val f1: 0.5492353894179476
meta,dgl,1,419,14.5091,0.5492

epoch:421/50, training loss:1.5529649257659912
Train Acc 0.5492
 Acc 0.5496
new best val f1: 0.5495778506605551
meta,dgl,1,420,14.5436,0.5496

epoch:422/50, training loss:1.5520542860031128
Train Acc 0.5495
 Acc 0.5498
new best val f1: 0.5498149392131294
meta,dgl,1,421,14.5832,0.5498

epoch:423/50, training loss:1.5511503219604492
Train Acc 0.5497
 Acc 0.5501
new best val f1: 0.5500520277657038
meta,dgl,1,422,14.6168,0.5501

epoch:424/50, training loss:1.5502485036849976
Train Acc 0.5499
 Acc 0.5502
new best val f1: 0.5501837436282452
meta,dgl,1,423,14.6502,0.5502

epoch:425/50, training loss:1.5493450164794922
Train Acc 0.5501
 Acc 0.5503
new best val f1: 0.5503022879045324
meta,dgl,1,424,14.6837,0.5503

epoch:426/50, training loss:1.5484490394592285
Train Acc 0.5503
 Acc 0.5505
new best val f1: 0.5504932759052172
meta,dgl,1,425,14.7181,0.5505

epoch:427/50, training loss:1.5475544929504395
Train Acc 0.5505
 Acc 0.5506
new best val f1: 0.5506315775608857
meta,dgl,1,426,14.7525,0.5506

epoch:428/50, training loss:1.5466636419296265
Train Acc 0.5506
 Acc 0.5509
new best val f1: 0.5508884234928413
meta,dgl,1,427,14.7861,0.5509

epoch:429/50, training loss:1.5457751750946045
Train Acc 0.5508
 Acc 0.5511
new best val f1: 0.5511255120454156
meta,dgl,1,428,14.8198,0.5511

epoch:430/50, training loss:1.544886827468872
Train Acc 0.5510
 Acc 0.5512
new best val f1: 0.5512308847354487
meta,dgl,1,429,14.8534,0.5512

epoch:431/50, training loss:1.5440047979354858
Train Acc 0.5512
 Acc 0.5515
new best val f1: 0.5514811448742772
meta,dgl,1,430,14.8871,0.5515

epoch:432/50, training loss:1.5431240797042847
Train Acc 0.5514
 Acc 0.5516
new best val f1: 0.5515996891505645
meta,dgl,1,431,14.9216,0.5516

epoch:433/50, training loss:1.5422439575195312
Train Acc 0.5515
 Acc 0.5518
new best val f1: 0.5518433634962658
meta,dgl,1,432,14.9565,0.5518

epoch:434/50, training loss:1.5413670539855957
Train Acc 0.5517
 Acc 0.5520
new best val f1: 0.5519948367381884
meta,dgl,1,433,14.9909,0.5520

epoch:435/50, training loss:1.5404973030090332
Train Acc 0.5518
 Acc 0.5523
new best val f1: 0.5522780258426522
meta,dgl,1,434,15.0246,0.5523

epoch:436/50, training loss:1.5396244525909424
Train Acc 0.5520
 Acc 0.5525
new best val f1: 0.5525282859814807
meta,dgl,1,435,15.0583,0.5525

epoch:437/50, training loss:1.5387579202651978
Train Acc 0.5522
 Acc 0.5527
new best val f1: 0.5526863450165304
meta,dgl,1,436,15.0918,0.5527

epoch:438/50, training loss:1.5378903150558472
Train Acc 0.5524
 Acc 0.5530
new best val f1: 0.5529695341209941
meta,dgl,1,437,15.1263,0.5530

epoch:439/50, training loss:1.5370274782180786
Train Acc 0.5526
 Acc 0.5530
new best val f1: 0.553048563638519
meta,dgl,1,438,15.1600,0.5530

epoch:440/50, training loss:1.5361688137054443
Train Acc 0.5527
 Acc 0.5533
new best val f1: 0.553252723225458
meta,dgl,1,439,15.1937,0.5533

epoch:441/50, training loss:1.5353115797042847
Train Acc 0.5529
 Acc 0.5534
new best val f1: 0.5533910248811265
meta,dgl,1,440,15.2273,0.5534

epoch:442/50, training loss:1.5344573259353638
Train Acc 0.5531
 Acc 0.5535
new best val f1: 0.5534700543986513
meta,dgl,1,441,15.2618,0.5535

epoch:443/50, training loss:1.533601999282837
Train Acc 0.5532
 Acc 0.5537
new best val f1: 0.553654456606209
meta,dgl,1,442,15.2953,0.5537

epoch:444/50, training loss:1.532753586769104
Train Acc 0.5534
 Acc 0.5538
new best val f1: 0.5538388588137669
meta,dgl,1,443,15.3290,0.5538

epoch:445/50, training loss:1.5319052934646606
Train Acc 0.5537
 Acc 0.5541
new best val f1: 0.5540561899869602
meta,dgl,1,444,15.3626,0.5541

epoch:446/50, training loss:1.5310612916946411
Train Acc 0.5538
 Acc 0.5543
new best val f1: 0.5542801069532803
meta,dgl,1,445,15.3962,0.5543

epoch:447/50, training loss:1.5302194356918335
Train Acc 0.5540
 Acc 0.5544
new best val f1: 0.5544315801952029
meta,dgl,1,446,15.4297,0.5544

epoch:448/50, training loss:1.5293796062469482
Train Acc 0.5542
 Acc 0.5546
new best val f1: 0.5545962250233796
meta,dgl,1,447,15.4642,0.5546

epoch:449/50, training loss:1.528542399406433
Train Acc 0.5543
 Acc 0.5549
new best val f1: 0.5548530709553352
meta,dgl,1,448,15.4986,0.5549

epoch:450/50, training loss:1.5277059078216553
Train Acc 0.5545
 Acc 0.5550
new best val f1: 0.5550111299903847
meta,dgl,1,449,15.5323,0.5550

epoch:451/50, training loss:1.5268737077713013
Train Acc 0.5547
 Acc 0.5552
new best val f1: 0.5552087037841967
meta,dgl,1,450,15.5659,0.5552

epoch:452/50, training loss:1.5260437726974487
Train Acc 0.5549
 Acc 0.5553
new best val f1: 0.5553404196467381
meta,dgl,1,451,15.5995,0.5553

epoch:453/50, training loss:1.5252139568328857
Train Acc 0.5550
 Acc 0.5556
new best val f1: 0.5555840939924395
meta,dgl,1,452,15.6341,0.5556

epoch:454/50, training loss:1.5243905782699585
Train Acc 0.5552
 Acc 0.5557
new best val f1: 0.555728981441235
meta,dgl,1,453,15.6677,0.5557

epoch:455/50, training loss:1.523568034172058
Train Acc 0.5554
 Acc 0.5558
new best val f1: 0.5558145967518868
meta,dgl,1,454,15.7022,0.5558

epoch:456/50, training loss:1.5227484703063965
Train Acc 0.5554
 Acc 0.5559
new best val f1: 0.555926555235047
meta,dgl,1,455,15.7359,0.5559

epoch:457/50, training loss:1.5219284296035767
Train Acc 0.5556
 Acc 0.5561
new best val f1: 0.5561109574426049
meta,dgl,1,456,15.7704,0.5561

epoch:458/50, training loss:1.5211127996444702
Train Acc 0.5558
 Acc 0.5562
new best val f1: 0.556236087512019
meta,dgl,1,457,15.8051,0.5562

epoch:459/50, training loss:1.520297646522522
Train Acc 0.5559
 Acc 0.5563
new best val f1: 0.556334874408925
meta,dgl,1,458,15.8387,0.5563

epoch:460/50, training loss:1.5194895267486572
Train Acc 0.5561
 Acc 0.5565
new best val f1: 0.5564995192371017
meta,dgl,1,459,15.8724,0.5565

epoch:461/50, training loss:1.518680214881897
Train Acc 0.5563
 Acc 0.5566
new best val f1: 0.5566444066858972
meta,dgl,1,460,15.9060,0.5566

epoch:462/50, training loss:1.5178735256195068
Train Acc 0.5565
 Acc 0.5568
new best val f1: 0.5567563651690574
meta,dgl,1,461,15.9401,0.5568

epoch:463/50, training loss:1.5170707702636719
Train Acc 0.5567
 Acc 0.5569
new best val f1: 0.556927595790361
meta,dgl,1,462,15.9737,0.5569

epoch:464/50, training loss:1.5162690877914429
Train Acc 0.5568
 Acc 0.5572
new best val f1: 0.557223956481079
meta,dgl,1,463,16.0074,0.5572

epoch:465/50, training loss:1.5154695510864258
Train Acc 0.5570
 Acc 0.5574
new best val f1: 0.557421530274891
meta,dgl,1,464,16.0410,0.5574

epoch:466/50, training loss:1.5146750211715698
Train Acc 0.5573
 Acc 0.5576
new best val f1: 0.5576454472412112
meta,dgl,1,465,16.0755,0.5576

epoch:467/50, training loss:1.5138766765594482
Train Acc 0.5574
 Acc 0.5578
new best val f1: 0.557823263655642
meta,dgl,1,466,16.1091,0.5578

epoch:468/50, training loss:1.5130865573883057
Train Acc 0.5576
 Acc 0.5579
new best val f1: 0.5578957073800398
meta,dgl,1,467,16.1427,0.5579

epoch:469/50, training loss:1.5122963190078735
Train Acc 0.5578
 Acc 0.5580
new best val f1: 0.5580208374494541
meta,dgl,1,468,16.1763,0.5580

epoch:470/50, training loss:1.5115102529525757
Train Acc 0.5579
 Acc 0.5582
new best val f1: 0.5582381686226472
meta,dgl,1,469,16.2108,0.5582

epoch:471/50, training loss:1.5107239484786987
Train Acc 0.5581
 Acc 0.5585
new best val f1: 0.5584554997958404
meta,dgl,1,470,16.2445,0.5585

epoch:472/50, training loss:1.5099427700042725
Train Acc 0.5583
 Acc 0.5586
new best val f1: 0.5586267304171442
meta,dgl,1,471,16.2781,0.5586

epoch:473/50, training loss:1.5091626644134521
Train Acc 0.5584
 Acc 0.5589
new best val f1: 0.5588704047628456
meta,dgl,1,472,16.3117,0.5589

epoch:474/50, training loss:1.508386254310608
Train Acc 0.5586
 Acc 0.5591
new best val f1: 0.5590548069704034
meta,dgl,1,473,16.3453,0.5591

epoch:475/50, training loss:1.5076074600219727
Train Acc 0.5588
 Acc 0.5593
new best val f1: 0.5592655523504696
meta,dgl,1,474,16.3790,0.5593

epoch:476/50, training loss:1.5068323612213135
Train Acc 0.5590
 Acc 0.5594
new best val f1: 0.559403854006138
meta,dgl,1,475,16.4134,0.5594

epoch:477/50, training loss:1.5060631036758423
Train Acc 0.5592
 Acc 0.5596
new best val f1: 0.5595882562136958
meta,dgl,1,476,16.4471,0.5596

epoch:478/50, training loss:1.5052921772003174
Train Acc 0.5594
 Acc 0.5598
new best val f1: 0.5597792442143807
meta,dgl,1,477,16.4807,0.5598

epoch:479/50, training loss:1.5045244693756104
Train Acc 0.5595
 Acc 0.5599
new best val f1: 0.5599175458700492
meta,dgl,1,478,16.5152,0.5599

epoch:480/50, training loss:1.5037599802017212
Train Acc 0.5597
 Acc 0.5600
new best val f1: 0.5600492617325905
meta,dgl,1,479,16.5494,0.5600

epoch:481/50, training loss:1.5029999017715454
Train Acc 0.5598
 Acc 0.5602
new best val f1: 0.5602270781470212
meta,dgl,1,480,16.5832,0.5602

epoch:482/50, training loss:1.5022369623184204
Train Acc 0.5601
 Acc 0.5605
new best val f1: 0.5604641666995956
meta,dgl,1,481,16.6170,0.5605

epoch:483/50, training loss:1.5014790296554565
Train Acc 0.5602
 Acc 0.5606
new best val f1: 0.5606156399415182
meta,dgl,1,482,16.6516,0.5606

epoch:484/50, training loss:1.5007232427597046
Train Acc 0.5604
 Acc 0.5608
new best val f1: 0.5607868705628218
meta,dgl,1,483,16.6860,0.5608

epoch:485/50, training loss:1.49997079372406
Train Acc 0.5606
 Acc 0.5610
new best val f1: 0.5609515153909985
meta,dgl,1,484,16.7197,0.5610

epoch:486/50, training loss:1.499218225479126
Train Acc 0.5608
 Acc 0.5611
new best val f1: 0.5610832312535399
meta,dgl,1,485,16.7534,0.5611

epoch:487/50, training loss:1.4984689950942993
Train Acc 0.5609
 Acc 0.5611
new best val f1: 0.5611490891848105
meta,dgl,1,486,16.7878,0.5611

epoch:488/50, training loss:1.4977208375930786
Train Acc 0.5611
 Acc 0.5613
new best val f1: 0.5613203198061143
meta,dgl,1,487,16.8224,0.5613

epoch:489/50, training loss:1.4969747066497803
Train Acc 0.5613
 Acc 0.5615
new best val f1: 0.5615310651861803
meta,dgl,1,488,16.8560,0.5615

epoch:490/50, training loss:1.4962351322174072
Train Acc 0.5615
 Acc 0.5617
new best val f1: 0.56168912422123
meta,dgl,1,489,16.8897,0.5617

epoch:491/50, training loss:1.495492935180664
Train Acc 0.5616
 Acc 0.5618
new best val f1: 0.5618340116700254
meta,dgl,1,490,16.9232,0.5618

epoch:492/50, training loss:1.4947539567947388
Train Acc 0.5618
 Acc 0.5620
new best val f1: 0.5619723133256939
meta,dgl,1,491,16.9577,0.5620

epoch:493/50, training loss:1.494017243385315
Train Acc 0.5619
 Acc 0.5621
new best val f1: 0.5620513428432186
meta,dgl,1,492,16.9912,0.5621

epoch:494/50, training loss:1.4932838678359985
Train Acc 0.5621
 Acc 0.5622
new best val f1: 0.5621633013263787
meta,dgl,1,493,17.0256,0.5622

epoch:495/50, training loss:1.492552399635315
Train Acc 0.5622
 Acc 0.5623
new best val f1: 0.56229501718892
meta,dgl,1,494,17.0592,0.5623

epoch:496/50, training loss:1.4918229579925537
Train Acc 0.5623
 Acc 0.5624
new best val f1: 0.5624267330514614
meta,dgl,1,495,17.0929,0.5624

epoch:497/50, training loss:1.4910929203033447
Train Acc 0.5625
 Acc 0.5626
new best val f1: 0.5626111352590193
meta,dgl,1,496,17.1266,0.5626

epoch:498/50, training loss:1.4903655052185059
Train Acc 0.5627
 Acc 0.5628
new best val f1: 0.5628350522253395
meta,dgl,1,497,17.1602,0.5628

epoch:499/50, training loss:1.4896430969238281
Train Acc 0.5628
 Acc 0.5630
new best val f1: 0.562986525467262
meta,dgl,1,498,17.1947,0.5630

epoch:500/50, training loss:1.4889215230941772
Train Acc 0.5630
 Acc 0.5631
new best val f1: 0.563078726571041
meta,dgl,1,499,17.2283,0.5631

epoch:501/50, training loss:1.4882044792175293
Train Acc 0.5632
 Acc 0.5632
new best val f1: 0.5632499571923447
meta,dgl,1,500,17.2620,0.5632

epoch:502/50, training loss:1.4874842166900635
Train Acc 0.5633
 Acc 0.5634
new best val f1: 0.5634277736067754
meta,dgl,1,501,17.2956,0.5634

epoch:503/50, training loss:1.4867699146270752
Train Acc 0.5635
 Acc 0.5635
new best val f1: 0.5635265605036814
meta,dgl,1,502,17.3301,0.5635

epoch:504/50, training loss:1.4860568046569824
Train Acc 0.5637
 Acc 0.5637
new best val f1: 0.563678033745604
meta,dgl,1,503,17.3646,0.5637

epoch:505/50, training loss:1.4853463172912598
Train Acc 0.5639
 Acc 0.5638
new best val f1: 0.5637965780218912
meta,dgl,1,504,17.3983,0.5638

epoch:506/50, training loss:1.4846354722976685
Train Acc 0.5640
 Acc 0.5639
new best val f1: 0.5639414654706867
meta,dgl,1,505,17.4320,0.5639

epoch:507/50, training loss:1.4839284420013428
Train Acc 0.5642
 Acc 0.5641
new best val f1: 0.5641258676782445
meta,dgl,1,506,17.4656,0.5641

epoch:508/50, training loss:1.4832236766815186
Train Acc 0.5644
 Acc 0.5643
new best val f1: 0.5642575835407858
meta,dgl,1,507,17.4993,0.5643

epoch:509/50, training loss:1.4825217723846436
Train Acc 0.5645
 Acc 0.5644
new best val f1: 0.5644090567827084
meta,dgl,1,508,17.5330,0.5644

epoch:510/50, training loss:1.4818198680877686
Train Acc 0.5647
 Acc 0.5645
new best val f1: 0.5645473584383768
meta,dgl,1,509,17.5675,0.5645

epoch:511/50, training loss:1.481121301651001
Train Acc 0.5648
 Acc 0.5648
new best val f1: 0.5647581038184428
meta,dgl,1,510,17.6020,0.5648

epoch:512/50, training loss:1.4804227352142334
Train Acc 0.5650
 Acc 0.5649
new best val f1: 0.5649029912672383
meta,dgl,1,511,17.6365,0.5649

epoch:513/50, training loss:1.4797286987304688
Train Acc 0.5652
 Acc 0.5651
new best val f1: 0.5650610503022879
meta,dgl,1,512,17.6701,0.5651

epoch:514/50, training loss:1.479036808013916
Train Acc 0.5654
 Acc 0.5653
new best val f1: 0.5652586240961
meta,dgl,1,513,17.7036,0.5653

epoch:515/50, training loss:1.4783453941345215
Train Acc 0.5655
 Acc 0.5654
new best val f1: 0.5653837541655141
meta,dgl,1,514,17.7381,0.5654

epoch:516/50, training loss:1.477657437324524
Train Acc 0.5656
 Acc 0.5656
new best val f1: 0.5656274285112156
meta,dgl,1,515,17.7725,0.5656

epoch:517/50, training loss:1.47697114944458
Train Acc 0.5659
 Acc 0.5659
new best val f1: 0.565857931270663
meta,dgl,1,516,17.8062,0.5659

epoch:518/50, training loss:1.4762871265411377
Train Acc 0.5660
 Acc 0.5660
new best val f1: 0.5660094045125854
meta,dgl,1,517,17.8399,0.5660

epoch:519/50, training loss:1.475602149963379
Train Acc 0.5662
 Acc 0.5662
new best val f1: 0.5662267356857786
meta,dgl,1,518,17.8735,0.5662

epoch:520/50, training loss:1.4749212265014648
Train Acc 0.5664
 Acc 0.5663
new best val f1: 0.5663452799620659
meta,dgl,1,519,17.9072,0.5663

epoch:521/50, training loss:1.4742416143417358
Train Acc 0.5665
 Acc 0.5665
new best val f1: 0.5665296821696237
meta,dgl,1,520,17.9417,0.5665

epoch:522/50, training loss:1.4735653400421143
Train Acc 0.5667
 Acc 0.5667
new best val f1: 0.5667272559634356
meta,dgl,1,521,17.9752,0.5667

epoch:523/50, training loss:1.472890019416809
Train Acc 0.5669
 Acc 0.5669
new best val f1: 0.5669182439641206
meta,dgl,1,522,18.0088,0.5669

epoch:524/50, training loss:1.4722161293029785
Train Acc 0.5671
 Acc 0.5671
new best val f1: 0.5671289893441868
meta,dgl,1,523,18.0424,0.5671

epoch:525/50, training loss:1.471545696258545
Train Acc 0.5672
 Acc 0.5673
new best val f1: 0.5673463205173799
meta,dgl,1,524,18.0760,0.5673

epoch:526/50, training loss:1.4708763360977173
Train Acc 0.5674
 Acc 0.5676
new best val f1: 0.5675636516905731
meta,dgl,1,525,18.1096,0.5676

epoch:527/50, training loss:1.4702086448669434
Train Acc 0.5676
 Acc 0.5677
new best val f1: 0.567748053898131
meta,dgl,1,526,18.1441,0.5677

epoch:528/50, training loss:1.4695435762405396
Train Acc 0.5677
 Acc 0.5680
new best val f1: 0.567958799278197
meta,dgl,1,527,18.1778,0.5680

epoch:529/50, training loss:1.468879222869873
Train Acc 0.5679
 Acc 0.5681
new best val f1: 0.5681234441063737
meta,dgl,1,528,18.2115,0.5681

epoch:530/50, training loss:1.468218207359314
Train Acc 0.5681
 Acc 0.5682
new best val f1: 0.5682222310032797
meta,dgl,1,529,18.2453,0.5682

epoch:531/50, training loss:1.4675590991973877
Train Acc 0.5682
 Acc 0.5684
new best val f1: 0.5684000474177106
meta,dgl,1,530,18.2789,0.5684

epoch:532/50, training loss:1.4668996334075928
Train Acc 0.5684
 Acc 0.5686
new best val f1: 0.5685976212115225
meta,dgl,1,531,18.3126,0.5686

epoch:533/50, training loss:1.4662446975708008
Train Acc 0.5685
 Acc 0.5688
new best val f1: 0.5687754376259533
meta,dgl,1,532,18.3471,0.5688

epoch:534/50, training loss:1.4655919075012207
Train Acc 0.5687
 Acc 0.5690
new best val f1: 0.5689861830060194
meta,dgl,1,533,18.3807,0.5690

epoch:535/50, training loss:1.4649393558502197
Train Acc 0.5689
 Acc 0.5692
new best val f1: 0.5692364431448479
meta,dgl,1,534,18.4152,0.5692

epoch:536/50, training loss:1.4642878770828247
Train Acc 0.5691
 Acc 0.5695
new best val f1: 0.5694735316974223
meta,dgl,1,535,18.4488,0.5695

epoch:537/50, training loss:1.4636399745941162
Train Acc 0.5693
 Acc 0.5697
new best val f1: 0.5697237918362509
meta,dgl,1,536,18.4825,0.5697

epoch:538/50, training loss:1.462992787361145
Train Acc 0.5695
 Acc 0.5699
new best val f1: 0.5698884366644275
meta,dgl,1,537,18.5162,0.5699

epoch:539/50, training loss:1.4623502492904663
Train Acc 0.5696
 Acc 0.5701
new best val f1: 0.5700794246651124
meta,dgl,1,538,18.5510,0.5701

epoch:540/50, training loss:1.461706519126892
Train Acc 0.5698
 Acc 0.5703
new best val f1: 0.5702638268726703
meta,dgl,1,539,18.5847,0.5703

epoch:541/50, training loss:1.461065411567688
Train Acc 0.5700
 Acc 0.5705
new best val f1: 0.5704548148733551
meta,dgl,1,540,18.6184,0.5705

epoch:542/50, training loss:1.4604243040084839
Train Acc 0.5701
 Acc 0.5706
new best val f1: 0.5706260454946589
meta,dgl,1,541,18.6521,0.5706

epoch:543/50, training loss:1.4597890377044678
Train Acc 0.5703
 Acc 0.5708
new best val f1: 0.5708499624609792
meta,dgl,1,542,18.6857,0.5708

epoch:544/50, training loss:1.4591538906097412
Train Acc 0.5705
 Acc 0.5710
new best val f1: 0.5710146072891559
meta,dgl,1,543,18.7194,0.5710

epoch:545/50, training loss:1.458518385887146
Train Acc 0.5707
 Acc 0.5711
new best val f1: 0.5710936368066807
meta,dgl,1,544,18.7530,0.5711

epoch:546/50, training loss:1.4578875303268433
Train Acc 0.5708
 Acc 0.5713
new best val f1: 0.5713241395661279
meta,dgl,1,545,18.7876,0.5713

epoch:547/50, training loss:1.457257628440857
Train Acc 0.5710
 Acc 0.5715
new best val f1: 0.571528299153067
meta,dgl,1,546,18.8221,0.5715

epoch:548/50, training loss:1.456628441810608
Train Acc 0.5712
 Acc 0.5717
new best val f1: 0.571725872946879
meta,dgl,1,547,18.8556,0.5717

epoch:549/50, training loss:1.4560041427612305
Train Acc 0.5714
 Acc 0.5719
new best val f1: 0.5718707603956744
meta,dgl,1,548,18.8891,0.5719

epoch:550/50, training loss:1.4553771018981934
Train Acc 0.5715
 Acc 0.5720
new best val f1: 0.5720419910169782
meta,dgl,1,549,18.9227,0.5720

epoch:551/50, training loss:1.4547573328018188
Train Acc 0.5717
 Acc 0.5722
new best val f1: 0.5721605352932654
meta,dgl,1,550,18.9564,0.5722

epoch:552/50, training loss:1.4541337490081787
Train Acc 0.5718
 Acc 0.5723
new best val f1: 0.5723054227420609
meta,dgl,1,551,18.9900,0.5723

epoch:553/50, training loss:1.4535139799118042
Train Acc 0.5720
 Acc 0.5725
new best val f1: 0.572522753915254
meta,dgl,1,552,19.0245,0.5725

epoch:554/50, training loss:1.4528963565826416
Train Acc 0.5721
 Acc 0.5726
new best val f1: 0.5726478839846683
meta,dgl,1,553,19.0581,0.5726

epoch:555/50, training loss:1.4522793292999268
Train Acc 0.5723
 Acc 0.5729
new best val f1: 0.5728520435716074
meta,dgl,1,554,19.0918,0.5729

epoch:556/50, training loss:1.4516656398773193
Train Acc 0.5725
 Acc 0.5730
new best val f1: 0.5730035168135299
meta,dgl,1,555,19.1254,0.5730

epoch:557/50, training loss:1.451053500175476
Train Acc 0.5726
 Acc 0.5731
new best val f1: 0.5731352326760711
meta,dgl,1,556,19.1591,0.5731

epoch:558/50, training loss:1.4504436254501343
Train Acc 0.5728
 Acc 0.5733
new best val f1: 0.5733328064698832
meta,dgl,1,557,19.1936,0.5733

epoch:559/50, training loss:1.4498331546783447
Train Acc 0.5730
 Acc 0.5735
new best val f1: 0.573517208677441
meta,dgl,1,558,19.2272,0.5735

epoch:560/50, training loss:1.4492249488830566
Train Acc 0.5732
 Acc 0.5737
new best val f1: 0.5737016108849988
meta,dgl,1,559,19.2618,0.5737

epoch:561/50, training loss:1.4486206769943237
Train Acc 0.5734
 Acc 0.5739
new best val f1: 0.5738596699200484
meta,dgl,1,560,19.2954,0.5739

epoch:562/50, training loss:1.4480164051055908
Train Acc 0.5736
 Acc 0.5740
new best val f1: 0.5740374863344793
meta,dgl,1,561,19.3290,0.5740

epoch:563/50, training loss:1.4474151134490967
Train Acc 0.5737
 Acc 0.5742
new best val f1: 0.5741692021970206
meta,dgl,1,562,19.3626,0.5742

epoch:564/50, training loss:1.4468141794204712
Train Acc 0.5739
 Acc 0.5743
new best val f1: 0.574300918059562
meta,dgl,1,563,19.3972,0.5743

epoch:565/50, training loss:1.4462144374847412
Train Acc 0.5741
 Acc 0.5744
new best val f1: 0.5744326339221032
meta,dgl,1,564,19.4310,0.5744

epoch:566/50, training loss:1.4456164836883545
Train Acc 0.5743
 Acc 0.5746
new best val f1: 0.5746367935090423
meta,dgl,1,565,19.4657,0.5746

epoch:567/50, training loss:1.4450194835662842
Train Acc 0.5745
 Acc 0.5748
new best val f1: 0.5748211957166002
meta,dgl,1,566,19.4994,0.5748

epoch:568/50, training loss:1.4444289207458496
Train Acc 0.5746
 Acc 0.5750
new best val f1: 0.5749792547516498
meta,dgl,1,567,19.5332,0.5750

epoch:569/50, training loss:1.4438360929489136
Train Acc 0.5748
 Acc 0.5752
new best val f1: 0.5751768285454617
meta,dgl,1,568,19.5669,0.5752

epoch:570/50, training loss:1.4432436227798462
Train Acc 0.5749
 Acc 0.5753
new best val f1: 0.5753217159942572
meta,dgl,1,569,19.6015,0.5753

epoch:571/50, training loss:1.44265615940094
Train Acc 0.5751
 Acc 0.5755
new best val f1: 0.5755061182018151
meta,dgl,1,570,19.6354,0.5755

epoch:572/50, training loss:1.4420678615570068
Train Acc 0.5753
 Acc 0.5757
new best val f1: 0.575703691995627
meta,dgl,1,571,19.6691,0.5757

epoch:573/50, training loss:1.4414832592010498
Train Acc 0.5754
 Acc 0.5759
new best val f1: 0.5759407805482014
meta,dgl,1,572,19.7027,0.5759

epoch:574/50, training loss:1.4408974647521973
Train Acc 0.5756
 Acc 0.5761
new best val f1: 0.5761120111695052
meta,dgl,1,573,19.7365,0.5761

epoch:575/50, training loss:1.4403159618377686
Train Acc 0.5758
 Acc 0.5763
new best val f1: 0.5763095849633171
meta,dgl,1,574,19.7702,0.5763

epoch:576/50, training loss:1.4397339820861816
Train Acc 0.5759
 Acc 0.5765
new best val f1: 0.5765071587571291
meta,dgl,1,575,19.8039,0.5765

epoch:577/50, training loss:1.4391549825668335
Train Acc 0.5761
 Acc 0.5766
new best val f1: 0.5766322888265434
meta,dgl,1,576,19.8385,0.5766

epoch:578/50, training loss:1.4385764598846436
Train Acc 0.5763
 Acc 0.5768
new best val f1: 0.576790347861593
meta,dgl,1,577,19.8721,0.5768

epoch:579/50, training loss:1.4380007982254028
Train Acc 0.5765
 Acc 0.5770
new best val f1: 0.5769549926897696
meta,dgl,1,578,19.9058,0.5770

epoch:580/50, training loss:1.4374268054962158
Train Acc 0.5767
 Acc 0.5770
new best val f1: 0.5770406080004215
meta,dgl,1,579,19.9394,0.5770

epoch:581/50, training loss:1.4368528127670288
Train Acc 0.5769
 Acc 0.5772
new best val f1: 0.5772052528285981
meta,dgl,1,580,19.9739,0.5772

epoch:582/50, training loss:1.436280608177185
Train Acc 0.5770
 Acc 0.5774
new best val f1: 0.5774225840017914
meta,dgl,1,581,20.0075,0.5774

epoch:583/50, training loss:1.4357093572616577
Train Acc 0.5772
 Acc 0.5775
new best val f1: 0.5775345424849515
meta,dgl,1,582,20.0412,0.5775

epoch:584/50, training loss:1.435141921043396
Train Acc 0.5773
 Acc 0.5778
new best val f1: 0.5777979742100341
meta,dgl,1,583,20.0748,0.5778

epoch:585/50, training loss:1.4345736503601074
Train Acc 0.5776
 Acc 0.5779
new best val f1: 0.5779165184863213
meta,dgl,1,584,20.1084,0.5779

epoch:586/50, training loss:1.4340100288391113
Train Acc 0.5777
 Acc 0.5781
new best val f1: 0.5781075064870063
meta,dgl,1,585,20.1421,0.5781

epoch:587/50, training loss:1.4334465265274048
Train Acc 0.5779
 Acc 0.5783
new best val f1: 0.5782721513151828
meta,dgl,1,586,20.1756,0.5783

epoch:588/50, training loss:1.432883381843567
Train Acc 0.5781
 Acc 0.5784
new best val f1: 0.5783709382120888
meta,dgl,1,587,20.2093,0.5784

epoch:589/50, training loss:1.4323211908340454
Train Acc 0.5782
 Acc 0.5785
new best val f1: 0.5785289972471385
meta,dgl,1,588,20.2437,0.5785

epoch:590/50, training loss:1.4317606687545776
Train Acc 0.5783
 Acc 0.5787
new best val f1: 0.5786541273165527
meta,dgl,1,589,20.2772,0.5787

epoch:591/50, training loss:1.431204080581665
Train Acc 0.5785
 Acc 0.5789
new best val f1: 0.578878044282873
meta,dgl,1,590,20.3116,0.5789

epoch:592/50, training loss:1.4306472539901733
Train Acc 0.5787
 Acc 0.5791
new best val f1: 0.5790690322835579
meta,dgl,1,591,20.3453,0.5791

epoch:593/50, training loss:1.4300919771194458
Train Acc 0.5790
 Acc 0.5792
new best val f1: 0.5792336771117346
meta,dgl,1,592,20.3789,0.5792

epoch:594/50, training loss:1.4295374155044556
Train Acc 0.5791
 Acc 0.5795
new best val f1: 0.5794641798711819
meta,dgl,1,593,20.4125,0.5795

epoch:595/50, training loss:1.428985357284546
Train Acc 0.5793
 Acc 0.5797
new best val f1: 0.5796551678718668
meta,dgl,1,594,20.4462,0.5797

epoch:596/50, training loss:1.4284337759017944
Train Acc 0.5795
 Acc 0.5798
new best val f1: 0.5798066411137893
meta,dgl,1,595,20.4808,0.5798

epoch:597/50, training loss:1.4278854131698608
Train Acc 0.5796
 Acc 0.5800
new best val f1: 0.5800108007007284
meta,dgl,1,596,20.5146,0.5800

epoch:598/50, training loss:1.4273358583450317
Train Acc 0.5798
 Acc 0.5802
new best val f1: 0.5801556881495239
meta,dgl,1,597,20.5491,0.5802

epoch:599/50, training loss:1.4267891645431519
Train Acc 0.5800
 Acc 0.5804
new best val f1: 0.5803532619433358
meta,dgl,1,598,20.5828,0.5804

epoch:600/50, training loss:1.4262446165084839
Train Acc 0.5802
 Acc 0.5806
new best val f1: 0.5805508357371478
meta,dgl,1,599,20.6164,0.5806

epoch:601/50, training loss:1.4256995916366577
Train Acc 0.5804
 Acc 0.5807
new best val f1: 0.5807352379447057
meta,dgl,1,600,20.6510,0.5807

epoch:602/50, training loss:1.4251567125320435
Train Acc 0.5807
 Acc 0.5808
new best val f1: 0.5808208532553576
meta,dgl,1,601,20.6846,0.5808

epoch:603/50, training loss:1.4246156215667725
Train Acc 0.5807
 Acc 0.5811
new best val f1: 0.581064527601059
meta,dgl,1,602,20.7183,0.5811

epoch:604/50, training loss:1.4240745306015015
Train Acc 0.5810
 Acc 0.5812
new best val f1: 0.5812225866361086
meta,dgl,1,603,20.7528,0.5812

epoch:605/50, training loss:1.4235353469848633
Train Acc 0.5811
 Acc 0.5814
new best val f1: 0.5814399178093018
meta,dgl,1,604,20.7866,0.5814

epoch:606/50, training loss:1.4230005741119385
Train Acc 0.5814
 Acc 0.5816
new best val f1: 0.5816177342237325
meta,dgl,1,605,20.8202,0.5816

epoch:607/50, training loss:1.4224635362625122
Train Acc 0.5816
 Acc 0.5818
new best val f1: 0.5818284796037987
meta,dgl,1,606,20.8538,0.5818

epoch:608/50, training loss:1.4219274520874023
Train Acc 0.5818
 Acc 0.5820
new best val f1: 0.58196019546634
meta,dgl,1,607,20.8874,0.5820

epoch:609/50, training loss:1.4213942289352417
Train Acc 0.5819
 Acc 0.5822
new best val f1: 0.5821709408464061
meta,dgl,1,608,20.9221,0.5822

epoch:610/50, training loss:1.4208627939224243
Train Acc 0.5821
 Acc 0.5824
new best val f1: 0.5823882720195993
meta,dgl,1,609,20.9558,0.5824

epoch:611/50, training loss:1.420331597328186
Train Acc 0.5823
 Acc 0.5826
new best val f1: 0.5826121889859196
meta,dgl,1,610,20.9896,0.5826

epoch:612/50, training loss:1.4198020696640015
Train Acc 0.5825
 Acc 0.5828
new best val f1: 0.5828229343659856
meta,dgl,1,611,21.0232,0.5828

epoch:613/50, training loss:1.419272780418396
Train Acc 0.5827
 Acc 0.5831
new best val f1: 0.583053437125433
meta,dgl,1,612,21.0570,0.5831

epoch:614/50, training loss:1.4187462329864502
Train Acc 0.5829
 Acc 0.5833
new best val f1: 0.5833036972642616
meta,dgl,1,613,21.0916,0.5833

epoch:615/50, training loss:1.4182204008102417
Train Acc 0.5832
 Acc 0.5836
new best val f1: 0.5836198153343607
meta,dgl,1,614,21.1251,0.5836

epoch:616/50, training loss:1.417695164680481
Train Acc 0.5835
 Acc 0.5837
new best val f1: 0.5836922590587584
meta,dgl,1,615,21.1595,0.5837

epoch:617/50, training loss:1.417170524597168
Train Acc 0.5836
 Acc 0.5839
new best val f1: 0.5838700754731893
meta,dgl,1,616,21.1932,0.5839

epoch:618/50, training loss:1.416649580001831
Train Acc 0.5838
 Acc 0.5841
new best val f1: 0.5840808208532554
meta,dgl,1,617,21.2269,0.5841

epoch:619/50, training loss:1.4161285161972046
Train Acc 0.5840
 Acc 0.5843
new best val f1: 0.5842718088539403
meta,dgl,1,618,21.2615,0.5843

epoch:620/50, training loss:1.415608286857605
Train Acc 0.5842
 Acc 0.5844
new best val f1: 0.5844101105096087
meta,dgl,1,619,21.3011,0.5844

epoch:621/50, training loss:1.415089726448059
Train Acc 0.5844
 Acc 0.5846
new best val f1: 0.5845549979584042
meta,dgl,1,620,21.3354,0.5846

epoch:622/50, training loss:1.4145721197128296
Train Acc 0.5846
 Acc 0.5847
new best val f1: 0.5847262285797079
meta,dgl,1,621,21.3691,0.5847

epoch:623/50, training loss:1.4140548706054688
Train Acc 0.5848
 Acc 0.5850
new best val f1: 0.5849501455460281
meta,dgl,1,622,21.4038,0.5850

epoch:624/50, training loss:1.4135416746139526
Train Acc 0.5850
 Acc 0.5852
new best val f1: 0.5852069914779837
meta,dgl,1,623,21.4374,0.5852

epoch:625/50, training loss:1.4130254983901978
Train Acc 0.5852
 Acc 0.5854
new best val f1: 0.5854309084443039
meta,dgl,1,624,21.4719,0.5854

epoch:626/50, training loss:1.4125124216079712
Train Acc 0.5855
 Acc 0.5856
new best val f1: 0.585635068031243
meta,dgl,1,625,21.5056,0.5856

epoch:627/50, training loss:1.4120008945465088
Train Acc 0.5857
 Acc 0.5858
new best val f1: 0.5857931270662926
meta,dgl,1,626,21.5392,0.5858

epoch:628/50, training loss:1.4114903211593628
Train Acc 0.5859
 Acc 0.5860
new best val f1: 0.5859907008601046
meta,dgl,1,627,21.5728,0.5860

epoch:629/50, training loss:1.4109808206558228
Train Acc 0.5861
 Acc 0.5862
new best val f1: 0.5861553456882812
meta,dgl,1,628,21.6073,0.5862

epoch:630/50, training loss:1.410473108291626
Train Acc 0.5862
 Acc 0.5864
new best val f1: 0.5864121916202368
meta,dgl,1,629,21.6417,0.5864

epoch:631/50, training loss:1.4099653959274292
Train Acc 0.5864
 Acc 0.5867
new best val f1: 0.5867217238972089
meta,dgl,1,630,21.6753,0.5867

epoch:632/50, training loss:1.4094589948654175
Train Acc 0.5867
 Acc 0.5869
new best val f1: 0.5868863687253856
meta,dgl,1,631,21.7089,0.5869

epoch:633/50, training loss:1.4089536666870117
Train Acc 0.5869
 Acc 0.5870
new best val f1: 0.5869785698291645
meta,dgl,1,632,21.7434,0.5870

epoch:634/50, training loss:1.4084489345550537
Train Acc 0.5870
 Acc 0.5872
new best val f1: 0.5872485873473743
meta,dgl,1,633,21.7778,0.5872

epoch:635/50, training loss:1.4079455137252808
Train Acc 0.5873
 Acc 0.5874
new best val f1: 0.5874329895549321
meta,dgl,1,634,21.8114,0.5874

epoch:636/50, training loss:1.407442569732666
Train Acc 0.5875
 Acc 0.5878
new best val f1: 0.5877754507975396
meta,dgl,1,635,21.8450,0.5878

epoch:637/50, training loss:1.4069406986236572
Train Acc 0.5877
 Acc 0.5880
new best val f1: 0.5880322967294951
meta,dgl,1,636,21.8786,0.5880

epoch:638/50, training loss:1.4064408540725708
Train Acc 0.5880
 Acc 0.5882
new best val f1: 0.58822328473018
meta,dgl,1,637,21.9130,0.5882

epoch:639/50, training loss:1.4059442281723022
Train Acc 0.5882
 Acc 0.5884
new best val f1: 0.5883945153514838
meta,dgl,1,638,21.9466,0.5884

epoch:640/50, training loss:1.4054454565048218
Train Acc 0.5884
 Acc 0.5885
new best val f1: 0.5885328170071522
meta,dgl,1,639,21.9802,0.5885

epoch:641/50, training loss:1.4049499034881592
Train Acc 0.5885
 Acc 0.5888
new best val f1: 0.5887896629391077
meta,dgl,1,640,22.0139,0.5888

epoch:642/50, training loss:1.4044532775878906
Train Acc 0.5888
 Acc 0.5890
new best val f1: 0.589013579905428
meta,dgl,1,641,22.0483,0.5890

epoch:643/50, training loss:1.4039555788040161
Train Acc 0.5890
 Acc 0.5892
new best val f1: 0.58921115369924
meta,dgl,1,642,22.0820,0.5892

epoch:644/50, training loss:1.4034624099731445
Train Acc 0.5892
 Acc 0.5894
new best val f1: 0.5894350706655602
meta,dgl,1,643,22.1156,0.5894

epoch:645/50, training loss:1.4029710292816162
Train Acc 0.5895
 Acc 0.5897
new best val f1: 0.5896524018387534
meta,dgl,1,644,22.1493,0.5897

epoch:646/50, training loss:1.402477741241455
Train Acc 0.5897
 Acc 0.5899
new best val f1: 0.5898631472188196
meta,dgl,1,645,22.1838,0.5899

epoch:647/50, training loss:1.4019862413406372
Train Acc 0.5898
 Acc 0.5901
new best val f1: 0.5901199931507751
meta,dgl,1,646,22.2174,0.5901

epoch:648/50, training loss:1.401496410369873
Train Acc 0.5900
 Acc 0.5903
new best val f1: 0.5902978095652059
meta,dgl,1,647,22.2511,0.5903

epoch:649/50, training loss:1.4010083675384521
Train Acc 0.5903
 Acc 0.5905
new best val f1: 0.590495383359018
meta,dgl,1,648,22.2847,0.5905

epoch:650/50, training loss:1.4005205631256104
Train Acc 0.5905
 Acc 0.5908
new best val f1: 0.5907522292909735
meta,dgl,1,649,22.3183,0.5908

epoch:651/50, training loss:1.4000319242477417
Train Acc 0.5908
 Acc 0.5909
new best val f1: 0.5909168741191502
meta,dgl,1,650,22.3520,0.5909

epoch:652/50, training loss:1.3995448350906372
Train Acc 0.5909
 Acc 0.5911
new best val f1: 0.5911210337060893
meta,dgl,1,651,22.3856,0.5911

epoch:653/50, training loss:1.399059772491455
Train Acc 0.5911
 Acc 0.5914
new best val f1: 0.5914371517761884
meta,dgl,1,652,22.4192,0.5914

epoch:654/50, training loss:1.398576021194458
Train Acc 0.5914
 Acc 0.5916
new best val f1: 0.5916413113631275
meta,dgl,1,653,22.4529,0.5916

epoch:655/50, training loss:1.3980919122695923
Train Acc 0.5916
 Acc 0.5918
new best val f1: 0.5918454709500666
meta,dgl,1,654,22.4866,0.5918

epoch:656/50, training loss:1.3976086378097534
Train Acc 0.5919
 Acc 0.5920
new best val f1: 0.5920364589507514
meta,dgl,1,655,22.5202,0.5920

epoch:657/50, training loss:1.397127389907837
Train Acc 0.5921
 Acc 0.5922
new best val f1: 0.5922274469514364
meta,dgl,1,656,22.5539,0.5922

epoch:658/50, training loss:1.3966457843780518
Train Acc 0.5923
 Acc 0.5924
new best val f1: 0.5924052633658672
meta,dgl,1,657,22.5876,0.5924

epoch:659/50, training loss:1.3961676359176636
Train Acc 0.5924
 Acc 0.5927
new best val f1: 0.5927411388153475
meta,dgl,1,658,22.6221,0.5927

epoch:660/50, training loss:1.3956875801086426
Train Acc 0.5927
 Acc 0.5930
new best val f1: 0.592984813161049
meta,dgl,1,659,22.6557,0.5930

epoch:661/50, training loss:1.3952094316482544
Train Acc 0.5929
 Acc 0.5931
new best val f1: 0.5931362864029716
meta,dgl,1,660,22.6893,0.5931

epoch:662/50, training loss:1.3947314023971558
Train Acc 0.5932
 Acc 0.5932
new best val f1: 0.5932153159204963
meta,dgl,1,661,22.7229,0.5932

epoch:663/50, training loss:1.3942532539367676
Train Acc 0.5933
 Acc 0.5934
new best val f1: 0.5934063039211812
meta,dgl,1,662,22.7565,0.5934

epoch:664/50, training loss:1.3937782049179077
Train Acc 0.5935
 Acc 0.5936
new best val f1: 0.5936104635081203
meta,dgl,1,663,22.7909,0.5936

epoch:665/50, training loss:1.3933025598526
Train Acc 0.5937
 Acc 0.5938
new best val f1: 0.5938409662675675
meta,dgl,1,664,22.8245,0.5938

epoch:666/50, training loss:1.392829179763794
Train Acc 0.5939
 Acc 0.5941
new best val f1: 0.594084640613269
meta,dgl,1,665,22.8588,0.5941

epoch:667/50, training loss:1.3923563957214355
Train Acc 0.5942
 Acc 0.5944
new best val f1: 0.5944468592352578
meta,dgl,1,666,22.8924,0.5944

epoch:668/50, training loss:1.3918828964233398
Train Acc 0.5946
 Acc 0.5946
new best val f1: 0.5946444330290697
meta,dgl,1,667,22.9259,0.5946

epoch:669/50, training loss:1.3914107084274292
Train Acc 0.5948
 Acc 0.5949
new best val f1: 0.59486834999539
meta,dgl,1,668,22.9604,0.5949

epoch:670/50, training loss:1.3909403085708618
Train Acc 0.5950
 Acc 0.5950
new best val f1: 0.5950329948235666
meta,dgl,1,669,22.9950,0.5950

epoch:671/50, training loss:1.390470027923584
Train Acc 0.5951
 Acc 0.5952
new best val f1: 0.5952437402036327
meta,dgl,1,670,23.0286,0.5952

epoch:672/50, training loss:1.3900004625320435
Train Acc 0.5953
 Acc 0.5955
new best val f1: 0.5954808287562071
meta,dgl,1,671,23.0623,0.5955

epoch:673/50, training loss:1.3895312547683716
Train Acc 0.5956
 Acc 0.5957
new best val f1: 0.5957047457225274
meta,dgl,1,672,23.0959,0.5957

epoch:674/50, training loss:1.3890633583068848
Train Acc 0.5958
 Acc 0.5959
new best val f1: 0.5959023195163393
meta,dgl,1,673,23.1303,0.5959

epoch:675/50, training loss:1.3885940313339233
Train Acc 0.5960
 Acc 0.5961
new best val f1: 0.5961328222757867
meta,dgl,1,674,23.1639,0.5961

epoch:676/50, training loss:1.388126254081726
Train Acc 0.5962
 Acc 0.5964
new best val f1: 0.5964423545527587
meta,dgl,1,675,23.1975,0.5964

epoch:677/50, training loss:1.3876627683639526
Train Acc 0.5964
 Acc 0.5966
new best val f1: 0.5965872420015542
meta,dgl,1,676,23.2311,0.5966

epoch:678/50, training loss:1.387197732925415
Train Acc 0.5966
 Acc 0.5967
new best val f1: 0.5967123720709685
meta,dgl,1,677,23.2647,0.5967

epoch:679/50, training loss:1.3867323398590088
Train Acc 0.5968
 Acc 0.5969
new best val f1: 0.5968770168991452
meta,dgl,1,678,23.2990,0.5969

epoch:680/50, training loss:1.3862683773040771
Train Acc 0.5969
 Acc 0.5971
new best val f1: 0.5971272770379736
meta,dgl,1,679,23.3327,0.5971

epoch:681/50, training loss:1.3858059644699097
Train Acc 0.5972
 Acc 0.5973
new best val f1: 0.5973050934524045
meta,dgl,1,680,23.3663,0.5973

epoch:682/50, training loss:1.3853445053100586
Train Acc 0.5974
 Acc 0.5975
new best val f1: 0.5974960814530894
meta,dgl,1,681,23.3999,0.5975

epoch:683/50, training loss:1.3848804235458374
Train Acc 0.5975
 Acc 0.5977
new best val f1: 0.5976804836606473
meta,dgl,1,682,23.4335,0.5977

epoch:684/50, training loss:1.3844202756881714
Train Acc 0.5978
 Acc 0.5978
new best val f1: 0.5978121995231885
meta,dgl,1,683,23.4679,0.5978

epoch:685/50, training loss:1.3839571475982666
Train Acc 0.5979
 Acc 0.5980
new best val f1: 0.597957086971984
meta,dgl,1,684,23.5015,0.5980

epoch:686/50, training loss:1.3835012912750244
Train Acc 0.5981
 Acc 0.5982
new best val f1: 0.5981678323520502
meta,dgl,1,685,23.5360,0.5982

epoch:687/50, training loss:1.3830403089523315
Train Acc 0.5983
 Acc 0.5984
new best val f1: 0.5984115066977516
meta,dgl,1,686,23.5705,0.5984

epoch:688/50, training loss:1.3825809955596924
Train Acc 0.5985
 Acc 0.5986
new best val f1: 0.5985827373190553
meta,dgl,1,687,23.6042,0.5986

epoch:689/50, training loss:1.382123589515686
Train Acc 0.5987
 Acc 0.5989
new best val f1: 0.5988856838029004
meta,dgl,1,688,23.6378,0.5989

epoch:690/50, training loss:1.3816648721694946
Train Acc 0.5990
 Acc 0.5990
new best val f1: 0.5989844706998064
meta,dgl,1,689,23.6714,0.5990

epoch:691/50, training loss:1.381209135055542
Train Acc 0.5992
 Acc 0.5992
new best val f1: 0.5991820444936183
meta,dgl,1,690,23.7050,0.5992

epoch:692/50, training loss:1.380755066871643
Train Acc 0.5994
 Acc 0.5994
new best val f1: 0.5994059614599386
meta,dgl,1,691,23.7387,0.5994

epoch:693/50, training loss:1.3802977800369263
Train Acc 0.5996
 Acc 0.5996
new best val f1: 0.5995969494606236
meta,dgl,1,692,23.7733,0.5996

epoch:694/50, training loss:1.3798421621322632
Train Acc 0.5998
 Acc 0.5999
new best val f1: 0.5998603811857062
meta,dgl,1,693,23.8070,0.5999

epoch:695/50, training loss:1.379386067390442
Train Acc 0.6000
 Acc 0.6000
new best val f1: 0.6000184402207558
meta,dgl,1,694,23.8407,0.6000

epoch:696/50, training loss:1.3789339065551758
Train Acc 0.6002
 Acc 0.6001
new best val f1: 0.600136984497043
meta,dgl,1,695,23.8751,0.6001

epoch:697/50, training loss:1.3784805536270142
Train Acc 0.6003
 Acc 0.6002
new best val f1: 0.6002423571870761
meta,dgl,1,696,23.9097,0.6002

epoch:698/50, training loss:1.378028392791748
Train Acc 0.6004
 Acc 0.6006
new best val f1: 0.6005518894640481
meta,dgl,1,697,23.9433,0.6006

epoch:699/50, training loss:1.377575159072876
Train Acc 0.6007
 Acc 0.6008
new best val f1: 0.600828492775385
meta,dgl,1,698,23.9778,0.6008

epoch:700/50, training loss:1.3771252632141113
Train Acc 0.6010
 Acc 0.6010
new best val f1: 0.6009997233966886
meta,dgl,1,699,24.0115,0.6010

epoch:701/50, training loss:1.3766709566116333
Train Acc 0.6011
 Acc 0.6012
new best val f1: 0.6011841256042465
meta,dgl,1,700,24.0451,0.6012

epoch:702/50, training loss:1.3762216567993164
Train Acc 0.6013
 Acc 0.6014
new best val f1: 0.6013751136049315
meta,dgl,1,701,24.0788,0.6014

epoch:703/50, training loss:1.3757721185684204
Train Acc 0.6015
 Acc 0.6016
new best val f1: 0.6015858589849976
meta,dgl,1,702,24.1133,0.6016

epoch:704/50, training loss:1.3753236532211304
Train Acc 0.6017
 Acc 0.6018
new best val f1: 0.6018031901581907
meta,dgl,1,703,24.1469,0.6018

epoch:705/50, training loss:1.374873161315918
Train Acc 0.6019
 Acc 0.6020
new best val f1: 0.6020073497451298
meta,dgl,1,704,24.1805,0.6020

epoch:706/50, training loss:1.3744243383407593
Train Acc 0.6022
 Acc 0.6022
new best val f1: 0.6021785803664336
meta,dgl,1,705,24.2141,0.6022

epoch:707/50, training loss:1.3739770650863647
Train Acc 0.6024
 Acc 0.6022
new best val f1: 0.6022444382977042
meta,dgl,1,706,24.2485,0.6022

epoch:708/50, training loss:1.3735291957855225
Train Acc 0.6025
 Acc 0.6025
new best val f1: 0.6024881126434056
meta,dgl,1,707,24.2821,0.6025

epoch:709/50, training loss:1.373082160949707
Train Acc 0.6027
 Acc 0.6027
new best val f1: 0.6027449585753613
meta,dgl,1,708,24.3158,0.6027

epoch:710/50, training loss:1.3726333379745483
Train Acc 0.6030
 Acc 0.6030
new best val f1: 0.6030347334729521
meta,dgl,1,709,24.3495,0.6030

epoch:711/50, training loss:1.372188687324524
Train Acc 0.6033
 Acc 0.6032
new best val f1: 0.6032454788530183
meta,dgl,1,710,24.3840,0.6032

epoch:712/50, training loss:1.371744155883789
Train Acc 0.6035
 Acc 0.6034
new best val f1: 0.6033771947155596
meta,dgl,1,711,24.4185,0.6034

epoch:713/50, training loss:1.3712983131408691
Train Acc 0.6036
 Acc 0.6036
new best val f1: 0.6035747685093715
meta,dgl,1,712,24.4530,0.6036

epoch:714/50, training loss:1.3708536624908447
Train Acc 0.6037
 Acc 0.6038
new best val f1: 0.6037920996825648
meta,dgl,1,713,24.4866,0.6038

epoch:715/50, training loss:1.370409607887268
Train Acc 0.6040
 Acc 0.6041
new best val f1: 0.604108217752664
meta,dgl,1,714,24.5205,0.6041

epoch:716/50, training loss:1.3699642419815063
Train Acc 0.6042
 Acc 0.6043
new best val f1: 0.6043387205121112
meta,dgl,1,715,24.5541,0.6043

epoch:717/50, training loss:1.3695231676101685
Train Acc 0.6045
 Acc 0.6046
new best val f1: 0.6045626374784315
meta,dgl,1,716,24.5886,0.6046

epoch:718/50, training loss:1.369079828262329
Train Acc 0.6047
 Acc 0.6047
new best val f1: 0.6047404538928624
meta,dgl,1,717,24.6231,0.6047

epoch:719/50, training loss:1.3686366081237793
Train Acc 0.6048
 Acc 0.6049
new best val f1: 0.6048919271347848
meta,dgl,1,718,24.6566,0.6049

epoch:720/50, training loss:1.368193507194519
Train Acc 0.6050
 Acc 0.6052
new best val f1: 0.6051553588598675
meta,dgl,1,719,24.6902,0.6052

epoch:721/50, training loss:1.367753267288208
Train Acc 0.6052
 Acc 0.6054
new best val f1: 0.6053661042399336
meta,dgl,1,720,24.7238,0.6054

epoch:722/50, training loss:1.3673099279403687
Train Acc 0.6055
 Acc 0.6055
new best val f1: 0.6055439206543644
meta,dgl,1,721,24.7583,0.6055

epoch:723/50, training loss:1.3668708801269531
Train Acc 0.6058
 Acc 0.6058
new best val f1: 0.6057546660344305
meta,dgl,1,722,24.7919,0.6058

epoch:724/50, training loss:1.3664294481277466
Train Acc 0.6059
 Acc 0.6059
new best val f1: 0.6059127250694801
meta,dgl,1,723,24.8256,0.6059

epoch:725/50, training loss:1.365989327430725
Train Acc 0.6061
 Acc 0.6061
new best val f1: 0.6061168846564192
meta,dgl,1,724,24.8600,0.6061

epoch:726/50, training loss:1.3655508756637573
Train Acc 0.6063
 Acc 0.6063
new best val f1: 0.6063144584502311
meta,dgl,1,725,24.8936,0.6063

epoch:727/50, training loss:1.3651121854782104
Train Acc 0.6064
 Acc 0.6067
new best val f1: 0.6066700912790928
meta,dgl,1,726,24.9272,0.6067

epoch:728/50, training loss:1.3646715879440308
Train Acc 0.6068
 Acc 0.6069
new best val f1: 0.6069466945904295
meta,dgl,1,727,24.9608,0.6069

epoch:729/50, training loss:1.3642330169677734
Train Acc 0.6070
 Acc 0.6071
new best val f1: 0.6071310967979874
meta,dgl,1,728,24.9952,0.6071

epoch:730/50, training loss:1.363795518875122
Train Acc 0.6072
 Acc 0.6072
new best val f1: 0.6072232979017663
meta,dgl,1,729,25.0289,0.6072

epoch:731/50, training loss:1.3633583784103394
Train Acc 0.6073
 Acc 0.6074
new best val f1: 0.6073945285230701
meta,dgl,1,730,25.0625,0.6074

epoch:732/50, training loss:1.3629201650619507
Train Acc 0.6075
 Acc 0.6078
new best val f1: 0.6077501613519316
meta,dgl,1,731,25.0961,0.6078

epoch:733/50, training loss:1.3624821901321411
Train Acc 0.6078
 Acc 0.6080
new best val f1: 0.6080004214907602
meta,dgl,1,732,25.1297,0.6080

epoch:734/50, training loss:1.3620480298995972
Train Acc 0.6081
 Acc 0.6082
new best val f1: 0.6081650663189367
meta,dgl,1,733,25.1634,0.6082

epoch:735/50, training loss:1.3616091012954712
Train Acc 0.6083
 Acc 0.6082
new best val f1: 0.6082375100433345
meta,dgl,1,734,25.1970,0.6082

epoch:736/50, training loss:1.361176609992981
Train Acc 0.6084
 Acc 0.6085
new best val f1: 0.608481184389036
meta,dgl,1,735,25.2306,0.6085

epoch:737/50, training loss:1.3607383966445923
Train Acc 0.6085
 Acc 0.6087
new best val f1: 0.6086853439759751
meta,dgl,1,736,25.2650,0.6087

epoch:738/50, training loss:1.3603039979934692
Train Acc 0.6087
 Acc 0.6090
new best val f1: 0.6089685330804389
meta,dgl,1,737,25.2995,0.6090

epoch:739/50, training loss:1.3598700761795044
Train Acc 0.6090
 Acc 0.6091
new best val f1: 0.6091331779086155
meta,dgl,1,738,25.3331,0.6091

epoch:740/50, training loss:1.3594332933425903
Train Acc 0.6093
 Acc 0.6093
new best val f1: 0.6092978227367922
meta,dgl,1,739,25.3667,0.6093

epoch:741/50, training loss:1.3589990139007568
Train Acc 0.6094
 Acc 0.6095
new best val f1: 0.6095349112893665
meta,dgl,1,740,25.4003,0.6095

epoch:742/50, training loss:1.3585642576217651
Train Acc 0.6096
 Acc 0.6097
new best val f1: 0.6097258992900515
meta,dgl,1,741,25.4349,0.6097

epoch:743/50, training loss:1.358131766319275
Train Acc 0.6098
 Acc 0.6099
new best val f1: 0.6099366446701177
meta,dgl,1,742,25.4687,0.6099

epoch:744/50, training loss:1.3576984405517578
Train Acc 0.6101
 Acc 0.6102
new best val f1: 0.6101869048089461
meta,dgl,1,743,25.5034,0.6102

epoch:745/50, training loss:1.3572640419006348
Train Acc 0.6103
 Acc 0.6104
new best val f1: 0.6103844786027581
meta,dgl,1,744,25.5371,0.6104

epoch:746/50, training loss:1.3568321466445923
Train Acc 0.6104
 Acc 0.6105
new best val f1: 0.6105227802584265
meta,dgl,1,745,25.5718,0.6105

epoch:747/50, training loss:1.3563988208770752
Train Acc 0.6106
 Acc 0.6107
new best val f1: 0.6107137682591115
meta,dgl,1,746,25.6054,0.6107

epoch:748/50, training loss:1.35596764087677
Train Acc 0.6108
 Acc 0.6109
new best val f1: 0.6109245136391775
meta,dgl,1,747,25.6391,0.6109

epoch:749/50, training loss:1.3555372953414917
Train Acc 0.6110
 Acc 0.6111
new best val f1: 0.6111089158467354
meta,dgl,1,748,25.6727,0.6111

epoch:750/50, training loss:1.3551064729690552
Train Acc 0.6112
 Acc 0.6114
new best val f1: 0.6113789333649451
meta,dgl,1,749,25.7072,0.6114

epoch:751/50, training loss:1.354674220085144
Train Acc 0.6115
 Acc 0.6116
new best val f1: 0.6115501639862488
meta,dgl,1,750,25.7407,0.6116

epoch:752/50, training loss:1.354243516921997
Train Acc 0.6116
 Acc 0.6118
new best val f1: 0.6117674951594421
meta,dgl,1,751,25.7751,0.6118

epoch:753/50, training loss:1.353813648223877
Train Acc 0.6118
 Acc 0.6120
new best val f1: 0.6119848263326353
meta,dgl,1,752,25.8087,0.6120

epoch:754/50, training loss:1.3533828258514404
Train Acc 0.6121
 Acc 0.6122
new best val f1: 0.6122285006783367
meta,dgl,1,753,25.8424,0.6122

epoch:755/50, training loss:1.352952480316162
Train Acc 0.6124
 Acc 0.6124
new best val f1: 0.6124129028858946
meta,dgl,1,754,25.8769,0.6124

epoch:756/50, training loss:1.3525234460830688
Train Acc 0.6126
 Acc 0.6126
new best val f1: 0.6125775477140712
meta,dgl,1,755,25.9114,0.6126

epoch:757/50, training loss:1.3520944118499756
Train Acc 0.6127
 Acc 0.6128
new best val f1: 0.6128014646803914
meta,dgl,1,756,25.9459,0.6128

epoch:758/50, training loss:1.3516628742218018
Train Acc 0.6129
 Acc 0.6130
new best val f1: 0.6130122100604576
meta,dgl,1,757,25.9796,0.6130

epoch:759/50, training loss:1.3512349128723145
Train Acc 0.6131
 Acc 0.6132
new best val f1: 0.613150511716126
meta,dgl,1,758,26.0132,0.6132

epoch:760/50, training loss:1.3508069515228271
Train Acc 0.6133
 Acc 0.6134
new best val f1: 0.6133744286824462
meta,dgl,1,759,26.0469,0.6134

epoch:761/50, training loss:1.3503775596618652
Train Acc 0.6135
 Acc 0.6137
new best val f1: 0.613651031993783
meta,dgl,1,760,26.0805,0.6137

epoch:762/50, training loss:1.3499494791030884
Train Acc 0.6138
 Acc 0.6138
new best val f1: 0.6138222626150868
meta,dgl,1,761,26.1142,0.6138

epoch:763/50, training loss:1.3495216369628906
Train Acc 0.6140
 Acc 0.6140
new best val f1: 0.6140000790295175
meta,dgl,1,762,26.1488,0.6140

epoch:764/50, training loss:1.349094271659851
Train Acc 0.6142
 Acc 0.6142
new best val f1: 0.6141910670302024
meta,dgl,1,763,26.1824,0.6142

epoch:765/50, training loss:1.3486639261245728
Train Acc 0.6144
 Acc 0.6145
new best val f1: 0.6145203566865558
meta,dgl,1,764,26.2160,0.6145

epoch:766/50, training loss:1.3482376337051392
Train Acc 0.6147
 Acc 0.6148
new best val f1: 0.6147837884116384
meta,dgl,1,765,26.2497,0.6148

epoch:767/50, training loss:1.3478097915649414
Train Acc 0.6149
 Acc 0.6150
new best val f1: 0.6149550190329421
meta,dgl,1,766,26.2833,0.6150

epoch:768/50, training loss:1.3473848104476929
Train Acc 0.6151
 Acc 0.6152
new best val f1: 0.6152118649648978
meta,dgl,1,767,26.3169,0.6152

epoch:769/50, training loss:1.3469575643539429
Train Acc 0.6153
 Acc 0.6155
new best val f1: 0.6154752966899804
meta,dgl,1,768,26.3513,0.6155

epoch:770/50, training loss:1.3465298414230347
Train Acc 0.6155
 Acc 0.6157
new best val f1: 0.6157453142081901
meta,dgl,1,769,26.3850,0.6157

epoch:771/50, training loss:1.3461036682128906
Train Acc 0.6158
 Acc 0.6159
new best val f1: 0.6158967874501127
meta,dgl,1,770,26.4195,0.6159

epoch:772/50, training loss:1.3456780910491943
Train Acc 0.6159
 Acc 0.6160
new best val f1: 0.6160285033126539
meta,dgl,1,771,26.4532,0.6160

epoch:773/50, training loss:1.3452517986297607
Train Acc 0.6161
 Acc 0.6163
new best val f1: 0.6162919350377366
meta,dgl,1,772,26.4879,0.6163

epoch:774/50, training loss:1.344826340675354
Train Acc 0.6164
 Acc 0.6166
new best val f1: 0.6166146389009628
meta,dgl,1,773,26.5224,0.6166

epoch:775/50, training loss:1.3443996906280518
Train Acc 0.6167
 Acc 0.6168
new best val f1: 0.6167924553153936
meta,dgl,1,774,26.5561,0.6168

epoch:776/50, training loss:1.3439749479293823
Train Acc 0.6168
 Acc 0.6170
new best val f1: 0.617022958074841
meta,dgl,1,775,26.5897,0.6170

epoch:777/50, training loss:1.3435500860214233
Train Acc 0.6171
 Acc 0.6172
new best val f1: 0.6171810171098906
meta,dgl,1,776,26.6242,0.6172

epoch:778/50, training loss:1.3431241512298584
Train Acc 0.6173
 Acc 0.6174
new best val f1: 0.6174444488349732
meta,dgl,1,777,26.6579,0.6174

epoch:779/50, training loss:1.3426997661590576
Train Acc 0.6175
 Acc 0.6177
new best val f1: 0.61772105214631
meta,dgl,1,778,26.6915,0.6177

epoch:780/50, training loss:1.3422749042510986
Train Acc 0.6178
 Acc 0.6179
new best val f1: 0.6179449691126302
meta,dgl,1,779,26.7251,0.6179

epoch:781/50, training loss:1.341852068901062
Train Acc 0.6180
 Acc 0.6181
new best val f1: 0.6180766849751715
meta,dgl,1,780,26.7596,0.6181

epoch:782/50, training loss:1.3414283990859985
Train Acc 0.6181
 Acc 0.6183
new best val f1: 0.6182874303552377
meta,dgl,1,781,26.7933,0.6183

epoch:783/50, training loss:1.3410019874572754
Train Acc 0.6183
 Acc 0.6185
new best val f1: 0.6184718325627956
meta,dgl,1,782,26.8269,0.6185

epoch:784/50, training loss:1.3405787944793701
Train Acc 0.6185
 Acc 0.6187
new best val f1: 0.6186694063566075
meta,dgl,1,783,26.8604,0.6187

epoch:785/50, training loss:1.3401557207107544
Train Acc 0.6188
 Acc 0.6189
new best val f1: 0.6189196664954361
meta,dgl,1,784,26.8949,0.6189

epoch:786/50, training loss:1.3397315740585327
Train Acc 0.6190
 Acc 0.6190
new best val f1: 0.6189986960129609
meta,dgl,1,785,26.9285,0.6190

epoch:787/50, training loss:1.3393083810806274
Train Acc 0.6191
 Acc 0.6193
new best val f1: 0.6192621277380435
meta,dgl,1,786,26.9622,0.6193

epoch:788/50, training loss:1.338884949684143
Train Acc 0.6193
 Acc 0.6194
new best val f1: 0.6194267725662201
meta,dgl,1,787,26.9958,0.6194

epoch:789/50, training loss:1.3384634256362915
Train Acc 0.6195
 Acc 0.6198
new best val f1: 0.6197560622225735
meta,dgl,1,788,27.0303,0.6198

epoch:790/50, training loss:1.338039755821228
Train Acc 0.6199
 Acc 0.6198
new best val f1: 0.6198482633263523
meta,dgl,1,789,27.0639,0.6198

epoch:791/50, training loss:1.3376166820526123
Train Acc 0.6200
 Acc 0.6201
new best val f1: 0.620111695051435
meta,dgl,1,790,27.0976,0.6201

epoch:792/50, training loss:1.3371938467025757
Train Acc 0.6202
 Acc 0.6203
new best val f1: 0.6202895114658659
meta,dgl,1,791,27.1322,0.6203

epoch:793/50, training loss:1.336772084236145
Train Acc 0.6204
 Acc 0.6205
new best val f1: 0.6204541562940425
meta,dgl,1,792,27.1658,0.6205

epoch:794/50, training loss:1.3363523483276367
Train Acc 0.6206
 Acc 0.6208
new best val f1: 0.6207505169847605
meta,dgl,1,793,27.2002,0.6208

epoch:795/50, training loss:1.335928201675415
Train Acc 0.6209
 Acc 0.6210
new best val f1: 0.6209744339510808
meta,dgl,1,794,27.2339,0.6210

epoch:796/50, training loss:1.3355077505111694
Train Acc 0.6211
 Acc 0.6211
new best val f1: 0.621092978227368
meta,dgl,1,795,27.2674,0.6211

epoch:797/50, training loss:1.3350852727890015
Train Acc 0.6213
 Acc 0.6212
new best val f1: 0.6212444514692904
meta,dgl,1,796,27.3010,0.6212

epoch:798/50, training loss:1.3346630334854126
Train Acc 0.6215
 Acc 0.6215
new best val f1: 0.6215144689875002
meta,dgl,1,797,27.3346,0.6215

epoch:799/50, training loss:1.3342432975769043
Train Acc 0.6217
 Acc 0.6217
new best val f1: 0.6217383859538205
meta,dgl,1,798,27.3682,0.6217

epoch:800/50, training loss:1.3338218927383423
Train Acc 0.6220
 Acc 0.6220
new best val f1: 0.6219623029201407
meta,dgl,1,799,27.4018,0.6220

epoch:801/50, training loss:1.333399772644043
Train Acc 0.6222
 Acc 0.6221
new best val f1: 0.6221401193345715
meta,dgl,1,800,27.4353,0.6221

epoch:802/50, training loss:1.332978367805481
Train Acc 0.6224
 Acc 0.6223
new best val f1: 0.6222981783696211
meta,dgl,1,801,27.4689,0.6223

epoch:803/50, training loss:1.3325603008270264
Train Acc 0.6226
 Acc 0.6225
new best val f1: 0.6224759947840518
meta,dgl,1,802,27.5033,0.6225

epoch:804/50, training loss:1.3321400880813599
Train Acc 0.6228
 Acc 0.6227
new best val f1: 0.6227130833366262
meta,dgl,1,803,27.5369,0.6227

epoch:805/50, training loss:1.3317195177078247
Train Acc 0.6230
 Acc 0.6230
new best val f1: 0.6229501718892007
meta,dgl,1,804,27.5706,0.6230

epoch:806/50, training loss:1.331299066543579
Train Acc 0.6232
 Acc 0.6232
new best val f1: 0.6232333609936644
meta,dgl,1,805,27.6041,0.6232

epoch:807/50, training loss:1.3308775424957275
Train Acc 0.6234
 Acc 0.6235
new best val f1: 0.6234770353393659
meta,dgl,1,806,27.6386,0.6235

epoch:808/50, training loss:1.3304587602615356
Train Acc 0.6236
 Acc 0.6236
new best val f1: 0.6235626506500178
meta,dgl,1,807,27.6722,0.6236

epoch:809/50, training loss:1.3300395011901855
Train Acc 0.6237
 Acc 0.6239
new best val f1: 0.6239446266513876
meta,dgl,1,808,27.7066,0.6239

epoch:810/50, training loss:1.3296185731887817
Train Acc 0.6241
 Acc 0.6241
new best val f1: 0.6241026856864372
meta,dgl,1,809,27.7403,0.6241

epoch:811/50, training loss:1.3291988372802734
Train Acc 0.6243
 Acc 0.6244
new best val f1: 0.624385874790901
meta,dgl,1,810,27.7739,0.6244

epoch:812/50, training loss:1.328781247138977
Train Acc 0.6246
 Acc 0.6245
new best val f1: 0.6245373480328236
meta,dgl,1,811,27.8075,0.6245

epoch:813/50, training loss:1.3283615112304688
Train Acc 0.6247
 Acc 0.6247
new best val f1: 0.6247217502403815
meta,dgl,1,812,27.8411,0.6247

epoch:814/50, training loss:1.3279436826705933
Train Acc 0.6249
 Acc 0.6249
new best val f1: 0.6249061524479393
meta,dgl,1,813,27.8747,0.6249

epoch:815/50, training loss:1.3275234699249268
Train Acc 0.6250
 Acc 0.6252
new best val f1: 0.6252090989317843
meta,dgl,1,814,27.9091,0.6252

epoch:816/50, training loss:1.3271043300628662
Train Acc 0.6253
 Acc 0.6254
new best val f1: 0.6254000869324693
meta,dgl,1,815,27.9427,0.6254

epoch:817/50, training loss:1.3266855478286743
Train Acc 0.6255
 Acc 0.6256
new best val f1: 0.6255515601743918
meta,dgl,1,816,27.9763,0.6256

epoch:818/50, training loss:1.3262670040130615
Train Acc 0.6257
 Acc 0.6257
new best val f1: 0.6257030334163143
meta,dgl,1,817,28.0100,0.6257

epoch:819/50, training loss:1.3258490562438965
Train Acc 0.6259
 Acc 0.6259
new best val f1: 0.6259203645895075
meta,dgl,1,818,28.0438,0.6259

epoch:820/50, training loss:1.3254307508468628
Train Acc 0.6261
 Acc 0.6262
new best val f1: 0.6261772105214631
meta,dgl,1,819,28.0784,0.6262

epoch:821/50, training loss:1.3250116109848022
Train Acc 0.6264
 Acc 0.6264
new best val f1: 0.6264011274877833
meta,dgl,1,820,28.1129,0.6264

epoch:822/50, training loss:1.324594259262085
Train Acc 0.6266
 Acc 0.6266
new best val f1: 0.6265526007297059
meta,dgl,1,821,28.1467,0.6266

epoch:823/50, training loss:1.3241747617721558
Train Acc 0.6267
 Acc 0.6267
new best val f1: 0.6267106597647555
meta,dgl,1,822,28.1803,0.6267

epoch:824/50, training loss:1.3237593173980713
Train Acc 0.6269
 Acc 0.6269
new best val f1: 0.6268950619723134
meta,dgl,1,823,28.2140,0.6269

epoch:825/50, training loss:1.3233397006988525
Train Acc 0.6271
 Acc 0.6271
new best val f1: 0.6271387363180148
meta,dgl,1,824,28.2477,0.6271

epoch:826/50, training loss:1.3229224681854248
Train Acc 0.6273
 Acc 0.6273
new best val f1: 0.6273165527324456
meta,dgl,1,825,28.2815,0.6273

epoch:827/50, training loss:1.3225051164627075
Train Acc 0.6276
 Acc 0.6275
new best val f1: 0.6274811975606223
meta,dgl,1,826,28.3160,0.6275

epoch:828/50, training loss:1.3220880031585693
Train Acc 0.6278
 Acc 0.6278
new best val f1: 0.6277512150788319
meta,dgl,1,827,28.3496,0.6278

epoch:829/50, training loss:1.321669578552246
Train Acc 0.6280
 Acc 0.6279
new best val f1: 0.6279422030795169
meta,dgl,1,828,28.3832,0.6279

epoch:830/50, training loss:1.3212519884109497
Train Acc 0.6282
 Acc 0.6282
new best val f1: 0.6281858774252184
meta,dgl,1,829,28.4169,0.6282

epoch:831/50, training loss:1.3208348751068115
Train Acc 0.6284
 Acc 0.6284
new best val f1: 0.6283900370121573
meta,dgl,1,830,28.4506,0.6284

epoch:832/50, training loss:1.320419192314148
Train Acc 0.6286
 Acc 0.6285
new best val f1: 0.6285283386678258
meta,dgl,1,831,28.4843,0.6285

epoch:833/50, training loss:1.320000171661377
Train Acc 0.6288
 Acc 0.6287
new best val f1: 0.6286600545303671
meta,dgl,1,832,28.5188,0.6287

epoch:834/50, training loss:1.3195832967758179
Train Acc 0.6290
 Acc 0.6290
new best val f1: 0.6290354447386098
meta,dgl,1,833,28.5526,0.6290

epoch:835/50, training loss:1.3191672563552856
Train Acc 0.6293
 Acc 0.6292
new best val f1: 0.6292132611530407
meta,dgl,1,834,28.5873,0.6292

epoch:836/50, training loss:1.3187487125396729
Train Acc 0.6295
 Acc 0.6294
new best val f1: 0.629437178119361
meta,dgl,1,835,28.6210,0.6294

epoch:837/50, training loss:1.3183320760726929
Train Acc 0.6297
 Acc 0.6296
new best val f1: 0.6295952371544105
meta,dgl,1,836,28.6548,0.6296

epoch:838/50, training loss:1.3179152011871338
Train Acc 0.6299
 Acc 0.6296
new best val f1: 0.629647923499427
meta,dgl,1,837,28.6895,0.6296

epoch:839/50, training loss:1.3175007104873657
Train Acc 0.6299
 Acc 0.6299
new best val f1: 0.6298915978451285
meta,dgl,1,838,28.7232,0.6299

epoch:840/50, training loss:1.3170828819274902
Train Acc 0.6302
 Acc 0.6300
new best val f1: 0.6300496568801781
meta,dgl,1,839,28.7577,0.6300

epoch:841/50, training loss:1.3166669607162476
Train Acc 0.6305
 Acc 0.6303
new best val f1: 0.6302669880533712
meta,dgl,1,840,28.7914,0.6303

epoch:842/50, training loss:1.3162492513656616
Train Acc 0.6307
 Acc 0.6304
new best val f1: 0.6303987039159126
meta,dgl,1,841,28.8251,0.6304

epoch:843/50, training loss:1.3158340454101562
Train Acc 0.6308
 Acc 0.6306
new best val f1: 0.6305896919165975
meta,dgl,1,842,28.8587,0.6306

epoch:844/50, training loss:1.3154181241989136
Train Acc 0.6310
 Acc 0.6309
new best val f1: 0.6308597094348072
meta,dgl,1,843,28.8932,0.6309

epoch:845/50, training loss:1.3150006532669067
Train Acc 0.6313
 Acc 0.6311
new best val f1: 0.6311099695736357
meta,dgl,1,844,28.9268,0.6311

epoch:846/50, training loss:1.3145862817764282
Train Acc 0.6316
 Acc 0.6313
new best val f1: 0.6312746144018124
meta,dgl,1,845,28.9605,0.6313

epoch:847/50, training loss:1.314168930053711
Train Acc 0.6317
 Acc 0.6314
new best val f1: 0.6313997444712267
meta,dgl,1,846,28.9941,0.6314

epoch:848/50, training loss:1.3137532472610474
Train Acc 0.6318
 Acc 0.6316
new best val f1: 0.631636833023801
meta,dgl,1,847,29.0277,0.6316

epoch:849/50, training loss:1.3133381605148315
Train Acc 0.6320
 Acc 0.6319
new best val f1: 0.6318805073695025
meta,dgl,1,848,29.0612,0.6319

epoch:850/50, training loss:1.3129225969314575
Train Acc 0.6323
 Acc 0.6321
new best val f1: 0.6320714953701875
meta,dgl,1,849,29.0957,0.6321

epoch:851/50, training loss:1.3125076293945312
Train Acc 0.6325
 Acc 0.6321
new best val f1: 0.632117595922077
meta,dgl,1,850,29.1293,0.6321

epoch:852/50, training loss:1.312092661857605
Train Acc 0.6326
 Acc 0.6323
new best val f1: 0.6322624833708723
meta,dgl,1,851,29.1630,0.6323

epoch:853/50, training loss:1.3116761445999146
Train Acc 0.6327
 Acc 0.6325
new best val f1: 0.6324600571646843
meta,dgl,1,852,29.1967,0.6325

epoch:854/50, training loss:1.3112611770629883
Train Acc 0.6329
 Acc 0.6327
new best val f1: 0.6327498320622753
meta,dgl,1,853,29.2312,0.6327

epoch:855/50, training loss:1.3108490705490112
Train Acc 0.6331
 Acc 0.6329
new best val f1: 0.6329342342698331
meta,dgl,1,854,29.2648,0.6329

epoch:856/50, training loss:1.3104362487792969
Train Acc 0.6333
 Acc 0.6331
new best val f1: 0.6330527785461203
meta,dgl,1,855,29.3025,0.6331

epoch:857/50, training loss:1.310021162033081
Train Acc 0.6335
 Acc 0.6332
new best val f1: 0.6331844944086616
meta,dgl,1,856,29.3361,0.6332

epoch:858/50, training loss:1.3096070289611816
Train Acc 0.6336
 Acc 0.6334
new best val f1: 0.6334018255818549
meta,dgl,1,857,29.3706,0.6334

epoch:859/50, training loss:1.3091915845870972
Train Acc 0.6338
 Acc 0.6337
new best val f1: 0.6336520857206833
meta,dgl,1,858,29.4042,0.6337

epoch:860/50, training loss:1.3087769746780396
Train Acc 0.6341
 Acc 0.6338
new best val f1: 0.6337903873763517
meta,dgl,1,859,29.4379,0.6338

epoch:861/50, training loss:1.3083642721176147
Train Acc 0.6342
 Acc 0.6339
new best val f1: 0.6339484464114014
meta,dgl,1,860,29.4716,0.6339

epoch:862/50, training loss:1.307949185371399
Train Acc 0.6344
 Acc 0.6340
new best val f1: 0.6340208901357991
meta,dgl,1,861,29.5059,0.6340

epoch:863/50, training loss:1.3075374364852905
Train Acc 0.6345
 Acc 0.6343
new best val f1: 0.6342513928952463
meta,dgl,1,862,29.5397,0.6343

epoch:864/50, training loss:1.3071237802505493
Train Acc 0.6347
 Acc 0.6345
new best val f1: 0.6345214104134561
meta,dgl,1,863,29.5735,0.6345

epoch:865/50, training loss:1.3067113161087036
Train Acc 0.6350
 Acc 0.6347
new best val f1: 0.6346531262759975
meta,dgl,1,864,29.6073,0.6347

epoch:866/50, training loss:1.3062968254089355
Train Acc 0.6352
 Acc 0.6348
new best val f1: 0.6348309426904282
meta,dgl,1,865,29.6411,0.6348

epoch:867/50, training loss:1.3058863878250122
Train Acc 0.6353
 Acc 0.6349
new best val f1: 0.6349033864148259
meta,dgl,1,866,29.6757,0.6349

epoch:868/50, training loss:1.3054723739624023
Train Acc 0.6355
 Acc 0.6352
new best val f1: 0.635206332898671
meta,dgl,1,867,29.7094,0.6352

epoch:869/50, training loss:1.3050600290298462
Train Acc 0.6357
 Acc 0.6354
new best val f1: 0.6354236640718641
meta,dgl,1,868,29.7431,0.6354

epoch:870/50, training loss:1.3046486377716064
Train Acc 0.6360
 Acc 0.6355
new best val f1: 0.6355487941412784
meta,dgl,1,869,29.7768,0.6355

epoch:871/50, training loss:1.3042347431182861
Train Acc 0.6361
 Acc 0.6357
new best val f1: 0.6357463679350904
meta,dgl,1,870,29.8105,0.6357

epoch:872/50, training loss:1.3038231134414673
Train Acc 0.6363
 Acc 0.6359
new best val f1: 0.635897841177013
meta,dgl,1,871,29.8442,0.6359

epoch:873/50, training loss:1.3034120798110962
Train Acc 0.6365
 Acc 0.6360
new best val f1: 0.6360295570395543
meta,dgl,1,872,29.8787,0.6360

epoch:874/50, training loss:1.3030003309249878
Train Acc 0.6366
 Acc 0.6363
new best val f1: 0.636299574557764
meta,dgl,1,873,29.9125,0.6363

epoch:875/50, training loss:1.3025872707366943
Train Acc 0.6368
 Acc 0.6366
new best val f1: 0.6365761778691008
meta,dgl,1,874,29.9461,0.6366

epoch:876/50, training loss:1.3021764755249023
Train Acc 0.6371
 Acc 0.6368
new best val f1: 0.6368066806285481
meta,dgl,1,875,29.9805,0.6368

epoch:877/50, training loss:1.3017646074295044
Train Acc 0.6373
 Acc 0.6369
new best val f1: 0.636898881732327
meta,dgl,1,876,30.0139,0.6369

epoch:878/50, training loss:1.3013536930084229
Train Acc 0.6374
 Acc 0.6370
new best val f1: 0.6370305975948684
meta,dgl,1,877,30.0478,0.6370

epoch:879/50, training loss:1.3009421825408936
Train Acc 0.6376
 Acc 0.6373
new best val f1: 0.6373203724924593
meta,dgl,1,878,30.0816,0.6373

epoch:880/50, training loss:1.3005315065383911
Train Acc 0.6378
 Acc 0.6376
new best val f1: 0.6376035615969231
meta,dgl,1,879,30.1163,0.6376

epoch:881/50, training loss:1.3001211881637573
Train Acc 0.6381
 Acc 0.6376
new best val f1: 0.6376430763556855
meta,dgl,1,880,30.1500,0.6376

epoch:882/50, training loss:1.299710988998413
Train Acc 0.6381
 Acc 0.6378
new best val f1: 0.6377813780113539
meta,dgl,1,881,30.1838,0.6378

epoch:883/50, training loss:1.2993005514144897
Train Acc 0.6383
 Acc 0.6381
new best val f1: 0.6380579813226906
meta,dgl,1,882,30.2175,0.6381

epoch:884/50, training loss:1.298888087272644
Train Acc 0.6385
 Acc 0.6382
new best val f1: 0.6382028687714861
meta,dgl,1,883,30.2513,0.6382

epoch:885/50, training loss:1.2984788417816162
Train Acc 0.6387
 Acc 0.6384
new best val f1: 0.638380685185917
meta,dgl,1,884,30.2859,0.6384

epoch:886/50, training loss:1.298068881034851
Train Acc 0.6389
 Acc 0.6386
new best val f1: 0.638584844772856
meta,dgl,1,885,30.3195,0.6386

epoch:887/50, training loss:1.2976588010787964
Train Acc 0.6391
 Acc 0.6388
new best val f1: 0.6387692469804138
meta,dgl,1,886,30.3540,0.6388

epoch:888/50, training loss:1.2972503900527954
Train Acc 0.6392
 Acc 0.6389
new best val f1: 0.6389338918085905
meta,dgl,1,887,30.3877,0.6389

epoch:889/50, training loss:1.296838402748108
Train Acc 0.6394
 Acc 0.6393
new best val f1: 0.6392565956718168
meta,dgl,1,888,30.4213,0.6393

epoch:890/50, training loss:1.2964297533035278
Train Acc 0.6397
 Acc 0.6394
new best val f1: 0.6394278262931204
meta,dgl,1,889,30.4549,0.6394

epoch:891/50, training loss:1.2960196733474731
Train Acc 0.6398
 Acc 0.6396
new best val f1: 0.6395529563625347
meta,dgl,1,890,30.4895,0.6396

epoch:892/50, training loss:1.2956104278564453
Train Acc 0.6400
 Acc 0.6397
new best val f1: 0.6396846722250761
meta,dgl,1,891,30.5231,0.6397

epoch:893/50, training loss:1.2952005863189697
Train Acc 0.6401
 Acc 0.6399
new best val f1: 0.6399217607776505
meta,dgl,1,892,30.5567,0.6399

epoch:894/50, training loss:1.2947920560836792
Train Acc 0.6404
 Acc 0.6401
new best val f1: 0.6400864056058271
meta,dgl,1,893,30.5911,0.6401

epoch:895/50, training loss:1.2943822145462036
Train Acc 0.6406
 Acc 0.6402
new best val f1: 0.6402378788477496
meta,dgl,1,894,30.6247,0.6402

epoch:896/50, training loss:1.293972373008728
Train Acc 0.6408
 Acc 0.6403
new best val f1: 0.6402576362271308
meta,dgl,1,895,30.6584,0.6403

epoch:897/50, training loss:1.2935645580291748
Train Acc 0.6409
 Acc 0.6405
new best val f1: 0.6405078963659594
meta,dgl,1,896,30.6920,0.6405

epoch:898/50, training loss:1.293156623840332
Train Acc 0.6411
 Acc 0.6408
new best val f1: 0.6408437718154397
meta,dgl,1,897,30.7257,0.6408

epoch:899/50, training loss:1.292747139930725
Train Acc 0.6413
 Acc 0.6409
new best val f1: 0.6409030439535833
meta,dgl,1,898,30.7593,0.6409

epoch:900/50, training loss:1.2923388481140137
Train Acc 0.6414
 Acc 0.6412
new best val f1: 0.6411533040924119
meta,dgl,1,899,30.7938,0.6412

epoch:901/50, training loss:1.291930079460144
Train Acc 0.6416
 Acc 0.6413
new best val f1: 0.6413442920930967
meta,dgl,1,900,30.8283,0.6413

epoch:902/50, training loss:1.2915236949920654
Train Acc 0.6418
 Acc 0.6415
new best val f1: 0.6414825937487652
meta,dgl,1,901,30.8619,0.6415

epoch:903/50, training loss:1.2911149263381958
Train Acc 0.6419
 Acc 0.6417
new best val f1: 0.6417262680944666
meta,dgl,1,902,30.8954,0.6417

epoch:904/50, training loss:1.2907074689865112
Train Acc 0.6422
 Acc 0.6420
new best val f1: 0.641963356647041
meta,dgl,1,903,30.9291,0.6420

epoch:905/50, training loss:1.2902984619140625
Train Acc 0.6424
 Acc 0.6421
new best val f1: 0.6421345872683447
meta,dgl,1,904,30.9626,0.6421

epoch:906/50, training loss:1.2898908853530884
Train Acc 0.6425
 Acc 0.6423
new best val f1: 0.6422926463033943
meta,dgl,1,905,30.9972,0.6423

epoch:907/50, training loss:1.289482593536377
Train Acc 0.6426
 Acc 0.6425
new best val f1: 0.642457291131571
meta,dgl,1,906,31.0308,0.6425

epoch:908/50, training loss:1.2890746593475342
Train Acc 0.6428
 Acc 0.6428
new best val f1: 0.6427668234085431
meta,dgl,1,907,31.0644,0.6428

epoch:909/50, training loss:1.2886682748794556
Train Acc 0.6431
 Acc 0.6429
new best val f1: 0.6429446398229739
meta,dgl,1,908,31.0981,0.6429

epoch:910/50, training loss:1.288259506225586
Train Acc 0.6432
 Acc 0.6431
new best val f1: 0.6430697698923882
meta,dgl,1,909,31.1326,0.6431

epoch:911/50, training loss:1.2878541946411133
Train Acc 0.6434
 Acc 0.6432
new best val f1: 0.6432212431343106
meta,dgl,1,910,31.1669,0.6432

epoch:912/50, training loss:1.2874460220336914
Train Acc 0.6436
 Acc 0.6434
new best val f1: 0.6434319885143768
meta,dgl,1,911,31.2006,0.6434

epoch:913/50, training loss:1.287039875984192
Train Acc 0.6438
 Acc 0.6435
new best val f1: 0.6435373612044099
meta,dgl,1,912,31.2350,0.6435

epoch:914/50, training loss:1.2866320610046387
Train Acc 0.6439
 Acc 0.6437
new best val f1: 0.6436690770669512
meta,dgl,1,913,31.2687,0.6437

epoch:915/50, training loss:1.2862255573272705
Train Acc 0.6440
 Acc 0.6440
new best val f1: 0.6439720235507962
meta,dgl,1,914,31.3024,0.6440

epoch:916/50, training loss:1.2858185768127441
Train Acc 0.6444
 Acc 0.6442
new best val f1: 0.6441630115514811
meta,dgl,1,915,31.3360,0.6442

epoch:917/50, training loss:1.285410761833191
Train Acc 0.6445
 Acc 0.6444
new best val f1: 0.6444066858971826
meta,dgl,1,916,31.3706,0.6444

epoch:918/50, training loss:1.2850059270858765
Train Acc 0.6447
 Acc 0.6446
new best val f1: 0.644643774449757
meta,dgl,1,917,31.4049,0.6446

epoch:919/50, training loss:1.2845978736877441
Train Acc 0.6450
 Acc 0.6448
new best val f1: 0.6448215908641878
meta,dgl,1,918,31.4385,0.6448

epoch:920/50, training loss:1.2841919660568237
Train Acc 0.6451
 Acc 0.6449
new best val f1: 0.6449203777610938
meta,dgl,1,919,31.4722,0.6449

epoch:921/50, training loss:1.2837862968444824
Train Acc 0.6453
 Acc 0.6452
new best val f1: 0.6451574663136681
meta,dgl,1,920,31.5058,0.6452

epoch:922/50, training loss:1.2833791971206665
Train Acc 0.6455
 Acc 0.6454
new best val f1: 0.6453945548662425
meta,dgl,1,921,31.5395,0.6454

epoch:923/50, training loss:1.282974362373352
Train Acc 0.6457
 Acc 0.6456
new best val f1: 0.6455657854875463
meta,dgl,1,922,31.5740,0.6456

epoch:924/50, training loss:1.2825688123703003
Train Acc 0.6459
 Acc 0.6456
new best val f1: 0.6456448150050711
meta,dgl,1,923,31.6076,0.6456

epoch:925/50, training loss:1.2821629047393799
Train Acc 0.6460
 Acc 0.6459
new best val f1: 0.6459477614889161
meta,dgl,1,924,31.6419,0.6459

epoch:926/50, training loss:1.2817569971084595
Train Acc 0.6463
 Acc 0.6461
new best val f1: 0.646138749489601
meta,dgl,1,925,31.6756,0.6461

epoch:927/50, training loss:1.2813512086868286
Train Acc 0.6465
 Acc 0.6462
new best val f1: 0.64623095059338
meta,dgl,1,926,31.7100,0.6462

epoch:928/50, training loss:1.2809449434280396
Train Acc 0.6466
 Acc 0.6464
new best val f1: 0.6464021812146837
meta,dgl,1,927,31.7437,0.6464

epoch:929/50, training loss:1.2805413007736206
Train Acc 0.6467
 Acc 0.6466
new best val f1: 0.6465931692153686
meta,dgl,1,928,31.7781,0.6466

epoch:930/50, training loss:1.280136227607727
Train Acc 0.6469
 Acc 0.6468
new best val f1: 0.6467512282504182
meta,dgl,1,929,31.8117,0.6468

epoch:931/50, training loss:1.2797305583953857
Train Acc 0.6471
 Acc 0.6470
new best val f1: 0.6469817310098656
meta,dgl,1,930,31.8453,0.6470

epoch:932/50, training loss:1.2793259620666504
Train Acc 0.6473
 Acc 0.6472
new best val f1: 0.6472122337693128
meta,dgl,1,931,31.8788,0.6472

epoch:933/50, training loss:1.2789207696914673
Train Acc 0.6475
 Acc 0.6473
new best val f1: 0.6473307780456
meta,dgl,1,932,31.9125,0.6473

epoch:934/50, training loss:1.2785155773162842
Train Acc 0.6476
 Acc 0.6475
new best val f1: 0.6474822512875226
meta,dgl,1,933,31.9460,0.6475

epoch:935/50, training loss:1.2781109809875488
Train Acc 0.6478
 Acc 0.6477
new best val f1: 0.6477259256332241
meta,dgl,1,934,31.9805,0.6477

epoch:936/50, training loss:1.2777079343795776
Train Acc 0.6480
 Acc 0.6480
new best val f1: 0.6479695999789254
meta,dgl,1,935,32.0142,0.6480

epoch:937/50, training loss:1.2773029804229736
Train Acc 0.6482
 Acc 0.6481
new best val f1: 0.6481079016345939
meta,dgl,1,936,32.0478,0.6481

epoch:938/50, training loss:1.2768980264663696
Train Acc 0.6483
 Acc 0.6483
new best val f1: 0.6482988896352788
meta,dgl,1,937,32.0814,0.6483

epoch:939/50, training loss:1.2764958143234253
Train Acc 0.6485
 Acc 0.6484
new best val f1: 0.6484371912909471
meta,dgl,1,938,32.1160,0.6484

epoch:940/50, training loss:1.276090383529663
Train Acc 0.6487
 Acc 0.6486
new best val f1: 0.6485689071534885
meta,dgl,1,939,32.1506,0.6486

epoch:941/50, training loss:1.2756868600845337
Train Acc 0.6489
 Acc 0.6487
new best val f1: 0.6487269661885381
meta,dgl,1,940,32.1842,0.6487

epoch:942/50, training loss:1.2752838134765625
Train Acc 0.6490
 Acc 0.6488
new best val f1: 0.6487928241198088
meta,dgl,1,941,32.2179,0.6488

epoch:943/50, training loss:1.2748774290084839
Train Acc 0.6491
 Acc 0.6491
new best val f1: 0.6490562558448913
meta,dgl,1,942,32.2516,0.6491

epoch:944/50, training loss:1.274476408958435
Train Acc 0.6494
 Acc 0.6493
new best val f1: 0.6492933443974658
meta,dgl,1,943,32.2861,0.6493

epoch:945/50, training loss:1.2740727663040161
Train Acc 0.6495
 Acc 0.6495
new best val f1: 0.6495172613637861
meta,dgl,1,944,32.3197,0.6495

epoch:946/50, training loss:1.2736680507659912
Train Acc 0.6497
 Acc 0.6496
new best val f1: 0.6496292198469462
meta,dgl,1,945,32.3532,0.6496

epoch:947/50, training loss:1.2732652425765991
Train Acc 0.6499
 Acc 0.6498
new best val f1: 0.6497741072957416
meta,dgl,1,946,32.3868,0.6498

epoch:948/50, training loss:1.2728617191314697
Train Acc 0.6500
 Acc 0.6500
new best val f1: 0.6499848526758077
meta,dgl,1,947,32.4204,0.6500

epoch:949/50, training loss:1.2724595069885254
Train Acc 0.6502
 Acc 0.6501
new best val f1: 0.6501363259177303
meta,dgl,1,948,32.4540,0.6501

epoch:950/50, training loss:1.272057056427002
Train Acc 0.6504
 Acc 0.6504
new best val f1: 0.6503536570909234
meta,dgl,1,949,32.4884,0.6504

epoch:951/50, training loss:1.2716537714004517
Train Acc 0.6505
 Acc 0.6506
new best val f1: 0.6505512308847354
meta,dgl,1,950,32.5220,0.6506

epoch:952/50, training loss:1.2712492942810059
Train Acc 0.6507
 Acc 0.6507
new best val f1: 0.6506631893678956
meta,dgl,1,951,32.5565,0.6507

epoch:953/50, training loss:1.2708467245101929
Train Acc 0.6509
 Acc 0.6509
new best val f1: 0.6508607631617076
meta,dgl,1,952,32.5901,0.6509

epoch:954/50, training loss:1.270445704460144
Train Acc 0.6510
 Acc 0.6511
new best val f1: 0.6510715085417736
meta,dgl,1,953,32.6236,0.6511

epoch:955/50, training loss:1.2700426578521729
Train Acc 0.6513
 Acc 0.6513
new best val f1: 0.6512559107493315
meta,dgl,1,954,32.6572,0.6513

epoch:956/50, training loss:1.2696396112442017
Train Acc 0.6514
 Acc 0.6514
new best val f1: 0.6513876266118729
meta,dgl,1,955,32.6917,0.6514

epoch:957/50, training loss:1.2692363262176514
Train Acc 0.6515
 Acc 0.6516
new best val f1: 0.6515654430263037
meta,dgl,1,956,32.7253,0.6516

epoch:958/50, training loss:1.2688356637954712
Train Acc 0.6517
 Acc 0.6519
new best val f1: 0.6519474190276735
meta,dgl,1,957,32.7590,0.6519

epoch:959/50, training loss:1.268433690071106
Train Acc 0.6520
 Acc 0.6520
new best val f1: 0.6519869337864359
meta,dgl,1,958,32.7926,0.6520

epoch:960/50, training loss:1.2680304050445557
Train Acc 0.6521
 Acc 0.6521
new best val f1: 0.6520857206833419
meta,dgl,1,959,32.8263,0.6521

epoch:961/50, training loss:1.2676284313201904
Train Acc 0.6522
 Acc 0.6523
new best val f1: 0.6523228092359162
meta,dgl,1,960,32.8599,0.6523

epoch:962/50, training loss:1.2672260999679565
Train Acc 0.6524
 Acc 0.6525
new best val f1: 0.6524676966847117
meta,dgl,1,961,32.8944,0.6525

epoch:963/50, training loss:1.2668248414993286
Train Acc 0.6525
 Acc 0.6526
new best val f1: 0.6525664835816177
meta,dgl,1,962,32.9280,0.6526

epoch:964/50, training loss:1.266422152519226
Train Acc 0.6527
 Acc 0.6529
new best val f1: 0.6528628442723358
meta,dgl,1,963,32.9617,0.6529

epoch:965/50, training loss:1.2660200595855713
Train Acc 0.6529
 Acc 0.6529
new best val f1: 0.6529023590310982
meta,dgl,1,964,32.9954,0.6529

epoch:966/50, training loss:1.2656186819076538
Train Acc 0.6530
 Acc 0.6531
new best val f1: 0.6530670038592747
meta,dgl,1,965,33.0299,0.6531

epoch:967/50, training loss:1.2652170658111572
Train Acc 0.6532
 Acc 0.6532
new best val f1: 0.6532184771011973
meta,dgl,1,966,33.0636,0.6532

epoch:968/50, training loss:1.2648144960403442
Train Acc 0.6534
 Acc 0.6534
new best val f1: 0.653389707722501
meta,dgl,1,967,33.0973,0.6534

epoch:969/50, training loss:1.2644126415252686
Train Acc 0.6536
 Acc 0.6535
new best val f1: 0.6535082519987883
meta,dgl,1,968,33.1308,0.6535

epoch:970/50, training loss:1.2640125751495361
Train Acc 0.6537
 Acc 0.6536
new best val f1: 0.6536465536544566
meta,dgl,1,969,33.1654,0.6536

epoch:971/50, training loss:1.2636101245880127
Train Acc 0.6538
 Acc 0.6539
new best val f1: 0.6539429143451746
meta,dgl,1,970,33.1998,0.6539

epoch:972/50, training loss:1.2632081508636475
Train Acc 0.6541
 Acc 0.6541
new best val f1: 0.6541404881389866
meta,dgl,1,971,33.2333,0.6541

epoch:973/50, training loss:1.2628092765808105
Train Acc 0.6543
 Acc 0.6543
new best val f1: 0.6542985471740361
meta,dgl,1,972,33.2669,0.6543

epoch:974/50, training loss:1.2624067068099976
Train Acc 0.6544
 Acc 0.6545
new best val f1: 0.6545027067609752
meta,dgl,1,973,33.3005,0.6545

epoch:975/50, training loss:1.2620066404342651
Train Acc 0.6546
 Acc 0.6546
new best val f1: 0.6546212510372624
meta,dgl,1,974,33.3340,0.6546

epoch:976/50, training loss:1.2616064548492432
Train Acc 0.6547
 Acc 0.6548
new best val f1: 0.6548056532448203
meta,dgl,1,975,33.3684,0.6548

epoch:977/50, training loss:1.2612051963806152
Train Acc 0.6549
 Acc 0.6550
new best val f1: 0.654983469659251
meta,dgl,1,976,33.4019,0.6550

epoch:978/50, training loss:1.2608047723770142
Train Acc 0.6551
 Acc 0.6552
new best val f1: 0.6551942150393172
meta,dgl,1,977,33.4355,0.6552

epoch:979/50, training loss:1.26040518283844
Train Acc 0.6553
 Acc 0.6553
new best val f1: 0.6553127593156044
meta,dgl,1,978,33.4691,0.6553

epoch:980/50, training loss:1.2600032091140747
Train Acc 0.6554
 Acc 0.6555
new best val f1: 0.6554839899369082
meta,dgl,1,979,33.5028,0.6555

epoch:981/50, training loss:1.2596051692962646
Train Acc 0.6556
 Acc 0.6556
new best val f1: 0.6556420489719577
meta,dgl,1,980,33.5364,0.6556

epoch:982/50, training loss:1.259201169013977
Train Acc 0.6558
 Acc 0.6558
new best val f1: 0.6557935222138802
meta,dgl,1,981,33.5700,0.6558

epoch:983/50, training loss:1.2588045597076416
Train Acc 0.6560
 Acc 0.6559
new best val f1: 0.6559449954558028
meta,dgl,1,982,33.6036,0.6559

epoch:984/50, training loss:1.2584046125411987
Train Acc 0.6561
 Acc 0.6561
new best val f1: 0.6560767113183441
meta,dgl,1,983,33.6372,0.6561

epoch:985/50, training loss:1.258004903793335
Train Acc 0.6563
 Acc 0.6562
new best val f1: 0.6561557408358689
meta,dgl,1,984,33.6709,0.6562

epoch:986/50, training loss:1.2576048374176025
Train Acc 0.6564
 Acc 0.6563
new best val f1: 0.6562611135259019
meta,dgl,1,985,33.7045,0.6563

epoch:987/50, training loss:1.2572052478790283
Train Acc 0.6565
 Acc 0.6564
new best val f1: 0.656373072009062
meta,dgl,1,986,33.7380,0.6564

epoch:988/50, training loss:1.2568079233169556
Train Acc 0.6567
 Acc 0.6568
new best val f1: 0.6568011485623214
meta,dgl,1,987,33.7716,0.6568

epoch:989/50, training loss:1.2564071416854858
Train Acc 0.6570
 Acc 0.6569
new best val f1: 0.6568538349073378
meta,dgl,1,988,33.8051,0.6569

epoch:990/50, training loss:1.2560062408447266
Train Acc 0.6571
 Acc 0.6569
new best val f1: 0.656873592286719
meta,dgl,1,989,33.8395,0.6569

epoch:991/50, training loss:1.2556078433990479
Train Acc 0.6571
 Acc 0.6571
new best val f1: 0.6571370240118017
meta,dgl,1,990,33.8731,0.6571

epoch:992/50, training loss:1.2552099227905273
Train Acc 0.6574
 Acc 0.6573
new best val f1: 0.6573082546331055
meta,dgl,1,991,33.9068,0.6573

epoch:993/50, training loss:1.2548108100891113
Train Acc 0.6576
 Acc 0.6575
new best val f1: 0.6575190000131715
meta,dgl,1,992,33.9404,0.6575

epoch:994/50, training loss:1.2544139623641968
Train Acc 0.6578
 Acc 0.6576
new best val f1: 0.6576243727032046
meta,dgl,1,993,33.9750,0.6576

epoch:995/50, training loss:1.2540150880813599
Train Acc 0.6579
 Acc 0.6578
new best val f1: 0.6578153607038896
meta,dgl,1,994,34.0086,0.6578

epoch:996/50, training loss:1.2536157369613647
Train Acc 0.6581
 Acc 0.6579
new best val f1: 0.6578680470489061
meta,dgl,1,995,34.0422,0.6579

epoch:997/50, training loss:1.2532168626785278
Train Acc 0.6581
 Acc 0.6580
new best val f1: 0.6580195202908287
meta,dgl,1,996,34.0759,0.6580

epoch:998/50, training loss:1.2528208494186401
Train Acc 0.6583
 Acc 0.6582
new best val f1: 0.6582236798777676
meta,dgl,1,997,34.1102,0.6582

epoch:999/50, training loss:1.2524216175079346
Train Acc 0.6584
 Acc 0.6584
new best val f1: 0.6584014962921985
meta,dgl,1,998,34.1438,0.6584

epoch:1000/50, training loss:1.2520239353179932
Train Acc 0.6586
 Acc 0.6585
new best val f1: 0.6585332121547398
meta,dgl,1,999,34.1775,0.6585

epoch:1001/50, training loss:1.2516264915466309
Train Acc 0.6587
 Acc 0.6587
new best val f1: 0.6586715138104082
meta,dgl,1,1000,34.2111,0.6587

epoch:1002/50, training loss:1.2512292861938477
Train Acc 0.6589
 Acc 0.6588
new best val f1: 0.6588229870523307
meta,dgl,1,1001,34.2449,0.6588

epoch:1003/50, training loss:1.2508313655853271
Train Acc 0.6591
 Acc 0.6590
new best val f1: 0.6589678745011261
meta,dgl,1,1002,34.2794,0.6590

epoch:1004/50, training loss:1.2504312992095947
Train Acc 0.6592
 Acc 0.6590
new best val f1: 0.6590271466392698
meta,dgl,1,1003,34.3129,0.6590

epoch:1005/50, training loss:1.2500367164611816
Train Acc 0.6593
 Acc 0.6592
new best val f1: 0.6591588625018111
meta,dgl,1,1004,34.3467,0.6592

epoch:1006/50, training loss:1.2496403455734253
Train Acc 0.6595
 Acc 0.6595
new best val f1: 0.6594618089856561
meta,dgl,1,1005,34.3803,0.6595

epoch:1007/50, training loss:1.2492433786392212
Train Acc 0.6597
 Acc 0.6595
new best val f1: 0.6595342527100538
meta,dgl,1,1006,34.4148,0.6595

epoch:1008/50, training loss:1.2488471269607544
Train Acc 0.6598
 Acc 0.6597
new best val f1: 0.6597054833313576
meta,dgl,1,1007,34.4485,0.6597

epoch:1009/50, training loss:1.2484498023986816
Train Acc 0.6599
 Acc 0.6599
new best val f1: 0.6598898855389155
meta,dgl,1,1008,34.4822,0.6599

epoch:1010/50, training loss:1.2480518817901611
Train Acc 0.6602
 Acc 0.6600
new best val f1: 0.6600018440220756
meta,dgl,1,1009,34.5162,0.6600

epoch:1011/50, training loss:1.247655987739563
Train Acc 0.6603
 Acc 0.6601
new best val f1: 0.6601072167121086
meta,dgl,1,1010,34.5508,0.6601

epoch:1012/50, training loss:1.24726140499115
Train Acc 0.6605
 Acc 0.6604
new best val f1: 0.6603838200234454
meta,dgl,1,1011,34.5846,0.6604

epoch:1013/50, training loss:1.2468633651733398
Train Acc 0.6607
 Acc 0.6604
new best val f1: 0.6604167489890808
meta,dgl,1,1012,34.6191,0.6604

epoch:1014/50, training loss:1.2464672327041626
Train Acc 0.6608
 Acc 0.6606
new best val f1: 0.6605879796103845
meta,dgl,1,1013,34.6528,0.6606

epoch:1015/50, training loss:1.2460709810256958
Train Acc 0.6609
 Acc 0.6608
new best val f1: 0.6608118965767047
meta,dgl,1,1014,34.6865,0.6608

epoch:1016/50, training loss:1.2456765174865723
Train Acc 0.6611
 Acc 0.6610
new best val f1: 0.6609501982323731
meta,dgl,1,1015,34.7201,0.6610

epoch:1017/50, training loss:1.2452822923660278
Train Acc 0.6613
 Acc 0.6610
new best val f1: 0.6610489851292791
meta,dgl,1,1016,34.7546,0.6610

epoch:1018/50, training loss:1.2448852062225342
Train Acc 0.6614
 Acc 0.6612
new best val f1: 0.6611675294055663
meta,dgl,1,1017,34.7883,0.6612

epoch:1019/50, training loss:1.2444920539855957
Train Acc 0.6615
 Acc 0.6613
new best val f1: 0.6613190026474889
meta,dgl,1,1018,34.8221,0.6613

epoch:1020/50, training loss:1.2440941333770752
Train Acc 0.6617
 Acc 0.6614
new best val f1: 0.6613980321650136
meta,dgl,1,1019,34.8565,0.6614

epoch:1021/50, training loss:1.243699312210083
Train Acc 0.6617
 Acc 0.6616
new best val f1: 0.6616021917519527
meta,dgl,1,1020,34.8906,0.6616

epoch:1022/50, training loss:1.2433058023452759
Train Acc 0.6619
 Acc 0.6618
new best val f1: 0.6617602507870023
meta,dgl,1,1021,34.9244,0.6618

epoch:1023/50, training loss:1.2429101467132568
Train Acc 0.6621
 Acc 0.6619
new best val f1: 0.6618524518907812
meta,dgl,1,1022,34.9581,0.6619

epoch:1024/50, training loss:1.242516279220581
Train Acc 0.6622
 Acc 0.6620
new best val f1: 0.6619973393395767
meta,dgl,1,1023,34.9917,0.6620

epoch:1025/50, training loss:1.2421221733093262
Train Acc 0.6623
 Acc 0.6621
new best val f1: 0.6621158836158638
meta,dgl,1,1024,35.0263,0.6621

epoch:1026/50, training loss:1.2417272329330444
Train Acc 0.6625
 Acc 0.6623
new best val f1: 0.662333214789057
meta,dgl,1,1025,35.0599,0.6623

epoch:1027/50, training loss:1.2413315773010254
Train Acc 0.6627
 Acc 0.6624
new best val f1: 0.6623793153409465
meta,dgl,1,1026,35.0935,0.6624

epoch:1028/50, training loss:1.2409392595291138
Train Acc 0.6627
 Acc 0.6627
new best val f1: 0.6626625044454103
meta,dgl,1,1027,35.1271,0.6627

epoch:1029/50, training loss:1.2405450344085693
Train Acc 0.6629
 Acc 0.6627
new best val f1: 0.6627151907904268
meta,dgl,1,1028,35.1607,0.6627

epoch:1030/50, training loss:1.2401492595672607
Train Acc 0.6630
 Acc 0.6628
new best val f1: 0.6627612913423163
meta,dgl,1,1029,35.1944,0.6628

epoch:1031/50, training loss:1.2397555112838745
Train Acc 0.6631
 Acc 0.6631
new best val f1: 0.6630708236192885
meta,dgl,1,1030,35.2280,0.6631

epoch:1032/50, training loss:1.239362120628357
Train Acc 0.6634
 Acc 0.6632
new best val f1: 0.663215711068084
meta,dgl,1,1031,35.2616,0.6632

epoch:1033/50, training loss:1.238966464996338
Train Acc 0.6635
 Acc 0.6632
meta,dgl,1,1032,35.2952,0.6632

epoch:1034/50, training loss:1.2385748624801636
Train Acc 0.6636
 Acc 0.6633
new best val f1: 0.6633408411374981
meta,dgl,1,1033,35.3288,0.6633

epoch:1035/50, training loss:1.2381789684295654
Train Acc 0.6637
 Acc 0.6635
new best val f1: 0.6635318291381831
meta,dgl,1,1034,35.3624,0.6635

epoch:1036/50, training loss:1.237784504890442
Train Acc 0.6639
 Acc 0.6635
meta,dgl,1,1035,35.3959,0.6635

epoch:1037/50, training loss:1.2373913526535034
Train Acc 0.6640
 Acc 0.6638
new best val f1: 0.6637886750701387
meta,dgl,1,1036,35.4294,0.6638

epoch:1038/50, training loss:1.2369972467422485
Train Acc 0.6642
 Acc 0.6640
new best val f1: 0.6639533198983154
meta,dgl,1,1037,35.4630,0.6640

epoch:1039/50, training loss:1.236604928970337
Train Acc 0.6643
 Acc 0.6639
meta,dgl,1,1038,35.5009,0.6639

epoch:1040/50, training loss:1.2362076044082642
Train Acc 0.6643
 Acc 0.6641
new best val f1: 0.6641047931402378
meta,dgl,1,1039,35.5354,0.6641

epoch:1041/50, training loss:1.2358132600784302
Train Acc 0.6645
 Acc 0.6644
new best val f1: 0.664407739624083
meta,dgl,1,1040,35.5690,0.6644

epoch:1042/50, training loss:1.23542058467865
Train Acc 0.6647
 Acc 0.6644
new best val f1: 0.6644209112103371
meta,dgl,1,1041,35.6035,0.6644

epoch:1043/50, training loss:1.2350248098373413
Train Acc 0.6648
 Acc 0.6646
new best val f1: 0.6645855560385138
meta,dgl,1,1042,35.6371,0.6646

epoch:1044/50, training loss:1.2346309423446655
Train Acc 0.6649
 Acc 0.6648
new best val f1: 0.6648028872117069
meta,dgl,1,1043,35.6706,0.6648

epoch:1045/50, training loss:1.2342368364334106
Train Acc 0.6651
 Acc 0.6648
meta,dgl,1,1044,35.7042,0.6648

epoch:1046/50, training loss:1.233838677406311
Train Acc 0.6651
 Acc 0.6649
new best val f1: 0.6649411888673753
meta,dgl,1,1045,35.7379,0.6649

epoch:1047/50, training loss:1.233445405960083
Train Acc 0.6653
 Acc 0.6653
new best val f1: 0.6652638927306015
meta,dgl,1,1046,35.7714,0.6653

epoch:1048/50, training loss:1.233051061630249
Train Acc 0.6656
 Acc 0.6652
meta,dgl,1,1047,35.8058,0.6652

epoch:1049/50, training loss:1.232654094696045
Train Acc 0.6655
 Acc 0.6654
new best val f1: 0.6654417091450323
meta,dgl,1,1048,35.8394,0.6654

epoch:1050/50, training loss:1.2322593927383423
Train Acc 0.6658
 Acc 0.6655
new best val f1: 0.6655339102488113
meta,dgl,1,1049,35.8731,0.6655

epoch:1051/50, training loss:1.231858491897583
Train Acc 0.6659
 Acc 0.6657
new best val f1: 0.6656985550769879
meta,dgl,1,1050,35.9067,0.6657

epoch:1052/50, training loss:1.2314642667770386
Train Acc 0.6660
 Acc 0.6658
new best val f1: 0.665797341973894
meta,dgl,1,1051,35.9413,0.6658

epoch:1053/50, training loss:1.2310689687728882
Train Acc 0.6661
 Acc 0.6660
new best val f1: 0.6660146731470871
meta,dgl,1,1052,35.9750,0.6660

epoch:1054/50, training loss:1.2306697368621826
Train Acc 0.6663
 Acc 0.6661
new best val f1: 0.6660937026646119
meta,dgl,1,1053,36.0096,0.6661

epoch:1055/50, training loss:1.230272889137268
Train Acc 0.6664
 Acc 0.6662
new best val f1: 0.6661661463890096
meta,dgl,1,1054,36.0433,0.6662

epoch:1056/50, training loss:1.2298762798309326
Train Acc 0.6666
 Acc 0.6664
new best val f1: 0.6663768917690758
meta,dgl,1,1055,36.0778,0.6664

epoch:1057/50, training loss:1.229477882385254
Train Acc 0.6668
 Acc 0.6666
new best val f1: 0.6665612939766336
meta,dgl,1,1056,36.1116,0.6666

epoch:1058/50, training loss:1.2290817499160767
Train Acc 0.6669
 Acc 0.6667
new best val f1: 0.6667325245979373
meta,dgl,1,1057,36.1461,0.6667

epoch:1059/50, training loss:1.2286853790283203
Train Acc 0.6671
 Acc 0.6667
meta,dgl,1,1058,36.1799,0.6667

epoch:1060/50, training loss:1.2282865047454834
Train Acc 0.6671
 Acc 0.6670
new best val f1: 0.66699595632302
meta,dgl,1,1059,36.2136,0.6670

epoch:1061/50, training loss:1.227888584136963
Train Acc 0.6673
 Acc 0.6671
new best val f1: 0.6671145005993072
meta,dgl,1,1060,36.2473,0.6671

epoch:1062/50, training loss:1.2274919748306274
Train Acc 0.6674
 Acc 0.6673
new best val f1: 0.6673054885999921
meta,dgl,1,1061,36.2819,0.6673

epoch:1063/50, training loss:1.2270928621292114
Train Acc 0.6676
 Acc 0.6675
new best val f1: 0.6674701334281687
meta,dgl,1,1062,36.3155,0.6675

epoch:1064/50, training loss:1.2266974449157715
Train Acc 0.6678
 Acc 0.6675
new best val f1: 0.66748989080755
meta,dgl,1,1063,36.3491,0.6675

epoch:1065/50, training loss:1.226298451423645
Train Acc 0.6678
 Acc 0.6677
new best val f1: 0.6676742930151078
meta,dgl,1,1064,36.3828,0.6677

epoch:1066/50, training loss:1.2259023189544678
Train Acc 0.6680
 Acc 0.6679
new best val f1: 0.6678784526020468
meta,dgl,1,1065,36.4164,0.6679

epoch:1067/50, training loss:1.2255077362060547
Train Acc 0.6682
 Acc 0.6678
meta,dgl,1,1066,36.4500,0.6678

epoch:1068/50, training loss:1.2251108884811401
Train Acc 0.6682
 Acc 0.6681
new best val f1: 0.6681155411546212
meta,dgl,1,1067,36.4844,0.6681

epoch:1069/50, training loss:1.2247157096862793
Train Acc 0.6685
 Acc 0.6682
new best val f1: 0.6682011564652731
meta,dgl,1,1068,36.5180,0.6682

epoch:1070/50, training loss:1.2243194580078125
Train Acc 0.6686
 Acc 0.6682
meta,dgl,1,1069,36.5518,0.6682

epoch:1071/50, training loss:1.2239248752593994
Train Acc 0.6686
 Acc 0.6685
new best val f1: 0.6685304461216264
meta,dgl,1,1070,36.5855,0.6685

epoch:1072/50, training loss:1.2235314846038818
Train Acc 0.6689
 Acc 0.6687
new best val f1: 0.6686555761910407
meta,dgl,1,1071,36.6192,0.6687

epoch:1073/50, training loss:1.2231374979019165
Train Acc 0.6691
 Acc 0.6686
meta,dgl,1,1072,36.6538,0.6686

epoch:1074/50, training loss:1.2227426767349243
Train Acc 0.6690
 Acc 0.6689
new best val f1: 0.6688992505367422
meta,dgl,1,1073,36.6875,0.6689

epoch:1075/50, training loss:1.222350835800171
Train Acc 0.6693
 Acc 0.6691
new best val f1: 0.6690704811580459
meta,dgl,1,1074,36.7212,0.6691

epoch:1076/50, training loss:1.2219560146331787
Train Acc 0.6695
 Acc 0.6690
meta,dgl,1,1075,36.7549,0.6690

epoch:1077/50, training loss:1.2215629816055298
Train Acc 0.6694
 Acc 0.6693
new best val f1: 0.6693075697106202
meta,dgl,1,1076,36.7894,0.6693

epoch:1078/50, training loss:1.2211692333221436
Train Acc 0.6697
 Acc 0.6694
new best val f1: 0.6694326997800345
meta,dgl,1,1077,36.8230,0.6694

epoch:1079/50, training loss:1.2207776308059692
Train Acc 0.6698
 Acc 0.6695
new best val f1: 0.6694524571594157
meta,dgl,1,1078,36.8566,0.6695

epoch:1080/50, training loss:1.2203874588012695
Train Acc 0.6698
 Acc 0.6696
new best val f1: 0.6696302735738465
meta,dgl,1,1079,36.8902,0.6696

epoch:1081/50, training loss:1.2199950218200684
Train Acc 0.6699
 Acc 0.6701
new best val f1: 0.6701044506789953
meta,dgl,1,1080,36.9248,0.6701

epoch:1082/50, training loss:1.219604730606079
Train Acc 0.6704
 Acc 0.6700
meta,dgl,1,1081,36.9593,0.6700

epoch:1083/50, training loss:1.219212293624878
Train Acc 0.6703
 Acc 0.6700
meta,dgl,1,1082,36.9930,0.6700

epoch:1084/50, training loss:1.21882164478302
Train Acc 0.6703
 Acc 0.6704
new best val f1: 0.6703612966109509
meta,dgl,1,1083,37.0266,0.6704

epoch:1085/50, training loss:1.2184334993362427
Train Acc 0.6706
 Acc 0.6706
new best val f1: 0.6705522846116357
meta,dgl,1,1084,37.0612,0.6706

epoch:1086/50, training loss:1.2180439233779907
Train Acc 0.6707
 Acc 0.6706
new best val f1: 0.6706247283360335
meta,dgl,1,1085,37.0956,0.6706

epoch:1087/50, training loss:1.2176555395126343
Train Acc 0.6708
 Acc 0.6708
new best val f1: 0.6707827873710831
meta,dgl,1,1086,37.1292,0.6708

epoch:1088/50, training loss:1.2172638177871704
Train Acc 0.6710
 Acc 0.6710
new best val f1: 0.6709803611648951
meta,dgl,1,1087,37.1627,0.6710

epoch:1089/50, training loss:1.216878056526184
Train Acc 0.6711
 Acc 0.6709
meta,dgl,1,1088,37.1963,0.6709

epoch:1090/50, training loss:1.2164902687072754
Train Acc 0.6710
 Acc 0.6712
new best val f1: 0.6711647633724529
meta,dgl,1,1089,37.2299,0.6712

epoch:1091/50, training loss:1.2161023616790771
Train Acc 0.6713
 Acc 0.6713
new best val f1: 0.6713294082006296
meta,dgl,1,1090,37.2635,0.6713

epoch:1092/50, training loss:1.2157156467437744
Train Acc 0.6715
 Acc 0.6713
meta,dgl,1,1091,37.2980,0.6713

epoch:1093/50, training loss:1.215327262878418
Train Acc 0.6715
 Acc 0.6715
new best val f1: 0.6714545382700439
meta,dgl,1,1092,37.3324,0.6715

epoch:1094/50, training loss:1.2149394750595093
Train Acc 0.6717
 Acc 0.6716
new best val f1: 0.6716257688913476
meta,dgl,1,1093,37.3661,0.6716

epoch:1095/50, training loss:1.2145540714263916
Train Acc 0.6718
 Acc 0.6718
new best val f1: 0.6717640705470159
meta,dgl,1,1094,37.3998,0.6718

epoch:1096/50, training loss:1.2141653299331665
Train Acc 0.6719
 Acc 0.6720
new best val f1: 0.6719879875133362
meta,dgl,1,1095,37.4334,0.6720

epoch:1097/50, training loss:1.2137800455093384
Train Acc 0.6722
 Acc 0.6721
new best val f1: 0.6720999459964964
meta,dgl,1,1096,37.4679,0.6721

epoch:1098/50, training loss:1.2133948802947998
Train Acc 0.6723
 Acc 0.6721
meta,dgl,1,1097,37.5016,0.6721

epoch:1099/50, training loss:1.2130088806152344
Train Acc 0.6723
 Acc 0.6723
new best val f1: 0.6722843482040542
meta,dgl,1,1098,37.5353,0.6723

epoch:1100/50, training loss:1.2126245498657227
Train Acc 0.6725
 Acc 0.6724
new best val f1: 0.6724094782734684
meta,dgl,1,1099,37.5691,0.6724

epoch:1101/50, training loss:1.2122397422790527
Train Acc 0.6726
 Acc 0.6726
new best val f1: 0.6726004662741534
meta,dgl,1,1100,37.6036,0.6726

epoch:1102/50, training loss:1.2118544578552246
Train Acc 0.6728
 Acc 0.6727
new best val f1: 0.6726992531710594
meta,dgl,1,1101,37.6373,0.6727

epoch:1103/50, training loss:1.2114695310592651
Train Acc 0.6729
 Acc 0.6728
new best val f1: 0.6727914542748383
meta,dgl,1,1102,37.6708,0.6728

epoch:1104/50, training loss:1.2110868692398071
Train Acc 0.6730
 Acc 0.6729
new best val f1: 0.6729297559305067
meta,dgl,1,1103,37.7044,0.6729

epoch:1105/50, training loss:1.2107017040252686
Train Acc 0.6732
 Acc 0.6732
new best val f1: 0.673160258689954
meta,dgl,1,1104,37.7380,0.6732

epoch:1106/50, training loss:1.210317850112915
Train Acc 0.6733
 Acc 0.6733
new best val f1: 0.6732985603456224
meta,dgl,1,1105,37.7715,0.6733

epoch:1107/50, training loss:1.2099350690841675
Train Acc 0.6735
 Acc 0.6734
new best val f1: 0.673351246690639
meta,dgl,1,1106,37.8051,0.6734

epoch:1108/50, training loss:1.2095513343811035
Train Acc 0.6735
 Acc 0.6735
new best val f1: 0.6734697909669262
meta,dgl,1,1107,37.8395,0.6735

epoch:1109/50, training loss:1.2091726064682007
Train Acc 0.6736
 Acc 0.6737
new best val f1: 0.6737332226920087
meta,dgl,1,1108,37.8732,0.6737

epoch:1110/50, training loss:1.2087883949279785
Train Acc 0.6739
 Acc 0.6738
new best val f1: 0.673759565864517
meta,dgl,1,1109,37.9069,0.6738

epoch:1111/50, training loss:1.2084065675735474
Train Acc 0.6740
 Acc 0.6740
new best val f1: 0.6740032402102185
meta,dgl,1,1110,37.9407,0.6740

epoch:1112/50, training loss:1.2080246210098267
Train Acc 0.6742
 Acc 0.6741
new best val f1: 0.6741020271071245
meta,dgl,1,1111,37.9752,0.6741

epoch:1113/50, training loss:1.2076417207717896
Train Acc 0.6743
 Acc 0.6741
new best val f1: 0.6741283702796328
meta,dgl,1,1112,38.0090,0.6741

epoch:1114/50, training loss:1.2072618007659912
Train Acc 0.6743
 Acc 0.6744
new best val f1: 0.6743786304184612
meta,dgl,1,1113,38.0436,0.6744

epoch:1115/50, training loss:1.2068809270858765
Train Acc 0.6746
 Acc 0.6746
new best val f1: 0.6745696184191462
meta,dgl,1,1114,38.0773,0.6746

epoch:1116/50, training loss:1.2064985036849976
Train Acc 0.6748
 Acc 0.6745
meta,dgl,1,1115,38.1110,0.6745

epoch:1117/50, training loss:1.206118106842041
Train Acc 0.6748
 Acc 0.6748
new best val f1: 0.6748198785579748
meta,dgl,1,1116,38.1447,0.6748

epoch:1118/50, training loss:1.2057374715805054
Train Acc 0.6750
 Acc 0.6751
new best val f1: 0.6750964818693115
meta,dgl,1,1117,38.1793,0.6751

epoch:1119/50, training loss:1.2053583860397339
Train Acc 0.6752
 Acc 0.6751
new best val f1: 0.6751294108349468
meta,dgl,1,1118,38.2138,0.6751

epoch:1120/50, training loss:1.2049795389175415
Train Acc 0.6753
 Acc 0.6753
new best val f1: 0.6752874698699964
meta,dgl,1,1119,38.2474,0.6753

epoch:1121/50, training loss:1.2045986652374268
Train Acc 0.6754
 Acc 0.6754
new best val f1: 0.6754125999394107
meta,dgl,1,1120,38.2810,0.6754

epoch:1122/50, training loss:1.2042192220687866
Train Acc 0.6755
 Acc 0.6754
meta,dgl,1,1121,38.3146,0.6754

epoch:1123/50, training loss:1.2038395404815674
Train Acc 0.6755
 Acc 0.6757
new best val f1: 0.6757353038026369
meta,dgl,1,1122,38.3490,0.6757

epoch:1124/50, training loss:1.2034610509872437
Train Acc 0.6758
 Acc 0.6759
new best val f1: 0.6759131202170677
meta,dgl,1,1123,38.3827,0.6759

epoch:1125/50, training loss:1.203081488609314
Train Acc 0.6760
 Acc 0.6759
meta,dgl,1,1124,38.4163,0.6759

epoch:1126/50, training loss:1.2027039527893066
Train Acc 0.6760
 Acc 0.6760
new best val f1: 0.6760448360796091
meta,dgl,1,1125,38.4509,0.6760

epoch:1127/50, training loss:1.2023266553878784
Train Acc 0.6761
 Acc 0.6762
new best val f1: 0.6761897235284046
meta,dgl,1,1126,38.4846,0.6762

epoch:1128/50, training loss:1.2019500732421875
Train Acc 0.6762
 Acc 0.6763
new best val f1: 0.6762819246321835
meta,dgl,1,1127,38.5183,0.6763

epoch:1129/50, training loss:1.2015708684921265
Train Acc 0.6764
 Acc 0.6765
new best val f1: 0.6764794984259954
meta,dgl,1,1128,38.5521,0.6765

epoch:1130/50, training loss:1.2011929750442505
Train Acc 0.6766
 Acc 0.6767
new best val f1: 0.6766902438060616
meta,dgl,1,1129,38.5857,0.6767

epoch:1131/50, training loss:1.20081627368927
Train Acc 0.6768
 Acc 0.6767
meta,dgl,1,1130,38.6193,0.6767

epoch:1132/50, training loss:1.2004377841949463
Train Acc 0.6767
 Acc 0.6769
new best val f1: 0.6768614744273653
meta,dgl,1,1131,38.6538,0.6769

epoch:1133/50, training loss:1.2000616788864136
Train Acc 0.6769
 Acc 0.6771
new best val f1: 0.6771249061524479
meta,dgl,1,1132,38.6875,0.6771

epoch:1134/50, training loss:1.199684739112854
Train Acc 0.6771
 Acc 0.6771
meta,dgl,1,1133,38.7212,0.6771

epoch:1135/50, training loss:1.1993087530136108
Train Acc 0.6772
 Acc 0.6772
new best val f1: 0.6771710067043374
meta,dgl,1,1134,38.7548,0.6772

epoch:1136/50, training loss:1.1989326477050781
Train Acc 0.6772
 Acc 0.6775
new best val f1: 0.6774937105675637
meta,dgl,1,1135,38.7893,0.6775

epoch:1137/50, training loss:1.198557734489441
Train Acc 0.6775
 Acc 0.6777
new best val f1: 0.6776912843613756
meta,dgl,1,1136,38.8239,0.6777

epoch:1138/50, training loss:1.1981793642044067
Train Acc 0.6777
 Acc 0.6776
meta,dgl,1,1137,38.8575,0.6776

epoch:1139/50, training loss:1.1978044509887695
Train Acc 0.6776
 Acc 0.6779
new best val f1: 0.6778822723620606
meta,dgl,1,1138,38.8909,0.6779

epoch:1140/50, training loss:1.1974271535873413
Train Acc 0.6778
 Acc 0.6782
new best val f1: 0.6781522898802703
meta,dgl,1,1139,38.9244,0.6782

epoch:1141/50, training loss:1.1970536708831787
Train Acc 0.6781
 Acc 0.6781
meta,dgl,1,1140,38.9588,0.6781

epoch:1142/50, training loss:1.196679949760437
Train Acc 0.6781
 Acc 0.6783
new best val f1: 0.6782905915359386
meta,dgl,1,1141,38.9923,0.6783

epoch:1143/50, training loss:1.1963032484054565
Train Acc 0.6783
 Acc 0.6785
new best val f1: 0.6785408516747672
meta,dgl,1,1142,39.0268,0.6785

epoch:1144/50, training loss:1.1959279775619507
Train Acc 0.6785
 Acc 0.6784
meta,dgl,1,1143,39.0665,0.6784

epoch:1145/50, training loss:1.1955554485321045
Train Acc 0.6784
 Acc 0.6786
new best val f1: 0.6786330527785461
meta,dgl,1,1144,39.1001,0.6786

epoch:1146/50, training loss:1.195180058479309
Train Acc 0.6786
 Acc 0.6788
new best val f1: 0.6788372123654852
meta,dgl,1,1145,39.1335,0.6788

epoch:1147/50, training loss:1.194806456565857
Train Acc 0.6788
 Acc 0.6790
new best val f1: 0.6790282003661701
meta,dgl,1,1146,39.1672,0.6790

epoch:1148/50, training loss:1.194434404373169
Train Acc 0.6790
 Acc 0.6789
meta,dgl,1,1147,39.2016,0.6789

epoch:1149/50, training loss:1.1940598487854004
Train Acc 0.6789
 Acc 0.6793
new best val f1: 0.6793048036775069
meta,dgl,1,1148,39.2404,0.6793

epoch:1150/50, training loss:1.1936858892440796
Train Acc 0.6793
 Acc 0.6793
meta,dgl,1,1149,39.2739,0.6793

epoch:1151/50, training loss:1.193314790725708
Train Acc 0.6792
 Acc 0.6794
new best val f1: 0.679416762160667
meta,dgl,1,1150,39.3075,0.6794

epoch:1152/50, training loss:1.1929404735565186
Train Acc 0.6794
 Acc 0.6797
new best val f1: 0.6797460518170203
meta,dgl,1,1151,39.3411,0.6797

epoch:1153/50, training loss:1.1925686597824097
Train Acc 0.6796
 Acc 0.6797
meta,dgl,1,1152,39.3747,0.6797

epoch:1154/50, training loss:1.1921955347061157
Train Acc 0.6796
 Acc 0.6797
meta,dgl,1,1153,39.4082,0.6797

epoch:1155/50, training loss:1.1918226480484009
Train Acc 0.6797
 Acc 0.6801
new best val f1: 0.6800555840939925
meta,dgl,1,1154,39.4417,0.6801

epoch:1156/50, training loss:1.1914522647857666
Train Acc 0.6800
 Acc 0.6802
new best val f1: 0.6802004715427878
meta,dgl,1,1155,39.4763,0.6802

epoch:1157/50, training loss:1.1910802125930786
Train Acc 0.6801
 Acc 0.6802
meta,dgl,1,1156,39.5107,0.6802

epoch:1158/50, training loss:1.1907083988189697
Train Acc 0.6801
 Acc 0.6803
new best val f1: 0.6803058442328209
meta,dgl,1,1157,39.5501,0.6803

epoch:1159/50, training loss:1.190335988998413
Train Acc 0.6802
 Acc 0.6806
new best val f1: 0.6805561043716495
meta,dgl,1,1158,39.5837,0.6806

epoch:1160/50, training loss:1.1899667978286743
Train Acc 0.6805
 Acc 0.6804
meta,dgl,1,1159,39.6173,0.6804

epoch:1161/50, training loss:1.1895945072174072
Train Acc 0.6804
 Acc 0.6807
new best val f1: 0.6806746486479367
meta,dgl,1,1160,39.6510,0.6807

epoch:1162/50, training loss:1.1892249584197998
Train Acc 0.6806
 Acc 0.6809
new best val f1: 0.6808985656142569
meta,dgl,1,1161,39.6855,0.6809

epoch:1163/50, training loss:1.188854455947876
Train Acc 0.6809
 Acc 0.6808
meta,dgl,1,1162,39.7198,0.6808

epoch:1164/50, training loss:1.1884832382202148
Train Acc 0.6807
 Acc 0.6810
new best val f1: 0.6809512519592734
meta,dgl,1,1163,39.7534,0.6810

epoch:1165/50, training loss:1.1881150007247925
Train Acc 0.6809
 Acc 0.6812
new best val f1: 0.6812212694774832
meta,dgl,1,1164,39.7869,0.6812

epoch:1166/50, training loss:1.1877444982528687
Train Acc 0.6812
 Acc 0.6812
meta,dgl,1,1165,39.8205,0.6812

epoch:1167/50, training loss:1.1873725652694702
Train Acc 0.6812
 Acc 0.6813
new best val f1: 0.6812607842362456
meta,dgl,1,1166,39.8549,0.6813

epoch:1168/50, training loss:1.1870038509368896
Train Acc 0.6813
 Acc 0.6814
new best val f1: 0.6814254290644223
meta,dgl,1,1167,39.8885,0.6814

epoch:1169/50, training loss:1.1866345405578613
Train Acc 0.6815
 Acc 0.6816
new best val f1: 0.68160983127198
meta,dgl,1,1168,39.9229,0.6816

epoch:1170/50, training loss:1.1862659454345703
Train Acc 0.6817
 Acc 0.6816
meta,dgl,1,1169,39.9565,0.6816

epoch:1171/50, training loss:1.1858969926834106
Train Acc 0.6816
 Acc 0.6819
new best val f1: 0.681899606169571
meta,dgl,1,1170,39.9901,0.6819

epoch:1172/50, training loss:1.1855275630950928
Train Acc 0.6819
 Acc 0.6819
new best val f1: 0.6819391209283334
meta,dgl,1,1171,40.0237,0.6819

epoch:1173/50, training loss:1.1851592063903809
Train Acc 0.6820
 Acc 0.6820
new best val f1: 0.6819786356870958
meta,dgl,1,1172,40.0574,0.6820

epoch:1174/50, training loss:1.1847920417785645
Train Acc 0.6820
 Acc 0.6823
new best val f1: 0.6822815821709408
meta,dgl,1,1173,40.0919,0.6823

epoch:1175/50, training loss:1.184423565864563
Train Acc 0.6823
 Acc 0.6825
new best val f1: 0.6824528127922446
meta,dgl,1,1174,40.1254,0.6825

epoch:1176/50, training loss:1.184056282043457
Train Acc 0.6825
 Acc 0.6823
meta,dgl,1,1175,40.1590,0.6823

epoch:1177/50, training loss:1.1836868524551392
Train Acc 0.6824
 Acc 0.6827
new best val f1: 0.6827228303104543
meta,dgl,1,1176,40.1926,0.6827

epoch:1178/50, training loss:1.1833194494247437
Train Acc 0.6828
 Acc 0.6827
meta,dgl,1,1177,40.2261,0.6827

epoch:1179/50, training loss:1.1829513311386108
Train Acc 0.6827
 Acc 0.6828
new best val f1: 0.682801859827979
meta,dgl,1,1178,40.2598,0.6828

epoch:1180/50, training loss:1.1825859546661377
Train Acc 0.6828
 Acc 0.6830
new best val f1: 0.6830323625874264
meta,dgl,1,1179,40.2934,0.6830

epoch:1181/50, training loss:1.182218074798584
Train Acc 0.6831
 Acc 0.6831
new best val f1: 0.6830784631393159
meta,dgl,1,1180,40.3271,0.6831

epoch:1182/50, training loss:1.1818515062332153
Train Acc 0.6833
 Acc 0.6830
meta,dgl,1,1181,40.3606,0.6830

epoch:1183/50, training loss:1.1814862489700317
Train Acc 0.6831
 Acc 0.6834
new best val f1: 0.6833682380369068
meta,dgl,1,1182,40.3941,0.6834

epoch:1184/50, training loss:1.1811200380325317
Train Acc 0.6836
 Acc 0.6835
new best val f1: 0.6834604391406857
meta,dgl,1,1183,40.4278,0.6835

epoch:1185/50, training loss:1.1807527542114258
Train Acc 0.6836
 Acc 0.6834
meta,dgl,1,1184,40.4614,0.6834

epoch:1186/50, training loss:1.1803871393203735
Train Acc 0.6836
 Acc 0.6837
new best val f1: 0.6836645987276248
meta,dgl,1,1185,40.4958,0.6837

epoch:1187/50, training loss:1.1800211668014526
Train Acc 0.6838
 Acc 0.6838
new best val f1: 0.6838160719695473
meta,dgl,1,1186,40.5295,0.6838

epoch:1188/50, training loss:1.1796554327011108
Train Acc 0.6840
 Acc 0.6838
meta,dgl,1,1187,40.5632,0.6838

epoch:1189/50, training loss:1.1792902946472168
Train Acc 0.6839
 Acc 0.6840
new best val f1: 0.6839938883839781
meta,dgl,1,1188,40.5968,0.6840

epoch:1190/50, training loss:1.1789261102676392
Train Acc 0.6841
 Acc 0.6843
new best val f1: 0.684290249074696
meta,dgl,1,1189,40.6314,0.6843

epoch:1191/50, training loss:1.1785615682601929
Train Acc 0.6844
 Acc 0.6841
meta,dgl,1,1190,40.6659,0.6841

epoch:1192/50, training loss:1.1781952381134033
Train Acc 0.6842
 Acc 0.6844
new best val f1: 0.6844219649372374
meta,dgl,1,1191,40.6996,0.6844

epoch:1193/50, training loss:1.1778312921524048
Train Acc 0.6846
 Acc 0.6847
new best val f1: 0.6846524676966848
meta,dgl,1,1192,40.7333,0.6847

epoch:1194/50, training loss:1.1774661540985107
Train Acc 0.6848
 Acc 0.6845
meta,dgl,1,1193,40.7669,0.6845

epoch:1195/50, training loss:1.1771016120910645
Train Acc 0.6846
 Acc 0.6848
new best val f1: 0.684784183559226
meta,dgl,1,1194,40.8014,0.6848

epoch:1196/50, training loss:1.176737904548645
Train Acc 0.6849
 Acc 0.6850
new best val f1: 0.6849685857667839
meta,dgl,1,1195,40.8350,0.6850

epoch:1197/50, training loss:1.1763752698898315
Train Acc 0.6851
 Acc 0.6849
meta,dgl,1,1196,40.8687,0.6849

epoch:1198/50, training loss:1.176011085510254
Train Acc 0.6850
 Acc 0.6852
new best val f1: 0.6852188459056124
meta,dgl,1,1197,40.9024,0.6852

epoch:1199/50, training loss:1.1756483316421509
Train Acc 0.6853
 Acc 0.6855
new best val f1: 0.6855086208032033
meta,dgl,1,1198,40.9360,0.6855

epoch:1200/50, training loss:1.1752855777740479
Train Acc 0.6855
 Acc 0.6854
meta,dgl,1,1199,40.9696,0.6854

epoch:1201/50, training loss:1.174921989440918
Train Acc 0.6854
 Acc 0.6856
new best val f1: 0.6856337508726176
meta,dgl,1,1200,41.0032,0.6856

epoch:1202/50, training loss:1.1745582818984985
Train Acc 0.6856
 Acc 0.6858
new best val f1: 0.6858379104595567
meta,dgl,1,1201,41.0368,0.6858

epoch:1203/50, training loss:1.1741952896118164
Train Acc 0.6859
 Acc 0.6857
meta,dgl,1,1202,41.0704,0.6857

epoch:1204/50, training loss:1.1738338470458984
Train Acc 0.6857
 Acc 0.6861
new best val f1: 0.6860618274258768
meta,dgl,1,1203,41.1040,0.6861

epoch:1205/50, training loss:1.1734709739685059
Train Acc 0.6861
 Acc 0.6861
new best val f1: 0.6861276853571475
meta,dgl,1,1204,41.1384,0.6861

epoch:1206/50, training loss:1.1731065511703491
Train Acc 0.6862
 Acc 0.6862
new best val f1: 0.686173785909037
meta,dgl,1,1205,41.1721,0.6862

epoch:1207/50, training loss:1.1727497577667236
Train Acc 0.6862
 Acc 0.6863
new best val f1: 0.6863055017715783
meta,dgl,1,1206,41.2057,0.6863

epoch:1208/50, training loss:1.1723839044570923
Train Acc 0.6864
 Acc 0.6866
new best val f1: 0.6865557619104069
meta,dgl,1,1207,41.2393,0.6866

epoch:1209/50, training loss:1.1720240116119385
Train Acc 0.6866
 Acc 0.6865
meta,dgl,1,1208,41.2739,0.6865

epoch:1210/50, training loss:1.1716606616973877
Train Acc 0.6865
 Acc 0.6868
new best val f1: 0.6867533357042188
meta,dgl,1,1209,41.3075,0.6868

epoch:1211/50, training loss:1.171301245689392
Train Acc 0.6868
 Acc 0.6870
new best val f1: 0.6869970100499203
meta,dgl,1,1210,41.3410,0.6870

epoch:1212/50, training loss:1.170937418937683
Train Acc 0.6870
 Acc 0.6869
meta,dgl,1,1211,41.3747,0.6869

epoch:1213/50, training loss:1.1705763339996338
Train Acc 0.6870
 Acc 0.6871
new best val f1: 0.6871155543262075
meta,dgl,1,1212,41.4083,0.6871

epoch:1214/50, training loss:1.1702165603637695
Train Acc 0.6872
 Acc 0.6874
new best val f1: 0.6874053292237984
meta,dgl,1,1213,41.4427,0.6874

epoch:1215/50, training loss:1.1698552370071411
Train Acc 0.6874
 Acc 0.6874
new best val f1: 0.6874382581894337
meta,dgl,1,1214,41.4763,0.6874

epoch:1216/50, training loss:1.1694952249526978
Train Acc 0.6875
 Acc 0.6875
new best val f1: 0.6875304592932127
meta,dgl,1,1215,41.5099,0.6875

epoch:1217/50, training loss:1.1691360473632812
Train Acc 0.6875
 Acc 0.6878
new best val f1: 0.6878136483976766
meta,dgl,1,1216,41.5436,0.6878

epoch:1218/50, training loss:1.168776035308838
Train Acc 0.6879
 Acc 0.6877
meta,dgl,1,1217,41.5771,0.6877

epoch:1219/50, training loss:1.168415904045105
Train Acc 0.6877
 Acc 0.6878
new best val f1: 0.6878399915701848
meta,dgl,1,1218,41.6151,0.6878

epoch:1220/50, training loss:1.1680573225021362
Train Acc 0.6879
 Acc 0.6880
new best val f1: 0.6880309795708697
meta,dgl,1,1219,41.6488,0.6880

epoch:1221/50, training loss:1.1676958799362183
Train Acc 0.6882
 Acc 0.6881
new best val f1: 0.6881297664677757
meta,dgl,1,1220,41.6834,0.6881

epoch:1222/50, training loss:1.1673340797424316
Train Acc 0.6883
 Acc 0.6882
new best val f1: 0.6881890386059193
meta,dgl,1,1221,41.7171,0.6882

epoch:1223/50, training loss:1.1669752597808838
Train Acc 0.6883
 Acc 0.6884
new best val f1: 0.688360269227223
meta,dgl,1,1222,41.7508,0.6884

epoch:1224/50, training loss:1.1666189432144165
Train Acc 0.6885
 Acc 0.6885
new best val f1: 0.6885051566760185
meta,dgl,1,1223,41.7844,0.6885

epoch:1225/50, training loss:1.166259527206421
Train Acc 0.6887
 Acc 0.6885
new best val f1: 0.6885446714347809
meta,dgl,1,1224,41.8190,0.6885

epoch:1226/50, training loss:1.1659005880355835
Train Acc 0.6887
 Acc 0.6888
new best val f1: 0.6888212747461177
meta,dgl,1,1225,41.8534,0.6888

epoch:1227/50, training loss:1.1655421257019043
Train Acc 0.6889
 Acc 0.6888
new best val f1: 0.6888344463323718
meta,dgl,1,1226,41.8870,0.6888

epoch:1228/50, training loss:1.1651841402053833
Train Acc 0.6889
 Acc 0.6890
new best val f1: 0.6889793337811673
meta,dgl,1,1227,41.9207,0.6890

epoch:1229/50, training loss:1.1648260354995728
Train Acc 0.6891
 Acc 0.6891
new best val f1: 0.6890715348849462
meta,dgl,1,1228,41.9544,0.6891

epoch:1230/50, training loss:1.1644655466079712
Train Acc 0.6892
 Acc 0.6892
new best val f1: 0.6891900791612334
meta,dgl,1,1229,41.9889,0.6892

epoch:1231/50, training loss:1.1641088724136353
Train Acc 0.6894
 Acc 0.6896
new best val f1: 0.6895918125419844
meta,dgl,1,1230,42.0225,0.6896

epoch:1232/50, training loss:1.1637502908706665
Train Acc 0.6897
 Acc 0.6894
meta,dgl,1,1231,42.0561,0.6894

epoch:1233/50, training loss:1.1633927822113037
Train Acc 0.6895
 Acc 0.6899
new best val f1: 0.6898684158533213
meta,dgl,1,1232,42.0906,0.6899

epoch:1234/50, training loss:1.1630367040634155
Train Acc 0.6899
 Acc 0.6899
new best val f1: 0.6899211021983378
meta,dgl,1,1233,42.1242,0.6899

epoch:1235/50, training loss:1.1626805067062378
Train Acc 0.6900
 Acc 0.6899
meta,dgl,1,1234,42.1578,0.6899

epoch:1236/50, training loss:1.162322759628296
Train Acc 0.6900
 Acc 0.6903
new best val f1: 0.6902767350271993
meta,dgl,1,1235,42.1914,0.6903

epoch:1237/50, training loss:1.1619634628295898
Train Acc 0.6903
 Acc 0.6905
new best val f1: 0.6904677230278843
meta,dgl,1,1236,42.2258,0.6905

epoch:1238/50, training loss:1.1616075038909912
Train Acc 0.6904
 Acc 0.6903
meta,dgl,1,1237,42.2594,0.6903

epoch:1239/50, training loss:1.1612508296966553
Train Acc 0.6903
 Acc 0.6906
new best val f1: 0.6905862673041714
meta,dgl,1,1238,42.2929,0.6906

epoch:1240/50, training loss:1.1608922481536865
Train Acc 0.6906
 Acc 0.6908
new best val f1: 0.690836527443
meta,dgl,1,1239,42.3265,0.6908

epoch:1241/50, training loss:1.160536766052246
Train Acc 0.6909
 Acc 0.6907
meta,dgl,1,1240,42.3603,0.6907

epoch:1242/50, training loss:1.1601828336715698
Train Acc 0.6908
 Acc 0.6908
new best val f1: 0.690843113236127
meta,dgl,1,1241,42.3939,0.6908

epoch:1243/50, training loss:1.1598244905471802
Train Acc 0.6909
 Acc 0.6912
new best val f1: 0.691244846616878
meta,dgl,1,1242,42.4275,0.6912

epoch:1244/50, training loss:1.1594703197479248
Train Acc 0.6913
 Acc 0.6909
meta,dgl,1,1243,42.4611,0.6909

epoch:1245/50, training loss:1.1591157913208008
Train Acc 0.6910
 Acc 0.6913
new best val f1: 0.6912843613756404
meta,dgl,1,1244,42.4947,0.6913

epoch:1246/50, training loss:1.1587581634521484
Train Acc 0.6913
 Acc 0.6917
new best val f1: 0.6917124379288998
meta,dgl,1,1245,42.5283,0.6917

epoch:1247/50, training loss:1.1584022045135498
Train Acc 0.6918
 Acc 0.6915
meta,dgl,1,1246,42.5619,0.6915

epoch:1248/50, training loss:1.1580487489700317
Train Acc 0.6916
 Acc 0.6917
meta,dgl,1,1247,42.5955,0.6917

epoch:1249/50, training loss:1.1576915979385376
Train Acc 0.6917
 Acc 0.6920
new best val f1: 0.6919626980677283
meta,dgl,1,1248,42.6291,0.6920

epoch:1250/50, training loss:1.1573394536972046
Train Acc 0.6921
 Acc 0.6919
meta,dgl,1,1249,42.6627,0.6919

epoch:1251/50, training loss:1.1569825410842896
Train Acc 0.6920
 Acc 0.6921
new best val f1: 0.6920812423440155
meta,dgl,1,1250,42.6972,0.6921

epoch:1252/50, training loss:1.1566303968429565
Train Acc 0.6922
 Acc 0.6924
new best val f1: 0.6924368751728771
meta,dgl,1,1251,42.7308,0.6924

epoch:1253/50, training loss:1.1562741994857788
Train Acc 0.6925
 Acc 0.6924
meta,dgl,1,1252,42.7644,0.6924

epoch:1254/50, training loss:1.1559195518493652
Train Acc 0.6925
 Acc 0.6924
meta,dgl,1,1253,42.7980,0.6924

epoch:1255/50, training loss:1.1555662155151367
Train Acc 0.6925
 Acc 0.6929
new best val f1: 0.6928583659330093
meta,dgl,1,1254,42.8323,0.6929

epoch:1256/50, training loss:1.155210018157959
Train Acc 0.6929
 Acc 0.6928
meta,dgl,1,1255,42.8660,0.6928

epoch:1257/50, training loss:1.1548575162887573
Train Acc 0.6929
 Acc 0.6929
new best val f1: 0.6929110522780259
meta,dgl,1,1256,42.8997,0.6929

epoch:1258/50, training loss:1.154503345489502
Train Acc 0.6930
 Acc 0.6931
new best val f1: 0.6930756971062025
meta,dgl,1,1257,42.9342,0.6931

epoch:1259/50, training loss:1.1541510820388794
Train Acc 0.6932
 Acc 0.6932
new best val f1: 0.6931678982099815
meta,dgl,1,1258,42.9678,0.6932

epoch:1260/50, training loss:1.1537989377975464
Train Acc 0.6933
 Acc 0.6935
new best val f1: 0.6934971878663347
meta,dgl,1,1259,43.0013,0.6935

epoch:1261/50, training loss:1.1534453630447388
Train Acc 0.6936
 Acc 0.6934
meta,dgl,1,1260,43.0349,0.6934

epoch:1262/50, training loss:1.1530910730361938
Train Acc 0.6935
 Acc 0.6936
new best val f1: 0.693622317935749
meta,dgl,1,1261,43.0686,0.6936

epoch:1263/50, training loss:1.1527361869812012
Train Acc 0.6938
 Acc 0.6937
new best val f1: 0.693714519039528
meta,dgl,1,1262,43.1022,0.6937

epoch:1264/50, training loss:1.1523858308792114
Train Acc 0.6939
 Acc 0.6939
new best val f1: 0.693918678626467
meta,dgl,1,1263,43.1366,0.6939

epoch:1265/50, training loss:1.1520321369171143
Train Acc 0.6941
 Acc 0.6940
new best val f1: 0.6939647791783564
meta,dgl,1,1264,43.1702,0.6940

epoch:1266/50, training loss:1.1516804695129395
Train Acc 0.6941
 Acc 0.6945
new best val f1: 0.6944587136628865
meta,dgl,1,1265,43.2039,0.6945

epoch:1267/50, training loss:1.151328444480896
Train Acc 0.6945
 Acc 0.6946
new best val f1: 0.6946299442841901
meta,dgl,1,1266,43.2376,0.6946

epoch:1268/50, training loss:1.1509757041931152
Train Acc 0.6947
 Acc 0.6940
meta,dgl,1,1267,43.2721,0.6940

epoch:1269/50, training loss:1.1506260633468628
Train Acc 0.6941
 Acc 0.6948
new best val f1: 0.6948341038711292
meta,dgl,1,1268,43.3066,0.6948

epoch:1270/50, training loss:1.1502729654312134
Train Acc 0.6950
 Acc 0.6950
new best val f1: 0.6949724055267976
meta,dgl,1,1269,43.3402,0.6950

epoch:1271/50, training loss:1.1499203443527222
Train Acc 0.6950
 Acc 0.6945
meta,dgl,1,1270,43.3738,0.6945

epoch:1272/50, training loss:1.1495717763900757
Train Acc 0.6945
 Acc 0.6954
new best val f1: 0.6954465826319464
meta,dgl,1,1271,43.4075,0.6954

epoch:1273/50, training loss:1.1492177248001099
Train Acc 0.6955
 Acc 0.6953
meta,dgl,1,1272,43.4419,0.6953

epoch:1274/50, training loss:1.1488666534423828
Train Acc 0.6954
 Acc 0.6951
meta,dgl,1,1273,43.4764,0.6951

epoch:1275/50, training loss:1.148516058921814
Train Acc 0.6952
 Acc 0.6957
new best val f1: 0.695703428563902
meta,dgl,1,1274,43.5100,0.6957

epoch:1276/50, training loss:1.148165225982666
Train Acc 0.6958
 Acc 0.6956
meta,dgl,1,1275,43.5437,0.6956

epoch:1277/50, training loss:1.1478145122528076
Train Acc 0.6957
 Acc 0.6955
meta,dgl,1,1276,43.5773,0.6955

epoch:1278/50, training loss:1.1474639177322388
Train Acc 0.6956
 Acc 0.6962
new best val f1: 0.6962237062209402
meta,dgl,1,1277,43.6111,0.6962

epoch:1279/50, training loss:1.1471141576766968
Train Acc 0.6963
 Acc 0.6958
meta,dgl,1,1278,43.6447,0.6958

epoch:1280/50, training loss:1.1467640399932861
Train Acc 0.6959
 Acc 0.6961
meta,dgl,1,1279,43.6783,0.6961

epoch:1281/50, training loss:1.1464110612869263
Train Acc 0.6963
 Acc 0.6966
new best val f1: 0.6965793390498017
meta,dgl,1,1280,43.7120,0.6966

epoch:1282/50, training loss:1.1460620164871216
Train Acc 0.6967
 Acc 0.6961
meta,dgl,1,1281,43.7466,0.6961

epoch:1283/50, training loss:1.1457135677337646
Train Acc 0.6962
 Acc 0.6966
new best val f1: 0.6965859248429288
meta,dgl,1,1282,43.7802,0.6966

epoch:1284/50, training loss:1.1453641653060913
Train Acc 0.6967
 Acc 0.6969
new best val f1: 0.6968756997405198
meta,dgl,1,1283,43.8137,0.6969

epoch:1285/50, training loss:1.1450132131576538
Train Acc 0.6970
 Acc 0.6967
meta,dgl,1,1284,43.8474,0.6967

epoch:1286/50, training loss:1.1446633338928223
Train Acc 0.6968
 Acc 0.6969
new best val f1: 0.6968822855336468
meta,dgl,1,1285,43.8819,0.6969

epoch:1287/50, training loss:1.1443159580230713
Train Acc 0.6969
 Acc 0.6973
new best val f1: 0.6973367052594144
meta,dgl,1,1286,43.9156,0.6973

epoch:1288/50, training loss:1.1439669132232666
Train Acc 0.6975
 Acc 0.6971
meta,dgl,1,1287,43.9501,0.6971

epoch:1289/50, training loss:1.1436153650283813
Train Acc 0.6972
 Acc 0.6973
meta,dgl,1,1288,43.9838,0.6973

epoch:1290/50, training loss:1.1432666778564453
Train Acc 0.6974
 Acc 0.6977
new best val f1: 0.6977252670539114
meta,dgl,1,1289,44.0183,0.6977

epoch:1291/50, training loss:1.1429194211959839
Train Acc 0.6978
 Acc 0.6976
meta,dgl,1,1290,44.0518,0.6976

epoch:1292/50, training loss:1.1425690650939941
Train Acc 0.6977
 Acc 0.6978
new best val f1: 0.6977713676058007
meta,dgl,1,1291,44.0856,0.6978

epoch:1293/50, training loss:1.142220377922058
Train Acc 0.6979
 Acc 0.6980
new best val f1: 0.6979623556064857
meta,dgl,1,1292,44.1191,0.6980

epoch:1294/50, training loss:1.141871452331543
Train Acc 0.6981
 Acc 0.6981
new best val f1: 0.6981270004346624
meta,dgl,1,1293,44.1527,0.6981

epoch:1295/50, training loss:1.1415232419967651
Train Acc 0.6983
 Acc 0.6979
meta,dgl,1,1294,44.1864,0.6979

epoch:1296/50, training loss:1.1411769390106201
Train Acc 0.6980
 Acc 0.6984
new best val f1: 0.6983575031941097
meta,dgl,1,1295,44.2209,0.6984

epoch:1297/50, training loss:1.140829086303711
Train Acc 0.6985
 Acc 0.6984
new best val f1: 0.6984497042978886
meta,dgl,1,1296,44.2546,0.6984

epoch:1298/50, training loss:1.1404802799224854
Train Acc 0.6986
 Acc 0.6985
new best val f1: 0.6985023906429051
meta,dgl,1,1297,44.2892,0.6985

epoch:1299/50, training loss:1.1401324272155762
Train Acc 0.6986
 Acc 0.6988
new best val f1: 0.6987921655404961
meta,dgl,1,1298,44.3238,0.6988

epoch:1300/50, training loss:1.1397861242294312
Train Acc 0.6989
 Acc 0.6989
new best val f1: 0.6988514376786397
meta,dgl,1,1299,44.3574,0.6989

epoch:1301/50, training loss:1.1394370794296265
Train Acc 0.6990
 Acc 0.6990
new best val f1: 0.6989502245755457
meta,dgl,1,1300,44.3912,0.6990

epoch:1302/50, training loss:1.1390901803970337
Train Acc 0.6990
 Acc 0.6993
new best val f1: 0.6993190289906613
meta,dgl,1,1301,44.4249,0.6993

epoch:1303/50, training loss:1.1387416124343872
Train Acc 0.6994
 Acc 0.6994
new best val f1: 0.699378301128805
meta,dgl,1,1302,44.4593,0.6994

epoch:1304/50, training loss:1.1383945941925049
Train Acc 0.6995
 Acc 0.6993
meta,dgl,1,1303,44.4937,0.6993

epoch:1305/50, training loss:1.1380482912063599
Train Acc 0.6994
 Acc 0.6997
new best val f1: 0.6996614902332688
meta,dgl,1,1304,44.5273,0.6997

epoch:1306/50, training loss:1.1377012729644775
Train Acc 0.6998
 Acc 0.6996
meta,dgl,1,1305,44.5608,0.6996

epoch:1307/50, training loss:1.137355923652649
Train Acc 0.6997
 Acc 0.6998
new best val f1: 0.6997932060958101
meta,dgl,1,1306,44.5943,0.6998

epoch:1308/50, training loss:1.1370084285736084
Train Acc 0.6999
 Acc 0.7000
new best val f1: 0.6999973656827492
meta,dgl,1,1307,44.6279,0.7000

epoch:1309/50, training loss:1.136662244796753
Train Acc 0.7002
 Acc 0.7001
new best val f1: 0.7000566378208928
meta,dgl,1,1308,44.6615,0.7001

epoch:1310/50, training loss:1.1363154649734497
Train Acc 0.7002
 Acc 0.7002
new best val f1: 0.7001685963040529
meta,dgl,1,1309,44.6951,0.7002

epoch:1311/50, training loss:1.135970115661621
Train Acc 0.7003
 Acc 0.7002
new best val f1: 0.7002278684421965
meta,dgl,1,1310,44.7287,0.7002

epoch:1312/50, training loss:1.1356254816055298
Train Acc 0.7004
 Acc 0.7006
new best val f1: 0.7005571580985498
meta,dgl,1,1311,44.7632,0.7006

epoch:1313/50, training loss:1.135277271270752
Train Acc 0.7008
 Acc 0.7004
meta,dgl,1,1312,44.7968,0.7004

epoch:1314/50, training loss:1.1349341869354248
Train Acc 0.7006
 Acc 0.7007
new best val f1: 0.7006625307885829
meta,dgl,1,1313,44.8305,0.7007

epoch:1315/50, training loss:1.1345878839492798
Train Acc 0.7009
 Acc 0.7008
new best val f1: 0.7008337614098866
meta,dgl,1,1314,44.8642,0.7008

epoch:1316/50, training loss:1.1342437267303467
Train Acc 0.7011
 Acc 0.7008
meta,dgl,1,1315,44.8978,0.7008

epoch:1317/50, training loss:1.1338952779769897
Train Acc 0.7010
 Acc 0.7011
new best val f1: 0.7011367078937316
meta,dgl,1,1316,44.9314,0.7011

epoch:1318/50, training loss:1.1335499286651611
Train Acc 0.7014
 Acc 0.7013
new best val f1: 0.7012552521700188
meta,dgl,1,1317,44.9651,0.7013

epoch:1319/50, training loss:1.1332076787948608
Train Acc 0.7015
 Acc 0.7012
meta,dgl,1,1318,44.9987,0.7012

epoch:1320/50, training loss:1.1328613758087158
Train Acc 0.7014
 Acc 0.7014
new best val f1: 0.7014133112050684
meta,dgl,1,1319,45.0331,0.7014

epoch:1321/50, training loss:1.1325162649154663
Train Acc 0.7017
 Acc 0.7017
new best val f1: 0.7017491866545488
meta,dgl,1,1320,45.0668,0.7017

epoch:1322/50, training loss:1.1321721076965332
Train Acc 0.7020
 Acc 0.7014
meta,dgl,1,1321,45.1004,0.7014

epoch:1323/50, training loss:1.1318310499191284
Train Acc 0.7016
 Acc 0.7020
new best val f1: 0.7019796894139961
meta,dgl,1,1322,45.1349,0.7020

epoch:1324/50, training loss:1.1314815282821655
Train Acc 0.7022
 Acc 0.7020
new best val f1: 0.7020389615521397
meta,dgl,1,1323,45.1686,0.7020

epoch:1325/50, training loss:1.1311379671096802
Train Acc 0.7023
 Acc 0.7019
meta,dgl,1,1324,45.2022,0.7019

epoch:1326/50, training loss:1.130795955657959
Train Acc 0.7021
 Acc 0.7023
new best val f1: 0.7023419080359847
meta,dgl,1,1325,45.2367,0.7023

epoch:1327/50, training loss:1.1304516792297363
Train Acc 0.7026
 Acc 0.7025
new best val f1: 0.7024736238985261
meta,dgl,1,1326,45.2711,0.7025

epoch:1328/50, training loss:1.1301079988479614
Train Acc 0.7027
 Acc 0.7022
meta,dgl,1,1327,45.3048,0.7022

epoch:1329/50, training loss:1.1297658681869507
Train Acc 0.7024
 Acc 0.7027
new best val f1: 0.702664611899211
meta,dgl,1,1328,45.3384,0.7027

epoch:1330/50, training loss:1.1294209957122803
Train Acc 0.7029
 Acc 0.7027
new best val f1: 0.7027172982442276
meta,dgl,1,1329,45.3721,0.7027

epoch:1331/50, training loss:1.1290758848190308
Train Acc 0.7030
 Acc 0.7026
meta,dgl,1,1330,45.4058,0.7026

epoch:1332/50, training loss:1.1287355422973633
Train Acc 0.7028
 Acc 0.7031
new best val f1: 0.7031058600387244
meta,dgl,1,1331,45.4402,0.7031

epoch:1333/50, training loss:1.1283893585205078
Train Acc 0.7033
 Acc 0.7032
new best val f1: 0.7031783037631222
meta,dgl,1,1332,45.4738,0.7032

epoch:1334/50, training loss:1.1280455589294434
Train Acc 0.7034
 Acc 0.7030
meta,dgl,1,1333,45.5084,0.7030

epoch:1335/50, training loss:1.1277050971984863
Train Acc 0.7032
 Acc 0.7035
new best val f1: 0.7035141792126026
meta,dgl,1,1334,45.5420,0.7035

epoch:1336/50, training loss:1.1273595094680786
Train Acc 0.7037
 Acc 0.7036
new best val f1: 0.7035734513507461
meta,dgl,1,1335,45.5757,0.7036

epoch:1337/50, training loss:1.1270191669464111
Train Acc 0.7037
 Acc 0.7037
new best val f1: 0.7037315103857957
meta,dgl,1,1336,45.6094,0.7037

epoch:1338/50, training loss:1.1266764402389526
Train Acc 0.7039
 Acc 0.7038
new best val f1: 0.7037776109376852
meta,dgl,1,1337,45.6430,0.7038

epoch:1339/50, training loss:1.126334309577942
Train Acc 0.7039
 Acc 0.7039
new best val f1: 0.7038895694208454
meta,dgl,1,1338,45.6767,0.7039

epoch:1340/50, training loss:1.1259922981262207
Train Acc 0.7041
 Acc 0.7043
new best val f1: 0.704258373835961
meta,dgl,1,1339,45.7112,0.7043

epoch:1341/50, training loss:1.1256496906280518
Train Acc 0.7044
 Acc 0.7042
meta,dgl,1,1340,45.7449,0.7042

epoch:1342/50, training loss:1.1253081560134888
Train Acc 0.7043
 Acc 0.7044
new best val f1: 0.7043769181122482
meta,dgl,1,1341,45.7785,0.7044

epoch:1343/50, training loss:1.1249650716781616
Train Acc 0.7046
 Acc 0.7045
new best val f1: 0.7045086339747896
meta,dgl,1,1342,45.8122,0.7045

epoch:1344/50, training loss:1.1246236562728882
Train Acc 0.7047
 Acc 0.7046
new best val f1: 0.7045679061129332
meta,dgl,1,1343,45.8459,0.7046

epoch:1345/50, training loss:1.1242809295654297
Train Acc 0.7047
 Acc 0.7048
new best val f1: 0.7047918230792535
meta,dgl,1,1344,45.8805,0.7048

epoch:1346/50, training loss:1.1239405870437622
Train Acc 0.7050
 Acc 0.7053
new best val f1: 0.7052725859775293
meta,dgl,1,1345,45.9141,0.7053

epoch:1347/50, training loss:1.1235980987548828
Train Acc 0.7054
 Acc 0.7047
meta,dgl,1,1346,45.9478,0.7047

epoch:1348/50, training loss:1.123256802558899
Train Acc 0.7049
 Acc 0.7053
new best val f1: 0.7053055149431646
meta,dgl,1,1347,45.9815,0.7053

epoch:1349/50, training loss:1.1229180097579956
Train Acc 0.7055
 Acc 0.7055
new best val f1: 0.7054899171507225
meta,dgl,1,1348,46.0160,0.7055

epoch:1350/50, training loss:1.1225742101669312
Train Acc 0.7056
 Acc 0.7051
meta,dgl,1,1349,46.0505,0.7051

epoch:1351/50, training loss:1.122235655784607
Train Acc 0.7052
 Acc 0.7060
new best val f1: 0.7059838516352525
meta,dgl,1,1350,46.0841,0.7060

epoch:1352/50, training loss:1.1218934059143066
Train Acc 0.7061
 Acc 0.7057
meta,dgl,1,1351,46.1177,0.7057

epoch:1353/50, training loss:1.1215518712997437
Train Acc 0.7058
 Acc 0.7057
meta,dgl,1,1352,46.1513,0.7057

epoch:1354/50, training loss:1.1212104558944702
Train Acc 0.7058
 Acc 0.7063
new best val f1: 0.7062867981190974
meta,dgl,1,1353,46.1849,0.7063

epoch:1355/50, training loss:1.12087082862854
Train Acc 0.7064
 Acc 0.7061
meta,dgl,1,1354,46.2185,0.7061

epoch:1356/50, training loss:1.1205308437347412
Train Acc 0.7062
 Acc 0.7061
meta,dgl,1,1355,46.2522,0.7061

epoch:1357/50, training loss:1.1201916933059692
Train Acc 0.7062
 Acc 0.7066
new best val f1: 0.7065963303960696
meta,dgl,1,1356,46.2860,0.7066

epoch:1358/50, training loss:1.1198500394821167
Train Acc 0.7067
 Acc 0.7065
meta,dgl,1,1357,46.3197,0.7065

epoch:1359/50, training loss:1.119509220123291
Train Acc 0.7066
 Acc 0.7066
meta,dgl,1,1358,46.3534,0.7066

epoch:1360/50, training loss:1.119170904159546
Train Acc 0.7067
 Acc 0.7068
new best val f1: 0.7068465905348981
meta,dgl,1,1359,46.3870,0.7068

epoch:1361/50, training loss:1.1188275814056396
Train Acc 0.7069
 Acc 0.7069
new best val f1: 0.7068861052936605
meta,dgl,1,1360,46.4215,0.7069

epoch:1362/50, training loss:1.118490219116211
Train Acc 0.7069
 Acc 0.7071
new best val f1: 0.7071429512256161
meta,dgl,1,1361,46.4551,0.7071

epoch:1363/50, training loss:1.1181517839431763
Train Acc 0.7072
 Acc 0.7071
meta,dgl,1,1362,46.4895,0.7071

epoch:1364/50, training loss:1.117810845375061
Train Acc 0.7071
 Acc 0.7076
new best val f1: 0.7075973709513836
meta,dgl,1,1363,46.5232,0.7076

epoch:1365/50, training loss:1.117473840713501
Train Acc 0.7076
 Acc 0.7074
meta,dgl,1,1364,46.5577,0.7074

epoch:1366/50, training loss:1.1171342134475708
Train Acc 0.7073
 Acc 0.7077
new best val f1: 0.7076895720551626
meta,dgl,1,1365,46.5912,0.7077

epoch:1367/50, training loss:1.1167943477630615
Train Acc 0.7077
 Acc 0.7078
new best val f1: 0.7078015305383227
meta,dgl,1,1366,46.6257,0.7078

epoch:1368/50, training loss:1.116457462310791
Train Acc 0.7079
 Acc 0.7077
meta,dgl,1,1367,46.6593,0.7077

epoch:1369/50, training loss:1.1161166429519653
Train Acc 0.7078
 Acc 0.7082
new best val f1: 0.7081835065396925
meta,dgl,1,1368,46.6929,0.7082

epoch:1370/50, training loss:1.1157785654067993
Train Acc 0.7082
 Acc 0.7081
meta,dgl,1,1369,46.7265,0.7081

epoch:1371/50, training loss:1.1154377460479736
Train Acc 0.7082
 Acc 0.7082
meta,dgl,1,1370,46.7609,0.7082

epoch:1372/50, training loss:1.1151002645492554
Train Acc 0.7082
 Acc 0.7087
new best val f1: 0.7086971984036038
meta,dgl,1,1371,46.7945,0.7087

epoch:1373/50, training loss:1.114761233329773
Train Acc 0.7087
 Acc 0.7085
meta,dgl,1,1372,46.8282,0.7085

epoch:1374/50, training loss:1.1144235134124756
Train Acc 0.7085
 Acc 0.7087
new best val f1: 0.7087235415761121
meta,dgl,1,1373,46.8619,0.7087

epoch:1375/50, training loss:1.1140862703323364
Train Acc 0.7087
 Acc 0.7089
new best val f1: 0.7089408727493052
meta,dgl,1,1374,46.8956,0.7089

epoch:1376/50, training loss:1.1137480735778809
Train Acc 0.7089
 Acc 0.7088
meta,dgl,1,1375,46.9292,0.7088

epoch:1377/50, training loss:1.1134090423583984
Train Acc 0.7089
 Acc 0.7091
new best val f1: 0.7090989317843548
meta,dgl,1,1376,46.9636,0.7091

epoch:1378/50, training loss:1.1130722761154175
Train Acc 0.7091
 Acc 0.7093
new best val f1: 0.7093294345438022
meta,dgl,1,1377,46.9982,0.7093

epoch:1379/50, training loss:1.1127346754074097
Train Acc 0.7094
 Acc 0.7089
meta,dgl,1,1378,47.0320,0.7089

epoch:1380/50, training loss:1.1123979091644287
Train Acc 0.7089
 Acc 0.7097
new best val f1: 0.709717996338299
meta,dgl,1,1379,47.0657,0.7097

epoch:1381/50, training loss:1.1120586395263672
Train Acc 0.7098
 Acc 0.7094
meta,dgl,1,1380,47.0995,0.7094

epoch:1382/50, training loss:1.111722707748413
Train Acc 0.7094
 Acc 0.7096
meta,dgl,1,1381,47.1332,0.7096

epoch:1383/50, training loss:1.1113828420639038
Train Acc 0.7096
 Acc 0.7099
new best val f1: 0.7098760553733486
meta,dgl,1,1382,47.1669,0.7099

epoch:1384/50, training loss:1.1110481023788452
Train Acc 0.7100
 Acc 0.7098
meta,dgl,1,1383,47.2015,0.7098

epoch:1385/50, training loss:1.1107102632522583
Train Acc 0.7099
 Acc 0.7100
new best val f1: 0.7099945996496358
meta,dgl,1,1384,47.2351,0.7100

epoch:1386/50, training loss:1.1103739738464355
Train Acc 0.7101
 Acc 0.7104
new best val f1: 0.7103897472372598
meta,dgl,1,1385,47.2687,0.7104

epoch:1387/50, training loss:1.1100339889526367
Train Acc 0.7105
 Acc 0.7101
meta,dgl,1,1386,47.3024,0.7101

epoch:1388/50, training loss:1.1096992492675781
Train Acc 0.7101
 Acc 0.7103
meta,dgl,1,1387,47.3363,0.7103

epoch:1389/50, training loss:1.1093641519546509
Train Acc 0.7104
 Acc 0.7107
new best val f1: 0.7106729363417237
meta,dgl,1,1388,47.3700,0.7107

epoch:1390/50, training loss:1.1090233325958252
Train Acc 0.7108
 Acc 0.7105
meta,dgl,1,1389,47.4045,0.7105

epoch:1391/50, training loss:1.1086878776550293
Train Acc 0.7107
 Acc 0.7105
meta,dgl,1,1390,47.4382,0.7105

epoch:1392/50, training loss:1.1083533763885498
Train Acc 0.7106
 Acc 0.7111
new best val f1: 0.7111471134468724
meta,dgl,1,1391,47.4719,0.7111

epoch:1393/50, training loss:1.1080169677734375
Train Acc 0.7113
 Acc 0.7107
meta,dgl,1,1392,47.5057,0.7107

epoch:1394/50, training loss:1.1076807975769043
Train Acc 0.7108
 Acc 0.7111
meta,dgl,1,1393,47.5401,0.7111

epoch:1395/50, training loss:1.1073423624038696
Train Acc 0.7112
 Acc 0.7114
new best val f1: 0.7113842019994467
meta,dgl,1,1394,47.5737,0.7114

epoch:1396/50, training loss:1.107006549835205
Train Acc 0.7116
 Acc 0.7110
meta,dgl,1,1395,47.6073,0.7110

epoch:1397/50, training loss:1.1066715717315674
Train Acc 0.7112
 Acc 0.7115
new best val f1: 0.7115356752413693
meta,dgl,1,1396,47.6409,0.7115

epoch:1398/50, training loss:1.1063380241394043
Train Acc 0.7118
 Acc 0.7117
new best val f1: 0.7117332490351813
meta,dgl,1,1397,47.6745,0.7117

epoch:1399/50, training loss:1.1060020923614502
Train Acc 0.7119
 Acc 0.7113
meta,dgl,1,1398,47.7079,0.7113

epoch:1400/50, training loss:1.1056653261184692
Train Acc 0.7115
 Acc 0.7121
new best val f1: 0.7120559528984075
meta,dgl,1,1399,47.7424,0.7121

epoch:1401/50, training loss:1.105331540107727
Train Acc 0.7123
 Acc 0.7120
meta,dgl,1,1400,47.7761,0.7120

epoch:1402/50, training loss:1.1049927473068237
Train Acc 0.7121
 Acc 0.7116
meta,dgl,1,1401,47.8106,0.7116

epoch:1403/50, training loss:1.1046600341796875
Train Acc 0.7118
 Acc 0.7126
new best val f1: 0.7125630589691917
meta,dgl,1,1402,47.8442,0.7126

epoch:1404/50, training loss:1.1043237447738647
Train Acc 0.7127
 Acc 0.7124
meta,dgl,1,1403,47.8788,0.7124

epoch:1405/50, training loss:1.1039881706237793
Train Acc 0.7126
 Acc 0.7119
meta,dgl,1,1404,47.9124,0.7119

epoch:1406/50, training loss:1.1036542654037476
Train Acc 0.7121
 Acc 0.7130
new best val f1: 0.7130174786949592
meta,dgl,1,1405,47.9459,0.7130

epoch:1407/50, training loss:1.1033189296722412
Train Acc 0.7132
 Acc 0.7127
meta,dgl,1,1406,47.9795,0.7127

epoch:1408/50, training loss:1.1029834747314453
Train Acc 0.7129
 Acc 0.7124
meta,dgl,1,1407,48.0132,0.7124

epoch:1409/50, training loss:1.102648377418518
Train Acc 0.7126
 Acc 0.7134
new best val f1: 0.713399454696329
meta,dgl,1,1408,48.0477,0.7134

epoch:1410/50, training loss:1.1023166179656982
Train Acc 0.7135
 Acc 0.7130
meta,dgl,1,1409,48.0814,0.7130

epoch:1411/50, training loss:1.1019816398620605
Train Acc 0.7131
 Acc 0.7132
meta,dgl,1,1410,48.1151,0.7132

epoch:1412/50, training loss:1.1016448736190796
Train Acc 0.7133
 Acc 0.7137
new best val f1: 0.7136826438007929
meta,dgl,1,1411,48.1497,0.7137

epoch:1413/50, training loss:1.101311445236206
Train Acc 0.7138
 Acc 0.7133
meta,dgl,1,1412,48.1833,0.7133

epoch:1414/50, training loss:1.1009770631790161
Train Acc 0.7134
 Acc 0.7138
new best val f1: 0.713788016490826
meta,dgl,1,1413,48.2168,0.7138

epoch:1415/50, training loss:1.1006413698196411
Train Acc 0.7139
 Acc 0.7137
meta,dgl,1,1414,48.2512,0.7137

epoch:1416/50, training loss:1.1003082990646362
Train Acc 0.7139
 Acc 0.7140
new best val f1: 0.7140053476640191
meta,dgl,1,1415,48.2848,0.7140

epoch:1417/50, training loss:1.0999733209609985
Train Acc 0.7141
 Acc 0.7140
new best val f1: 0.7140382766296545
meta,dgl,1,1416,48.3193,0.7140

epoch:1418/50, training loss:1.0996392965316772
Train Acc 0.7141
 Acc 0.7144
new best val f1: 0.7144070810447702
meta,dgl,1,1417,48.3530,0.7144

epoch:1419/50, training loss:1.099306344985962
Train Acc 0.7145
 Acc 0.7142
meta,dgl,1,1418,48.3867,0.7142

epoch:1420/50, training loss:1.0989710092544556
Train Acc 0.7143
 Acc 0.7145
new best val f1: 0.7144926963554221
meta,dgl,1,1419,48.4204,0.7145

epoch:1421/50, training loss:1.0986400842666626
Train Acc 0.7146
 Acc 0.7148
new best val f1: 0.7148351575980295
meta,dgl,1,1420,48.4549,0.7148

epoch:1422/50, training loss:1.0983052253723145
Train Acc 0.7149
 Acc 0.7146
meta,dgl,1,1421,48.4901,0.7146

epoch:1423/50, training loss:1.097971796989441
Train Acc 0.7147
 Acc 0.7150
new best val f1: 0.7150129740124603
meta,dgl,1,1422,48.5238,0.7150

epoch:1424/50, training loss:1.0976382493972778
Train Acc 0.7150
 Acc 0.7153
new best val f1: 0.7153422636688136
meta,dgl,1,1423,48.5574,0.7153

epoch:1425/50, training loss:1.097305417060852
Train Acc 0.7153
 Acc 0.7150
meta,dgl,1,1424,48.5910,0.7150

epoch:1426/50, training loss:1.096971869468689
Train Acc 0.7150
 Acc 0.7157
new best val f1: 0.7157044822908023
meta,dgl,1,1425,48.6254,0.7157

epoch:1427/50, training loss:1.0966390371322632
Train Acc 0.7157
 Acc 0.7155
meta,dgl,1,1426,48.6600,0.7155

epoch:1428/50, training loss:1.0963062047958374
Train Acc 0.7155
 Acc 0.7155
meta,dgl,1,1427,48.6937,0.7155

epoch:1429/50, training loss:1.095973014831543
Train Acc 0.7154
 Acc 0.7163
new best val f1: 0.7162972036722383
meta,dgl,1,1428,48.7273,0.7163

epoch:1430/50, training loss:1.0956406593322754
Train Acc 0.7161
 Acc 0.7158
meta,dgl,1,1429,48.7610,0.7158

epoch:1431/50, training loss:1.0953081846237183
Train Acc 0.7158
 Acc 0.7160
meta,dgl,1,1430,48.7947,0.7160

epoch:1432/50, training loss:1.0949745178222656
Train Acc 0.7160
 Acc 0.7167
new best val f1: 0.7166528365010998
meta,dgl,1,1431,48.8293,0.7167

epoch:1433/50, training loss:1.0946452617645264
Train Acc 0.7165
 Acc 0.7160
meta,dgl,1,1432,48.8638,0.7160

epoch:1434/50, training loss:1.0943119525909424
Train Acc 0.7160
 Acc 0.7164
meta,dgl,1,1433,48.8974,0.7164

epoch:1435/50, training loss:1.0939788818359375
Train Acc 0.7164
 Acc 0.7169
new best val f1: 0.7169360256055637
meta,dgl,1,1434,48.9319,0.7169

epoch:1436/50, training loss:1.0936477184295654
Train Acc 0.7168
 Acc 0.7164
meta,dgl,1,1435,48.9656,0.7164

epoch:1437/50, training loss:1.0933161973953247
Train Acc 0.7164
 Acc 0.7171
new best val f1: 0.7171335993993757
meta,dgl,1,1436,49.0001,0.7171

epoch:1438/50, training loss:1.092982292175293
Train Acc 0.7171
 Acc 0.7170
meta,dgl,1,1437,49.0337,0.7170

epoch:1439/50, training loss:1.0926513671875
Train Acc 0.7170
 Acc 0.7171
meta,dgl,1,1438,49.0673,0.7171

epoch:1440/50, training loss:1.0923198461532593
Train Acc 0.7170
 Acc 0.7173
new best val f1: 0.717265315261917
meta,dgl,1,1439,49.1008,0.7173

epoch:1441/50, training loss:1.0919889211654663
Train Acc 0.7172
 Acc 0.7176
new best val f1: 0.7175814333320162
meta,dgl,1,1440,49.1344,0.7176

epoch:1442/50, training loss:1.0916551351547241
Train Acc 0.7175
 Acc 0.7174
meta,dgl,1,1441,49.1681,0.7174

epoch:1443/50, training loss:1.0913243293762207
Train Acc 0.7173
 Acc 0.7178
new best val f1: 0.7177987645052094
meta,dgl,1,1442,49.2026,0.7178

epoch:1444/50, training loss:1.0909920930862427
Train Acc 0.7178
 Acc 0.7181
new best val f1: 0.7180951251959273
meta,dgl,1,1443,49.2361,0.7181

epoch:1445/50, training loss:1.0906643867492676
Train Acc 0.7180
 Acc 0.7176
meta,dgl,1,1444,49.2706,0.7176

epoch:1446/50, training loss:1.0903314352035522
Train Acc 0.7176
 Acc 0.7182
new best val f1: 0.7182334268515957
meta,dgl,1,1445,49.3042,0.7182

epoch:1447/50, training loss:1.090000867843628
Train Acc 0.7181
 Acc 0.7186
new best val f1: 0.7186417460254738
meta,dgl,1,1446,49.3379,0.7186

epoch:1448/50, training loss:1.0896708965301514
Train Acc 0.7184
 Acc 0.7182
meta,dgl,1,1447,49.3716,0.7182

epoch:1449/50, training loss:1.0893398523330688
Train Acc 0.7180
 Acc 0.7188
new best val f1: 0.7188195624399046
meta,dgl,1,1448,49.4061,0.7188

epoch:1450/50, training loss:1.0890077352523804
Train Acc 0.7186
 Acc 0.7188
new best val f1: 0.7188459056124129
meta,dgl,1,1449,49.4407,0.7188

epoch:1451/50, training loss:1.0886781215667725
Train Acc 0.7186
 Acc 0.7186
meta,dgl,1,1450,49.4743,0.7186

epoch:1452/50, training loss:1.0883476734161377
Train Acc 0.7184
 Acc 0.7195
new best val f1: 0.719451798580103
meta,dgl,1,1451,49.5081,0.7195

epoch:1453/50, training loss:1.088019609451294
Train Acc 0.7191
 Acc 0.7191
meta,dgl,1,1452,49.5418,0.7191

epoch:1454/50, training loss:1.0876882076263428
Train Acc 0.7188
 Acc 0.7192
meta,dgl,1,1453,49.5755,0.7192

epoch:1455/50, training loss:1.0873576402664185
Train Acc 0.7189
 Acc 0.7199
new best val f1: 0.7198601177539811
meta,dgl,1,1454,49.6100,0.7199

epoch:1456/50, training loss:1.087027668952942
Train Acc 0.7196
 Acc 0.7193
meta,dgl,1,1455,49.6435,0.7193

epoch:1457/50, training loss:1.0866957902908325
Train Acc 0.7190
 Acc 0.7200
new best val f1: 0.7200050052027765
meta,dgl,1,1456,49.6772,0.7200

epoch:1458/50, training loss:1.086367130279541
Train Acc 0.7197
 Acc 0.7199
meta,dgl,1,1457,49.7107,0.7199

epoch:1459/50, training loss:1.086037039756775
Train Acc 0.7195
 Acc 0.7201
new best val f1: 0.7200642773409202
meta,dgl,1,1458,49.7452,0.7201

epoch:1460/50, training loss:1.0857069492340088
Train Acc 0.7197
 Acc 0.7202
new best val f1: 0.7201630642378262
meta,dgl,1,1459,49.7788,0.7202

epoch:1461/50, training loss:1.0853779315948486
Train Acc 0.7199
 Acc 0.7205
new best val f1: 0.7205252828598148
meta,dgl,1,1460,49.8125,0.7205

epoch:1462/50, training loss:1.0850495100021362
Train Acc 0.7202
 Acc 0.7201
meta,dgl,1,1461,49.8461,0.7201

epoch:1463/50, training loss:1.0847194194793701
Train Acc 0.7199
 Acc 0.7206
new best val f1: 0.7206306555498478
meta,dgl,1,1462,49.8798,0.7206

epoch:1464/50, training loss:1.0843884944915771
Train Acc 0.7203
 Acc 0.7208
new best val f1: 0.7208479867230411
meta,dgl,1,1463,49.9135,0.7208

epoch:1465/50, training loss:1.0840591192245483
Train Acc 0.7206
 Acc 0.7204
meta,dgl,1,1464,49.9472,0.7204

epoch:1466/50, training loss:1.0837346315383911
Train Acc 0.7201
 Acc 0.7213
new best val f1: 0.7213089922419357
meta,dgl,1,1465,49.9809,0.7213

epoch:1467/50, training loss:1.0834037065505981
Train Acc 0.7210
 Acc 0.7208
meta,dgl,1,1466,50.0154,0.7208

epoch:1468/50, training loss:1.0830777883529663
Train Acc 0.7205
 Acc 0.7212
meta,dgl,1,1467,50.0489,0.7212

epoch:1469/50, training loss:1.082747459411621
Train Acc 0.7209
 Acc 0.7214
new best val f1: 0.7214275365182229
meta,dgl,1,1468,50.0825,0.7214

epoch:1470/50, training loss:1.0824183225631714
Train Acc 0.7211
 Acc 0.7211
meta,dgl,1,1469,50.1161,0.7211

epoch:1471/50, training loss:1.082089900970459
Train Acc 0.7209
 Acc 0.7216
new best val f1: 0.721631696105162
meta,dgl,1,1470,50.1497,0.7216

epoch:1472/50, training loss:1.0817598104476929
Train Acc 0.7214
 Acc 0.7218
new best val f1: 0.7217568261745761
meta,dgl,1,1471,50.1832,0.7218

epoch:1473/50, training loss:1.0814323425292969
Train Acc 0.7215
 Acc 0.7216
meta,dgl,1,1472,50.2168,0.7216

epoch:1474/50, training loss:1.0811024904251099
Train Acc 0.7213
 Acc 0.7221
new best val f1: 0.7220729442446754
meta,dgl,1,1473,50.2505,0.7221

epoch:1475/50, training loss:1.0807744264602661
Train Acc 0.7219
 Acc 0.7217
meta,dgl,1,1474,50.2841,0.7217

epoch:1476/50, training loss:1.0804466009140015
Train Acc 0.7215
 Acc 0.7221
new best val f1: 0.7221058732103107
meta,dgl,1,1475,50.3186,0.7221

epoch:1477/50, training loss:1.0801191329956055
Train Acc 0.7218
 Acc 0.7224
new best val f1: 0.7224219912804098
meta,dgl,1,1476,50.3523,0.7224

epoch:1478/50, training loss:1.0797920227050781
Train Acc 0.7223
 Acc 0.7220
meta,dgl,1,1477,50.3859,0.7220

epoch:1479/50, training loss:1.079461693763733
Train Acc 0.7218
 Acc 0.7225
new best val f1: 0.7225141923841888
meta,dgl,1,1478,50.4195,0.7225

epoch:1480/50, training loss:1.0791327953338623
Train Acc 0.7223
 Acc 0.7229
new best val f1: 0.7228698252130504
meta,dgl,1,1479,50.4532,0.7229

epoch:1481/50, training loss:1.0788068771362305
Train Acc 0.7227
 Acc 0.7224
meta,dgl,1,1480,50.4868,0.7224

epoch:1482/50, training loss:1.0784814357757568
Train Acc 0.7221
 Acc 0.7229
new best val f1: 0.7229422689374482
meta,dgl,1,1481,50.5205,0.7229

epoch:1483/50, training loss:1.0781521797180176
Train Acc 0.7228
 Acc 0.7234
new best val f1: 0.7233901028700886
meta,dgl,1,1482,50.5541,0.7234

epoch:1484/50, training loss:1.0778250694274902
Train Acc 0.7232
 Acc 0.7226
meta,dgl,1,1483,50.5885,0.7226

epoch:1485/50, training loss:1.0774985551834106
Train Acc 0.7223
 Acc 0.7235
new best val f1: 0.7235481619051383
meta,dgl,1,1484,50.6221,0.7235

epoch:1486/50, training loss:1.077170968055725
Train Acc 0.7233
 Acc 0.7236
new best val f1: 0.7236271914226631
meta,dgl,1,1485,50.6558,0.7236

epoch:1487/50, training loss:1.0768425464630127
Train Acc 0.7234
 Acc 0.7229
meta,dgl,1,1486,50.6894,0.7229

epoch:1488/50, training loss:1.0765204429626465
Train Acc 0.7226
 Acc 0.7243
new best val f1: 0.7242725991491156
meta,dgl,1,1487,50.7240,0.7243

epoch:1489/50, training loss:1.07619047164917
Train Acc 0.7240
 Acc 0.7237
meta,dgl,1,1488,50.7575,0.7237

epoch:1490/50, training loss:1.0758616924285889
Train Acc 0.7234
 Acc 0.7236
meta,dgl,1,1489,50.7912,0.7236

epoch:1491/50, training loss:1.075536847114563
Train Acc 0.7233
 Acc 0.7247
new best val f1: 0.7247072614955019
meta,dgl,1,1490,50.8249,0.7247

epoch:1492/50, training loss:1.075210452079773
Train Acc 0.7244
 Acc 0.7238
meta,dgl,1,1491,50.8585,0.7238

epoch:1493/50, training loss:1.0748835802078247
Train Acc 0.7234
 Acc 0.7245
meta,dgl,1,1492,50.8921,0.7245

epoch:1494/50, training loss:1.0745548009872437
Train Acc 0.7242
 Acc 0.7250
new best val f1: 0.7250102079793469
meta,dgl,1,1493,50.9258,0.7250

epoch:1495/50, training loss:1.0742284059524536
Train Acc 0.7246
 Acc 0.7239
meta,dgl,1,1494,50.9602,0.7239

epoch:1496/50, training loss:1.0739071369171143
Train Acc 0.7236
 Acc 0.7253
new best val f1: 0.7253263260494461
meta,dgl,1,1495,50.9938,0.7253

epoch:1497/50, training loss:1.073577642440796
Train Acc 0.7250
 Acc 0.7250
meta,dgl,1,1496,51.0274,0.7250

epoch:1498/50, training loss:1.0732483863830566
Train Acc 0.7247
 Acc 0.7244
meta,dgl,1,1497,51.0611,0.7244

epoch:1499/50, training loss:1.072925329208374
Train Acc 0.7241
 Acc 0.7258
new best val f1: 0.7257609883958325
meta,dgl,1,1498,51.0957,0.7258

epoch:1500/50, training loss:1.0725994110107422
Train Acc 0.7255
 Acc 0.7254
meta,dgl,1,1499,51.1303,0.7254

epoch:1501/50, training loss:1.0722707509994507
Train Acc 0.7250
 Acc 0.7250
meta,dgl,1,1500,51.1639,0.7250

epoch:1502/50, training loss:1.0719460248947144
Train Acc 0.7246
 Acc 0.7261
new best val f1: 0.7261495501903295
meta,dgl,1,1501,51.1975,0.7261

epoch:1503/50, training loss:1.0716190338134766
Train Acc 0.7259
 Acc 0.7254
meta,dgl,1,1502,51.2320,0.7254

epoch:1504/50, training loss:1.0712934732437134
Train Acc 0.7251
 Acc 0.7258
meta,dgl,1,1503,51.2656,0.7258

epoch:1505/50, training loss:1.0709655284881592
Train Acc 0.7254
 Acc 0.7262
new best val f1: 0.7262219939147272
meta,dgl,1,1504,51.3000,0.7262

epoch:1506/50, training loss:1.0706424713134766
Train Acc 0.7259
 Acc 0.7260
meta,dgl,1,1505,51.3337,0.7260

epoch:1507/50, training loss:1.0703169107437134
Train Acc 0.7257
 Acc 0.7262
meta,dgl,1,1506,51.3673,0.7262

epoch:1508/50, training loss:1.0699900388717651
Train Acc 0.7259
 Acc 0.7265
new best val f1: 0.7265315261916993
meta,dgl,1,1507,51.4019,0.7265

epoch:1509/50, training loss:1.0696667432785034
Train Acc 0.7263
 Acc 0.7263
meta,dgl,1,1508,51.4355,0.7263

epoch:1510/50, training loss:1.069339394569397
Train Acc 0.7259
 Acc 0.7266
new best val f1: 0.7265973841229699
meta,dgl,1,1509,51.4691,0.7266

epoch:1511/50, training loss:1.0690163373947144
Train Acc 0.7264
 Acc 0.7267
new best val f1: 0.7267225141923842
meta,dgl,1,1510,51.5027,0.7267

epoch:1512/50, training loss:1.0686888694763184
Train Acc 0.7265
 Acc 0.7267
new best val f1: 0.7267488573648925
meta,dgl,1,1511,51.5363,0.7267

epoch:1513/50, training loss:1.068363904953003
Train Acc 0.7265
 Acc 0.7267
meta,dgl,1,1512,51.5699,0.7267

epoch:1514/50, training loss:1.0680395364761353
Train Acc 0.7265
 Acc 0.7274
new best val f1: 0.7273613361257096
meta,dgl,1,1513,51.6044,0.7274

epoch:1515/50, training loss:1.0677149295806885
Train Acc 0.7272
 Acc 0.7268
meta,dgl,1,1514,51.6379,0.7268

epoch:1516/50, training loss:1.0673894882202148
Train Acc 0.7266
 Acc 0.7273
meta,dgl,1,1515,51.6716,0.7273

epoch:1517/50, training loss:1.0670640468597412
Train Acc 0.7271
 Acc 0.7275
new best val f1: 0.7274667088157427
meta,dgl,1,1516,51.7053,0.7275

epoch:1518/50, training loss:1.0667400360107422
Train Acc 0.7273
 Acc 0.7275
new best val f1: 0.7275457383332675
meta,dgl,1,1517,51.7398,0.7275

epoch:1519/50, training loss:1.0664122104644775
Train Acc 0.7274
 Acc 0.7273
meta,dgl,1,1518,51.7733,0.7273

epoch:1520/50, training loss:1.0660911798477173
Train Acc 0.7271
 Acc 0.7283
new best val f1: 0.72830310454288
meta,dgl,1,1519,51.8078,0.7283

epoch:1521/50, training loss:1.0657687187194824
Train Acc 0.7281
 Acc 0.7274
meta,dgl,1,1520,51.8424,0.7274

epoch:1522/50, training loss:1.065442681312561
Train Acc 0.7272
 Acc 0.7280
meta,dgl,1,1521,51.8761,0.7280

epoch:1523/50, training loss:1.065114974975586
Train Acc 0.7279
 Acc 0.7285
new best val f1: 0.7285138499229462
meta,dgl,1,1522,51.9098,0.7285

epoch:1524/50, training loss:1.064794659614563
Train Acc 0.7284
 Acc 0.7278
meta,dgl,1,1523,51.9435,0.7278

epoch:1525/50, training loss:1.064470887184143
Train Acc 0.7276
 Acc 0.7285
meta,dgl,1,1524,51.9781,0.7285

epoch:1526/50, training loss:1.064143180847168
Train Acc 0.7284
 Acc 0.7288
new best val f1: 0.7287641100617748
meta,dgl,1,1525,52.0117,0.7288

epoch:1527/50, training loss:1.0638197660446167
Train Acc 0.7287
 Acc 0.7279
meta,dgl,1,1526,52.0455,0.7279

epoch:1528/50, training loss:1.063498616218567
Train Acc 0.7278
 Acc 0.7290
new best val f1: 0.7290011986143491
meta,dgl,1,1527,52.0792,0.7290

epoch:1529/50, training loss:1.063170075416565
Train Acc 0.7289
 Acc 0.7289
meta,dgl,1,1528,52.1129,0.7289

epoch:1530/50, training loss:1.0628471374511719
Train Acc 0.7289
 Acc 0.7284
meta,dgl,1,1529,52.1467,0.7284

epoch:1531/50, training loss:1.0625256299972534
Train Acc 0.7283
 Acc 0.7292
new best val f1: 0.7292053582012882
meta,dgl,1,1530,52.1812,0.7292

epoch:1532/50, training loss:1.0622003078460693
Train Acc 0.7291
 Acc 0.7294
new best val f1: 0.7293897604088461
meta,dgl,1,1531,52.2149,0.7294

epoch:1533/50, training loss:1.061874508857727
Train Acc 0.7293
 Acc 0.7285
meta,dgl,1,1532,52.2486,0.7285

epoch:1534/50, training loss:1.0615586042404175
Train Acc 0.7285
 Acc 0.7297
new best val f1: 0.7297453932377076
meta,dgl,1,1533,52.2823,0.7297

epoch:1535/50, training loss:1.0612313747406006
Train Acc 0.7297
 Acc 0.7294
meta,dgl,1,1534,52.3159,0.7294

epoch:1536/50, training loss:1.0609058141708374
Train Acc 0.7293
 Acc 0.7290
meta,dgl,1,1535,52.3505,0.7290

epoch:1537/50, training loss:1.0605847835540771
Train Acc 0.7290
 Acc 0.7304
new best val f1: 0.7303710435847789
meta,dgl,1,1536,52.3841,0.7304

epoch:1538/50, training loss:1.060264229774475
Train Acc 0.7303
 Acc 0.7292
meta,dgl,1,1537,52.4179,0.7292

epoch:1539/50, training loss:1.059938907623291
Train Acc 0.7291
 Acc 0.7298
meta,dgl,1,1538,52.4515,0.7298

epoch:1540/50, training loss:1.0596109628677368
Train Acc 0.7297
 Acc 0.7305
new best val f1: 0.7305093452404473
meta,dgl,1,1539,52.4861,0.7305

epoch:1541/50, training loss:1.0592916011810303
Train Acc 0.7304
 Acc 0.7296
meta,dgl,1,1540,52.5205,0.7296

epoch:1542/50, training loss:1.0589648485183716
Train Acc 0.7295
 Acc 0.7302
meta,dgl,1,1541,52.5541,0.7302

epoch:1543/50, training loss:1.0586438179016113
Train Acc 0.7301
 Acc 0.7308
new best val f1: 0.7308057059311653
meta,dgl,1,1542,52.5877,0.7308

epoch:1544/50, training loss:1.0583217144012451
Train Acc 0.7307
 Acc 0.7300
meta,dgl,1,1543,52.6214,0.7300

epoch:1545/50, training loss:1.0579992532730103
Train Acc 0.7299
 Acc 0.7305
meta,dgl,1,1544,52.6550,0.7305

epoch:1546/50, training loss:1.0576757192611694
Train Acc 0.7304
 Acc 0.7313
new best val f1: 0.7313062262088224
meta,dgl,1,1545,52.6886,0.7313

epoch:1547/50, training loss:1.0573562383651733
Train Acc 0.7312
 Acc 0.7303
meta,dgl,1,1546,52.7222,0.7303

epoch:1548/50, training loss:1.0570310354232788
Train Acc 0.7302
 Acc 0.7308
meta,dgl,1,1547,52.7558,0.7308

epoch:1549/50, training loss:1.0567057132720947
Train Acc 0.7308
 Acc 0.7316
new best val f1: 0.7315630721407779
meta,dgl,1,1548,52.7894,0.7316

epoch:1550/50, training loss:1.0563857555389404
Train Acc 0.7315
 Acc 0.7307
meta,dgl,1,1549,52.8231,0.7307

epoch:1551/50, training loss:1.05606210231781
Train Acc 0.7307
 Acc 0.7312
meta,dgl,1,1550,52.8567,0.7312

epoch:1552/50, training loss:1.0557373762130737
Train Acc 0.7311
 Acc 0.7317
new best val f1: 0.7317343027620816
meta,dgl,1,1551,52.8904,0.7317

epoch:1553/50, training loss:1.0554155111312866
Train Acc 0.7318
 Acc 0.7311
meta,dgl,1,1552,52.9240,0.7311

epoch:1554/50, training loss:1.0550941228866577
Train Acc 0.7311
 Acc 0.7315
meta,dgl,1,1553,52.9576,0.7315

epoch:1555/50, training loss:1.0547707080841064
Train Acc 0.7315
 Acc 0.7320
new best val f1: 0.7319845629009102
meta,dgl,1,1554,52.9920,0.7320

epoch:1556/50, training loss:1.054449439048767
Train Acc 0.7320
 Acc 0.7313
meta,dgl,1,1555,53.0257,0.7313

epoch:1557/50, training loss:1.0541273355484009
Train Acc 0.7314
 Acc 0.7321
new best val f1: 0.7320504208321809
meta,dgl,1,1556,53.0593,0.7321

epoch:1558/50, training loss:1.0538047552108765
Train Acc 0.7320
 Acc 0.7323
new best val f1: 0.7323336099366446
meta,dgl,1,1557,53.0930,0.7323

epoch:1559/50, training loss:1.0534815788269043
Train Acc 0.7323
 Acc 0.7318
meta,dgl,1,1558,53.1276,0.7318

epoch:1560/50, training loss:1.0531606674194336
Train Acc 0.7319
 Acc 0.7324
new best val f1: 0.7323533673160258
meta,dgl,1,1559,53.1622,0.7324

epoch:1561/50, training loss:1.0528367757797241
Train Acc 0.7324
 Acc 0.7325
new best val f1: 0.7324521542129319
meta,dgl,1,1560,53.1957,0.7325

epoch:1562/50, training loss:1.052514672279358
Train Acc 0.7325
 Acc 0.7324
meta,dgl,1,1561,53.2294,0.7324

epoch:1563/50, training loss:1.052192211151123
Train Acc 0.7325
 Acc 0.7326
new best val f1: 0.7326167990411085
meta,dgl,1,1562,53.2630,0.7326

epoch:1564/50, training loss:1.0518708229064941
Train Acc 0.7326
 Acc 0.7331
new best val f1: 0.733064632973749
meta,dgl,1,1563,53.2967,0.7331

epoch:1565/50, training loss:1.051548957824707
Train Acc 0.7331
 Acc 0.7325
meta,dgl,1,1564,53.3303,0.7325

epoch:1566/50, training loss:1.0512282848358154
Train Acc 0.7325
 Acc 0.7333
new best val f1: 0.733262206767561
meta,dgl,1,1565,53.3639,0.7333

epoch:1567/50, training loss:1.050906777381897
Train Acc 0.7332
 Acc 0.7333
new best val f1: 0.7333280646988317
meta,dgl,1,1566,53.3976,0.7333

epoch:1568/50, training loss:1.050582766532898
Train Acc 0.7333
 Acc 0.7330
meta,dgl,1,1567,53.4321,0.7330

epoch:1569/50, training loss:1.0502625703811646
Train Acc 0.7330
 Acc 0.7339
new best val f1: 0.7339010287008865
meta,dgl,1,1568,53.4658,0.7339

epoch:1570/50, training loss:1.049943447113037
Train Acc 0.7337
 Acc 0.7333
meta,dgl,1,1569,53.4995,0.7333

epoch:1571/50, training loss:1.0496209859848022
Train Acc 0.7333
 Acc 0.7337
meta,dgl,1,1570,53.5332,0.7337

epoch:1572/50, training loss:1.0492967367172241
Train Acc 0.7336
 Acc 0.7341
new best val f1: 0.7341117740809525
meta,dgl,1,1571,53.5677,0.7341

epoch:1573/50, training loss:1.0489764213562012
Train Acc 0.7340
 Acc 0.7334
meta,dgl,1,1572,53.6023,0.7334

epoch:1574/50, training loss:1.0486559867858887
Train Acc 0.7334
 Acc 0.7343
new best val f1: 0.7342500757366209
meta,dgl,1,1573,53.6361,0.7343

epoch:1575/50, training loss:1.048333764076233
Train Acc 0.7341
 Acc 0.7345
new best val f1: 0.7345069216685766
meta,dgl,1,1574,53.6699,0.7345

epoch:1576/50, training loss:1.04801344871521
Train Acc 0.7344
 Acc 0.7337
meta,dgl,1,1575,53.7036,0.7337

epoch:1577/50, training loss:1.0476943254470825
Train Acc 0.7337
 Acc 0.7348
new best val f1: 0.7348098681524216
meta,dgl,1,1576,53.7374,0.7348

epoch:1578/50, training loss:1.0473718643188477
Train Acc 0.7347
 Acc 0.7347
meta,dgl,1,1577,53.7711,0.7347

epoch:1579/50, training loss:1.047049880027771
Train Acc 0.7346
 Acc 0.7343
meta,dgl,1,1578,53.8056,0.7343

epoch:1580/50, training loss:1.0467313528060913
Train Acc 0.7343
 Acc 0.7348
new best val f1: 0.7348230397386757
meta,dgl,1,1579,53.8403,0.7348

epoch:1581/50, training loss:1.046409010887146
Train Acc 0.7348
 Acc 0.7350
new best val f1: 0.7349876845668524
meta,dgl,1,1580,53.8741,0.7350

epoch:1582/50, training loss:1.0460890531539917
Train Acc 0.7350
 Acc 0.7349
meta,dgl,1,1581,53.9078,0.7349

epoch:1583/50, training loss:1.0457680225372314
Train Acc 0.7349
 Acc 0.7350
new best val f1: 0.7349942703599794
meta,dgl,1,1582,53.9425,0.7350

epoch:1584/50, training loss:1.0454462766647339
Train Acc 0.7350
 Acc 0.7355
new best val f1: 0.7354750332582553
meta,dgl,1,1583,53.9764,0.7355

epoch:1585/50, training loss:1.045128345489502
Train Acc 0.7355
 Acc 0.7352
meta,dgl,1,1584,54.0100,0.7352

epoch:1586/50, training loss:1.0448075532913208
Train Acc 0.7351
 Acc 0.7356
new best val f1: 0.7356199207070507
meta,dgl,1,1585,54.0436,0.7356

epoch:1587/50, training loss:1.04448664188385
Train Acc 0.7356
 Acc 0.7355
meta,dgl,1,1586,54.0782,0.7355

epoch:1588/50, training loss:1.0441697835922241
Train Acc 0.7355
 Acc 0.7356
meta,dgl,1,1587,54.1127,0.7356

epoch:1589/50, training loss:1.0438474416732788
Train Acc 0.7356
 Acc 0.7359
new best val f1: 0.7358965240183876
meta,dgl,1,1588,54.1473,0.7359

epoch:1590/50, training loss:1.043526530265808
Train Acc 0.7358
 Acc 0.7361
new best val f1: 0.7361336125709619
meta,dgl,1,1589,54.1809,0.7361

epoch:1591/50, training loss:1.0432075262069702
Train Acc 0.7361
 Acc 0.7357
meta,dgl,1,1590,54.2146,0.7357

epoch:1592/50, training loss:1.042889952659607
Train Acc 0.7357
 Acc 0.7364
new best val f1: 0.7363838727097904
meta,dgl,1,1591,54.2484,0.7364

epoch:1593/50, training loss:1.0425682067871094
Train Acc 0.7363
 Acc 0.7365
new best val f1: 0.7364760738135694
meta,dgl,1,1592,54.2821,0.7365

epoch:1594/50, training loss:1.042249083518982
Train Acc 0.7364
 Acc 0.7360
meta,dgl,1,1593,54.3158,0.7360

epoch:1595/50, training loss:1.041929841041565
Train Acc 0.7360
 Acc 0.7368
new best val f1: 0.7367592629180332
meta,dgl,1,1594,54.3503,0.7368

epoch:1596/50, training loss:1.041610598564148
Train Acc 0.7367
 Acc 0.7368
new best val f1: 0.7368317066424309
meta,dgl,1,1595,54.3840,0.7368

epoch:1597/50, training loss:1.0412911176681519
Train Acc 0.7368
 Acc 0.7364
meta,dgl,1,1596,54.4184,0.7364

epoch:1598/50, training loss:1.0409724712371826
Train Acc 0.7363
 Acc 0.7371
new best val f1: 0.737134653126276
meta,dgl,1,1597,54.4521,0.7371

epoch:1599/50, training loss:1.0406551361083984
Train Acc 0.7370
 Acc 0.7371
meta,dgl,1,1598,54.4858,0.7371

epoch:1600/50, training loss:1.0403327941894531
Train Acc 0.7370
 Acc 0.7371
meta,dgl,1,1599,54.5197,0.7371

epoch:1601/50, training loss:1.040015459060669
Train Acc 0.7369
 Acc 0.7374
new best val f1: 0.7373585700925962
meta,dgl,1,1600,54.5535,0.7374

epoch:1602/50, training loss:1.039695382118225
Train Acc 0.7372
 Acc 0.7377
new best val f1: 0.7376746881626954
meta,dgl,1,1601,54.5873,0.7377

epoch:1603/50, training loss:1.0393773317337036
Train Acc 0.7376
 Acc 0.7371
meta,dgl,1,1602,54.6210,0.7371

epoch:1604/50, training loss:1.0390585660934448
Train Acc 0.7370
 Acc 0.7376
meta,dgl,1,1603,54.6546,0.7376

epoch:1605/50, training loss:1.0387383699417114
Train Acc 0.7376
 Acc 0.7382
new best val f1: 0.7382081374059878
meta,dgl,1,1604,54.6882,0.7382

epoch:1606/50, training loss:1.0384199619293213
Train Acc 0.7381
 Acc 0.7372
meta,dgl,1,1605,54.7219,0.7372

epoch:1607/50, training loss:1.0381027460098267
Train Acc 0.7372
 Acc 0.7383
new best val f1: 0.7382674095441314
meta,dgl,1,1606,54.7557,0.7383

epoch:1608/50, training loss:1.037781834602356
Train Acc 0.7382
 Acc 0.7383
new best val f1: 0.7383200958891479
meta,dgl,1,1607,54.7903,0.7383

epoch:1609/50, training loss:1.0374634265899658
Train Acc 0.7382
 Acc 0.7380
meta,dgl,1,1608,54.8240,0.7380

epoch:1610/50, training loss:1.0371451377868652
Train Acc 0.7378
 Acc 0.7384
new best val f1: 0.7384188827860539
meta,dgl,1,1609,54.8587,0.7384

epoch:1611/50, training loss:1.0368244647979736
Train Acc 0.7383
 Acc 0.7388
new best val f1: 0.7388403735461861
meta,dgl,1,1610,54.8920,0.7388

epoch:1612/50, training loss:1.036508321762085
Train Acc 0.7386
 Acc 0.7384
meta,dgl,1,1611,54.9259,0.7384

epoch:1613/50, training loss:1.0361922979354858
Train Acc 0.7381
 Acc 0.7388
meta,dgl,1,1612,54.9605,0.7388

epoch:1614/50, training loss:1.0358728170394897
Train Acc 0.7387
 Acc 0.7390
new best val f1: 0.739018189960617
meta,dgl,1,1613,54.9944,0.7390

epoch:1615/50, training loss:1.035552740097046
Train Acc 0.7389
 Acc 0.7386
meta,dgl,1,1614,55.0289,0.7386

epoch:1616/50, training loss:1.0352379083633423
Train Acc 0.7383
 Acc 0.7398
new best val f1: 0.7398018993427379
meta,dgl,1,1615,55.0626,0.7398

epoch:1617/50, training loss:1.0349218845367432
Train Acc 0.7396
 Acc 0.7388
meta,dgl,1,1616,55.0963,0.7388

epoch:1618/50, training loss:1.0346002578735352
Train Acc 0.7386
 Acc 0.7391
meta,dgl,1,1617,55.1300,0.7391

epoch:1619/50, training loss:1.0342838764190674
Train Acc 0.7390
 Acc 0.7403
new best val f1: 0.740309005413522
meta,dgl,1,1618,55.1645,0.7403

epoch:1620/50, training loss:1.0339664220809937
Train Acc 0.7401
 Acc 0.7392
meta,dgl,1,1619,55.1982,0.7392

epoch:1621/50, training loss:1.0336475372314453
Train Acc 0.7389
 Acc 0.7397
meta,dgl,1,1620,55.2327,0.7397

epoch:1622/50, training loss:1.0333268642425537
Train Acc 0.7395
 Acc 0.7406
new best val f1: 0.7405724371386047
meta,dgl,1,1621,55.2664,0.7406

epoch:1623/50, training loss:1.033013939857483
Train Acc 0.7403
 Acc 0.7395
meta,dgl,1,1622,55.3001,0.7395

epoch:1624/50, training loss:1.032700777053833
Train Acc 0.7392
 Acc 0.7406
new best val f1: 0.7406317092767482
meta,dgl,1,1623,55.3338,0.7406

epoch:1625/50, training loss:1.03237783908844
Train Acc 0.7403
 Acc 0.7404
meta,dgl,1,1624,55.3684,0.7404

epoch:1626/50, training loss:1.0320582389831543
Train Acc 0.7401
 Acc 0.7401
meta,dgl,1,1625,55.4020,0.7401

epoch:1627/50, training loss:1.0317435264587402
Train Acc 0.7397
 Acc 0.7410
new best val f1: 0.7409939278987369
meta,dgl,1,1626,55.4357,0.7410

epoch:1628/50, training loss:1.0314244031906128
Train Acc 0.7407
 Acc 0.7405
meta,dgl,1,1627,55.4692,0.7405

epoch:1629/50, training loss:1.03110671043396
Train Acc 0.7403
 Acc 0.7403
meta,dgl,1,1628,55.5028,0.7403

epoch:1630/50, training loss:1.030791997909546
Train Acc 0.7401
 Acc 0.7416
new best val f1: 0.7416393356251894
meta,dgl,1,1629,55.5373,0.7416

epoch:1631/50, training loss:1.0304744243621826
Train Acc 0.7413
 Acc 0.7407
meta,dgl,1,1630,55.5710,0.7407

epoch:1632/50, training loss:1.0301555395126343
Train Acc 0.7404
 Acc 0.7407
meta,dgl,1,1631,55.6046,0.7407

epoch:1633/50, training loss:1.0298386812210083
Train Acc 0.7405
 Acc 0.7420
new best val f1: 0.7420344832128133
meta,dgl,1,1632,55.6382,0.7420

epoch:1634/50, training loss:1.029525637626648
Train Acc 0.7417
 Acc 0.7408
meta,dgl,1,1633,55.6727,0.7408

epoch:1635/50, training loss:1.0292079448699951
Train Acc 0.7406
 Acc 0.7412
meta,dgl,1,1634,55.7063,0.7412

epoch:1636/50, training loss:1.0288876295089722
Train Acc 0.7409
 Acc 0.7424
new best val f1: 0.7423703586622937
meta,dgl,1,1635,55.7399,0.7424

epoch:1637/50, training loss:1.0285755395889282
Train Acc 0.7420
 Acc 0.7412
meta,dgl,1,1636,55.7735,0.7412

epoch:1638/50, training loss:1.0282570123672485
Train Acc 0.7409
 Acc 0.7418
meta,dgl,1,1637,55.8080,0.7418

epoch:1639/50, training loss:1.0279383659362793
Train Acc 0.7414
 Acc 0.7427
new best val f1: 0.7426798909392658
meta,dgl,1,1638,55.8417,0.7427

epoch:1640/50, training loss:1.0276269912719727
Train Acc 0.7423
 Acc 0.7414
meta,dgl,1,1639,55.8754,0.7414

epoch:1641/50, training loss:1.027308464050293
Train Acc 0.7411
 Acc 0.7423
meta,dgl,1,1640,55.9091,0.7423

epoch:1642/50, training loss:1.0269898176193237
Train Acc 0.7419
 Acc 0.7430
new best val f1: 0.7429762516299838
meta,dgl,1,1641,55.9436,0.7430

epoch:1643/50, training loss:1.026678442955017
Train Acc 0.7426
 Acc 0.7418
meta,dgl,1,1642,55.9783,0.7418

epoch:1644/50, training loss:1.0263621807098389
Train Acc 0.7415
 Acc 0.7430
meta,dgl,1,1643,56.0120,0.7430

epoch:1645/50, training loss:1.026041865348816
Train Acc 0.7425
 Acc 0.7431
new best val f1: 0.743088210113144
meta,dgl,1,1644,56.0457,0.7431

epoch:1646/50, training loss:1.0257270336151123
Train Acc 0.7427
 Acc 0.7422
meta,dgl,1,1645,56.0795,0.7422

epoch:1647/50, training loss:1.0254122018814087
Train Acc 0.7418
 Acc 0.7433
new best val f1: 0.7433187128725912
meta,dgl,1,1646,56.1131,0.7433

epoch:1648/50, training loss:1.0250924825668335
Train Acc 0.7429
 Acc 0.7435
new best val f1: 0.7435162866664032
meta,dgl,1,1647,56.1468,0.7435

epoch:1649/50, training loss:1.0247772932052612
Train Acc 0.7431
 Acc 0.7426
meta,dgl,1,1648,56.1805,0.7426

epoch:1650/50, training loss:1.0244638919830322
Train Acc 0.7423
 Acc 0.7436
new best val f1: 0.7436216593564363
meta,dgl,1,1649,56.2142,0.7436

epoch:1651/50, training loss:1.024145245552063
Train Acc 0.7432
 Acc 0.7438
new best val f1: 0.7438192331502483
meta,dgl,1,1650,56.2486,0.7438

epoch:1652/50, training loss:1.0238298177719116
Train Acc 0.7434
 Acc 0.7431
meta,dgl,1,1651,56.2827,0.7431

epoch:1653/50, training loss:1.0235157012939453
Train Acc 0.7426
 Acc 0.7439
new best val f1: 0.743898262667773
meta,dgl,1,1652,56.3164,0.7439

epoch:1654/50, training loss:1.0231995582580566
Train Acc 0.7435
 Acc 0.7440
new best val f1: 0.7440233927371873
meta,dgl,1,1653,56.3501,0.7440

epoch:1655/50, training loss:1.0228849649429321
Train Acc 0.7436
 Acc 0.7437
meta,dgl,1,1654,56.3846,0.7437

epoch:1656/50, training loss:1.0225727558135986
Train Acc 0.7432
 Acc 0.7444
new best val f1: 0.7443658539797948
meta,dgl,1,1655,56.4184,0.7444

epoch:1657/50, training loss:1.0222522020339966
Train Acc 0.7440
 Acc 0.7442
meta,dgl,1,1656,56.4521,0.7442

epoch:1658/50, training loss:1.0219378471374512
Train Acc 0.7437
 Acc 0.7440
meta,dgl,1,1657,56.4857,0.7440

epoch:1659/50, training loss:1.0216240882873535
Train Acc 0.7435
 Acc 0.7448
new best val f1: 0.7448005163261812
meta,dgl,1,1658,56.5194,0.7448

epoch:1660/50, training loss:1.0213086605072021
Train Acc 0.7444
 Acc 0.7445
meta,dgl,1,1659,56.5531,0.7445

epoch:1661/50, training loss:1.0209912061691284
Train Acc 0.7441
 Acc 0.7443
meta,dgl,1,1660,56.5867,0.7443

epoch:1662/50, training loss:1.0206769704818726
Train Acc 0.7438
 Acc 0.7454
new best val f1: 0.7453537229488547
meta,dgl,1,1661,56.6205,0.7454

epoch:1663/50, training loss:1.0203641653060913
Train Acc 0.7449
 Acc 0.7447
meta,dgl,1,1662,56.6542,0.7447

epoch:1664/50, training loss:1.0200468301773071
Train Acc 0.7443
 Acc 0.7448
meta,dgl,1,1663,56.6879,0.7448

epoch:1665/50, training loss:1.0197312831878662
Train Acc 0.7443
 Acc 0.7457
new best val f1: 0.7457027699845893
meta,dgl,1,1664,56.7224,0.7457

epoch:1666/50, training loss:1.0194158554077148
Train Acc 0.7452
 Acc 0.7451
meta,dgl,1,1665,56.7560,0.7451

epoch:1667/50, training loss:1.019101858139038
Train Acc 0.7447
 Acc 0.7454
meta,dgl,1,1666,56.7896,0.7454

epoch:1668/50, training loss:1.0187857151031494
Train Acc 0.7450
 Acc 0.7458
new best val f1: 0.7458476574333847
meta,dgl,1,1667,56.8240,0.7458

epoch:1669/50, training loss:1.0184731483459473
Train Acc 0.7454
 Acc 0.7453
meta,dgl,1,1668,56.8577,0.7453

epoch:1670/50, training loss:1.0181595087051392
Train Acc 0.7449
 Acc 0.7461
new best val f1: 0.7460913317790862
meta,dgl,1,1669,56.8914,0.7461

epoch:1671/50, training loss:1.0178425312042236
Train Acc 0.7457
 Acc 0.7460
meta,dgl,1,1670,56.9250,0.7460

epoch:1672/50, training loss:1.017527461051941
Train Acc 0.7456
 Acc 0.7456
meta,dgl,1,1671,56.9587,0.7456

epoch:1673/50, training loss:1.0172098875045776
Train Acc 0.7452
 Acc 0.7466
new best val f1: 0.7466181952292514
meta,dgl,1,1672,56.9924,0.7466

epoch:1674/50, training loss:1.0168962478637695
Train Acc 0.7461
 Acc 0.7463
meta,dgl,1,1673,57.0270,0.7463

epoch:1675/50, training loss:1.0165787935256958
Train Acc 0.7458
 Acc 0.7464
meta,dgl,1,1674,57.0616,0.7464

epoch:1676/50, training loss:1.016262412071228
Train Acc 0.7459
 Acc 0.7467
new best val f1: 0.7467301537124116
meta,dgl,1,1675,57.0949,0.7467

epoch:1677/50, training loss:1.0159450769424438
Train Acc 0.7463
 Acc 0.7467
meta,dgl,1,1676,57.1286,0.7467

epoch:1678/50, training loss:1.0156294107437134
Train Acc 0.7462
 Acc 0.7468
new best val f1: 0.7468355264024447
meta,dgl,1,1677,57.1632,0.7468

epoch:1679/50, training loss:1.0153166055679321
Train Acc 0.7463
 Acc 0.7470
new best val f1: 0.7469738280581131
meta,dgl,1,1678,57.1969,0.7470

epoch:1680/50, training loss:1.0150021314620972
Train Acc 0.7465
 Acc 0.7470
new best val f1: 0.7470396859893838
meta,dgl,1,1679,57.2307,0.7470

epoch:1681/50, training loss:1.014687180519104
Train Acc 0.7466
 Acc 0.7472
new best val f1: 0.747177987645052
meta,dgl,1,1680,57.2644,0.7472

epoch:1682/50, training loss:1.014373540878296
Train Acc 0.7467
 Acc 0.7477
new best val f1: 0.7476719221295821
meta,dgl,1,1681,57.2981,0.7477

epoch:1683/50, training loss:1.0140619277954102
Train Acc 0.7472
 Acc 0.7470
meta,dgl,1,1682,57.3319,0.7470

epoch:1684/50, training loss:1.013745665550232
Train Acc 0.7465
 Acc 0.7479
new best val f1: 0.7479155964752835
meta,dgl,1,1683,57.3666,0.7479

epoch:1685/50, training loss:1.0134296417236328
Train Acc 0.7474
 Acc 0.7477
meta,dgl,1,1684,57.4003,0.7477

epoch:1686/50, training loss:1.0131137371063232
Train Acc 0.7471
 Acc 0.7475
meta,dgl,1,1685,57.4341,0.7475

epoch:1687/50, training loss:1.012803316116333
Train Acc 0.7470
 Acc 0.7483
new best val f1: 0.7482975724766534
meta,dgl,1,1686,57.4679,0.7483

epoch:1688/50, training loss:1.0124893188476562
Train Acc 0.7478
 Acc 0.7478
meta,dgl,1,1687,57.5017,0.7478

epoch:1689/50, training loss:1.0121721029281616
Train Acc 0.7473
 Acc 0.7482
meta,dgl,1,1688,57.5362,0.7482

epoch:1690/50, training loss:1.011859655380249
Train Acc 0.7477
 Acc 0.7485
new best val f1: 0.7485149036498465
meta,dgl,1,1689,57.5699,0.7485

epoch:1691/50, training loss:1.0115432739257812
Train Acc 0.7481
 Acc 0.7481
meta,dgl,1,1690,57.6036,0.7481

epoch:1692/50, training loss:1.0112305879592896
Train Acc 0.7476
 Acc 0.7488
new best val f1: 0.7487915069611834
meta,dgl,1,1691,57.6373,0.7488

epoch:1693/50, training loss:1.010917067527771
Train Acc 0.7483
 Acc 0.7487
meta,dgl,1,1692,57.6723,0.7487

epoch:1694/50, training loss:1.01060152053833
Train Acc 0.7482
 Acc 0.7487
meta,dgl,1,1693,57.7068,0.7487

epoch:1695/50, training loss:1.0102858543395996
Train Acc 0.7482
 Acc 0.7490
new best val f1: 0.7489824949618683
meta,dgl,1,1694,57.7404,0.7490

epoch:1696/50, training loss:1.0099740028381348
Train Acc 0.7484
 Acc 0.7492
new best val f1: 0.7492393408938238
meta,dgl,1,1695,57.7740,0.7492

epoch:1697/50, training loss:1.0096614360809326
Train Acc 0.7488
 Acc 0.7489
meta,dgl,1,1696,57.8077,0.7489

epoch:1698/50, training loss:1.0093446969985962
Train Acc 0.7484
 Acc 0.7493
new best val f1: 0.7493249562044757
meta,dgl,1,1697,57.8421,0.7493

epoch:1699/50, training loss:1.0090309381484985
Train Acc 0.7489
 Acc 0.7495
new best val f1: 0.7495159442051607
meta,dgl,1,1698,57.8766,0.7495

epoch:1700/50, training loss:1.0087178945541382
Train Acc 0.7490
 Acc 0.7493
meta,dgl,1,1699,57.9104,0.7493

epoch:1701/50, training loss:1.0084060430526733
Train Acc 0.7487
 Acc 0.7499
new best val f1: 0.7499440207584199
meta,dgl,1,1700,57.9441,0.7499

epoch:1702/50, training loss:1.008088231086731
Train Acc 0.7494
 Acc 0.7498
meta,dgl,1,1701,57.9778,0.7498

epoch:1703/50, training loss:1.0077753067016602
Train Acc 0.7492
 Acc 0.7497
meta,dgl,1,1702,58.0115,0.7497

epoch:1704/50, training loss:1.0074621438980103
Train Acc 0.7492
 Acc 0.7501
new best val f1: 0.7500889082072154
meta,dgl,1,1703,58.0461,0.7501

epoch:1705/50, training loss:1.0071468353271484
Train Acc 0.7496
 Acc 0.7503
new best val f1: 0.7502667246216462
meta,dgl,1,1704,58.0798,0.7503

epoch:1706/50, training loss:1.0068341493606567
Train Acc 0.7498
 Acc 0.7500
meta,dgl,1,1705,58.1135,0.7500

epoch:1707/50, training loss:1.006519079208374
Train Acc 0.7495
 Acc 0.7505
new best val f1: 0.7505367421398559
meta,dgl,1,1706,58.1471,0.7505

epoch:1708/50, training loss:1.0062053203582764
Train Acc 0.7500
 Acc 0.7507
new best val f1: 0.7506882153817784
meta,dgl,1,1707,58.1808,0.7507

epoch:1709/50, training loss:1.0058951377868652
Train Acc 0.7502
 Acc 0.7502
meta,dgl,1,1708,58.2146,0.7502

epoch:1710/50, training loss:1.0055811405181885
Train Acc 0.7497
 Acc 0.7510
new best val f1: 0.7509845760724964
meta,dgl,1,1709,58.2492,0.7510

epoch:1711/50, training loss:1.0052669048309326
Train Acc 0.7504
 Acc 0.7508
meta,dgl,1,1710,58.2830,0.7508

epoch:1712/50, training loss:1.0049506425857544
Train Acc 0.7503
 Acc 0.7508
meta,dgl,1,1711,58.3177,0.7508

epoch:1713/50, training loss:1.0046378374099731
Train Acc 0.7502
 Acc 0.7514
new best val f1: 0.7514324100051369
meta,dgl,1,1712,58.3522,0.7514

epoch:1714/50, training loss:1.0043253898620605
Train Acc 0.7508
 Acc 0.7511
meta,dgl,1,1713,58.3859,0.7511

epoch:1715/50, training loss:1.0040127038955688
Train Acc 0.7505
 Acc 0.7514
meta,dgl,1,1714,58.4196,0.7514

epoch:1716/50, training loss:1.0036969184875488
Train Acc 0.7508
 Acc 0.7515
new best val f1: 0.7515246111089159
meta,dgl,1,1715,58.4534,0.7515

epoch:1717/50, training loss:1.0033836364746094
Train Acc 0.7510
 Acc 0.7516
new best val f1: 0.7516365695920759
meta,dgl,1,1716,58.4871,0.7516

epoch:1718/50, training loss:1.003070592880249
Train Acc 0.7511
 Acc 0.7515
meta,dgl,1,1717,58.5216,0.7515

epoch:1719/50, training loss:1.0027573108673096
Train Acc 0.7510
 Acc 0.7520
new best val f1: 0.7519658592484293
meta,dgl,1,1718,58.5552,0.7520

epoch:1720/50, training loss:1.0024458169937134
Train Acc 0.7515
 Acc 0.7519
meta,dgl,1,1719,58.5897,0.7519

epoch:1721/50, training loss:1.0021312236785889
Train Acc 0.7513
 Acc 0.7521
new best val f1: 0.7520909893178436
meta,dgl,1,1720,58.6234,0.7521

epoch:1722/50, training loss:1.0018162727355957
Train Acc 0.7516
 Acc 0.7521
meta,dgl,1,1721,58.6578,0.7521

epoch:1723/50, training loss:1.001504898071289
Train Acc 0.7516
 Acc 0.7526
new best val f1: 0.7525915095955006
meta,dgl,1,1722,58.6916,0.7526

epoch:1724/50, training loss:1.0011931657791138
Train Acc 0.7521
 Acc 0.7523
meta,dgl,1,1723,58.7253,0.7523

epoch:1725/50, training loss:1.0008779764175415
Train Acc 0.7518
 Acc 0.7524
meta,dgl,1,1724,58.7590,0.7524

epoch:1726/50, training loss:1.0005645751953125
Train Acc 0.7519
 Acc 0.7529
new best val f1: 0.7528746986999645
meta,dgl,1,1725,58.7934,0.7529

epoch:1727/50, training loss:1.0002505779266357
Train Acc 0.7525
 Acc 0.7526
meta,dgl,1,1726,58.8270,0.7526

epoch:1728/50, training loss:0.9999381303787231
Train Acc 0.7520
 Acc 0.7529
new best val f1: 0.7528944560793457
meta,dgl,1,1727,58.8606,0.7529

epoch:1729/50, training loss:0.99962317943573
Train Acc 0.7524
 Acc 0.7529
new best val f1: 0.752933970838108
meta,dgl,1,1728,58.8943,0.7529

epoch:1730/50, training loss:0.9993124008178711
Train Acc 0.7525
 Acc 0.7532
new best val f1: 0.7531644735975553
meta,dgl,1,1729,58.9287,0.7532

epoch:1731/50, training loss:0.9989989399909973
Train Acc 0.7528
 Acc 0.7529
meta,dgl,1,1730,58.9623,0.7529

epoch:1732/50, training loss:0.9986858367919922
Train Acc 0.7525
 Acc 0.7534
new best val f1: 0.753421319529511
meta,dgl,1,1731,58.9958,0.7534

epoch:1733/50, training loss:0.9983722567558289
Train Acc 0.7530
 Acc 0.7534
meta,dgl,1,1732,59.0295,0.7534

epoch:1734/50, training loss:0.9980596899986267
Train Acc 0.7530
 Acc 0.7534
new best val f1: 0.7534476627020192
meta,dgl,1,1733,59.0630,0.7534

epoch:1735/50, training loss:0.9977465867996216
Train Acc 0.7530
 Acc 0.7539
new best val f1: 0.7538559818758973
meta,dgl,1,1734,59.0967,0.7539

epoch:1736/50, training loss:0.997435450553894
Train Acc 0.7534
 Acc 0.7536
meta,dgl,1,1735,59.1304,0.7536

epoch:1737/50, training loss:0.9971267580986023
Train Acc 0.7531
 Acc 0.7542
new best val f1: 0.7541786857391236
meta,dgl,1,1736,59.1641,0.7542

epoch:1738/50, training loss:0.9968093633651733
Train Acc 0.7537
 Acc 0.7541
meta,dgl,1,1737,59.1977,0.7541

epoch:1739/50, training loss:0.9964951872825623
Train Acc 0.7536
 Acc 0.7540
meta,dgl,1,1738,59.2322,0.7540

epoch:1740/50, training loss:0.9961840510368347
Train Acc 0.7534
 Acc 0.7544
new best val f1: 0.7544355316710791
meta,dgl,1,1739,59.2667,0.7544

epoch:1741/50, training loss:0.9958717823028564
Train Acc 0.7539
 Acc 0.7544
meta,dgl,1,1740,59.3003,0.7544

epoch:1742/50, training loss:0.9955596327781677
Train Acc 0.7539
 Acc 0.7545
new best val f1: 0.7544618748435874
meta,dgl,1,1741,59.3340,0.7545

epoch:1743/50, training loss:0.9952443838119507
Train Acc 0.7539
 Acc 0.7548
new best val f1: 0.7547516497411784
meta,dgl,1,1742,59.3685,0.7548

epoch:1744/50, training loss:0.9949349164962769
Train Acc 0.7542
 Acc 0.7548
meta,dgl,1,1743,59.4021,0.7548

epoch:1745/50, training loss:0.9946212768554688
Train Acc 0.7543
 Acc 0.7549
new best val f1: 0.7549426377418632
meta,dgl,1,1744,59.4366,0.7549

epoch:1746/50, training loss:0.9943094849586487
Train Acc 0.7544
 Acc 0.7548
meta,dgl,1,1745,59.4704,0.7548

epoch:1747/50, training loss:0.993996262550354
Train Acc 0.7542
 Acc 0.7554
new best val f1: 0.7554365722263933
meta,dgl,1,1746,59.5041,0.7554

epoch:1748/50, training loss:0.9936833381652832
Train Acc 0.7549
 Acc 0.7552
meta,dgl,1,1747,59.5378,0.7552

epoch:1749/50, training loss:0.9933711290359497
Train Acc 0.7546
 Acc 0.7553
meta,dgl,1,1748,59.5715,0.7553

epoch:1750/50, training loss:0.9930580854415894
Train Acc 0.7547
 Acc 0.7556
new best val f1: 0.7555814596751886
meta,dgl,1,1749,59.6060,0.7556

epoch:1751/50, training loss:0.9927493333816528
Train Acc 0.7550
 Acc 0.7554
meta,dgl,1,1750,59.6397,0.7554

epoch:1752/50, training loss:0.9924355745315552
Train Acc 0.7548
 Acc 0.7560
new best val f1: 0.7559831930559398
meta,dgl,1,1751,59.6734,0.7560

epoch:1753/50, training loss:0.9921205639839172
Train Acc 0.7554
 Acc 0.7556
meta,dgl,1,1752,59.7071,0.7556

epoch:1754/50, training loss:0.9918103814125061
Train Acc 0.7551
 Acc 0.7557
meta,dgl,1,1753,59.7417,0.7557

epoch:1755/50, training loss:0.9914964437484741
Train Acc 0.7552
 Acc 0.7563
new best val f1: 0.7563058969191659
meta,dgl,1,1754,59.7754,0.7563

epoch:1756/50, training loss:0.9911869764328003
Train Acc 0.7558
 Acc 0.7559
meta,dgl,1,1755,59.8090,0.7559

epoch:1757/50, training loss:0.990874707698822
Train Acc 0.7554
 Acc 0.7560
meta,dgl,1,1756,59.8427,0.7560

epoch:1758/50, training loss:0.9905595779418945
Train Acc 0.7554
 Acc 0.7565
new best val f1: 0.756510056506105
meta,dgl,1,1757,59.8771,0.7565

epoch:1759/50, training loss:0.9902474880218506
Train Acc 0.7560
 Acc 0.7565
new best val f1: 0.7565232280923592
meta,dgl,1,1758,59.9116,0.7565

epoch:1760/50, training loss:0.9899376630783081
Train Acc 0.7560
 Acc 0.7564
meta,dgl,1,1759,59.9452,0.7564

epoch:1761/50, training loss:0.989625096321106
Train Acc 0.7558
 Acc 0.7569
new best val f1: 0.756905204093729
meta,dgl,1,1760,59.9788,0.7569

epoch:1762/50, training loss:0.9893133044242859
Train Acc 0.7563
 Acc 0.7567
meta,dgl,1,1761,60.0124,0.7567

epoch:1763/50, training loss:0.9890046119689941
Train Acc 0.7561
 Acc 0.7570
new best val f1: 0.7569513046456184
meta,dgl,1,1762,60.0460,0.7570

epoch:1764/50, training loss:0.9886891841888428
Train Acc 0.7564
 Acc 0.7570
new best val f1: 0.757010576783762
meta,dgl,1,1763,60.0806,0.7570

epoch:1765/50, training loss:0.9883790612220764
Train Acc 0.7564
 Acc 0.7570
new best val f1: 0.7570237483700162
meta,dgl,1,1764,60.1151,0.7570

epoch:1766/50, training loss:0.9880694150924683
Train Acc 0.7565
 Acc 0.7577
new best val f1: 0.7577218424414852
meta,dgl,1,1765,60.1487,0.7577

epoch:1767/50, training loss:0.9877549409866333
Train Acc 0.7571
 Acc 0.7571
meta,dgl,1,1766,60.1824,0.7571

epoch:1768/50, training loss:0.9874410629272461
Train Acc 0.7565
 Acc 0.7573
meta,dgl,1,1767,60.2161,0.7573

epoch:1769/50, training loss:0.9871318936347961
Train Acc 0.7568
 Acc 0.7580
new best val f1: 0.7579852741665679
meta,dgl,1,1768,60.2498,0.7580

epoch:1770/50, training loss:0.9868192672729492
Train Acc 0.7574
 Acc 0.7576
meta,dgl,1,1769,60.2843,0.7576

epoch:1771/50, training loss:0.9865107536315918
Train Acc 0.7569
 Acc 0.7576
meta,dgl,1,1770,60.3179,0.7576

epoch:1772/50, training loss:0.9861969947814941
Train Acc 0.7571
 Acc 0.7583
new best val f1: 0.7583145638229212
meta,dgl,1,1771,60.3516,0.7583

epoch:1773/50, training loss:0.9858862161636353
Train Acc 0.7577
 Acc 0.7580
meta,dgl,1,1772,60.3852,0.7580

epoch:1774/50, training loss:0.9855754971504211
Train Acc 0.7573
 Acc 0.7582
meta,dgl,1,1773,60.4188,0.7582

epoch:1775/50, training loss:0.9852634072303772
Train Acc 0.7576
 Acc 0.7584
new best val f1: 0.7584133507198272
meta,dgl,1,1774,60.4524,0.7584

epoch:1776/50, training loss:0.984953761100769
Train Acc 0.7578
 Acc 0.7583
meta,dgl,1,1775,60.4870,0.7583

epoch:1777/50, training loss:0.9846422672271729
Train Acc 0.7576
 Acc 0.7586
new best val f1: 0.7586372676861475
meta,dgl,1,1776,60.5217,0.7586

epoch:1778/50, training loss:0.984330952167511
Train Acc 0.7580
 Acc 0.7589
new best val f1: 0.7589204567906113
meta,dgl,1,1777,60.5554,0.7589

epoch:1779/50, training loss:0.9840218424797058
Train Acc 0.7583
 Acc 0.7583
meta,dgl,1,1778,60.5890,0.7583

epoch:1780/50, training loss:0.9837170839309692
Train Acc 0.7577
 Acc 0.7595
new best val f1: 0.7595065923789202
meta,dgl,1,1779,60.6228,0.7595

epoch:1781/50, training loss:0.9834002256393433
Train Acc 0.7589
 Acc 0.7589
meta,dgl,1,1780,60.6565,0.7589

epoch:1782/50, training loss:0.9830892086029053
Train Acc 0.7582
 Acc 0.7587
meta,dgl,1,1781,60.6910,0.7587

epoch:1783/50, training loss:0.9827828407287598
Train Acc 0.7580
 Acc 0.7599
new best val f1: 0.7599214973459254
meta,dgl,1,1782,60.7246,0.7599

epoch:1784/50, training loss:0.9824752807617188
Train Acc 0.7594
 Acc 0.7591
meta,dgl,1,1783,60.7584,0.7591

epoch:1785/50, training loss:0.9821575880050659
Train Acc 0.7585
 Acc 0.7590
meta,dgl,1,1784,60.7921,0.7590

epoch:1786/50, training loss:0.9818510413169861
Train Acc 0.7584
 Acc 0.7604
new best val f1: 0.7604088460373283
meta,dgl,1,1785,60.8258,0.7604

epoch:1787/50, training loss:0.9815524816513062
Train Acc 0.7599
 Acc 0.7592
meta,dgl,1,1786,60.8596,0.7592

epoch:1788/50, training loss:0.9812324047088623
Train Acc 0.7585
 Acc 0.7597
meta,dgl,1,1787,60.8941,0.7597

epoch:1789/50, training loss:0.9809146523475647
Train Acc 0.7590
 Acc 0.7606
new best val f1: 0.7605603192792508
meta,dgl,1,1788,60.9278,0.7606

epoch:1790/50, training loss:0.9806131720542908
Train Acc 0.7600
 Acc 0.7594
meta,dgl,1,1789,60.9616,0.7594

epoch:1791/50, training loss:0.9803041815757751
Train Acc 0.7588
 Acc 0.7603
meta,dgl,1,1790,60.9953,0.7603

epoch:1792/50, training loss:0.9799825549125671
Train Acc 0.7596
 Acc 0.7608
new best val f1: 0.760784236245571
meta,dgl,1,1791,61.0298,0.7608

epoch:1793/50, training loss:0.9796786308288574
Train Acc 0.7601
 Acc 0.7597
meta,dgl,1,1792,61.0645,0.7597

epoch:1794/50, training loss:0.9793747067451477
Train Acc 0.7590
 Acc 0.7610
new best val f1: 0.7609620526600018
meta,dgl,1,1793,61.0981,0.7610

epoch:1795/50, training loss:0.9790554046630859
Train Acc 0.7603
 Acc 0.7609
meta,dgl,1,1794,61.1318,0.7609

epoch:1796/50, training loss:0.9787444472312927
Train Acc 0.7601
 Acc 0.7603
meta,dgl,1,1795,61.1655,0.7603

epoch:1797/50, training loss:0.9784452319145203
Train Acc 0.7595
 Acc 0.7612
new best val f1: 0.7612452417644657
meta,dgl,1,1796,61.2001,0.7612

epoch:1798/50, training loss:0.9781299829483032
Train Acc 0.7606
 Acc 0.7611
meta,dgl,1,1797,61.2337,0.7611

epoch:1799/50, training loss:0.9778163433074951
Train Acc 0.7604
 Acc 0.7607
meta,dgl,1,1798,61.2682,0.7607

epoch:1800/50, training loss:0.9775119423866272
Train Acc 0.7599
 Acc 0.7616
new best val f1: 0.7616074603864543
meta,dgl,1,1799,61.3026,0.7616

epoch:1801/50, training loss:0.9772011637687683
Train Acc 0.7610
 Acc 0.7614
meta,dgl,1,1800,61.3364,0.7614

epoch:1802/50, training loss:0.9768868684768677
Train Acc 0.7606
 Acc 0.7610
meta,dgl,1,1801,61.3701,0.7610

epoch:1803/50, training loss:0.9765859246253967
Train Acc 0.7602
 Acc 0.7620
new best val f1: 0.7620157795603324
meta,dgl,1,1802,61.4038,0.7620

epoch:1804/50, training loss:0.9762787222862244
Train Acc 0.7614
 Acc 0.7614
meta,dgl,1,1803,61.4375,0.7614

epoch:1805/50, training loss:0.9759611487388611
Train Acc 0.7607
 Acc 0.7614
meta,dgl,1,1804,61.4712,0.7614

epoch:1806/50, training loss:0.9756505489349365
Train Acc 0.7607
 Acc 0.7623
new best val f1: 0.7622594539060339
meta,dgl,1,1805,61.5057,0.7623

epoch:1807/50, training loss:0.9753515124320984
Train Acc 0.7616
 Acc 0.7618
meta,dgl,1,1806,61.5395,0.7618

epoch:1808/50, training loss:0.9750339984893799
Train Acc 0.7611
 Acc 0.7620
meta,dgl,1,1807,61.5732,0.7620

epoch:1809/50, training loss:0.9747269749641418
Train Acc 0.7612
 Acc 0.7625
new best val f1: 0.7625492288036249
meta,dgl,1,1808,61.6068,0.7625

epoch:1810/50, training loss:0.9744204878807068
Train Acc 0.7619
 Acc 0.7621
meta,dgl,1,1809,61.6415,0.7621

epoch:1811/50, training loss:0.974108874797821
Train Acc 0.7614
 Acc 0.7624
meta,dgl,1,1810,61.6752,0.7624

epoch:1812/50, training loss:0.9737992286682129
Train Acc 0.7616
 Acc 0.7627
new best val f1: 0.762667773079912
meta,dgl,1,1811,61.7098,0.7627

epoch:1813/50, training loss:0.9734938144683838
Train Acc 0.7621
 Acc 0.7623
meta,dgl,1,1812,61.7435,0.7623

epoch:1814/50, training loss:0.9731835126876831
Train Acc 0.7616
 Acc 0.7628
new best val f1: 0.7628258321149616
meta,dgl,1,1813,61.7772,0.7628

epoch:1815/50, training loss:0.9728705883026123
Train Acc 0.7621
 Acc 0.7630
new best val f1: 0.7629575479775029
meta,dgl,1,1814,61.8109,0.7630

epoch:1816/50, training loss:0.9725685715675354
Train Acc 0.7623
 Acc 0.7627
meta,dgl,1,1815,61.8456,0.7627

epoch:1817/50, training loss:0.9722592830657959
Train Acc 0.7620
 Acc 0.7633
new best val f1: 0.7633065950132375
meta,dgl,1,1816,61.8800,0.7633

epoch:1818/50, training loss:0.9719491004943848
Train Acc 0.7626
 Acc 0.7632
meta,dgl,1,1817,61.9137,0.7632

epoch:1819/50, training loss:0.9716387987136841
Train Acc 0.7625
 Acc 0.7630
meta,dgl,1,1818,61.9474,0.7630

epoch:1820/50, training loss:0.9713330268859863
Train Acc 0.7624
 Acc 0.7637
new best val f1: 0.7636688136352261
meta,dgl,1,1819,61.9811,0.7637

epoch:1821/50, training loss:0.9710239171981812
Train Acc 0.7631
 Acc 0.7634
meta,dgl,1,1820,62.0156,0.7634

epoch:1822/50, training loss:0.9707143902778625
Train Acc 0.7628
 Acc 0.7636
meta,dgl,1,1821,62.0502,0.7636

epoch:1823/50, training loss:0.9704042673110962
Train Acc 0.7629
 Acc 0.7640
new best val f1: 0.76395200273969
meta,dgl,1,1822,62.0839,0.7640

epoch:1824/50, training loss:0.9700993299484253
Train Acc 0.7633
 Acc 0.7637
meta,dgl,1,1823,62.1176,0.7637

epoch:1825/50, training loss:0.9697886109352112
Train Acc 0.7631
 Acc 0.7639
meta,dgl,1,1824,62.1513,0.7639

epoch:1826/50, training loss:0.9694807529449463
Train Acc 0.7632
 Acc 0.7645
new best val f1: 0.764544724121126
meta,dgl,1,1825,62.1858,0.7645

epoch:1827/50, training loss:0.9691770076751709
Train Acc 0.7639
 Acc 0.7639
meta,dgl,1,1826,62.2203,0.7639

epoch:1828/50, training loss:0.9688695073127747
Train Acc 0.7632
 Acc 0.7646
new best val f1: 0.7646435110180319
meta,dgl,1,1827,62.2539,0.7646

epoch:1829/50, training loss:0.9685609936714172
Train Acc 0.7641
 Acc 0.7644
meta,dgl,1,1828,62.2875,0.7644

epoch:1830/50, training loss:0.9682483673095703
Train Acc 0.7638
 Acc 0.7645
meta,dgl,1,1829,62.3212,0.7645

epoch:1831/50, training loss:0.9679429531097412
Train Acc 0.7638
 Acc 0.7649
new best val f1: 0.7648937711568604
meta,dgl,1,1830,62.3548,0.7649

epoch:1832/50, training loss:0.9676380157470703
Train Acc 0.7643
 Acc 0.7649
new best val f1: 0.7649069427431145
meta,dgl,1,1831,62.3884,0.7649

epoch:1833/50, training loss:0.9673240780830383
Train Acc 0.7642
 Acc 0.7649
meta,dgl,1,1832,62.4228,0.7649

epoch:1834/50, training loss:0.9670177102088928
Train Acc 0.7642
 Acc 0.7653
new best val f1: 0.7652559897788491
meta,dgl,1,1833,62.4564,0.7653

epoch:1835/50, training loss:0.966709554195404
Train Acc 0.7646
 Acc 0.7651
meta,dgl,1,1834,62.4900,0.7651

epoch:1836/50, training loss:0.966401994228363
Train Acc 0.7645
 Acc 0.7652
meta,dgl,1,1835,62.5237,0.7652

epoch:1837/50, training loss:0.966094970703125
Train Acc 0.7646
 Acc 0.7655
new best val f1: 0.7654930783314234
meta,dgl,1,1836,62.5582,0.7655

epoch:1838/50, training loss:0.9657894968986511
Train Acc 0.7650
 Acc 0.7654
meta,dgl,1,1837,62.5918,0.7654

epoch:1839/50, training loss:0.9654813408851624
Train Acc 0.7647
 Acc 0.7656
new best val f1: 0.7656116226077107
meta,dgl,1,1838,62.6255,0.7656

epoch:1840/50, training loss:0.9651695489883423
Train Acc 0.7650
 Acc 0.7659
new best val f1: 0.7659343264709368
meta,dgl,1,1839,62.6592,0.7659

epoch:1841/50, training loss:0.9648657441139221
Train Acc 0.7654
 Acc 0.7657
meta,dgl,1,1840,62.6929,0.7657

epoch:1842/50, training loss:0.9645577073097229
Train Acc 0.7650
 Acc 0.7661
new best val f1: 0.7661187286784947
meta,dgl,1,1841,62.7266,0.7661

epoch:1843/50, training loss:0.9642496705055237
Train Acc 0.7655
 Acc 0.7663
new best val f1: 0.7663163024723068
meta,dgl,1,1842,62.7611,0.7663

epoch:1844/50, training loss:0.9639430046081543
Train Acc 0.7656
 Acc 0.7661
meta,dgl,1,1843,62.7954,0.7661

epoch:1845/50, training loss:0.9636346101760864
Train Acc 0.7654
 Acc 0.7664
new best val f1: 0.7663624030241962
meta,dgl,1,1844,62.8291,0.7664

epoch:1846/50, training loss:0.9633283615112305
Train Acc 0.7657
 Acc 0.7666
new best val f1: 0.7665863199905164
meta,dgl,1,1845,62.8627,0.7666

epoch:1847/50, training loss:0.9630211591720581
Train Acc 0.7660
 Acc 0.7667
new best val f1: 0.7666587637149141
meta,dgl,1,1846,62.8964,0.7667

epoch:1848/50, training loss:0.9627152681350708
Train Acc 0.7660
 Acc 0.7666
meta,dgl,1,1847,62.9300,0.7666

epoch:1849/50, training loss:0.9624039530754089
Train Acc 0.7661
 Acc 0.7668
new best val f1: 0.7667970653705826
meta,dgl,1,1848,62.9644,0.7668

epoch:1850/50, training loss:0.9621003866195679
Train Acc 0.7662
 Acc 0.7670
new best val f1: 0.7670012249575217
meta,dgl,1,1849,62.9980,0.7670

epoch:1851/50, training loss:0.9617913365364075
Train Acc 0.7664
 Acc 0.7674
new best val f1: 0.7673897867520185
meta,dgl,1,1850,63.0317,0.7674

epoch:1852/50, training loss:0.9614861607551575
Train Acc 0.7668
 Acc 0.7670
meta,dgl,1,1851,63.0653,0.7670

epoch:1853/50, training loss:0.9611785411834717
Train Acc 0.7663
 Acc 0.7674
meta,dgl,1,1852,63.0999,0.7674

epoch:1854/50, training loss:0.9608700275421143
Train Acc 0.7667
 Acc 0.7678
new best val f1: 0.7677849343396426
meta,dgl,1,1853,63.1335,0.7678

epoch:1855/50, training loss:0.960565447807312
Train Acc 0.7672
 Acc 0.7672
meta,dgl,1,1854,63.1671,0.7672

epoch:1856/50, training loss:0.9602605700492859
Train Acc 0.7665
 Acc 0.7679
new best val f1: 0.7678507922709132
meta,dgl,1,1855,63.2007,0.7679

epoch:1857/50, training loss:0.9599523544311523
Train Acc 0.7672
 Acc 0.7680
new best val f1: 0.7680022655128357
meta,dgl,1,1856,63.2350,0.7680

epoch:1858/50, training loss:0.9596418738365173
Train Acc 0.7674
 Acc 0.7676
meta,dgl,1,1857,63.2693,0.7676

epoch:1859/50, training loss:0.9593504071235657
Train Acc 0.7670
 Acc 0.7683
new best val f1: 0.7683447267554432
meta,dgl,1,1858,63.3039,0.7683

epoch:1860/50, training loss:0.9590396881103516
Train Acc 0.7678
 Acc 0.7680
meta,dgl,1,1859,63.3376,0.7680

epoch:1861/50, training loss:0.9587256908416748
Train Acc 0.7673
 Acc 0.7680
meta,dgl,1,1860,63.3713,0.7680

epoch:1862/50, training loss:0.9584189653396606
Train Acc 0.7674
 Acc 0.7688
new best val f1: 0.7687728033087025
meta,dgl,1,1861,63.4050,0.7688

epoch:1863/50, training loss:0.958118736743927
Train Acc 0.7683
 Acc 0.7682
meta,dgl,1,1862,63.4387,0.7682

epoch:1864/50, training loss:0.9578063488006592
Train Acc 0.7675
 Acc 0.7686
meta,dgl,1,1863,63.4724,0.7686

epoch:1865/50, training loss:0.9574986100196838
Train Acc 0.7680
 Acc 0.7691
new best val f1: 0.7690625782062934
meta,dgl,1,1864,63.5070,0.7691

epoch:1866/50, training loss:0.9571961164474487
Train Acc 0.7686
 Acc 0.7684
meta,dgl,1,1865,63.5407,0.7684

epoch:1867/50, training loss:0.9568884372711182
Train Acc 0.7678
 Acc 0.7689
meta,dgl,1,1866,63.5743,0.7689

epoch:1868/50, training loss:0.9565833806991577
Train Acc 0.7684
 Acc 0.7692
new best val f1: 0.76922722303447
meta,dgl,1,1867,63.6081,0.7692

epoch:1869/50, training loss:0.9562755227088928
Train Acc 0.7688
 Acc 0.7687
meta,dgl,1,1868,63.6418,0.7687

epoch:1870/50, training loss:0.9559761881828308
Train Acc 0.7681
 Acc 0.7694
new best val f1: 0.7693721104832655
meta,dgl,1,1869,63.6763,0.7694

epoch:1871/50, training loss:0.955665647983551
Train Acc 0.7690
 Acc 0.7692
meta,dgl,1,1870,63.7100,0.7692

epoch:1872/50, training loss:0.955356776714325
Train Acc 0.7687
 Acc 0.7693
meta,dgl,1,1871,63.7444,0.7693

epoch:1873/50, training loss:0.955051064491272
Train Acc 0.7688
 Acc 0.7698
new best val f1: 0.7697870154502707
meta,dgl,1,1872,63.7780,0.7698

epoch:1874/50, training loss:0.9547492861747742
Train Acc 0.7694
 Acc 0.7694
meta,dgl,1,1873,63.8118,0.7694

epoch:1875/50, training loss:0.9544408917427063
Train Acc 0.7688
 Acc 0.7697
meta,dgl,1,1874,63.8455,0.7697

epoch:1876/50, training loss:0.9541335701942444
Train Acc 0.7693
 Acc 0.7699
new best val f1: 0.7699384886921932
meta,dgl,1,1875,63.8800,0.7699

epoch:1877/50, training loss:0.953827977180481
Train Acc 0.7695
 Acc 0.7699
meta,dgl,1,1876,63.9137,0.7699

epoch:1878/50, training loss:0.9535238146781921
Train Acc 0.7694
 Acc 0.7701
new best val f1: 0.7700636187616074
meta,dgl,1,1877,63.9482,0.7701

epoch:1879/50, training loss:0.9532181024551392
Train Acc 0.7696
 Acc 0.7702
new best val f1: 0.7701755772447676
meta,dgl,1,1878,63.9820,0.7702

epoch:1880/50, training loss:0.9529125094413757
Train Acc 0.7698
 Acc 0.7701
meta,dgl,1,1879,64.0157,0.7701

epoch:1881/50, training loss:0.9526092410087585
Train Acc 0.7696
 Acc 0.7706
new best val f1: 0.7706102395911539
meta,dgl,1,1880,64.0502,0.7706

epoch:1882/50, training loss:0.9523016214370728
Train Acc 0.7702
 Acc 0.7704
meta,dgl,1,1881,64.0839,0.7704

epoch:1883/50, training loss:0.951995313167572
Train Acc 0.7700
 Acc 0.7707
new best val f1: 0.7706958549018058
meta,dgl,1,1882,64.1176,0.7707

epoch:1884/50, training loss:0.951689600944519
Train Acc 0.7702
 Acc 0.7707
meta,dgl,1,1883,64.1514,0.7707

epoch:1885/50, training loss:0.9513845443725586
Train Acc 0.7703
 Acc 0.7710
new best val f1: 0.7709790440062697
meta,dgl,1,1884,64.1858,0.7710

epoch:1886/50, training loss:0.9510778188705444
Train Acc 0.7706
 Acc 0.7706
meta,dgl,1,1885,64.2194,0.7706

epoch:1887/50, training loss:0.9507765173912048
Train Acc 0.7702
 Acc 0.7712
new best val f1: 0.7711963751794628
meta,dgl,1,1886,64.2538,0.7712

epoch:1888/50, training loss:0.9504689574241638
Train Acc 0.7708
 Acc 0.7711
meta,dgl,1,1887,64.2881,0.7711

epoch:1889/50, training loss:0.9501641988754272
Train Acc 0.7706
 Acc 0.7712
meta,dgl,1,1888,64.3218,0.7712

epoch:1890/50, training loss:0.949856162071228
Train Acc 0.7708
 Acc 0.7714
new best val f1: 0.771413706352656
meta,dgl,1,1889,64.3555,0.7714

epoch:1891/50, training loss:0.9495561122894287
Train Acc 0.7711
 Acc 0.7713
meta,dgl,1,1890,64.3892,0.7713

epoch:1892/50, training loss:0.949251115322113
Train Acc 0.7709
 Acc 0.7715
new best val f1: 0.7715190790426891
meta,dgl,1,1891,64.4229,0.7715

epoch:1893/50, training loss:0.9489409327507019
Train Acc 0.7712
 Acc 0.7718
new best val f1: 0.7718351971127883
meta,dgl,1,1892,64.4574,0.7718

epoch:1894/50, training loss:0.9486455321311951
Train Acc 0.7716
 Acc 0.7712
meta,dgl,1,1893,64.4910,0.7712

epoch:1895/50, training loss:0.9483453631401062
Train Acc 0.7708
 Acc 0.7719
new best val f1: 0.7719208124234401
meta,dgl,1,1894,64.5255,0.7719

epoch:1896/50, training loss:0.9480277895927429
Train Acc 0.7717
 Acc 0.7720
new best val f1: 0.7720393566997273
meta,dgl,1,1895,64.5592,0.7720

epoch:1897/50, training loss:0.9477263689041138
Train Acc 0.7718
 Acc 0.7717
meta,dgl,1,1896,64.5929,0.7717

epoch:1898/50, training loss:0.9474323987960815
Train Acc 0.7712
 Acc 0.7723
new best val f1: 0.7723093742179371
meta,dgl,1,1897,64.6266,0.7723

epoch:1899/50, training loss:0.9471213817596436
Train Acc 0.7721
 Acc 0.7723
meta,dgl,1,1898,64.6612,0.7723

epoch:1900/50, training loss:0.9468088150024414
Train Acc 0.7720
 Acc 0.7720
meta,dgl,1,1899,64.6948,0.7720

epoch:1901/50, training loss:0.9465124011039734
Train Acc 0.7715
 Acc 0.7727
new best val f1: 0.7727308649780693
meta,dgl,1,1900,64.7292,0.7727

epoch:1902/50, training loss:0.9462143182754517
Train Acc 0.7726
 Acc 0.7725
meta,dgl,1,1901,64.7629,0.7725

epoch:1903/50, training loss:0.9458968043327332
Train Acc 0.7722
 Acc 0.7725
meta,dgl,1,1902,64.7966,0.7725

epoch:1904/50, training loss:0.9455937147140503
Train Acc 0.7722
 Acc 0.7729
new best val f1: 0.7728691666337377
meta,dgl,1,1903,64.8303,0.7729

epoch:1905/50, training loss:0.9452928304672241
Train Acc 0.7727
 Acc 0.7727
meta,dgl,1,1904,64.8640,0.7727

epoch:1906/50, training loss:0.9449832439422607
Train Acc 0.7724
 Acc 0.7729
new best val f1: 0.7729284387718813
meta,dgl,1,1905,64.8986,0.7729

epoch:1907/50, training loss:0.9446782469749451
Train Acc 0.7727
 Acc 0.7731
new best val f1: 0.7730535688412955
meta,dgl,1,1906,64.9331,0.7731

epoch:1908/50, training loss:0.944374680519104
Train Acc 0.7728
 Acc 0.7731
new best val f1: 0.7730733262206767
meta,dgl,1,1907,64.9667,0.7731

epoch:1909/50, training loss:0.944067120552063
Train Acc 0.7728
 Acc 0.7731
new best val f1: 0.7731457699450744
meta,dgl,1,1908,65.0005,0.7731

epoch:1910/50, training loss:0.9437669515609741
Train Acc 0.7730
 Acc 0.7735
new best val f1: 0.773494816980809
meta,dgl,1,1909,65.0342,0.7735

epoch:1911/50, training loss:0.9434627294540405
Train Acc 0.7733
 Acc 0.7733
meta,dgl,1,1910,65.0687,0.7733

epoch:1912/50, training loss:0.9431586265563965
Train Acc 0.7731
 Acc 0.7736
new best val f1: 0.7736199470502233
meta,dgl,1,1911,65.1031,0.7736

epoch:1913/50, training loss:0.9428561329841614
Train Acc 0.7734
 Acc 0.7736
meta,dgl,1,1912,65.1368,0.7736

epoch:1914/50, training loss:0.9425477981567383
Train Acc 0.7734
 Acc 0.7738
new best val f1: 0.7737714202921457
meta,dgl,1,1913,65.1705,0.7738

epoch:1915/50, training loss:0.9422462582588196
Train Acc 0.7736
 Acc 0.7741
new best val f1: 0.7740677809828638
meta,dgl,1,1914,65.2043,0.7741

epoch:1916/50, training loss:0.9419370889663696
Train Acc 0.7739
 Acc 0.7739
meta,dgl,1,1915,65.2379,0.7739

epoch:1917/50, training loss:0.9416396021842957
Train Acc 0.7737
 Acc 0.7742
new best val f1: 0.7741731536728969
meta,dgl,1,1916,65.2716,0.7742

epoch:1918/50, training loss:0.941331684589386
Train Acc 0.7740
 Acc 0.7741
meta,dgl,1,1917,65.3053,0.7741

epoch:1919/50, training loss:0.9410274028778076
Train Acc 0.7740
 Acc 0.7745
new best val f1: 0.7744826859498689
meta,dgl,1,1918,65.3390,0.7745

epoch:1920/50, training loss:0.9407255053520203
Train Acc 0.7743
 Acc 0.7744
meta,dgl,1,1919,65.3726,0.7744

epoch:1921/50, training loss:0.9404194355010986
Train Acc 0.7742
 Acc 0.7747
new best val f1: 0.7747066029161892
meta,dgl,1,1920,65.4063,0.7747

epoch:1922/50, training loss:0.940117597579956
Train Acc 0.7745
 Acc 0.7746
meta,dgl,1,1921,65.4409,0.7746

epoch:1923/50, training loss:0.9398111701011658
Train Acc 0.7744
 Acc 0.7750
new best val f1: 0.7749897920206531
meta,dgl,1,1922,65.4746,0.7750

epoch:1924/50, training loss:0.9395076036453247
Train Acc 0.7747
 Acc 0.7749
meta,dgl,1,1923,65.5083,0.7749

epoch:1925/50, training loss:0.9392054677009583
Train Acc 0.7747
 Acc 0.7748
meta,dgl,1,1924,65.5420,0.7748

epoch:1926/50, training loss:0.9389021992683411
Train Acc 0.7746
 Acc 0.7753
new best val f1: 0.7753256674701334
meta,dgl,1,1925,65.5756,0.7753

epoch:1927/50, training loss:0.9385982155799866
Train Acc 0.7752
 Acc 0.7752
meta,dgl,1,1926,65.6101,0.7752

epoch:1928/50, training loss:0.9382947683334351
Train Acc 0.7750
 Acc 0.7750
meta,dgl,1,1927,65.6446,0.7750

epoch:1929/50, training loss:0.9379962086677551
Train Acc 0.7748
 Acc 0.7759
new best val f1: 0.775885459885934
meta,dgl,1,1928,65.6783,0.7759

epoch:1930/50, training loss:0.9376925230026245
Train Acc 0.7757
 Acc 0.7754
meta,dgl,1,1929,65.7120,0.7754

epoch:1931/50, training loss:0.9373818039894104
Train Acc 0.7752
 Acc 0.7753
meta,dgl,1,1930,65.7458,0.7753

epoch:1932/50, training loss:0.9370852708816528
Train Acc 0.7751
 Acc 0.7764
new best val f1: 0.7764123233360993
meta,dgl,1,1931,65.7804,0.7764

epoch:1933/50, training loss:0.9367919564247131
Train Acc 0.7762
 Acc 0.7757
meta,dgl,1,1932,65.8141,0.7757

epoch:1934/50, training loss:0.9364768862724304
Train Acc 0.7754
 Acc 0.7758
meta,dgl,1,1933,65.8479,0.7758

epoch:1935/50, training loss:0.9361711144447327
Train Acc 0.7756
 Acc 0.7764
new best val f1: 0.7764320807154805
meta,dgl,1,1934,65.8824,0.7764

epoch:1936/50, training loss:0.9358721375465393
Train Acc 0.7763
 Acc 0.7759
meta,dgl,1,1935,65.9169,0.7759

epoch:1937/50, training loss:0.9355652332305908
Train Acc 0.7757
 Acc 0.7765
new best val f1: 0.77647818126737
meta,dgl,1,1936,65.9505,0.7765

epoch:1938/50, training loss:0.9352578520774841
Train Acc 0.7762
 Acc 0.7766
new best val f1: 0.776570382371149
meta,dgl,1,1937,65.9842,0.7766

epoch:1939/50, training loss:0.9349535703659058
Train Acc 0.7764
 Acc 0.7765
meta,dgl,1,1938,66.0186,0.7765

epoch:1940/50, training loss:0.9346514344215393
Train Acc 0.7763
 Acc 0.7769
new best val f1: 0.7769194294068835
meta,dgl,1,1939,66.0522,0.7769

epoch:1941/50, training loss:0.9343453645706177
Train Acc 0.7767
 Acc 0.7770
new best val f1: 0.7769721157519
meta,dgl,1,1940,66.0867,0.7770

epoch:1942/50, training loss:0.9340437650680542
Train Acc 0.7767
 Acc 0.7769
meta,dgl,1,1941,66.1204,0.7769

epoch:1943/50, training loss:0.9337418079376221
Train Acc 0.7767
 Acc 0.7773
new best val f1: 0.777268476442618
meta,dgl,1,1942,66.1541,0.7773

epoch:1944/50, training loss:0.9334393739700317
Train Acc 0.7770
 Acc 0.7772
meta,dgl,1,1943,66.1877,0.7772

epoch:1945/50, training loss:0.9331339597702026
Train Acc 0.7770
 Acc 0.7775
new best val f1: 0.7774594644433029
meta,dgl,1,1944,66.2213,0.7775

epoch:1946/50, training loss:0.9328321218490601
Train Acc 0.7772
 Acc 0.7774
meta,dgl,1,1945,66.2550,0.7774

epoch:1947/50, training loss:0.9325287938117981
Train Acc 0.7771
 Acc 0.7777
new best val f1: 0.7777294819615126
meta,dgl,1,1946,66.2888,0.7777

epoch:1948/50, training loss:0.9322254061698914
Train Acc 0.7774
 Acc 0.7777
meta,dgl,1,1947,66.3233,0.7777

epoch:1949/50, training loss:0.9319234490394592
Train Acc 0.7775
 Acc 0.7778
new best val f1: 0.7778282688584186
meta,dgl,1,1948,66.3570,0.7778

epoch:1950/50, training loss:0.931617021560669
Train Acc 0.7776
 Acc 0.7779
new best val f1: 0.7778611978240539
meta,dgl,1,1949,66.3906,0.7779

epoch:1951/50, training loss:0.9313178658485413
Train Acc 0.7776
 Acc 0.7783
new best val f1: 0.7782563454116779
meta,dgl,1,1950,66.4243,0.7783

epoch:1952/50, training loss:0.9310112595558167
Train Acc 0.7779
 Acc 0.7782
meta,dgl,1,1951,66.4589,0.7782

epoch:1953/50, training loss:0.9307088255882263
Train Acc 0.7779
 Acc 0.7782
meta,dgl,1,1952,66.4935,0.7782

epoch:1954/50, training loss:0.9304085969924927
Train Acc 0.7779
 Acc 0.7785
new best val f1: 0.7784934339642523
meta,dgl,1,1953,66.5282,0.7785

epoch:1955/50, training loss:0.9301043748855591
Train Acc 0.7782
 Acc 0.7786
new best val f1: 0.7785988066542854
meta,dgl,1,1954,66.5619,0.7786

epoch:1956/50, training loss:0.9298028945922852
Train Acc 0.7782
 Acc 0.7786
meta,dgl,1,1955,66.5955,0.7786

epoch:1957/50, training loss:0.9294993281364441
Train Acc 0.7783
 Acc 0.7788
new best val f1: 0.7788029662412245
meta,dgl,1,1956,66.6293,0.7788

epoch:1958/50, training loss:0.9291954636573792
Train Acc 0.7784
 Acc 0.7789
new best val f1: 0.7788819957587493
meta,dgl,1,1957,66.6630,0.7789

epoch:1959/50, training loss:0.9288955330848694
Train Acc 0.7786
 Acc 0.7789
meta,dgl,1,1958,66.6975,0.7789

epoch:1960/50, training loss:0.9285925626754761
Train Acc 0.7786
 Acc 0.7789
new best val f1: 0.7789149247243845
meta,dgl,1,1959,66.7311,0.7789

epoch:1961/50, training loss:0.9282905459403992
Train Acc 0.7786
 Acc 0.7792
new best val f1: 0.7791981138288484
meta,dgl,1,1960,66.7648,0.7792

epoch:1962/50, training loss:0.9279853105545044
Train Acc 0.7789
 Acc 0.7792
meta,dgl,1,1961,66.7984,0.7792

epoch:1963/50, training loss:0.9276856780052185
Train Acc 0.7789
 Acc 0.7792
meta,dgl,1,1962,66.8330,0.7792

epoch:1964/50, training loss:0.9273813366889954
Train Acc 0.7789
 Acc 0.7796
new best val f1: 0.7796130187958535
meta,dgl,1,1963,66.8668,0.7796

epoch:1965/50, training loss:0.927075982093811
Train Acc 0.7793
 Acc 0.7795
meta,dgl,1,1964,66.9006,0.7795

epoch:1966/50, training loss:0.9267755150794983
Train Acc 0.7793
 Acc 0.7796
meta,dgl,1,1965,66.9343,0.7796

epoch:1967/50, training loss:0.9264710545539856
Train Acc 0.7792
 Acc 0.7798
new best val f1: 0.7798171783827926
meta,dgl,1,1966,66.9681,0.7798

epoch:1968/50, training loss:0.9261696934700012
Train Acc 0.7796
 Acc 0.7799
new best val f1: 0.7798962079003174
meta,dgl,1,1967,67.0019,0.7799

epoch:1969/50, training loss:0.9258682727813721
Train Acc 0.7797
 Acc 0.7798
meta,dgl,1,1968,67.0357,0.7798

epoch:1970/50, training loss:0.925565242767334
Train Acc 0.7796
 Acc 0.7800
new best val f1: 0.7800147521766047
meta,dgl,1,1969,67.0704,0.7800

epoch:1971/50, training loss:0.9252628684043884
Train Acc 0.7798
 Acc 0.7804
new best val f1: 0.7803901423848474
meta,dgl,1,1970,67.1040,0.7804

epoch:1972/50, training loss:0.924962043762207
Train Acc 0.7802
 Acc 0.7802
meta,dgl,1,1971,67.1377,0.7802

epoch:1973/50, training loss:0.9246609807014465
Train Acc 0.7798
 Acc 0.7804
new best val f1: 0.7804362429367369
meta,dgl,1,1972,67.1713,0.7804

epoch:1974/50, training loss:0.9243553876876831
Train Acc 0.7801
 Acc 0.7808
new best val f1: 0.7808116331449797
meta,dgl,1,1973,67.2059,0.7808

epoch:1975/50, training loss:0.9240593910217285
Train Acc 0.7806
 Acc 0.7804
meta,dgl,1,1974,67.2395,0.7804

epoch:1976/50, training loss:0.9237634539604187
Train Acc 0.7800
 Acc 0.7810
new best val f1: 0.7809631063869021
meta,dgl,1,1975,67.2732,0.7810

epoch:1977/50, training loss:0.9234578609466553
Train Acc 0.7807
 Acc 0.7809
meta,dgl,1,1976,67.3068,0.7809

epoch:1978/50, training loss:0.9231507778167725
Train Acc 0.7807
 Acc 0.7808
meta,dgl,1,1977,67.3413,0.7808

epoch:1979/50, training loss:0.9228508472442627
Train Acc 0.7804
 Acc 0.7815
new best val f1: 0.7815097272164486
meta,dgl,1,1978,67.3755,0.7815

epoch:1980/50, training loss:0.9225624203681946
Train Acc 0.7812
 Acc 0.7811
meta,dgl,1,1979,67.4093,0.7811

epoch:1981/50, training loss:0.9222524166107178
Train Acc 0.7808
 Acc 0.7814
meta,dgl,1,1980,67.4430,0.7814

epoch:1982/50, training loss:0.9219416379928589
Train Acc 0.7812
 Acc 0.7817
new best val f1: 0.7816809578377524
meta,dgl,1,1981,67.4767,0.7817

epoch:1983/50, training loss:0.9216501116752625
Train Acc 0.7813
 Acc 0.7815
meta,dgl,1,1982,67.5113,0.7815

epoch:1984/50, training loss:0.9213463068008423
Train Acc 0.7811
 Acc 0.7819
new best val f1: 0.7819312179765809
meta,dgl,1,1983,67.5458,0.7819

epoch:1985/50, training loss:0.9210395812988281
Train Acc 0.7816
 Acc 0.7819
meta,dgl,1,1984,67.5795,0.7819

epoch:1986/50, training loss:0.9207422137260437
Train Acc 0.7816
 Acc 0.7816
meta,dgl,1,1985,67.6132,0.7816

epoch:1987/50, training loss:0.9204491376876831
Train Acc 0.7812
 Acc 0.7824
new best val f1: 0.7823527087367131
meta,dgl,1,1986,67.6468,0.7824

epoch:1988/50, training loss:0.9201449751853943
Train Acc 0.7821
 Acc 0.7821
meta,dgl,1,1987,67.6805,0.7821

epoch:1989/50, training loss:0.9198349714279175
Train Acc 0.7818
 Acc 0.7821
meta,dgl,1,1988,67.7141,0.7821

epoch:1990/50, training loss:0.9195376038551331
Train Acc 0.7816
 Acc 0.7828
new best val f1: 0.7827544421174643
meta,dgl,1,1989,67.7477,0.7828

epoch:1991/50, training loss:0.9192392826080322
Train Acc 0.7825
 Acc 0.7825
meta,dgl,1,1990,67.7822,0.7825

epoch:1992/50, training loss:0.9189348220825195
Train Acc 0.7821
 Acc 0.7825
meta,dgl,1,1991,67.8160,0.7825

epoch:1993/50, training loss:0.9186302423477173
Train Acc 0.7821
 Acc 0.7829
new best val f1: 0.782932258531895
meta,dgl,1,1992,67.8497,0.7829

epoch:1994/50, training loss:0.9183273911476135
Train Acc 0.7826
 Acc 0.7829
meta,dgl,1,1993,67.8835,0.7829

epoch:1995/50, training loss:0.918028712272644
Train Acc 0.7826
 Acc 0.7828
meta,dgl,1,1994,67.9172,0.7828

epoch:1996/50, training loss:0.9177271127700806
Train Acc 0.7825
 Acc 0.7831
new best val f1: 0.783136418118834
meta,dgl,1,1995,67.9509,0.7831

epoch:1997/50, training loss:0.91743004322052
Train Acc 0.7829
 Acc 0.7831
meta,dgl,1,1996,67.9854,0.7831

epoch:1998/50, training loss:0.9171293377876282
Train Acc 0.7829
 Acc 0.7832
new best val f1: 0.7832220334294859
meta,dgl,1,1997,68.0191,0.7832

epoch:1999/50, training loss:0.9168252348899841
Train Acc 0.7830
 Acc 0.7836
new best val f1: 0.783630352603364
meta,dgl,1,1998,68.0527,0.7836

epoch:2000/50, training loss:0.916532576084137
Train Acc 0.7833
 Acc 0.7833
meta,dgl,1,1999,68.0864,0.7833

training using time 227.91465306282043
Traceback (most recent call last):
  File "dgl/train_full_load.py", line 340, in <module>
    main(args)
  File "dgl/train_full_load.py", line 317, in main
    model, g, g.ndata['label'], test_mask, False)
TypeError: cannot unpack non-iterable float object
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2122072) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2122072 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     dgl/train_full_load.py FAILED     
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:16:04
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2122072)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

Namespace(csv='full_new.csv', dataset='arctic25', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
Inited proc group
Max label: tensor(32.)
----Data statistics------'
    #Nodes 500044
    #Edges 46177615
    #Classes/Labels (multi binary labels) 33
    #Train samples 166681
    #Val samples 0
    #Test samples 166682
Running on: 0
GCN(
  (layers): ModuleList(
    (0): GraphConv(in=64, out=128, normalization=both, activation=<function relu at 0x1491402e5f80>)
    (1): GraphConv(in=128, out=33, normalization=both, activation=None)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
Namespace(csv='full_new.csv', dataset='arctic25', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
epoch:1/50, training loss:36.12286376953125
Train Acc 0.0012
 Acc 0.0045
new best val f1: 0.004541814888226723
arctic25,dgl,1,0,0.3716,0.0045

epoch:2/50, training loss:29.29082489013672
Train Acc 0.0046
 Acc 0.0025
arctic25,dgl,1,1,0.4147,0.0025

epoch:3/50, training loss:23.881919860839844
Train Acc 0.0026
 Acc 0.0013
arctic25,dgl,1,2,0.4573,0.0013

epoch:4/50, training loss:19.083036422729492
Train Acc 0.0014
 Acc 0.0028
arctic25,dgl,1,3,0.5001,0.0028

epoch:5/50, training loss:14.713728904724121
Train Acc 0.0029
 Acc 0.0449
new best val f1: 0.04489964689358239
arctic25,dgl,1,4,0.5429,0.0449

epoch:6/50, training loss:11.992022514343262
Train Acc 0.0456
 Acc 0.0578
new best val f1: 0.05782938002914113
arctic25,dgl,1,5,0.5856,0.0578

epoch:7/50, training loss:10.712259292602539
Train Acc 0.0584
 Acc 0.0595
new best val f1: 0.05952927895406991
arctic25,dgl,1,6,0.6284,0.0595

epoch:8/50, training loss:9.849554061889648
Train Acc 0.0596
 Acc 0.0656
new best val f1: 0.0656200364920387
arctic25,dgl,1,7,0.6711,0.0656

epoch:9/50, training loss:8.780444145202637
Train Acc 0.0658
 Acc 0.1569
new best val f1: 0.15686325986794608
arctic25,dgl,1,8,0.7138,0.1569

epoch:10/50, training loss:8.37876033782959
Train Acc 0.1554
 Acc 0.2445
new best val f1: 0.2444900959556845
arctic25,dgl,1,9,0.7566,0.2445

epoch:11/50, training loss:7.843497276306152
Train Acc 0.2422
 Acc 0.2585
new best val f1: 0.25850933959911265
arctic25,dgl,1,10,0.7992,0.2585

epoch:12/50, training loss:7.189307689666748
Train Acc 0.2591
 Acc 0.2774
new best val f1: 0.27735918404851606
arctic25,dgl,1,11,0.8419,0.2774

epoch:13/50, training loss:6.81313943862915
Train Acc 0.2785
 Acc 0.2668
arctic25,dgl,1,12,0.8846,0.2668

epoch:14/50, training loss:6.516043186187744
Train Acc 0.2652
 Acc 0.2959
new best val f1: 0.29588742599860857
arctic25,dgl,1,13,0.9272,0.2959

epoch:15/50, training loss:6.157543659210205
Train Acc 0.2935
 Acc 0.3610
new best val f1: 0.3610283403998372
arctic25,dgl,1,14,0.9753,0.3610

epoch:16/50, training loss:5.6971330642700195
Train Acc 0.3580
 Acc 0.3956
new best val f1: 0.3955579475197228
arctic25,dgl,1,15,1.0181,0.3956

epoch:17/50, training loss:5.35676908493042
Train Acc 0.3937
 Acc 0.3887
arctic25,dgl,1,16,1.0608,0.3887

epoch:18/50, training loss:5.054515838623047
Train Acc 0.3884
 Acc 0.3889
arctic25,dgl,1,17,1.1034,0.3889

epoch:19/50, training loss:4.691064834594727
Train Acc 0.3904
 Acc 0.3894
arctic25,dgl,1,18,1.1460,0.3894

epoch:20/50, training loss:4.289119720458984
Train Acc 0.3903
 Acc 0.4149
new best val f1: 0.4148803507436237
arctic25,dgl,1,19,1.1886,0.4149

epoch:21/50, training loss:4.018261909484863
Train Acc 0.4149
 Acc 0.4695
new best val f1: 0.46951995904490623
arctic25,dgl,1,20,1.2312,0.4695

epoch:22/50, training loss:3.758556365966797
Train Acc 0.4697
 Acc 0.5326
new best val f1: 0.5326196820729578
arctic25,dgl,1,21,1.2739,0.5326

epoch:23/50, training loss:3.382573127746582
Train Acc 0.5332
 Acc 0.5448
new best val f1: 0.544768380567333
arctic25,dgl,1,22,1.3164,0.5448

epoch:24/50, training loss:3.0741426944732666
Train Acc 0.5444
 Acc 0.5371
arctic25,dgl,1,23,1.3590,0.5371

epoch:25/50, training loss:2.833404302597046
Train Acc 0.5377
 Acc 0.5176
arctic25,dgl,1,24,1.4016,0.5176

epoch:26/50, training loss:2.6387553215026855
Train Acc 0.5162
 Acc 0.4594
arctic25,dgl,1,25,1.4443,0.4594

epoch:27/50, training loss:2.594593048095703
Train Acc 0.4576
 Acc 0.4394
arctic25,dgl,1,26,1.4869,0.4394

epoch:28/50, training loss:2.5696589946746826
Train Acc 0.4378
 Acc 0.4879
arctic25,dgl,1,27,1.5295,0.4879

epoch:29/50, training loss:2.4095420837402344
Train Acc 0.4861
 Acc 0.5830
new best val f1: 0.5829734448221998
arctic25,dgl,1,28,1.5721,0.5830

epoch:30/50, training loss:2.2448618412017822
Train Acc 0.5837
 Acc 0.6593
new best val f1: 0.6592851235872461
arctic25,dgl,1,29,1.6147,0.6593

epoch:31/50, training loss:2.116481304168701
Train Acc 0.6571
 Acc 0.6907
new best val f1: 0.6907102820913351
arctic25,dgl,1,30,1.6574,0.6907

epoch:32/50, training loss:1.9882054328918457
Train Acc 0.6890
 Acc 0.6616
arctic25,dgl,1,31,1.7000,0.6616

epoch:33/50, training loss:1.923944354057312
Train Acc 0.6605
 Acc 0.6576
arctic25,dgl,1,32,1.7426,0.6576

epoch:34/50, training loss:1.8448224067687988
Train Acc 0.6558
 Acc 0.6806
arctic25,dgl,1,33,1.7852,0.6806

epoch:35/50, training loss:1.7549257278442383
Train Acc 0.6800
 Acc 0.6977
new best val f1: 0.6977264672293616
arctic25,dgl,1,34,1.8278,0.6977

epoch:36/50, training loss:1.6922262907028198
Train Acc 0.6967
 Acc 0.7048
new best val f1: 0.7047951588978879
arctic25,dgl,1,35,1.8704,0.7048

epoch:37/50, training loss:1.6702576875686646
Train Acc 0.7044
 Acc 0.7286
new best val f1: 0.7286265604284533
arctic25,dgl,1,36,1.9129,0.7286

epoch:38/50, training loss:1.622780203819275
Train Acc 0.7266
 Acc 0.7625
new best val f1: 0.7625457791312794
arctic25,dgl,1,37,1.9555,0.7625

epoch:39/50, training loss:1.5382031202316284
Train Acc 0.7623
 Acc 0.7791
new best val f1: 0.7790525196571324
arctic25,dgl,1,38,1.9981,0.7791

epoch:40/50, training loss:1.4707529544830322
Train Acc 0.7802
 Acc 0.7789
arctic25,dgl,1,39,2.0408,0.7789

epoch:41/50, training loss:1.4488211870193481
Train Acc 0.7795
 Acc 0.8027
new best val f1: 0.8026870216983237
arctic25,dgl,1,40,2.0833,0.8027

epoch:42/50, training loss:1.4069119691848755
Train Acc 0.8035
 Acc 0.8040
new best val f1: 0.8039603050629422
arctic25,dgl,1,41,2.1259,0.8040

epoch:43/50, training loss:1.3886243104934692
Train Acc 0.8041
 Acc 0.8144
new best val f1: 0.8144222312650136
arctic25,dgl,1,42,2.1684,0.8144

epoch:44/50, training loss:1.3517507314682007
Train Acc 0.8140
 Acc 0.8193
new best val f1: 0.8192659587036137
arctic25,dgl,1,43,2.2110,0.8193

epoch:45/50, training loss:1.3203195333480835
Train Acc 0.8193
 Acc 0.8317
new best val f1: 0.8317362596972998
arctic25,dgl,1,44,2.2536,0.8317

epoch:46/50, training loss:1.2849394083023071
Train Acc 0.8329
 Acc 0.8627
new best val f1: 0.8627085493758286
arctic25,dgl,1,45,2.2962,0.8627

epoch:47/50, training loss:1.2330715656280518
Train Acc 0.8637
 Acc 0.8798
new best val f1: 0.8797797351045537
arctic25,dgl,1,46,2.3387,0.8798

epoch:48/50, training loss:1.1912137269973755
Train Acc 0.8799
 Acc 0.8713
arctic25,dgl,1,47,2.3813,0.8713

epoch:49/50, training loss:1.1724532842636108
Train Acc 0.8718
 Acc 0.8893
new best val f1: 0.88927029049238
arctic25,dgl,1,48,2.4238,0.8893

epoch:50/50, training loss:1.134680986404419
Train Acc 0.8908
 Acc 0.8979
new best val f1: 0.8979076147595857
arctic25,dgl,1,49,2.4664,0.8979

epoch:51/50, training loss:1.1206694841384888
Train Acc 0.8993
 Acc 0.9078
new best val f1: 0.9078116590750974
arctic25,dgl,1,50,2.5089,0.9078

epoch:52/50, training loss:1.0943177938461304
Train Acc 0.9092
 Acc 0.9063
arctic25,dgl,1,51,2.5563,0.9063

epoch:53/50, training loss:1.0714715719223022
Train Acc 0.9083
 Acc 0.9096
new best val f1: 0.9096428243262756
arctic25,dgl,1,52,2.5990,0.9096

epoch:54/50, training loss:1.0496699810028076
Train Acc 0.9116
 Acc 0.9224
new best val f1: 0.9224150378703351
arctic25,dgl,1,53,2.6418,0.9224

epoch:55/50, training loss:1.0297077894210815
Train Acc 0.9248
 Acc 0.9341
new best val f1: 0.9340977409065252
arctic25,dgl,1,54,2.6845,0.9341

epoch:56/50, training loss:1.003279209136963
Train Acc 0.9357
 Acc 0.9294
arctic25,dgl,1,55,2.7271,0.9294

epoch:57/50, training loss:0.9890250563621521
Train Acc 0.9314
 Acc 0.9356
new best val f1: 0.9356204302910175
arctic25,dgl,1,56,2.7698,0.9356

epoch:58/50, training loss:0.9658063054084778
Train Acc 0.9382
 Acc 0.9441
new best val f1: 0.9440542917525367
arctic25,dgl,1,57,2.8125,0.9441

epoch:59/50, training loss:0.9471981525421143
Train Acc 0.9467
 Acc 0.9468
new best val f1: 0.9468437011853349
arctic25,dgl,1,58,2.8551,0.9468

epoch:60/50, training loss:0.9317660927772522
Train Acc 0.9494
 Acc 0.9498
new best val f1: 0.94975781362807
arctic25,dgl,1,59,2.8977,0.9498

epoch:61/50, training loss:0.911456823348999
Train Acc 0.9530
 Acc 0.9585
new best val f1: 0.9584804610073377
arctic25,dgl,1,60,2.9404,0.9585

epoch:62/50, training loss:0.8977643847465515
Train Acc 0.9614
 Acc 0.9607
new best val f1: 0.960666045339389
arctic25,dgl,1,61,2.9830,0.9607

epoch:63/50, training loss:0.8809865713119507
Train Acc 0.9641
 Acc 0.9630
new best val f1: 0.9629632060487523
arctic25,dgl,1,62,3.0256,0.9630

epoch:64/50, training loss:0.8611129522323608
Train Acc 0.9670
 Acc 0.9569
arctic25,dgl,1,63,3.0682,0.9569

epoch:65/50, training loss:0.8527857065200806
Train Acc 0.9603
 Acc 0.9723
new best val f1: 0.9722568619472047
arctic25,dgl,1,64,3.1109,0.9723

epoch:66/50, training loss:0.8328956365585327
Train Acc 0.9751
 Acc 0.9700
arctic25,dgl,1,65,3.1534,0.9700

epoch:67/50, training loss:0.8251840472221375
Train Acc 0.9728
 Acc 0.9771
new best val f1: 0.9771005893858049
arctic25,dgl,1,66,3.1961,0.9771

epoch:68/50, training loss:0.8062425255775452
Train Acc 0.9806
 Acc 0.9706
arctic25,dgl,1,67,3.2387,0.9706

epoch:69/50, training loss:0.7988463044166565
Train Acc 0.9733
 Acc 0.9782
new best val f1: 0.9782294797915491
arctic25,dgl,1,68,3.2813,0.9782

epoch:70/50, training loss:0.7826173901557922
Train Acc 0.9817
 Acc 0.9811
new best val f1: 0.9810779590711595
arctic25,dgl,1,69,3.3238,0.9811

epoch:71/50, training loss:0.7743875980377197
Train Acc 0.9848
 Acc 0.9838
new best val f1: 0.9838411152387078
arctic25,dgl,1,70,3.3663,0.9838

epoch:72/50, training loss:0.7605779767036438
Train Acc 0.9873
 Acc 0.9825
arctic25,dgl,1,71,3.4089,0.9825

epoch:73/50, training loss:0.7519298791885376
Train Acc 0.9857
 Acc 0.9864
new best val f1: 0.9863614287026949
arctic25,dgl,1,72,3.4515,0.9864

epoch:74/50, training loss:0.7393112778663635
Train Acc 0.9903
 Acc 0.9895
new best val f1: 0.9894855672674289
arctic25,dgl,1,73,3.4940,0.9895

epoch:75/50, training loss:0.7310847043991089
Train Acc 0.9931
 Acc 0.9930
new best val f1: 0.9930100681272234
arctic25,dgl,1,74,3.5367,0.9930

epoch:76/50, training loss:0.7198510766029358
Train Acc 0.9961
 Acc 0.9909
arctic25,dgl,1,75,3.5793,0.9909

epoch:77/50, training loss:0.7116490006446838
Train Acc 0.9943
 Acc 0.9952
new best val f1: 0.995215342408212
arctic25,dgl,1,76,3.6219,0.9952

epoch:78/50, training loss:0.7007456421852112
Train Acc 0.9987
 Acc 0.9972
new best val f1: 0.9971777739856394
arctic25,dgl,1,77,3.6645,0.9972

epoch:79/50, training loss:0.6931017637252808
Train Acc 1.0010
 Acc 0.9986
new best val f1: 0.998634830207007
arctic25,dgl,1,78,3.7070,0.9986

epoch:80/50, training loss:0.6840450763702393
Train Acc 1.0022
 Acc 0.9991
new best val f1: 0.999127078930442
arctic25,dgl,1,79,3.7496,0.9991

epoch:81/50, training loss:0.6774852871894836
Train Acc 1.0021
 Acc 0.9973
arctic25,dgl,1,80,3.7922,0.9973

epoch:82/50, training loss:0.6700549125671387
Train Acc 1.0010
 Acc 1.0025
new best val f1: 1.002546566729237
arctic25,dgl,1,81,3.8348,1.0025

epoch:83/50, training loss:0.6624515056610107
Train Acc 1.0055
 Acc 1.0028
new best val f1: 1.0028090993817356
arctic25,dgl,1,82,3.8774,1.0028

epoch:84/50, training loss:0.6540693044662476
Train Acc 1.0062
 Acc 1.0020
arctic25,dgl,1,83,3.9199,1.0020

epoch:85/50, training loss:0.6476690769195557
Train Acc 1.0055
 Acc 1.0054
new best val f1: 1.0054212992740972
arctic25,dgl,1,84,3.9625,1.0054

epoch:86/50, training loss:0.6411147713661194
Train Acc 1.0089
 Acc 1.0061
new best val f1: 1.0061235741195311
arctic25,dgl,1,85,4.0051,1.0061

epoch:87/50, training loss:0.6342082023620605
Train Acc 1.0100
 Acc 1.0071
new best val f1: 1.0070818183011512
arctic25,dgl,1,86,4.0476,1.0071

epoch:88/50, training loss:0.6276177167892456
Train Acc 1.0107
 Acc 1.0085
new best val f1: 1.0085191845735813
arctic25,dgl,1,87,4.0902,1.0085

epoch:89/50, training loss:0.621944785118103
Train Acc 1.0115
 Acc 1.0086
new best val f1: 1.0085913810530185
arctic25,dgl,1,88,4.1327,1.0086

epoch:90/50, training loss:0.6159443259239197
Train Acc 1.0126
 Acc 1.0124
new best val f1: 1.0124374844121238
arctic25,dgl,1,89,4.1753,1.0124

epoch:91/50, training loss:0.6098214983940125
Train Acc 1.0161
 Acc 1.0133
new best val f1: 1.0133497853795566
arctic25,dgl,1,90,4.2179,1.0133

epoch:92/50, training loss:0.604579746723175
Train Acc 1.0167
 Acc 1.0108
arctic25,dgl,1,91,4.2605,1.0108

epoch:93/50, training loss:0.599668562412262
Train Acc 1.0148
 Acc 1.0144
new best val f1: 1.0143933526732387
arctic25,dgl,1,92,4.3031,1.0144

epoch:94/50, training loss:0.5939716100692749
Train Acc 1.0179
 Acc 1.0161
new best val f1: 1.0160604350166051
arctic25,dgl,1,93,4.3456,1.0161

epoch:95/50, training loss:0.5886696577072144
Train Acc 1.0197
 Acc 1.0145
arctic25,dgl,1,94,4.3881,1.0145

epoch:96/50, training loss:0.5838119387626648
Train Acc 1.0183
 Acc 1.0167
new best val f1: 1.0167495832294142
arctic25,dgl,1,95,4.4307,1.0167

epoch:97/50, training loss:0.5787428617477417
Train Acc 1.0199
 Acc 1.0170
new best val f1: 1.0169792993003506
arctic25,dgl,1,96,4.4732,1.0170

epoch:98/50, training loss:0.5735741853713989
Train Acc 1.0208
 Acc 1.0155
arctic25,dgl,1,97,4.5157,1.0155

epoch:99/50, training loss:0.5694214701652527
Train Acc 1.0192
 Acc 1.0192
new best val f1: 1.0192042635302765
arctic25,dgl,1,98,4.5583,1.0192

epoch:100/50, training loss:0.565192699432373
Train Acc 1.0221
 Acc 1.0191
arctic25,dgl,1,99,4.6009,1.0191

epoch:101/50, training loss:0.559837281703949
Train Acc 1.0227
 Acc 1.0183
arctic25,dgl,1,100,4.6435,1.0183

epoch:102/50, training loss:0.5560857653617859
Train Acc 1.0220
 Acc 1.0209
new best val f1: 1.0209107257715178
arctic25,dgl,1,101,4.6860,1.0209

epoch:103/50, training loss:0.5519386529922485
Train Acc 1.0243
 Acc 1.0202
arctic25,dgl,1,102,4.7286,1.0202

epoch:104/50, training loss:0.547347903251648
Train Acc 1.0238
 Acc 1.0215
new best val f1: 1.0215473674538271
arctic25,dgl,1,103,4.7711,1.0215

epoch:105/50, training loss:0.5435230135917664
Train Acc 1.0251
 Acc 1.0219
new best val f1: 1.0218886599020753
arctic25,dgl,1,104,4.8137,1.0219

epoch:106/50, training loss:0.5393082499504089
Train Acc 1.0252
 Acc 1.0217
arctic25,dgl,1,105,4.8562,1.0217

epoch:107/50, training loss:0.5354087352752686
Train Acc 1.0255
 Acc 1.0244
new best val f1: 1.0244220999986873
arctic25,dgl,1,106,4.8987,1.0244

epoch:108/50, training loss:0.5313644409179688
Train Acc 1.0281
 Acc 1.0236
arctic25,dgl,1,107,4.9412,1.0236

epoch:109/50, training loss:0.5275912284851074
Train Acc 1.0271
 Acc 1.0244
new best val f1: 1.0244286633149997
arctic25,dgl,1,108,4.9837,1.0244

epoch:110/50, training loss:0.5234900116920471
Train Acc 1.0282
 Acc 1.0257
new best val f1: 1.025656003465431
arctic25,dgl,1,109,5.0262,1.0257

epoch:111/50, training loss:0.5201740264892578
Train Acc 1.0291
 Acc 1.0245
arctic25,dgl,1,110,5.0687,1.0245

epoch:112/50, training loss:0.5167433023452759
Train Acc 1.0281
 Acc 1.0267
new best val f1: 1.0266733174938634
arctic25,dgl,1,111,5.1112,1.0267

epoch:113/50, training loss:0.513233482837677
Train Acc 1.0303
 Acc 1.0268
new best val f1: 1.0267717672385503
arctic25,dgl,1,112,5.1537,1.0268

epoch:114/50, training loss:0.5097872018814087
Train Acc 1.0305
 Acc 1.0270
new best val f1: 1.0269555400952994
arctic25,dgl,1,113,5.1963,1.0270

epoch:115/50, training loss:0.5066143870353699
Train Acc 1.0307
 Acc 1.0285
new best val f1: 1.0284913561124165
arctic25,dgl,1,114,5.2388,1.0285

epoch:116/50, training loss:0.5035062432289124
Train Acc 1.0322
 Acc 1.0281
arctic25,dgl,1,115,5.2813,1.0281

epoch:117/50, training loss:0.5001791715621948
Train Acc 1.0314
 Acc 1.0288
new best val f1: 1.0288129586117274
arctic25,dgl,1,116,5.3238,1.0288

epoch:118/50, training loss:0.4970645308494568
Train Acc 1.0321
 Acc 1.0292
new best val f1: 1.0292395741720377
arctic25,dgl,1,117,5.3663,1.0292

epoch:119/50, training loss:0.4938894212245941
Train Acc 1.0328
 Acc 1.0290
arctic25,dgl,1,118,5.4088,1.0290

epoch:120/50, training loss:0.4910047650337219
Train Acc 1.0324
 Acc 1.0305
new best val f1: 1.0304866042714063
arctic25,dgl,1,119,5.4512,1.0305

epoch:121/50, training loss:0.4880536198616028
Train Acc 1.0339
 Acc 1.0300
arctic25,dgl,1,120,5.4937,1.0300

epoch:122/50, training loss:0.48508644104003906
Train Acc 1.0336
 Acc 1.0306
new best val f1: 1.0305522374345308
arctic25,dgl,1,121,5.5362,1.0306

epoch:123/50, training loss:0.48222047090530396
Train Acc 1.0340
 Acc 1.0311
new best val f1: 1.031142935902653
arctic25,dgl,1,122,5.5788,1.0311

epoch:124/50, training loss:0.47957611083984375
Train Acc 1.0347
 Acc 1.0305
arctic25,dgl,1,123,5.6213,1.0305

epoch:125/50, training loss:0.4767811894416809
Train Acc 1.0341
 Acc 1.0320
new best val f1: 1.0319633504417112
arctic25,dgl,1,124,5.6639,1.0320

epoch:126/50, training loss:0.4741823375225067
Train Acc 1.0355
 Acc 1.0315
arctic25,dgl,1,125,5.7064,1.0315

epoch:127/50, training loss:0.47141382098197937
Train Acc 1.0348
 Acc 1.0326
new best val f1: 1.0326131187566454
arctic25,dgl,1,126,5.7489,1.0326

epoch:128/50, training loss:0.4688445031642914
Train Acc 1.0363
 Acc 1.0320
arctic25,dgl,1,127,5.7915,1.0320

epoch:129/50, training loss:0.4662144184112549
Train Acc 1.0353
 Acc 1.0335
new best val f1: 1.033466349877266
arctic25,dgl,1,128,5.8340,1.0335

epoch:130/50, training loss:0.46378257870674133
Train Acc 1.0368
 Acc 1.0323
arctic25,dgl,1,129,5.8765,1.0323

epoch:131/50, training loss:0.4612484872341156
Train Acc 1.0354
 Acc 1.0338
new best val f1: 1.0337945156928894
arctic25,dgl,1,130,5.9190,1.0338

epoch:132/50, training loss:0.45909973978996277
Train Acc 1.0373
 Acc 1.0327
arctic25,dgl,1,131,5.9615,1.0327

epoch:133/50, training loss:0.456355482339859
Train Acc 1.0358
 Acc 1.0346
new best val f1: 1.0346280568645725
arctic25,dgl,1,132,6.0040,1.0346

epoch:134/50, training loss:0.45396605134010315
Train Acc 1.0379
 Acc 1.0337
arctic25,dgl,1,133,6.0465,1.0337

epoch:135/50, training loss:0.45140427350997925
Train Acc 1.0368
 Acc 1.0349
new best val f1: 1.0348577729355088
arctic25,dgl,1,134,6.0891,1.0349

epoch:136/50, training loss:0.4490133821964264
Train Acc 1.0382
 Acc 1.0348
arctic25,dgl,1,135,6.1316,1.0348

epoch:137/50, training loss:0.4466293454170227
Train Acc 1.0380
 Acc 1.0352
new best val f1: 1.0351793754348197
arctic25,dgl,1,136,6.1741,1.0352

epoch:138/50, training loss:0.44440948963165283
Train Acc 1.0383
 Acc 1.0356
new best val f1: 1.03557973772988
arctic25,dgl,1,137,6.2166,1.0356

epoch:139/50, training loss:0.44224342703819275
Train Acc 1.0389
 Acc 1.0349
arctic25,dgl,1,138,6.2591,1.0349

epoch:140/50, training loss:0.4401685893535614
Train Acc 1.0381
 Acc 1.0365
new best val f1: 1.0364723487483756
arctic25,dgl,1,139,6.3016,1.0365

epoch:141/50, training loss:0.4380766451358795
Train Acc 1.0397
 Acc 1.0355
arctic25,dgl,1,140,6.3442,1.0355

epoch:142/50, training loss:0.43563902378082275
Train Acc 1.0384
 Acc 1.0374
new best val f1: 1.0373977763484334
arctic25,dgl,1,141,6.3867,1.0374

epoch:143/50, training loss:0.43355926871299744
Train Acc 1.0404
 Acc 1.0355
arctic25,dgl,1,142,6.4292,1.0355

epoch:144/50, training loss:0.4312276244163513
Train Acc 1.0382
 Acc 1.0372
arctic25,dgl,1,143,6.4717,1.0372

epoch:145/50, training loss:0.42947185039520264
Train Acc 1.0402
 Acc 1.0361
arctic25,dgl,1,144,6.5142,1.0361

epoch:146/50, training loss:0.427097886800766
Train Acc 1.0387
 Acc 1.0378
new best val f1: 1.0378243919087438
arctic25,dgl,1,145,6.5567,1.0378

epoch:147/50, training loss:0.42498812079429626
Train Acc 1.0405
 Acc 1.0373
arctic25,dgl,1,146,6.5992,1.0373

epoch:148/50, training loss:0.42259183526039124
Train Acc 1.0399
 Acc 1.0377
arctic25,dgl,1,147,6.6417,1.0377

epoch:149/50, training loss:0.42051053047180176
Train Acc 1.0402
 Acc 1.0382
new best val f1: 1.0382247542038041
arctic25,dgl,1,148,6.6842,1.0382

epoch:150/50, training loss:0.41881147027015686
Train Acc 1.0412
 Acc 1.0376
arctic25,dgl,1,149,6.7267,1.0376

epoch:151/50, training loss:0.4170014560222626
Train Acc 1.0402
 Acc 1.0392
new best val f1: 1.0392092516506741
arctic25,dgl,1,150,6.7692,1.0392

epoch:152/50, training loss:0.4155205190181732
Train Acc 1.0422
 Acc 1.0380
arctic25,dgl,1,151,6.8118,1.0380

epoch:153/50, training loss:0.413176953792572
Train Acc 1.0407
 Acc 1.0393
new best val f1: 1.0393339546606108
arctic25,dgl,1,152,6.8543,1.0393

epoch:154/50, training loss:0.41099438071250916
Train Acc 1.0421
 Acc 1.0392
arctic25,dgl,1,153,6.8969,1.0392

epoch:155/50, training loss:0.4090537130832672
Train Acc 1.0420
 Acc 1.0392
arctic25,dgl,1,154,6.9395,1.0392

epoch:156/50, training loss:0.4072414040565491
Train Acc 1.0419
 Acc 1.0397
new best val f1: 1.039714627006734
arctic25,dgl,1,155,6.9821,1.0397

epoch:157/50, training loss:0.4053952097892761
Train Acc 1.0423
 Acc 1.0394
arctic25,dgl,1,156,7.0246,1.0394

epoch:158/50, training loss:0.4037792980670929
Train Acc 1.0418
 Acc 1.0396
arctic25,dgl,1,157,7.0672,1.0396

epoch:159/50, training loss:0.4023195505142212
Train Acc 1.0424
 Acc 1.0392
arctic25,dgl,1,158,7.1098,1.0392

epoch:160/50, training loss:0.4006805121898651
Train Acc 1.0417
 Acc 1.0407
new best val f1: 1.040685997820979
arctic25,dgl,1,159,7.1524,1.0407

epoch:161/50, training loss:0.3995983898639679
Train Acc 1.0435
 Acc 1.0394
arctic25,dgl,1,160,7.1949,1.0394

epoch:162/50, training loss:0.3973865509033203
Train Acc 1.0418
 Acc 1.0407
new best val f1: 1.0407253777188539
arctic25,dgl,1,161,7.2374,1.0407

epoch:163/50, training loss:0.3956466019153595
Train Acc 1.0436
 Acc 1.0406
arctic25,dgl,1,162,7.2800,1.0406

epoch:164/50, training loss:0.3936147689819336
Train Acc 1.0432
 Acc 1.0411
new best val f1: 1.0410929234323518
arctic25,dgl,1,163,7.3226,1.0411

epoch:165/50, training loss:0.39188212156295776
Train Acc 1.0436
 Acc 1.0415
new best val f1: 1.0414735957784749
arctic25,dgl,1,164,7.3651,1.0415

epoch:166/50, training loss:0.39029404520988464
Train Acc 1.0441
 Acc 1.0407
arctic25,dgl,1,165,7.4077,1.0407

epoch:167/50, training loss:0.388925164937973
Train Acc 1.0434
 Acc 1.0415
new best val f1: 1.0414932857274124
arctic25,dgl,1,166,7.4502,1.0415

epoch:168/50, training loss:0.3879252076148987
Train Acc 1.0444
 Acc 1.0407
arctic25,dgl,1,167,7.4928,1.0407

epoch:169/50, training loss:0.3861628770828247
Train Acc 1.0432
 Acc 1.0422
new best val f1: 1.0421693073075964
arctic25,dgl,1,168,7.5354,1.0422

epoch:170/50, training loss:0.38490161299705505
Train Acc 1.0449
 Acc 1.0410
arctic25,dgl,1,169,7.5779,1.0410

epoch:171/50, training loss:0.3827650547027588
Train Acc 1.0435
 Acc 1.0421
arctic25,dgl,1,170,7.6205,1.0421

epoch:172/50, training loss:0.38102999329566956
Train Acc 1.0446
 Acc 1.0421
arctic25,dgl,1,171,7.6630,1.0421

epoch:173/50, training loss:0.3794649839401245
Train Acc 1.0446
 Acc 1.0423
new best val f1: 1.042346516848033
arctic25,dgl,1,172,7.7056,1.0423

epoch:174/50, training loss:0.3779603838920593
Train Acc 1.0449
 Acc 1.0427
new best val f1: 1.042668119347344
arctic25,dgl,1,173,7.7481,1.0427

epoch:175/50, training loss:0.3765190839767456
Train Acc 1.0454
 Acc 1.0424
arctic25,dgl,1,174,7.7906,1.0424

epoch:176/50, training loss:0.3751489818096161
Train Acc 1.0449
 Acc 1.0428
new best val f1: 1.0428059489899055
arctic25,dgl,1,175,7.8332,1.0428

epoch:177/50, training loss:0.3739025592803955
Train Acc 1.0456
 Acc 1.0421
arctic25,dgl,1,176,7.8757,1.0421

epoch:178/50, training loss:0.3726266622543335
Train Acc 1.0448
 Acc 1.0432
new best val f1: 1.0432325645502158
arctic25,dgl,1,177,7.9183,1.0432

epoch:179/50, training loss:0.3719147741794586
Train Acc 1.0461
 Acc 1.0418
arctic25,dgl,1,178,7.9608,1.0418

epoch:180/50, training loss:0.3703562915325165
Train Acc 1.0443
 Acc 1.0435
new best val f1: 1.043501660519027
arctic25,dgl,1,179,8.0034,1.0435

epoch:181/50, training loss:0.36988669633865356
Train Acc 1.0464
 Acc 1.0424
arctic25,dgl,1,180,8.0460,1.0424

epoch:182/50, training loss:0.3671809732913971
Train Acc 1.0449
 Acc 1.0435
new best val f1: 1.0435082238353395
arctic25,dgl,1,181,8.0885,1.0435

epoch:183/50, training loss:0.3654159605503082
Train Acc 1.0464
 Acc 1.0436
new best val f1: 1.0436198002126515
arctic25,dgl,1,182,8.1311,1.0436

epoch:184/50, training loss:0.36409157514572144
Train Acc 1.0465
 Acc 1.0429
arctic25,dgl,1,183,8.1736,1.0429

epoch:185/50, training loss:0.3631702661514282
Train Acc 1.0455
 Acc 1.0440
new best val f1: 1.0440398524566492
arctic25,dgl,1,184,8.2162,1.0440

epoch:186/50, training loss:0.3632091283798218
Train Acc 1.0471
 Acc 1.0424
arctic25,dgl,1,185,8.2588,1.0424

epoch:187/50, training loss:0.361294686794281
Train Acc 1.0449
 Acc 1.0443
new best val f1: 1.0443155117417728
arctic25,dgl,1,186,8.3013,1.0443

epoch:188/50, training loss:0.3608974814414978
Train Acc 1.0471
 Acc 1.0438
arctic25,dgl,1,187,8.3438,1.0438

epoch:189/50, training loss:0.3580333888530731
Train Acc 1.0464
 Acc 1.0442
arctic25,dgl,1,188,8.3864,1.0442

epoch:190/50, training loss:0.3563898801803589
Train Acc 1.0471
 Acc 1.0447
new best val f1: 1.0446699308226461
arctic25,dgl,1,189,8.4290,1.0447

epoch:191/50, training loss:0.3553631603717804
Train Acc 1.0475
 Acc 1.0432
arctic25,dgl,1,190,8.4715,1.0432

epoch:192/50, training loss:0.35478147864341736
Train Acc 1.0459
 Acc 1.0448
new best val f1: 1.0447618172510207
arctic25,dgl,1,191,8.5140,1.0448

epoch:193/50, training loss:0.3556203544139862
Train Acc 1.0476
 Acc 1.0442
arctic25,dgl,1,192,8.5566,1.0442

epoch:194/50, training loss:0.35203611850738525
Train Acc 1.0470
 Acc 1.0447
arctic25,dgl,1,193,8.5991,1.0447

epoch:195/50, training loss:0.350397527217865
Train Acc 1.0475
 Acc 1.0451
new best val f1: 1.0451227996482062
arctic25,dgl,1,194,8.6417,1.0451

epoch:196/50, training loss:0.34978780150413513
Train Acc 1.0479
 Acc 1.0431
arctic25,dgl,1,195,8.6842,1.0431

epoch:197/50, training loss:0.34944626688957214
Train Acc 1.0461
 Acc 1.0452
new best val f1: 1.0452081227602683
arctic25,dgl,1,196,8.7268,1.0452

epoch:198/50, training loss:0.35158011317253113
Train Acc 1.0481
 Acc 1.0455
new best val f1: 1.0455362885758916
arctic25,dgl,1,197,8.7693,1.0455

epoch:199/50, training loss:0.3458869755268097
Train Acc 1.0485
 Acc 1.0392
arctic25,dgl,1,198,8.8119,1.0392

epoch:200/50, training loss:0.3504945933818817
Train Acc 1.0423
 Acc 1.0436
arctic25,dgl,1,199,8.8545,1.0436

epoch:201/50, training loss:0.36951008439064026
Train Acc 1.0467
 Acc 1.0455
arctic25,dgl,1,200,8.8972,1.0455

epoch:202/50, training loss:0.3648541271686554
Train Acc 1.0487
 Acc 1.0430
arctic25,dgl,1,201,8.9397,1.0430

epoch:203/50, training loss:0.3476298749446869
Train Acc 1.0457
 Acc 1.0322
arctic25,dgl,1,202,8.9823,1.0322

epoch:204/50, training loss:0.35501378774642944
Train Acc 1.0361
 Acc 1.0418
arctic25,dgl,1,203,9.0249,1.0418

epoch:205/50, training loss:0.36150220036506653
Train Acc 1.0446
 Acc 1.0446
arctic25,dgl,1,204,9.0674,1.0446

epoch:206/50, training loss:0.35451045632362366
Train Acc 1.0476
 Acc 1.0416
arctic25,dgl,1,205,9.1100,1.0416

epoch:207/50, training loss:0.3459531366825104
Train Acc 1.0446
 Acc 1.0458
new best val f1: 1.045838201126265
arctic25,dgl,1,206,9.1525,1.0458

epoch:208/50, training loss:0.33890753984451294
Train Acc 1.0488
 Acc 1.0449
arctic25,dgl,1,207,9.1950,1.0449

epoch:209/50, training loss:0.3409319818019867
Train Acc 1.0478
 Acc 1.0435
arctic25,dgl,1,208,9.2375,1.0435

epoch:210/50, training loss:0.3374268114566803
Train Acc 1.0465
 Acc 1.0398
arctic25,dgl,1,209,9.2805,1.0398

epoch:211/50, training loss:0.33997419476509094
Train Acc 1.0433
 Acc 1.0466
new best val f1: 1.046625799083761
arctic25,dgl,1,210,9.3231,1.0466

epoch:212/50, training loss:0.352262020111084
Train Acc 1.0500
 Acc 1.0464
arctic25,dgl,1,211,9.3656,1.0464

epoch:213/50, training loss:0.3532443344593048
Train Acc 1.0492
 Acc 1.0457
arctic25,dgl,1,212,9.4081,1.0457

epoch:214/50, training loss:0.33426234126091003
Train Acc 1.0485
 Acc 1.0111
arctic25,dgl,1,213,9.4507,1.0111

epoch:215/50, training loss:0.3823697566986084
Train Acc 1.0150
 Acc 1.0426
arctic25,dgl,1,214,9.4932,1.0426

epoch:216/50, training loss:0.33661001920700073
Train Acc 1.0452
 Acc 1.0425
arctic25,dgl,1,215,9.5357,1.0425

epoch:217/50, training loss:0.35253211855888367
Train Acc 1.0455
 Acc 1.0453
arctic25,dgl,1,216,9.5782,1.0453

epoch:218/50, training loss:0.3500102162361145
Train Acc 1.0482
 Acc 1.0455
arctic25,dgl,1,217,9.6208,1.0455

epoch:219/50, training loss:0.3340566158294678
Train Acc 1.0486
 Acc 1.0174
arctic25,dgl,1,218,9.6634,1.0174

epoch:220/50, training loss:0.370602011680603
Train Acc 1.0211
 Acc 1.0464
arctic25,dgl,1,219,9.7060,1.0464

epoch:221/50, training loss:0.3311006724834442
Train Acc 1.0490
 Acc 1.0420
arctic25,dgl,1,220,9.7485,1.0420

epoch:222/50, training loss:0.3410676121711731
Train Acc 1.0447
 Acc 1.0433
arctic25,dgl,1,221,9.7911,1.0433

epoch:223/50, training loss:0.3353058099746704
Train Acc 1.0462
 Acc 1.0319
arctic25,dgl,1,222,9.8337,1.0319

epoch:224/50, training loss:0.34242162108421326
Train Acc 1.0357
 Acc 1.0456
arctic25,dgl,1,223,9.8762,1.0456

epoch:225/50, training loss:0.338214248418808
Train Acc 1.0488
 Acc 1.0469
new best val f1: 1.0469080216851971
arctic25,dgl,1,224,9.9189,1.0469

epoch:226/50, training loss:0.32752734422683716
Train Acc 1.0503
 Acc 1.0403
arctic25,dgl,1,225,9.9614,1.0403

epoch:227/50, training loss:0.33431383967399597
Train Acc 1.0430
 Acc 1.0432
arctic25,dgl,1,226,10.0039,1.0432

epoch:228/50, training loss:0.3255649209022522
Train Acc 1.0457
 Acc 1.0452
arctic25,dgl,1,227,10.0464,1.0452

epoch:229/50, training loss:0.33348777890205383
Train Acc 1.0484
 Acc 1.0489
new best val f1: 1.0489426497420618
arctic25,dgl,1,228,10.0889,1.0489

epoch:230/50, training loss:0.3204541802406311
Train Acc 1.0519
 Acc 1.0396
arctic25,dgl,1,229,10.1315,1.0396

epoch:231/50, training loss:0.33149802684783936
Train Acc 1.0425
 Acc 1.0466
arctic25,dgl,1,230,10.1741,1.0466

epoch:232/50, training loss:0.3304654657840729
Train Acc 1.0497
 Acc 1.0465
arctic25,dgl,1,231,10.2166,1.0465

epoch:233/50, training loss:0.33024507761001587
Train Acc 1.0497
 Acc 1.0450
arctic25,dgl,1,232,10.2590,1.0450

epoch:234/50, training loss:0.3205721974372864
Train Acc 1.0479
 Acc 1.0438
arctic25,dgl,1,233,10.3016,1.0438

epoch:235/50, training loss:0.3207588195800781
Train Acc 1.0465
 Acc 1.0478
arctic25,dgl,1,234,10.3441,1.0478

epoch:236/50, training loss:0.3294411599636078
Train Acc 1.0507
 Acc 1.0482
arctic25,dgl,1,235,10.3866,1.0482

epoch:237/50, training loss:0.33016321063041687
Train Acc 1.0514
 Acc 1.0479
arctic25,dgl,1,236,10.4292,1.0479

epoch:238/50, training loss:0.315420001745224
Train Acc 1.0511
 Acc 1.0175
arctic25,dgl,1,237,10.4718,1.0175

epoch:239/50, training loss:0.35534828901290894
Train Acc 1.0213
 Acc 1.0459
arctic25,dgl,1,238,10.5144,1.0459

epoch:240/50, training loss:0.3148142397403717
Train Acc 1.0488
 Acc 1.0466
arctic25,dgl,1,239,10.5569,1.0466

epoch:241/50, training loss:0.33028146624565125
Train Acc 1.0495
 Acc 1.0473
arctic25,dgl,1,240,10.5994,1.0473

epoch:242/50, training loss:0.33099159598350525
Train Acc 1.0503
 Acc 1.0491
new best val f1: 1.049073916068311
arctic25,dgl,1,241,10.6420,1.0491

epoch:243/50, training loss:0.3129514157772064
Train Acc 1.0519
 Acc 1.0130
arctic25,dgl,1,242,10.6846,1.0130

epoch:244/50, training loss:0.36557072401046753
Train Acc 1.0170
 Acc 1.0371
arctic25,dgl,1,243,10.7271,1.0371

epoch:245/50, training loss:0.32607710361480713
Train Acc 1.0405
 Acc 1.0467
arctic25,dgl,1,244,10.7697,1.0467

epoch:246/50, training loss:0.3351111114025116
Train Acc 1.0499
 Acc 1.0449
arctic25,dgl,1,245,10.8123,1.0449

epoch:247/50, training loss:0.34781745076179504
Train Acc 1.0478
 Acc 1.0463
arctic25,dgl,1,246,10.8549,1.0463

epoch:248/50, training loss:0.33351844549179077
Train Acc 1.0489
 Acc 1.0434
arctic25,dgl,1,247,10.8974,1.0434

epoch:249/50, training loss:0.31569206714630127
Train Acc 1.0461
 Acc 1.0446
arctic25,dgl,1,248,10.9400,1.0446

epoch:250/50, training loss:0.31837016344070435
Train Acc 1.0474
 Acc 1.0482
arctic25,dgl,1,249,10.9826,1.0482

epoch:251/50, training loss:0.3189818263053894
Train Acc 1.0512
 Acc 1.0451
arctic25,dgl,1,250,11.0252,1.0451

epoch:252/50, training loss:0.32209664583206177
Train Acc 1.0480
 Acc 1.0451
arctic25,dgl,1,251,11.0677,1.0451

epoch:253/50, training loss:0.31182530522346497
Train Acc 1.0478
 Acc 1.0290
arctic25,dgl,1,252,11.1103,1.0290

epoch:254/50, training loss:0.3297911286354065
Train Acc 1.0322
 Acc 1.0477
arctic25,dgl,1,253,11.1528,1.0477

epoch:255/50, training loss:0.31574416160583496
Train Acc 1.0505
 Acc 1.0500
new best val f1: 1.0500059069846812
arctic25,dgl,1,254,11.1953,1.0500

epoch:256/50, training loss:0.30824926495552063
Train Acc 1.0528
 Acc 1.0444
arctic25,dgl,1,255,11.2378,1.0444

epoch:257/50, training loss:0.3129951059818268
Train Acc 1.0470
 Acc 1.0392
arctic25,dgl,1,256,11.2803,1.0392

epoch:258/50, training loss:0.3137909471988678
Train Acc 1.0421
 Acc 1.0489
arctic25,dgl,1,257,11.3230,1.0489

epoch:259/50, training loss:0.30626407265663147
Train Acc 1.0520
 Acc 1.0494
arctic25,dgl,1,258,11.3655,1.0494

epoch:260/50, training loss:0.3111269474029541
Train Acc 1.0521
 Acc 1.0467
arctic25,dgl,1,259,11.4080,1.0467

epoch:261/50, training loss:0.30386027693748474
Train Acc 1.0496
 Acc 1.0481
arctic25,dgl,1,260,11.4506,1.0481

epoch:262/50, training loss:0.3042544424533844
Train Acc 1.0509
 Acc 1.0489
arctic25,dgl,1,261,11.4932,1.0489

epoch:263/50, training loss:0.3020257353782654
Train Acc 1.0519
 Acc 1.0488
arctic25,dgl,1,262,11.5357,1.0488

epoch:264/50, training loss:0.2987198531627655
Train Acc 1.0517
 Acc 1.0474
arctic25,dgl,1,263,11.5783,1.0474

epoch:265/50, training loss:0.30159467458724976
Train Acc 1.0504
 Acc 1.0502
new best val f1: 1.0502290597393051
arctic25,dgl,1,264,11.6208,1.0502

epoch:266/50, training loss:0.29890525341033936
Train Acc 1.0532
 Acc 1.0493
arctic25,dgl,1,265,11.6634,1.0493

epoch:267/50, training loss:0.2996193766593933
Train Acc 1.0522
 Acc 1.0467
arctic25,dgl,1,266,11.7059,1.0467

epoch:268/50, training loss:0.2998805344104767
Train Acc 1.0496
 Acc 1.0504
new best val f1: 1.0503931426471167
arctic25,dgl,1,267,11.7484,1.0504

epoch:269/50, training loss:0.295937716960907
Train Acc 1.0533
 Acc 1.0499
arctic25,dgl,1,268,11.7909,1.0499

epoch:270/50, training loss:0.2976256310939789
Train Acc 1.0529
 Acc 1.0468
arctic25,dgl,1,269,11.8334,1.0468

epoch:271/50, training loss:0.296319842338562
Train Acc 1.0497
 Acc 1.0497
arctic25,dgl,1,270,11.8760,1.0497

epoch:272/50, training loss:0.2958418130874634
Train Acc 1.0527
 Acc 1.0504
arctic25,dgl,1,271,11.9185,1.0504

epoch:273/50, training loss:0.2950794994831085
Train Acc 1.0532
 Acc 1.0494
arctic25,dgl,1,272,11.9610,1.0494

epoch:274/50, training loss:0.2928997278213501
Train Acc 1.0521
 Acc 1.0507
new best val f1: 1.0506950551974903
arctic25,dgl,1,273,12.0035,1.0507

epoch:275/50, training loss:0.29163989424705505
Train Acc 1.0536
 Acc 1.0503
arctic25,dgl,1,274,12.0461,1.0503

epoch:276/50, training loss:0.2909204661846161
Train Acc 1.0535
 Acc 1.0493
arctic25,dgl,1,275,12.0886,1.0493

epoch:277/50, training loss:0.29004448652267456
Train Acc 1.0523
 Acc 1.0494
arctic25,dgl,1,276,12.1312,1.0494

epoch:278/50, training loss:0.28977471590042114
Train Acc 1.0524
 Acc 1.0513
new best val f1: 1.0512726270329873
arctic25,dgl,1,277,12.1736,1.0513

epoch:279/50, training loss:0.28898707032203674
Train Acc 1.0540
 Acc 1.0512
arctic25,dgl,1,278,12.2162,1.0512

epoch:280/50, training loss:0.2880266010761261
Train Acc 1.0541
 Acc 1.0497
arctic25,dgl,1,279,12.2587,1.0497

epoch:281/50, training loss:0.2881402373313904
Train Acc 1.0528
 Acc 1.0505
arctic25,dgl,1,280,12.3012,1.0505

epoch:282/50, training loss:0.2875126302242279
Train Acc 1.0537
 Acc 1.0502
arctic25,dgl,1,281,12.3437,1.0502

epoch:283/50, training loss:0.28653189539909363
Train Acc 1.0532
 Acc 1.0491
arctic25,dgl,1,282,12.3863,1.0491

epoch:284/50, training loss:0.2869134545326233
Train Acc 1.0520
 Acc 1.0516
new best val f1: 1.0516007928486106
arctic25,dgl,1,283,12.4287,1.0516

epoch:285/50, training loss:0.2859969139099121
Train Acc 1.0544
 Acc 1.0515
arctic25,dgl,1,284,12.4712,1.0515

epoch:286/50, training loss:0.2847089171409607
Train Acc 1.0546
 Acc 1.0485
arctic25,dgl,1,285,12.5138,1.0485

epoch:287/50, training loss:0.28602921962738037
Train Acc 1.0514
 Acc 1.0509
arctic25,dgl,1,286,12.5563,1.0509

epoch:288/50, training loss:0.28515323996543884
Train Acc 1.0538
 Acc 1.0511
arctic25,dgl,1,287,12.5989,1.0511

epoch:289/50, training loss:0.28382259607315063
Train Acc 1.0540
 Acc 1.0480
arctic25,dgl,1,288,12.6414,1.0480

epoch:290/50, training loss:0.2852555811405182
Train Acc 1.0506
 Acc 1.0518
new best val f1: 1.0517583124401098
arctic25,dgl,1,289,12.6839,1.0518

epoch:291/50, training loss:0.28374117612838745
Train Acc 1.0548
 Acc 1.0518
new best val f1: 1.0517976923379846
arctic25,dgl,1,290,12.7265,1.0518

epoch:292/50, training loss:0.28263694047927856
Train Acc 1.0548
 Acc 1.0480
arctic25,dgl,1,291,12.7690,1.0480

epoch:293/50, training loss:0.28331536054611206
Train Acc 1.0509
 Acc 1.0514
arctic25,dgl,1,292,12.8116,1.0514

epoch:294/50, training loss:0.280996710062027
Train Acc 1.0543
 Acc 1.0514
arctic25,dgl,1,293,12.8541,1.0514

epoch:295/50, training loss:0.2802119553089142
Train Acc 1.0543
 Acc 1.0490
arctic25,dgl,1,294,12.8967,1.0490

epoch:296/50, training loss:0.280587375164032
Train Acc 1.0521
 Acc 1.0519
new best val f1: 1.0518961420826716
arctic25,dgl,1,295,12.9391,1.0519

epoch:297/50, training loss:0.27870112657546997
Train Acc 1.0549
 Acc 1.0519
arctic25,dgl,1,296,12.9817,1.0519

epoch:298/50, training loss:0.2780059278011322
Train Acc 1.0549
 Acc 1.0497
arctic25,dgl,1,297,13.0242,1.0497

epoch:299/50, training loss:0.27813929319381714
Train Acc 1.0528
 Acc 1.0516
arctic25,dgl,1,298,13.0668,1.0516

epoch:300/50, training loss:0.27658599615097046
Train Acc 1.0547
 Acc 1.0518
arctic25,dgl,1,299,13.1093,1.0518

epoch:301/50, training loss:0.27583828568458557
Train Acc 1.0550
 Acc 1.0504
arctic25,dgl,1,300,13.1519,1.0504

epoch:302/50, training loss:0.2760353684425354
Train Acc 1.0535
 Acc 1.0521
new best val f1: 1.052099604888358
arctic25,dgl,1,301,13.1945,1.0521

epoch:303/50, training loss:0.2748306393623352
Train Acc 1.0550
 Acc 1.0520
arctic25,dgl,1,302,13.2370,1.0520

epoch:304/50, training loss:0.2740401327610016
Train Acc 1.0550
 Acc 1.0504
arctic25,dgl,1,303,13.2795,1.0504

epoch:305/50, training loss:0.27429038286209106
Train Acc 1.0535
 Acc 1.0519
arctic25,dgl,1,304,13.3221,1.0519

epoch:306/50, training loss:0.2731616497039795
Train Acc 1.0550
 Acc 1.0518
arctic25,dgl,1,305,13.3646,1.0518

epoch:307/50, training loss:0.2722451388835907
Train Acc 1.0549
 Acc 1.0504
arctic25,dgl,1,306,13.4071,1.0504

epoch:308/50, training loss:0.2727029323577881
Train Acc 1.0537
 Acc 1.0522
new best val f1: 1.0522177445819825
arctic25,dgl,1,307,13.4496,1.0522

epoch:309/50, training loss:0.2717318534851074
Train Acc 1.0553
 Acc 1.0524
new best val f1: 1.0523687008571692
arctic25,dgl,1,308,13.4921,1.0524

epoch:310/50, training loss:0.2707274556159973
Train Acc 1.0554
 Acc 1.0503
arctic25,dgl,1,309,13.5346,1.0503

epoch:311/50, training loss:0.2714231014251709
Train Acc 1.0535
 Acc 1.0521
arctic25,dgl,1,310,13.5772,1.0521

epoch:312/50, training loss:0.270805686712265
Train Acc 1.0552
 Acc 1.0520
arctic25,dgl,1,311,13.6197,1.0520

epoch:313/50, training loss:0.2692883312702179
Train Acc 1.0551
 Acc 1.0489
arctic25,dgl,1,312,13.6622,1.0489

epoch:314/50, training loss:0.27131387591362
Train Acc 1.0520
 Acc 1.0523
arctic25,dgl,1,313,13.7048,1.0523

epoch:315/50, training loss:0.27173128724098206
Train Acc 1.0552
 Acc 1.0525
new best val f1: 1.052480277234481
arctic25,dgl,1,314,13.7473,1.0525

epoch:316/50, training loss:0.2695927321910858
Train Acc 1.0555
 Acc 1.0469
arctic25,dgl,1,315,13.7898,1.0469

epoch:317/50, training loss:0.27233821153640747
Train Acc 1.0499
 Acc 1.0522
arctic25,dgl,1,316,13.8324,1.0522

epoch:318/50, training loss:0.2711247205734253
Train Acc 1.0553
 Acc 1.0524
arctic25,dgl,1,317,13.8750,1.0524

epoch:319/50, training loss:0.27050095796585083
Train Acc 1.0554
 Acc 1.0495
arctic25,dgl,1,318,13.9175,1.0495

epoch:320/50, training loss:0.26816263794898987
Train Acc 1.0525
 Acc 1.0525
arctic25,dgl,1,319,13.9600,1.0525

epoch:321/50, training loss:0.26479047536849976
Train Acc 1.0556
 Acc 1.0523
arctic25,dgl,1,320,14.0026,1.0523

epoch:322/50, training loss:0.26490160822868347
Train Acc 1.0556
 Acc 1.0514
arctic25,dgl,1,321,14.0451,1.0514

epoch:323/50, training loss:0.26411426067352295
Train Acc 1.0545
 Acc 1.0523
arctic25,dgl,1,322,14.0876,1.0523

epoch:324/50, training loss:0.26297852396965027
Train Acc 1.0553
 Acc 1.0525
arctic25,dgl,1,323,14.1301,1.0525

epoch:325/50, training loss:0.2627364993095398
Train Acc 1.0555
 Acc 1.0521
arctic25,dgl,1,324,14.1726,1.0521

epoch:326/50, training loss:0.26216626167297363
Train Acc 1.0551
 Acc 1.0526
new best val f1: 1.052578726979168
arctic25,dgl,1,325,14.2152,1.0526

epoch:327/50, training loss:0.26142552495002747
Train Acc 1.0557
 Acc 1.0528
new best val f1: 1.052788753101167
arctic25,dgl,1,326,14.2577,1.0528

epoch:328/50, training loss:0.26091763377189636
Train Acc 1.0558
 Acc 1.0522
arctic25,dgl,1,327,14.3003,1.0522

epoch:329/50, training loss:0.2605903446674347
Train Acc 1.0551
 Acc 1.0526
arctic25,dgl,1,328,14.3428,1.0526

epoch:330/50, training loss:0.25991156697273254
Train Acc 1.0556
 Acc 1.0526
arctic25,dgl,1,329,14.3853,1.0526

epoch:331/50, training loss:0.259320467710495
Train Acc 1.0555
 Acc 1.0524
arctic25,dgl,1,330,14.4279,1.0524

epoch:332/50, training loss:0.2590206563472748
Train Acc 1.0553
 Acc 1.0531
new best val f1: 1.0531169189167904
arctic25,dgl,1,331,14.4704,1.0531

epoch:333/50, training loss:0.25848492980003357
Train Acc 1.0560
 Acc 1.0528
arctic25,dgl,1,332,14.5130,1.0528

epoch:334/50, training loss:0.25771623849868774
Train Acc 1.0557
 Acc 1.0520
arctic25,dgl,1,333,14.5555,1.0520

epoch:335/50, training loss:0.2575927972793579
Train Acc 1.0550
 Acc 1.0529
arctic25,dgl,1,334,14.5981,1.0529

epoch:336/50, training loss:0.25700563192367554
Train Acc 1.0559
 Acc 1.0531
arctic25,dgl,1,335,14.6406,1.0531

epoch:337/50, training loss:0.2561931908130646
Train Acc 1.0560
 Acc 1.0523
arctic25,dgl,1,336,14.6832,1.0523

epoch:338/50, training loss:0.2562662363052368
Train Acc 1.0551
 Acc 1.0531
new best val f1: 1.0531300455494153
arctic25,dgl,1,337,14.7257,1.0531

epoch:339/50, training loss:0.2560482621192932
Train Acc 1.0561
 Acc 1.0530
arctic25,dgl,1,338,14.7683,1.0530

epoch:340/50, training loss:0.2545910179615021
Train Acc 1.0558
 Acc 1.0517
arctic25,dgl,1,339,14.8107,1.0517

epoch:341/50, training loss:0.2552146911621094
Train Acc 1.0547
 Acc 1.0533
new best val f1: 1.0532941284572268
arctic25,dgl,1,340,14.8533,1.0533

epoch:342/50, training loss:0.25531765818595886
Train Acc 1.0564
 Acc 1.0533
new best val f1: 1.0533138184061643
arctic25,dgl,1,341,14.8958,1.0533

epoch:343/50, training loss:0.2534301280975342
Train Acc 1.0564
 Acc 1.0493
arctic25,dgl,1,342,14.9383,1.0493

epoch:344/50, training loss:0.2567034959793091
Train Acc 1.0523
 Acc 1.0531
arctic25,dgl,1,343,14.9807,1.0531

epoch:345/50, training loss:0.25938645005226135
Train Acc 1.0563
 Acc 1.0535
new best val f1: 1.053477901313976
arctic25,dgl,1,344,15.0233,1.0535

epoch:346/50, training loss:0.25786373019218445
Train Acc 1.0564
 Acc 1.0493
arctic25,dgl,1,345,15.0659,1.0493

epoch:347/50, training loss:0.2556648850440979
Train Acc 1.0523
 Acc 1.0535
new best val f1: 1.0534975912629134
arctic25,dgl,1,346,15.1084,1.0535

epoch:348/50, training loss:0.2519361078739166
Train Acc 1.0565
 Acc 1.0534
arctic25,dgl,1,347,15.1510,1.0534

epoch:349/50, training loss:0.2515745460987091
Train Acc 1.0565
 Acc 1.0505
arctic25,dgl,1,348,15.1935,1.0505

epoch:350/50, training loss:0.2525750994682312
Train Acc 1.0536
 Acc 1.0537
new best val f1: 1.0536748008033499
arctic25,dgl,1,349,15.2361,1.0537

epoch:351/50, training loss:0.2521846890449524
Train Acc 1.0566
 Acc 1.0536
arctic25,dgl,1,350,15.2786,1.0536

epoch:352/50, training loss:0.2502687871456146
Train Acc 1.0566
 Acc 1.0480
arctic25,dgl,1,351,15.3211,1.0480

epoch:353/50, training loss:0.25397059321403503
Train Acc 1.0512
 Acc 1.0536
arctic25,dgl,1,352,15.3638,1.0536

epoch:354/50, training loss:0.25428101420402527
Train Acc 1.0567
 Acc 1.0538
new best val f1: 1.0538126304459117
arctic25,dgl,1,353,15.4063,1.0538

epoch:355/50, training loss:0.25329887866973877
Train Acc 1.0567
 Acc 1.0499
arctic25,dgl,1,354,15.4488,1.0499

epoch:356/50, training loss:0.25082021951675415
Train Acc 1.0530
 Acc 1.0538
new best val f1: 1.0538323203948492
arctic25,dgl,1,355,15.4914,1.0538

epoch:357/50, training loss:0.24720612168312073
Train Acc 1.0568
 Acc 1.0538
new best val f1: 1.0538388837111616
arctic25,dgl,1,356,15.5340,1.0538

epoch:358/50, training loss:0.24711336195468903
Train Acc 1.0568
 Acc 1.0516
arctic25,dgl,1,357,15.5765,1.0516

epoch:359/50, training loss:0.24760779738426208
Train Acc 1.0547
 Acc 1.0540
new best val f1: 1.054042346516848
arctic25,dgl,1,358,15.6190,1.0540

epoch:360/50, training loss:0.246769979596138
Train Acc 1.0569
 Acc 1.0539
arctic25,dgl,1,359,15.6614,1.0539

epoch:361/50, training loss:0.24531219899654388
Train Acc 1.0569
 Acc 1.0500
arctic25,dgl,1,360,15.7039,1.0500

epoch:362/50, training loss:0.24798306822776794
Train Acc 1.0531
 Acc 1.0540
arctic25,dgl,1,361,15.7465,1.0540

epoch:363/50, training loss:0.24947123229503632
Train Acc 1.0570
 Acc 1.0542
new best val f1: 1.0541867394757223
arctic25,dgl,1,362,15.7891,1.0542

epoch:364/50, training loss:0.2477893829345703
Train Acc 1.0571
 Acc 1.0484
arctic25,dgl,1,363,15.8321,1.0484

epoch:365/50, training loss:0.2490101307630539
Train Acc 1.0513
 Acc 1.0542
arctic25,dgl,1,364,15.8746,1.0542

epoch:366/50, training loss:0.24531599879264832
Train Acc 1.0571
 Acc 1.0541
arctic25,dgl,1,365,15.9172,1.0541

epoch:367/50, training loss:0.24504822492599487
Train Acc 1.0571
 Acc 1.0508
arctic25,dgl,1,366,15.9597,1.0508

epoch:368/50, training loss:0.244882270693779
Train Acc 1.0539
 Acc 1.0544
new best val f1: 1.054363949016159
arctic25,dgl,1,367,16.0023,1.0544

epoch:369/50, training loss:0.2428377866744995
Train Acc 1.0572
 Acc 1.0544
new best val f1: 1.0544098922303462
arctic25,dgl,1,368,16.0448,1.0544

epoch:370/50, training loss:0.24199390411376953
Train Acc 1.0573
 Acc 1.0508
arctic25,dgl,1,369,16.0873,1.0508

epoch:371/50, training loss:0.24365673959255219
Train Acc 1.0539
 Acc 1.0543
arctic25,dgl,1,370,16.1297,1.0543

epoch:372/50, training loss:0.24325604736804962
Train Acc 1.0572
 Acc 1.0544
new best val f1: 1.0544164555466586
arctic25,dgl,1,371,16.1723,1.0544

epoch:373/50, training loss:0.24176684021949768
Train Acc 1.0574
 Acc 1.0492
arctic25,dgl,1,372,16.2148,1.0492

epoch:374/50, training loss:0.24451656639575958
Train Acc 1.0523
 Acc 1.0544
new best val f1: 1.054449272128221
arctic25,dgl,1,373,16.2573,1.0544

epoch:375/50, training loss:0.2436174601316452
Train Acc 1.0574
 Acc 1.0544
arctic25,dgl,1,374,16.2999,1.0544

epoch:376/50, training loss:0.24297939240932465
Train Acc 1.0573
 Acc 1.0507
arctic25,dgl,1,375,16.3424,1.0507

epoch:377/50, training loss:0.2415938824415207
Train Acc 1.0538
 Acc 1.0547
new best val f1: 1.0546789881991572
arctic25,dgl,1,376,16.3849,1.0547

epoch:378/50, training loss:0.23855124413967133
Train Acc 1.0576
 Acc 1.0547
new best val f1: 1.054718368097032
arctic25,dgl,1,377,16.4274,1.0547

epoch:379/50, training loss:0.2382635623216629
Train Acc 1.0576
 Acc 1.0519
arctic25,dgl,1,378,16.4700,1.0519

epoch:380/50, training loss:0.23934464156627655
Train Acc 1.0548
 Acc 1.0546
arctic25,dgl,1,379,16.5125,1.0546

epoch:381/50, training loss:0.23893234133720398
Train Acc 1.0575
 Acc 1.0547
arctic25,dgl,1,380,16.5551,1.0547

epoch:382/50, training loss:0.23725439608097076
Train Acc 1.0576
 Acc 1.0499
arctic25,dgl,1,381,16.5976,1.0499

epoch:383/50, training loss:0.24050597846508026
Train Acc 1.0530
 Acc 1.0547
arctic25,dgl,1,382,16.6402,1.0547

epoch:384/50, training loss:0.24131673574447632
Train Acc 1.0577
 Acc 1.0548
new best val f1: 1.0547708746275317
arctic25,dgl,1,383,16.6827,1.0548

epoch:385/50, training loss:0.2407451868057251
Train Acc 1.0577
 Acc 1.0518
arctic25,dgl,1,384,16.7252,1.0518

epoch:386/50, training loss:0.23716865479946136
Train Acc 1.0548
 Acc 1.0548
arctic25,dgl,1,385,16.7677,1.0548

epoch:387/50, training loss:0.23375827074050903
Train Acc 1.0576
 Acc 1.0549
new best val f1: 1.0549218309027186
arctic25,dgl,1,386,16.8103,1.0549

epoch:388/50, training loss:0.23469728231430054
Train Acc 1.0579
 Acc 1.0544
arctic25,dgl,1,387,16.8529,1.0544

epoch:389/50, training loss:0.23320209980010986
Train Acc 1.0574
 Acc 1.0544
arctic25,dgl,1,388,16.8954,1.0544

epoch:390/50, training loss:0.23292209208011627
Train Acc 1.0573
 Acc 1.0550
new best val f1: 1.0550334072800305
arctic25,dgl,1,389,16.9380,1.0550

epoch:391/50, training loss:0.2333814799785614
Train Acc 1.0580
 Acc 1.0549
arctic25,dgl,1,390,16.9805,1.0549

epoch:392/50, training loss:0.23181620240211487
Train Acc 1.0578
 Acc 1.0531
arctic25,dgl,1,391,17.0231,1.0531

epoch:393/50, training loss:0.23329555988311768
Train Acc 1.0561
 Acc 1.0552
new best val f1: 1.0551843635552172
arctic25,dgl,1,392,17.0656,1.0552

epoch:394/50, training loss:0.23466046154499054
Train Acc 1.0580
 Acc 1.0553
new best val f1: 1.0552893766162166
arctic25,dgl,1,393,17.1081,1.0553

epoch:395/50, training loss:0.23208342492580414
Train Acc 1.0581
 Acc 1.0485
arctic25,dgl,1,394,17.1506,1.0485

epoch:396/50, training loss:0.23768621683120728
Train Acc 1.0514
 Acc 1.0551
arctic25,dgl,1,395,17.1932,1.0551

epoch:397/50, training loss:0.2349785566329956
Train Acc 1.0580
 Acc 1.0553
arctic25,dgl,1,396,17.2357,1.0553

epoch:398/50, training loss:0.23493632674217224
Train Acc 1.0582
 Acc 1.0525
arctic25,dgl,1,397,17.2783,1.0525

epoch:399/50, training loss:0.23206622898578644
Train Acc 1.0556
 Acc 1.0554
new best val f1: 1.0554272062587784
arctic25,dgl,1,398,17.3208,1.0554

epoch:400/50, training loss:0.22898928821086884
Train Acc 1.0583
 Acc 1.0554
arctic25,dgl,1,399,17.3634,1.0554

epoch:401/50, training loss:0.2294951230287552
Train Acc 1.0582
 Acc 1.0543
arctic25,dgl,1,400,17.4060,1.0543

epoch:402/50, training loss:0.22880452871322632
Train Acc 1.0573
 Acc 1.0554
arctic25,dgl,1,401,17.4485,1.0554

epoch:403/50, training loss:0.22774407267570496
Train Acc 1.0582
 Acc 1.0556
new best val f1: 1.0555978524829026
arctic25,dgl,1,402,17.4913,1.0556

epoch:404/50, training loss:0.22766879200935364
Train Acc 1.0585
 Acc 1.0549
arctic25,dgl,1,403,17.5339,1.0549

epoch:405/50, training loss:0.22740264236927032
Train Acc 1.0577
 Acc 1.0553
arctic25,dgl,1,404,17.5764,1.0553

epoch:406/50, training loss:0.22674556076526642
Train Acc 1.0583
 Acc 1.0554
arctic25,dgl,1,405,17.6190,1.0554

epoch:407/50, training loss:0.22634142637252808
Train Acc 1.0583
 Acc 1.0551
arctic25,dgl,1,406,17.6615,1.0551

epoch:408/50, training loss:0.22624455392360687
Train Acc 1.0579
 Acc 1.0557
new best val f1: 1.0557422454417769
arctic25,dgl,1,407,17.7040,1.0557

epoch:409/50, training loss:0.22578725218772888
Train Acc 1.0586
 Acc 1.0556
arctic25,dgl,1,408,17.7464,1.0556

epoch:410/50, training loss:0.22520577907562256
Train Acc 1.0584
 Acc 1.0551
arctic25,dgl,1,409,17.7890,1.0551

epoch:411/50, training loss:0.2250867635011673
Train Acc 1.0579
 Acc 1.0556
arctic25,dgl,1,410,17.8316,1.0556

epoch:412/50, training loss:0.22467944025993347
Train Acc 1.0586
 Acc 1.0556
arctic25,dgl,1,411,17.8741,1.0556

epoch:413/50, training loss:0.2241310179233551
Train Acc 1.0585
 Acc 1.0552
arctic25,dgl,1,412,17.9165,1.0552

epoch:414/50, training loss:0.22411419451236725
Train Acc 1.0580
 Acc 1.0559
new best val f1: 1.0558669484517136
arctic25,dgl,1,413,17.9591,1.0559

epoch:415/50, training loss:0.22402256727218628
Train Acc 1.0587
 Acc 1.0556
arctic25,dgl,1,414,18.0016,1.0556

epoch:416/50, training loss:0.22307153046131134
Train Acc 1.0585
 Acc 1.0552
arctic25,dgl,1,415,18.0441,1.0552

epoch:417/50, training loss:0.22313648462295532
Train Acc 1.0581
 Acc 1.0559
new best val f1: 1.055926018298526
arctic25,dgl,1,416,18.0865,1.0559

epoch:418/50, training loss:0.22305186092853546
Train Acc 1.0589
 Acc 1.0558
arctic25,dgl,1,417,18.1291,1.0558

epoch:419/50, training loss:0.22203905880451202
Train Acc 1.0586
 Acc 1.0548
arctic25,dgl,1,418,18.1715,1.0548

epoch:420/50, training loss:0.22257134318351746
Train Acc 1.0578
 Acc 1.0560
new best val f1: 1.0560375946758378
arctic25,dgl,1,419,18.2140,1.0560

epoch:421/50, training loss:0.22302819788455963
Train Acc 1.0589
 Acc 1.0560
arctic25,dgl,1,420,18.2566,1.0560

epoch:422/50, training loss:0.22107693552970886
Train Acc 1.0588
 Acc 1.0534
arctic25,dgl,1,421,18.2990,1.0534

epoch:423/50, training loss:0.22322256863117218
Train Acc 1.0565
 Acc 1.0560
arctic25,dgl,1,422,18.3416,1.0560

epoch:424/50, training loss:0.22525930404663086
Train Acc 1.0588
 Acc 1.0562
new best val f1: 1.0561688610020872
arctic25,dgl,1,423,18.3841,1.0562

epoch:425/50, training loss:0.22283445298671722
Train Acc 1.0590
 Acc 1.0481
arctic25,dgl,1,424,18.4266,1.0481

epoch:426/50, training loss:0.2288832664489746
Train Acc 1.0511
 Acc 1.0563
new best val f1: 1.0562673107467742
arctic25,dgl,1,425,18.4692,1.0563

epoch:427/50, training loss:0.22201214730739594
Train Acc 1.0590
 Acc 1.0563
new best val f1: 1.0563263805935863
arctic25,dgl,1,426,18.5117,1.0563

epoch:428/50, training loss:0.22238144278526306
Train Acc 1.0590
 Acc 1.0529
arctic25,dgl,1,427,18.5542,1.0529

epoch:429/50, training loss:0.22207175195217133
Train Acc 1.0560
 Acc 1.0562
arctic25,dgl,1,428,18.5967,1.0562

epoch:430/50, training loss:0.22072017192840576
Train Acc 1.0591
 Acc 1.0563
arctic25,dgl,1,429,18.6392,1.0563

epoch:431/50, training loss:0.2193387895822525
Train Acc 1.0591
 Acc 1.0510
arctic25,dgl,1,430,18.6816,1.0510

epoch:432/50, training loss:0.22348076105117798
Train Acc 1.0541
 Acc 1.0562
arctic25,dgl,1,431,18.7242,1.0562

epoch:433/50, training loss:0.2245974987745285
Train Acc 1.0590
 Acc 1.0563
arctic25,dgl,1,432,18.7667,1.0563

epoch:434/50, training loss:0.22382338345050812
Train Acc 1.0591
 Acc 1.0526
arctic25,dgl,1,433,18.8091,1.0526

epoch:435/50, training loss:0.22079284489154816
Train Acc 1.0558
 Acc 1.0565
new best val f1: 1.0565232800829603
arctic25,dgl,1,434,18.8516,1.0565

epoch:436/50, training loss:0.21749985218048096
Train Acc 1.0595
 Acc 1.0565
arctic25,dgl,1,435,18.8942,1.0565

epoch:437/50, training loss:0.21717624366283417
Train Acc 1.0594
 Acc 1.0535
arctic25,dgl,1,436,18.9367,1.0535

epoch:438/50, training loss:0.21869079768657684
Train Acc 1.0566
 Acc 1.0564
arctic25,dgl,1,437,18.9792,1.0564

epoch:439/50, training loss:0.21927191317081451
Train Acc 1.0593
 Acc 1.0564
arctic25,dgl,1,438,19.0217,1.0564

epoch:440/50, training loss:0.21734435856342316
Train Acc 1.0594
 Acc 1.0500
arctic25,dgl,1,439,19.0642,1.0500

epoch:441/50, training loss:0.2223515659570694
Train Acc 1.0533
 Acc 1.0565
new best val f1: 1.0565298433992727
arctic25,dgl,1,440,19.1068,1.0565

epoch:442/50, training loss:0.21890097856521606
Train Acc 1.0594
 Acc 1.0566
new best val f1: 1.0566086031950224
arctic25,dgl,1,441,19.1493,1.0566

epoch:443/50, training loss:0.2186596542596817
Train Acc 1.0595
 Acc 1.0523
arctic25,dgl,1,442,19.1918,1.0523

epoch:444/50, training loss:0.2184736281633377
Train Acc 1.0554
 Acc 1.0567
new best val f1: 1.0566807996744596
arctic25,dgl,1,443,19.2343,1.0567

epoch:445/50, training loss:0.21685177087783813
Train Acc 1.0596
 Acc 1.0568
new best val f1: 1.0568055026843963
arctic25,dgl,1,444,19.2772,1.0568

epoch:446/50, training loss:0.21567071974277496
Train Acc 1.0598
 Acc 1.0516
arctic25,dgl,1,445,19.3196,1.0516

epoch:447/50, training loss:0.21872954070568085
Train Acc 1.0545
 Acc 1.0565
arctic25,dgl,1,446,19.3621,1.0565

epoch:448/50, training loss:0.21786071360111237
Train Acc 1.0595
 Acc 1.0566
arctic25,dgl,1,447,19.4047,1.0566

epoch:449/50, training loss:0.21709774434566498
Train Acc 1.0596
 Acc 1.0523
arctic25,dgl,1,448,19.4472,1.0523

epoch:450/50, training loss:0.21700206398963928
Train Acc 1.0555
 Acc 1.0569
new best val f1: 1.0568908257964584
arctic25,dgl,1,449,19.4897,1.0569

epoch:451/50, training loss:0.2151889055967331
Train Acc 1.0598
 Acc 1.0569
arctic25,dgl,1,450,19.5322,1.0569

epoch:452/50, training loss:0.21414795517921448
Train Acc 1.0598
 Acc 1.0521
arctic25,dgl,1,451,19.5746,1.0521

epoch:453/50, training loss:0.21640165150165558
Train Acc 1.0552
 Acc 1.0567
arctic25,dgl,1,452,19.6172,1.0567

epoch:454/50, training loss:0.21616780757904053
Train Acc 1.0595
 Acc 1.0568
arctic25,dgl,1,453,19.6597,1.0568

epoch:455/50, training loss:0.2152218371629715
Train Acc 1.0598
 Acc 1.0526
arctic25,dgl,1,454,19.7022,1.0526

epoch:456/50, training loss:0.21531419456005096
Train Acc 1.0556
 Acc 1.0569
arctic25,dgl,1,455,19.7447,1.0569

epoch:457/50, training loss:0.21387167274951935
Train Acc 1.0598
 Acc 1.0569
new best val f1: 1.0569236423780208
arctic25,dgl,1,456,19.7872,1.0569

epoch:458/50, training loss:0.21272647380828857
Train Acc 1.0597
 Acc 1.0521
arctic25,dgl,1,457,19.8297,1.0521

epoch:459/50, training loss:0.21491815149784088
Train Acc 1.0552
 Acc 1.0570
new best val f1: 1.0570220921227078
arctic25,dgl,1,458,19.8722,1.0570

epoch:460/50, training loss:0.21376928687095642
Train Acc 1.0599
 Acc 1.0571
new best val f1: 1.0571008519184573
arctic25,dgl,1,459,19.9148,1.0571

epoch:461/50, training loss:0.21302542090415955
Train Acc 1.0601
 Acc 1.0529
arctic25,dgl,1,460,19.9572,1.0529

epoch:462/50, training loss:0.2132766842842102
Train Acc 1.0559
 Acc 1.0570
arctic25,dgl,1,461,19.9997,1.0570

epoch:463/50, training loss:0.21175724267959595
Train Acc 1.0599
 Acc 1.0570
arctic25,dgl,1,462,20.0423,1.0570

epoch:464/50, training loss:0.21077725291252136
Train Acc 1.0599
 Acc 1.0524
arctic25,dgl,1,463,20.0848,1.0524

epoch:465/50, training loss:0.21300411224365234
Train Acc 1.0556
 Acc 1.0571
new best val f1: 1.0571139785510824
arctic25,dgl,1,464,20.1273,1.0571

epoch:466/50, training loss:0.2124241590499878
Train Acc 1.0601
 Acc 1.0572
new best val f1: 1.0572124282957693
arctic25,dgl,1,465,20.1698,1.0572

epoch:467/50, training loss:0.21157269179821014
Train Acc 1.0602
 Acc 1.0530
arctic25,dgl,1,466,20.2123,1.0530

epoch:468/50, training loss:0.21147866547107697
Train Acc 1.0562
 Acc 1.0571
arctic25,dgl,1,467,20.2548,1.0571

epoch:469/50, training loss:0.20944832265377045
Train Acc 1.0600
 Acc 1.0571
arctic25,dgl,1,468,20.2973,1.0571

epoch:470/50, training loss:0.20868146419525146
Train Acc 1.0601
 Acc 1.0534
arctic25,dgl,1,469,20.3398,1.0534

epoch:471/50, training loss:0.21033471822738647
Train Acc 1.0564
 Acc 1.0572
new best val f1: 1.057238681561019
arctic25,dgl,1,470,20.3823,1.0572

epoch:472/50, training loss:0.20998293161392212
Train Acc 1.0603
 Acc 1.0574
new best val f1: 1.057363384570956
arctic25,dgl,1,471,20.4247,1.0574

epoch:473/50, training loss:0.20880353450775146
Train Acc 1.0603
 Acc 1.0527
arctic25,dgl,1,472,20.4672,1.0527

epoch:474/50, training loss:0.21055707335472107
Train Acc 1.0557
 Acc 1.0572
arctic25,dgl,1,473,20.5097,1.0572

epoch:475/50, training loss:0.20888610184192657
Train Acc 1.0603
 Acc 1.0573
arctic25,dgl,1,474,20.5522,1.0573

epoch:476/50, training loss:0.2081340104341507
Train Acc 1.0603
 Acc 1.0536
arctic25,dgl,1,475,20.5947,1.0536

epoch:477/50, training loss:0.20862896740436554
Train Acc 1.0567
 Acc 1.0574
new best val f1: 1.057389637836206
arctic25,dgl,1,476,20.6372,1.0574

epoch:478/50, training loss:0.2070358842611313
Train Acc 1.0604
 Acc 1.0574
arctic25,dgl,1,477,20.6796,1.0574

epoch:479/50, training loss:0.20618291199207306
Train Acc 1.0604
 Acc 1.0533
arctic25,dgl,1,478,20.7222,1.0533

epoch:480/50, training loss:0.20829430222511292
Train Acc 1.0564
 Acc 1.0573
arctic25,dgl,1,479,20.7647,1.0573

epoch:481/50, training loss:0.20835959911346436
Train Acc 1.0604
 Acc 1.0575
new best val f1: 1.0574946508972054
arctic25,dgl,1,480,20.8072,1.0575

epoch:482/50, training loss:0.20743557810783386
Train Acc 1.0605
 Acc 1.0535
arctic25,dgl,1,481,20.8497,1.0535

epoch:483/50, training loss:0.20735937356948853
Train Acc 1.0566
 Acc 1.0576
new best val f1: 1.0575734106929549
arctic25,dgl,1,482,20.8922,1.0576

epoch:484/50, training loss:0.20510895550251007
Train Acc 1.0606
 Acc 1.0575
arctic25,dgl,1,483,20.9347,1.0575

epoch:485/50, training loss:0.204462930560112
Train Acc 1.0606
 Acc 1.0539
arctic25,dgl,1,484,20.9772,1.0539

epoch:486/50, training loss:0.20609687268733978
Train Acc 1.0569
 Acc 1.0575
arctic25,dgl,1,485,21.0197,1.0575

epoch:487/50, training loss:0.20560525357723236
Train Acc 1.0606
 Acc 1.0576
new best val f1: 1.0576062272745173
arctic25,dgl,1,486,21.0622,1.0576

epoch:488/50, training loss:0.20462296903133392
Train Acc 1.0607
 Acc 1.0537
arctic25,dgl,1,487,21.1047,1.0537

epoch:489/50, training loss:0.20576848089694977
Train Acc 1.0568
 Acc 1.0576
arctic25,dgl,1,488,21.1473,1.0576

epoch:490/50, training loss:0.20443812012672424
Train Acc 1.0606
 Acc 1.0576
arctic25,dgl,1,489,21.1897,1.0576

epoch:491/50, training loss:0.20363003015518188
Train Acc 1.0605
 Acc 1.0537
arctic25,dgl,1,490,21.2323,1.0537

epoch:492/50, training loss:0.20482350885868073
Train Acc 1.0568
 Acc 1.0577
new best val f1: 1.057658733805017
arctic25,dgl,1,491,21.2747,1.0577

epoch:493/50, training loss:0.20369885861873627
Train Acc 1.0608
 Acc 1.0578
new best val f1: 1.057783436814954
arctic25,dgl,1,492,21.3172,1.0578

epoch:494/50, training loss:0.20275910198688507
Train Acc 1.0609
 Acc 1.0537
arctic25,dgl,1,493,21.3597,1.0537

epoch:495/50, training loss:0.2043321132659912
Train Acc 1.0568
 Acc 1.0577
arctic25,dgl,1,494,21.4022,1.0577

epoch:496/50, training loss:0.20355285704135895
Train Acc 1.0607
 Acc 1.0577
arctic25,dgl,1,495,21.4446,1.0577

epoch:497/50, training loss:0.20280902087688446
Train Acc 1.0608
 Acc 1.0540
arctic25,dgl,1,496,21.4871,1.0540

epoch:498/50, training loss:0.20328716933727264
Train Acc 1.0571
 Acc 1.0579
new best val f1: 1.057940956406453
arctic25,dgl,1,497,21.5296,1.0579

epoch:499/50, training loss:0.20145371556282043
Train Acc 1.0610
 Acc 1.0579
arctic25,dgl,1,498,21.5721,1.0579

epoch:500/50, training loss:0.20067167282104492
Train Acc 1.0610
 Acc 1.0538
arctic25,dgl,1,499,21.6146,1.0538

epoch:501/50, training loss:0.2026694118976593
Train Acc 1.0569
 Acc 1.0578
arctic25,dgl,1,500,21.6572,1.0578

epoch:502/50, training loss:0.20255279541015625
Train Acc 1.0608
 Acc 1.0579
arctic25,dgl,1,501,21.6997,1.0579

epoch:503/50, training loss:0.20173603296279907
Train Acc 1.0610
 Acc 1.0539
arctic25,dgl,1,502,21.7421,1.0539

epoch:504/50, training loss:0.20218242704868317
Train Acc 1.0570
 Acc 1.0580
new best val f1: 1.0579934629369527
arctic25,dgl,1,503,21.7847,1.0580

epoch:505/50, training loss:0.20039725303649902
Train Acc 1.0612
 Acc 1.0579
arctic25,dgl,1,504,21.8271,1.0579

epoch:506/50, training loss:0.19979184865951538
Train Acc 1.0611
 Acc 1.0539
arctic25,dgl,1,505,21.8696,1.0539

epoch:507/50, training loss:0.2012166678905487
Train Acc 1.0570
 Acc 1.0580
new best val f1: 1.0580197162022027
arctic25,dgl,1,506,21.9121,1.0580

epoch:508/50, training loss:0.20044784247875214
Train Acc 1.0611
 Acc 1.0580
new best val f1: 1.05803940615114
arctic25,dgl,1,507,21.9546,1.0580

epoch:509/50, training loss:0.19964709877967834
Train Acc 1.0612
 Acc 1.0541
arctic25,dgl,1,508,21.9971,1.0541

epoch:510/50, training loss:0.20061716437339783
Train Acc 1.0572
 Acc 1.0581
new best val f1: 1.0580722227327024
arctic25,dgl,1,509,22.0396,1.0581

epoch:511/50, training loss:0.19923461973667145
Train Acc 1.0612
 Acc 1.0581
new best val f1: 1.0580984759979521
arctic25,dgl,1,510,22.0821,1.0581

epoch:512/50, training loss:0.1984586864709854
Train Acc 1.0612
 Acc 1.0542
arctic25,dgl,1,511,22.1246,1.0542

epoch:513/50, training loss:0.1995842009782791
Train Acc 1.0574
 Acc 1.0582
new best val f1: 1.0581706724773894
arctic25,dgl,1,512,22.1671,1.0582

epoch:514/50, training loss:0.19851641356945038
Train Acc 1.0613
 Acc 1.0582
new best val f1: 1.058223179007889
arctic25,dgl,1,513,22.2096,1.0582

epoch:515/50, training loss:0.1974872648715973
Train Acc 1.0613
 Acc 1.0540
arctic25,dgl,1,514,22.2521,1.0540

epoch:516/50, training loss:0.19935443997383118
Train Acc 1.0571
 Acc 1.0582
arctic25,dgl,1,515,22.2947,1.0582

epoch:517/50, training loss:0.19887679815292358
Train Acc 1.0612
 Acc 1.0582
new best val f1: 1.0582428689568264
arctic25,dgl,1,516,22.3372,1.0582

epoch:518/50, training loss:0.1981629878282547
Train Acc 1.0614
 Acc 1.0547
arctic25,dgl,1,517,22.3797,1.0547

epoch:519/50, training loss:0.1979251652956009
Train Acc 1.0579
 Acc 1.0584
new best val f1: 1.0583675719667633
arctic25,dgl,1,518,22.4221,1.0584

epoch:520/50, training loss:0.19548608362674713
Train Acc 1.0615
 Acc 1.0584
new best val f1: 1.058433205129888
arctic25,dgl,1,519,22.4646,1.0584

epoch:521/50, training loss:0.19510218501091003
Train Acc 1.0614
 Acc 1.0551
arctic25,dgl,1,520,22.5071,1.0551

epoch:522/50, training loss:0.1965590864419937
Train Acc 1.0583
 Acc 1.0584
arctic25,dgl,1,521,22.5497,1.0584

epoch:523/50, training loss:0.19611722230911255
Train Acc 1.0614
 Acc 1.0585
new best val f1: 1.05851852824195
arctic25,dgl,1,522,22.5922,1.0585

epoch:524/50, training loss:0.19479267299175262
Train Acc 1.0615
 Acc 1.0543
arctic25,dgl,1,523,22.6346,1.0543

epoch:525/50, training loss:0.19714416563510895
Train Acc 1.0574
 Acc 1.0584
arctic25,dgl,1,524,22.6772,1.0584

epoch:526/50, training loss:0.19687040150165558
Train Acc 1.0615
 Acc 1.0584
arctic25,dgl,1,525,22.7197,1.0584

epoch:527/50, training loss:0.1962101310491562
Train Acc 1.0614
 Acc 1.0550
arctic25,dgl,1,526,22.7622,1.0550

epoch:528/50, training loss:0.19550061225891113
Train Acc 1.0583
 Acc 1.0586
new best val f1: 1.058616977986637
arctic25,dgl,1,527,22.8047,1.0586

epoch:529/50, training loss:0.1928134262561798
Train Acc 1.0616
 Acc 1.0586
arctic25,dgl,1,528,22.8472,1.0586

epoch:530/50, training loss:0.1925657093524933
Train Acc 1.0616
 Acc 1.0559
arctic25,dgl,1,529,22.8897,1.0559

epoch:531/50, training loss:0.19357989728450775
Train Acc 1.0593
 Acc 1.0586
arctic25,dgl,1,530,22.9322,1.0586

epoch:532/50, training loss:0.19292877614498138
Train Acc 1.0616
 Acc 1.0586
arctic25,dgl,1,531,22.9746,1.0586

epoch:533/50, training loss:0.19174480438232422
Train Acc 1.0617
 Acc 1.0549
arctic25,dgl,1,532,23.0171,1.0549

epoch:534/50, training loss:0.19453677535057068
Train Acc 1.0581
 Acc 1.0586
new best val f1: 1.0586497945681994
arctic25,dgl,1,533,23.0597,1.0586

epoch:535/50, training loss:0.19546294212341309
Train Acc 1.0617
 Acc 1.0587
new best val f1: 1.0586629212008243
arctic25,dgl,1,534,23.1022,1.0587

epoch:536/50, training loss:0.1943809986114502
Train Acc 1.0616
 Acc 1.0548
arctic25,dgl,1,535,23.1447,1.0548

epoch:537/50, training loss:0.19410604238510132
Train Acc 1.0579
 Acc 1.0589
new best val f1: 1.0588598206901982
arctic25,dgl,1,536,23.1871,1.0589

epoch:538/50, training loss:0.19141247868537903
Train Acc 1.0618
 Acc 1.0590
new best val f1: 1.0589648337511979
arctic25,dgl,1,537,23.2295,1.0590

epoch:539/50, training loss:0.1911911517381668
Train Acc 1.0619
 Acc 1.0559
arctic25,dgl,1,538,23.2720,1.0559

epoch:540/50, training loss:0.19214750826358795
Train Acc 1.0592
 Acc 1.0588
arctic25,dgl,1,539,23.3145,1.0588

epoch:541/50, training loss:0.19112884998321533
Train Acc 1.0617
 Acc 1.0589
arctic25,dgl,1,540,23.3570,1.0589

epoch:542/50, training loss:0.19022680819034576
Train Acc 1.0618
 Acc 1.0549
arctic25,dgl,1,541,23.3995,1.0549

epoch:543/50, training loss:0.19287140667438507
Train Acc 1.0581
 Acc 1.0590
new best val f1: 1.0590107769653851
arctic25,dgl,1,542,23.4420,1.0590

epoch:544/50, training loss:0.19339200854301453
Train Acc 1.0619
 Acc 1.0589
arctic25,dgl,1,543,23.4844,1.0589

epoch:545/50, training loss:0.19252559542655945
Train Acc 1.0620
 Acc 1.0552
arctic25,dgl,1,544,23.5269,1.0552

epoch:546/50, training loss:0.19171880185604095
Train Acc 1.0586
 Acc 1.0590
new best val f1: 1.0590173402816976
arctic25,dgl,1,545,23.5698,1.0590

epoch:547/50, training loss:0.189020037651062
Train Acc 1.0619
 Acc 1.0591
new best val f1: 1.059122353342697
arctic25,dgl,1,546,23.6123,1.0591

epoch:548/50, training loss:0.18865525722503662
Train Acc 1.0620
 Acc 1.0563
arctic25,dgl,1,547,23.6548,1.0563

epoch:549/50, training loss:0.18988031148910522
Train Acc 1.0596
 Acc 1.0590
arctic25,dgl,1,548,23.6973,1.0590

epoch:550/50, training loss:0.18912926316261292
Train Acc 1.0620
 Acc 1.0591
arctic25,dgl,1,549,23.7398,1.0591

epoch:551/50, training loss:0.18803942203521729
Train Acc 1.0621
 Acc 1.0556
arctic25,dgl,1,550,23.7823,1.0556

epoch:552/50, training loss:0.19022248685359955
Train Acc 1.0588
 Acc 1.0591
arctic25,dgl,1,551,23.8247,1.0591

epoch:553/50, training loss:0.19002310931682587
Train Acc 1.0621
 Acc 1.0591
new best val f1: 1.0591354799753219
arctic25,dgl,1,552,23.8672,1.0591

epoch:554/50, training loss:0.1889667510986328
Train Acc 1.0621
 Acc 1.0546
arctic25,dgl,1,553,23.9097,1.0546

epoch:555/50, training loss:0.19086886942386627
Train Acc 1.0578
 Acc 1.0592
new best val f1: 1.0591748598731967
arctic25,dgl,1,554,23.9522,1.0592

epoch:556/50, training loss:0.18951795995235443
Train Acc 1.0622
 Acc 1.0593
new best val f1: 1.0593323794646958
arctic25,dgl,1,555,23.9954,1.0593

epoch:557/50, training loss:0.1889481097459793
Train Acc 1.0622
 Acc 1.0555
arctic25,dgl,1,556,24.0378,1.0555

epoch:558/50, training loss:0.1890919953584671
Train Acc 1.0588
 Acc 1.0593
arctic25,dgl,1,557,24.0803,1.0593

epoch:559/50, training loss:0.18707171082496643
Train Acc 1.0622
 Acc 1.0593
arctic25,dgl,1,558,24.1228,1.0593

epoch:560/50, training loss:0.18659119307994843
Train Acc 1.0622
 Acc 1.0560
arctic25,dgl,1,559,24.1653,1.0560

epoch:561/50, training loss:0.18795928359031677
Train Acc 1.0594
 Acc 1.0594
new best val f1: 1.059404575944133
arctic25,dgl,1,560,24.2078,1.0594

epoch:562/50, training loss:0.1870863288640976
Train Acc 1.0623
 Acc 1.0593
arctic25,dgl,1,561,24.2503,1.0593

epoch:563/50, training loss:0.1860104501247406
Train Acc 1.0623
 Acc 1.0556
arctic25,dgl,1,562,24.2927,1.0556

epoch:564/50, training loss:0.18806584179401398
Train Acc 1.0586
 Acc 1.0594
arctic25,dgl,1,563,24.3353,1.0594

epoch:565/50, training loss:0.18756309151649475
Train Acc 1.0623
 Acc 1.0594
new best val f1: 1.0594111392604455
arctic25,dgl,1,564,24.3777,1.0594

epoch:566/50, training loss:0.18663838505744934
Train Acc 1.0623
 Acc 1.0553
arctic25,dgl,1,565,24.4202,1.0553

epoch:567/50, training loss:0.18789228796958923
Train Acc 1.0585
 Acc 1.0595
new best val f1: 1.0594964623725076
arctic25,dgl,1,566,24.4627,1.0595

epoch:568/50, training loss:0.18647117912769318
Train Acc 1.0624
 Acc 1.0595
new best val f1: 1.059516152321445
arctic25,dgl,1,567,24.5052,1.0595

epoch:569/50, training loss:0.18601372838020325
Train Acc 1.0625
 Acc 1.0558
arctic25,dgl,1,568,24.5478,1.0558

epoch:570/50, training loss:0.18685920536518097
Train Acc 1.0588
 Acc 1.0596
new best val f1: 1.0595949121171946
arctic25,dgl,1,569,24.5902,1.0596

epoch:571/50, training loss:0.18526498973369598
Train Acc 1.0624
 Acc 1.0595
arctic25,dgl,1,570,24.6328,1.0595

epoch:572/50, training loss:0.18467022478580475
Train Acc 1.0624
 Acc 1.0555
arctic25,dgl,1,571,24.6752,1.0555

epoch:573/50, training loss:0.18658553063869476
Train Acc 1.0586
 Acc 1.0596
new best val f1: 1.059601475433507
arctic25,dgl,1,572,24.7178,1.0596

epoch:574/50, training loss:0.18606474995613098
Train Acc 1.0625
 Acc 1.0596
new best val f1: 1.059627728698757
arctic25,dgl,1,573,24.7603,1.0596

epoch:575/50, training loss:0.18539008498191833
Train Acc 1.0626
 Acc 1.0559
arctic25,dgl,1,574,24.8028,1.0559

epoch:576/50, training loss:0.18557389080524445
Train Acc 1.0590
 Acc 1.0597
new best val f1: 1.0596671085966316
arctic25,dgl,1,575,24.8453,1.0597

epoch:577/50, training loss:0.18338195979595184
Train Acc 1.0626
 Acc 1.0596
arctic25,dgl,1,576,24.8879,1.0596

epoch:578/50, training loss:0.1828947812318802
Train Acc 1.0626
 Acc 1.0567
arctic25,dgl,1,577,24.9303,1.0567

epoch:579/50, training loss:0.18406812846660614
Train Acc 1.0598
 Acc 1.0597
new best val f1: 1.0596933618618816
arctic25,dgl,1,578,24.9728,1.0597

epoch:580/50, training loss:0.18298064172267914
Train Acc 1.0627
 Acc 1.0597
new best val f1: 1.0597458683923813
arctic25,dgl,1,579,25.0153,1.0597

epoch:581/50, training loss:0.18223021924495697
Train Acc 1.0627
 Acc 1.0560
arctic25,dgl,1,580,25.0577,1.0560

epoch:582/50, training loss:0.18434524536132812
Train Acc 1.0591
 Acc 1.0598
new best val f1: 1.0597655583413188
arctic25,dgl,1,581,25.1002,1.0598

epoch:583/50, training loss:0.18401074409484863
Train Acc 1.0627
 Acc 1.0598
arctic25,dgl,1,582,25.1427,1.0598

epoch:584/50, training loss:0.18315421044826508
Train Acc 1.0627
 Acc 1.0561
arctic25,dgl,1,583,25.1852,1.0561

epoch:585/50, training loss:0.1839682161808014
Train Acc 1.0591
 Acc 1.0599
new best val f1: 1.0598705714023182
arctic25,dgl,1,584,25.2277,1.0599

epoch:586/50, training loss:0.1821073442697525
Train Acc 1.0628
 Acc 1.0598
arctic25,dgl,1,585,25.2702,1.0598

epoch:587/50, training loss:0.18163955211639404
Train Acc 1.0628
 Acc 1.0562
arctic25,dgl,1,586,25.3127,1.0562

epoch:588/50, training loss:0.18315745890140533
Train Acc 1.0593
 Acc 1.0599
arctic25,dgl,1,587,25.3551,1.0599

epoch:589/50, training loss:0.1823602169752121
Train Acc 1.0629
 Acc 1.0600
new best val f1: 1.05998214777963
arctic25,dgl,1,588,25.3977,1.0600

epoch:590/50, training loss:0.18137787282466888
Train Acc 1.0629
 Acc 1.0561
arctic25,dgl,1,589,25.4402,1.0561

epoch:591/50, training loss:0.18319742381572723
Train Acc 1.0592
 Acc 1.0599
arctic25,dgl,1,590,25.4827,1.0599

epoch:592/50, training loss:0.1821354180574417
Train Acc 1.0628
 Acc 1.0599
arctic25,dgl,1,591,25.5252,1.0599

epoch:593/50, training loss:0.18132488429546356
Train Acc 1.0629
 Acc 1.0564
arctic25,dgl,1,592,25.5678,1.0564

epoch:594/50, training loss:0.18224510550498962
Train Acc 1.0594
 Acc 1.0600
new best val f1: 1.06000840104488
arctic25,dgl,1,593,25.6102,1.0600

epoch:595/50, training loss:0.18059276044368744
Train Acc 1.0630
 Acc 1.0600
new best val f1: 1.0600280909938173
arctic25,dgl,1,594,25.6527,1.0600

epoch:596/50, training loss:0.17992621660232544
Train Acc 1.0630
 Acc 1.0564
arctic25,dgl,1,595,25.6952,1.0564

epoch:597/50, training loss:0.18139231204986572
Train Acc 1.0596
 Acc 1.0600
arctic25,dgl,1,596,25.7378,1.0600

epoch:598/50, training loss:0.180489644408226
Train Acc 1.0629
 Acc 1.0601
new best val f1: 1.060080597524317
arctic25,dgl,1,597,25.7803,1.0601

epoch:599/50, training loss:0.1796795129776001
Train Acc 1.0631
 Acc 1.0565
arctic25,dgl,1,598,25.8246,1.0565

epoch:600/50, training loss:0.181223526597023
Train Acc 1.0595
 Acc 1.0601
arctic25,dgl,1,599,25.8671,1.0601

epoch:601/50, training loss:0.1801358461380005
Train Acc 1.0631
 Acc 1.0600
arctic25,dgl,1,600,25.9096,1.0600

epoch:602/50, training loss:0.17943896353244781
Train Acc 1.0631
 Acc 1.0565
arctic25,dgl,1,601,25.9521,1.0565

epoch:603/50, training loss:0.18058833479881287
Train Acc 1.0595
 Acc 1.0602
new best val f1: 1.0601987372179416
arctic25,dgl,1,602,25.9946,1.0602

epoch:604/50, training loss:0.17930176854133606
Train Acc 1.0632
 Acc 1.0602
new best val f1: 1.0602249904831913
arctic25,dgl,1,603,26.0371,1.0602

epoch:605/50, training loss:0.17853063344955444
Train Acc 1.0633
 Acc 1.0566
arctic25,dgl,1,604,26.0796,1.0566

epoch:606/50, training loss:0.1798943132162094
Train Acc 1.0597
 Acc 1.0602
arctic25,dgl,1,605,26.1221,1.0602

epoch:607/50, training loss:0.17860548198223114
Train Acc 1.0631
 Acc 1.0603
new best val f1: 1.0602512437484413
arctic25,dgl,1,606,26.1646,1.0603

epoch:608/50, training loss:0.17793071269989014
Train Acc 1.0633
 Acc 1.0571
arctic25,dgl,1,607,26.2071,1.0571

epoch:609/50, training loss:0.17919893562793732
Train Acc 1.0601
 Acc 1.0603
new best val f1: 1.060303750278941
arctic25,dgl,1,608,26.2496,1.0603

epoch:610/50, training loss:0.17788682878017426
Train Acc 1.0634
 Acc 1.0603
arctic25,dgl,1,609,26.2919,1.0603

epoch:611/50, training loss:0.17712685465812683
Train Acc 1.0633
 Acc 1.0568
arctic25,dgl,1,610,26.3345,1.0568

epoch:612/50, training loss:0.17881344258785248
Train Acc 1.0598
 Acc 1.0604
new best val f1: 1.0603825100746906
arctic25,dgl,1,611,26.3770,1.0604

epoch:613/50, training loss:0.17783494293689728
Train Acc 1.0634
 Acc 1.0605
new best val f1: 1.060500649768315
arctic25,dgl,1,612,26.4196,1.0605

epoch:614/50, training loss:0.17716068029403687
Train Acc 1.0635
 Acc 1.0567
arctic25,dgl,1,613,26.4622,1.0567

epoch:615/50, training loss:0.17860004305839539
Train Acc 1.0598
 Acc 1.0603
arctic25,dgl,1,614,26.5048,1.0603

epoch:616/50, training loss:0.17744845151901245
Train Acc 1.0634
 Acc 1.0604
arctic25,dgl,1,615,26.5474,1.0604

epoch:617/50, training loss:0.17664490640163422
Train Acc 1.0635
 Acc 1.0570
arctic25,dgl,1,616,26.5900,1.0570

epoch:618/50, training loss:0.17788539826869965
Train Acc 1.0600
 Acc 1.0606
new best val f1: 1.0605531562988146
arctic25,dgl,1,617,26.6326,1.0606

epoch:619/50, training loss:0.1765209138393402
Train Acc 1.0636
 Acc 1.0606
new best val f1: 1.060559719615127
arctic25,dgl,1,618,26.6752,1.0606

epoch:620/50, training loss:0.175918310880661
Train Acc 1.0636
 Acc 1.0571
arctic25,dgl,1,619,26.7178,1.0571

epoch:621/50, training loss:0.17730791866779327
Train Acc 1.0600
 Acc 1.0605
arctic25,dgl,1,620,26.7604,1.0605

epoch:622/50, training loss:0.1762360781431198
Train Acc 1.0636
 Acc 1.0606
new best val f1: 1.0606450427271892
arctic25,dgl,1,621,26.8029,1.0606

epoch:623/50, training loss:0.17546790838241577
Train Acc 1.0637
 Acc 1.0572
arctic25,dgl,1,622,26.8455,1.0572

epoch:624/50, training loss:0.17677272856235504
Train Acc 1.0602
 Acc 1.0606
arctic25,dgl,1,623,26.8880,1.0606

epoch:625/50, training loss:0.1753857582807541
Train Acc 1.0636
 Acc 1.0606
arctic25,dgl,1,624,26.9306,1.0606

epoch:626/50, training loss:0.17470787465572357
Train Acc 1.0637
 Acc 1.0575
arctic25,dgl,1,625,26.9731,1.0575

epoch:627/50, training loss:0.17594152688980103
Train Acc 1.0605
 Acc 1.0608
new best val f1: 1.060756619104501
arctic25,dgl,1,626,27.0156,1.0608

epoch:628/50, training loss:0.17455658316612244
Train Acc 1.0637
 Acc 1.0608
new best val f1: 1.060782872369751
arctic25,dgl,1,627,27.0581,1.0608

epoch:629/50, training loss:0.17381639778614044
Train Acc 1.0638
 Acc 1.0576
arctic25,dgl,1,628,27.1006,1.0576

epoch:630/50, training loss:0.17525576055049896
Train Acc 1.0606
 Acc 1.0608
new best val f1: 1.0607959990023759
arctic25,dgl,1,629,27.1431,1.0608

epoch:631/50, training loss:0.1740543097257614
Train Acc 1.0637
 Acc 1.0609
new best val f1: 1.060868195481813
arctic25,dgl,1,630,27.1856,1.0609

epoch:632/50, training loss:0.17326666414737701
Train Acc 1.0638
 Acc 1.0578
arctic25,dgl,1,631,27.2281,1.0578

epoch:633/50, training loss:0.1747453510761261
Train Acc 1.0608
 Acc 1.0608
arctic25,dgl,1,632,27.2706,1.0608

epoch:634/50, training loss:0.17357932031154633
Train Acc 1.0638
 Acc 1.0609
new best val f1: 1.0609010120633753
arctic25,dgl,1,633,27.3131,1.0609

epoch:635/50, training loss:0.1727972775697708
Train Acc 1.0639
 Acc 1.0576
arctic25,dgl,1,634,27.3556,1.0576

epoch:636/50, training loss:0.17461305856704712
Train Acc 1.0604
 Acc 1.0609
arctic25,dgl,1,635,27.3981,1.0609

epoch:637/50, training loss:0.17392535507678986
Train Acc 1.0638
 Acc 1.0609
new best val f1: 1.0609272653286252
arctic25,dgl,1,636,27.4405,1.0609

epoch:638/50, training loss:0.17314468324184418
Train Acc 1.0639
 Acc 1.0573
arctic25,dgl,1,637,27.4830,1.0573

epoch:639/50, training loss:0.17460176348686218
Train Acc 1.0604
 Acc 1.0610
new best val f1: 1.0609732085428125
arctic25,dgl,1,638,27.5255,1.0610

epoch:640/50, training loss:0.1734241545200348
Train Acc 1.0640
 Acc 1.0610
arctic25,dgl,1,639,27.5681,1.0610

epoch:641/50, training loss:0.17273558676242828
Train Acc 1.0639
 Acc 1.0576
arctic25,dgl,1,640,27.6106,1.0576

epoch:642/50, training loss:0.1737453043460846
Train Acc 1.0604
 Acc 1.0610
new best val f1: 1.060979771859125
arctic25,dgl,1,641,27.6531,1.0610

epoch:643/50, training loss:0.17211177945137024
Train Acc 1.0639
 Acc 1.0611
new best val f1: 1.0611176015016868
arctic25,dgl,1,642,27.6956,1.0611

epoch:644/50, training loss:0.1716003566980362
Train Acc 1.0641
 Acc 1.0580
arctic25,dgl,1,643,27.7380,1.0580

epoch:645/50, training loss:0.17273172736167908
Train Acc 1.0610
 Acc 1.0612
new best val f1: 1.0611701080321865
arctic25,dgl,1,644,27.7805,1.0612

epoch:646/50, training loss:0.1713746339082718
Train Acc 1.0641
 Acc 1.0611
arctic25,dgl,1,645,27.8232,1.0611

epoch:647/50, training loss:0.17069779336452484
Train Acc 1.0641
 Acc 1.0576
arctic25,dgl,1,646,27.8657,1.0576

epoch:648/50, training loss:0.17281070351600647
Train Acc 1.0605
 Acc 1.0610
arctic25,dgl,1,647,27.9081,1.0610

epoch:649/50, training loss:0.17258593440055847
Train Acc 1.0639
 Acc 1.0612
new best val f1: 1.0612029246137489
arctic25,dgl,1,648,27.9507,1.0612

epoch:650/50, training loss:0.17160888016223907
Train Acc 1.0641
 Acc 1.0576
arctic25,dgl,1,649,27.9931,1.0576

epoch:651/50, training loss:0.1726357638835907
Train Acc 1.0606
 Acc 1.0613
new best val f1: 1.061275121093186
arctic25,dgl,1,650,28.0356,1.0613

epoch:652/50, training loss:0.17087581753730774
Train Acc 1.0642
 Acc 1.0612
arctic25,dgl,1,651,28.0781,1.0612

epoch:653/50, training loss:0.1702631562948227
Train Acc 1.0641
 Acc 1.0581
arctic25,dgl,1,652,28.1206,1.0581

epoch:654/50, training loss:0.17132364213466644
Train Acc 1.0611
 Acc 1.0612
arctic25,dgl,1,653,28.1631,1.0612

epoch:655/50, training loss:0.16972635686397552
Train Acc 1.0641
 Acc 1.0614
new best val f1: 1.0613932607868104
arctic25,dgl,1,654,28.2056,1.0614

epoch:656/50, training loss:0.16920161247253418
Train Acc 1.0643
 Acc 1.0587
arctic25,dgl,1,655,28.2481,1.0587

epoch:657/50, training loss:0.17049437761306763
Train Acc 1.0618
 Acc 1.0614
new best val f1: 1.0614129507357477
arctic25,dgl,1,656,28.2907,1.0614

epoch:658/50, training loss:0.16921164095401764
Train Acc 1.0643
 Acc 1.0613
arctic25,dgl,1,657,28.3333,1.0613

epoch:659/50, training loss:0.1685708612203598
Train Acc 1.0642
 Acc 1.0579
arctic25,dgl,1,658,28.3763,1.0579

epoch:660/50, training loss:0.17066514492034912
Train Acc 1.0609
 Acc 1.0613
arctic25,dgl,1,659,28.4215,1.0613

epoch:661/50, training loss:0.17036767303943634
Train Acc 1.0641
 Acc 1.0615
new best val f1: 1.061544217061997
arctic25,dgl,1,660,28.4641,1.0615

epoch:662/50, training loss:0.1693182736635208
Train Acc 1.0642
 Acc 1.0577
arctic25,dgl,1,661,28.5066,1.0577

epoch:663/50, training loss:0.17106232047080994
Train Acc 1.0608
 Acc 1.0615
arctic25,dgl,1,662,28.5492,1.0615

epoch:664/50, training loss:0.16975392401218414
Train Acc 1.0643
 Acc 1.0614
arctic25,dgl,1,663,28.5916,1.0614

epoch:665/50, training loss:0.16934731602668762
Train Acc 1.0643
 Acc 1.0580
arctic25,dgl,1,664,28.6342,1.0580

epoch:666/50, training loss:0.1700076311826706
Train Acc 1.0609
 Acc 1.0614
arctic25,dgl,1,665,28.6767,1.0614

epoch:667/50, training loss:0.1681220382452011
Train Acc 1.0643
 Acc 1.0616
new best val f1: 1.0616361034903716
arctic25,dgl,1,666,28.7192,1.0616

epoch:668/50, training loss:0.16755740344524384
Train Acc 1.0644
 Acc 1.0586
arctic25,dgl,1,667,28.7617,1.0586

epoch:669/50, training loss:0.16895177960395813
Train Acc 1.0618
 Acc 1.0616
arctic25,dgl,1,668,28.8042,1.0616

epoch:670/50, training loss:0.16769501566886902
Train Acc 1.0645
 Acc 1.0616
arctic25,dgl,1,669,28.8467,1.0616

epoch:671/50, training loss:0.16703873872756958
Train Acc 1.0645
 Acc 1.0579
arctic25,dgl,1,670,28.8892,1.0579

epoch:672/50, training loss:0.16914072632789612
Train Acc 1.0609
 Acc 1.0614
arctic25,dgl,1,671,28.9318,1.0614

epoch:673/50, training loss:0.16861332952976227
Train Acc 1.0642
 Acc 1.0617
new best val f1: 1.0616623567556214
arctic25,dgl,1,672,28.9743,1.0617

epoch:674/50, training loss:0.16779589653015137
Train Acc 1.0645
 Acc 1.0582
arctic25,dgl,1,673,29.0168,1.0582

epoch:675/50, training loss:0.16913792490959167
Train Acc 1.0611
 Acc 1.0618
new best val f1: 1.0617936230818708
arctic25,dgl,1,674,29.0593,1.0618

epoch:676/50, training loss:0.1673385053873062
Train Acc 1.0646
 Acc 1.0616
arctic25,dgl,1,675,29.1018,1.0616

epoch:677/50, training loss:0.166977196931839
Train Acc 1.0645
 Acc 1.0582
arctic25,dgl,1,676,29.1443,1.0582

epoch:678/50, training loss:0.16811762750148773
Train Acc 1.0612
 Acc 1.0615
arctic25,dgl,1,677,29.1868,1.0615

epoch:679/50, training loss:0.16655561327934265
Train Acc 1.0643
 Acc 1.0618
arctic25,dgl,1,678,29.2294,1.0618

epoch:680/50, training loss:0.16589654982089996
Train Acc 1.0645
 Acc 1.0583
arctic25,dgl,1,679,29.2719,1.0583

epoch:681/50, training loss:0.16813108325004578
Train Acc 1.0612
 Acc 1.0619
new best val f1: 1.0618789461939329
arctic25,dgl,1,680,29.3144,1.0619

epoch:682/50, training loss:0.16752555966377258
Train Acc 1.0647
 Acc 1.0618
arctic25,dgl,1,681,29.3569,1.0618

epoch:683/50, training loss:0.16677634418010712
Train Acc 1.0646
 Acc 1.0578
arctic25,dgl,1,682,29.3994,1.0578

epoch:684/50, training loss:0.16787022352218628
Train Acc 1.0609
 Acc 1.0615
arctic25,dgl,1,683,29.4419,1.0615

epoch:685/50, training loss:0.16624470055103302
Train Acc 1.0644
 Acc 1.0618
arctic25,dgl,1,684,29.4844,1.0618

epoch:686/50, training loss:0.16550612449645996
Train Acc 1.0645
 Acc 1.0582
arctic25,dgl,1,685,29.5270,1.0582

epoch:687/50, training loss:0.16739536821842194
Train Acc 1.0612
 Acc 1.0621
new best val f1: 1.0620824089996193
arctic25,dgl,1,686,29.5695,1.0621

epoch:688/50, training loss:0.16623803973197937
Train Acc 1.0649
 Acc 1.0619
arctic25,dgl,1,687,29.6121,1.0619

epoch:689/50, training loss:0.16572532057762146
Train Acc 1.0648
 Acc 1.0583
arctic25,dgl,1,688,29.6545,1.0583

epoch:690/50, training loss:0.1664908081293106
Train Acc 1.0614
 Acc 1.0616
arctic25,dgl,1,689,29.6971,1.0616

epoch:691/50, training loss:0.16451583802700043
Train Acc 1.0645
 Acc 1.0619
arctic25,dgl,1,690,29.7396,1.0619

epoch:692/50, training loss:0.16409721970558167
Train Acc 1.0647
 Acc 1.0589
arctic25,dgl,1,691,29.7821,1.0589

epoch:693/50, training loss:0.1654432713985443
Train Acc 1.0622
 Acc 1.0622
new best val f1: 1.0621546054790565
arctic25,dgl,1,692,29.8246,1.0622

epoch:694/50, training loss:0.1640348881483078
Train Acc 1.0651
 Acc 1.0620
arctic25,dgl,1,693,29.8671,1.0620

epoch:695/50, training loss:0.16346146166324615
Train Acc 1.0649
 Acc 1.0587
arctic25,dgl,1,694,29.9096,1.0587

epoch:696/50, training loss:0.16509541869163513
Train Acc 1.0619
 Acc 1.0618
arctic25,dgl,1,695,29.9521,1.0618

epoch:697/50, training loss:0.1641480177640915
Train Acc 1.0647
 Acc 1.0619
arctic25,dgl,1,696,29.9947,1.0619

epoch:698/50, training loss:0.16329634189605713
Train Acc 1.0649
 Acc 1.0586
arctic25,dgl,1,697,30.0372,1.0586

epoch:699/50, training loss:0.16525743901729584
Train Acc 1.0617
 Acc 1.0621
arctic25,dgl,1,698,30.0797,1.0621

epoch:700/50, training loss:0.16422128677368164
Train Acc 1.0650
 Acc 1.0621
arctic25,dgl,1,699,30.1222,1.0621

epoch:701/50, training loss:0.16372591257095337
Train Acc 1.0649
 Acc 1.0589
arctic25,dgl,1,700,30.1648,1.0589

epoch:702/50, training loss:0.1640535295009613
Train Acc 1.0622
 Acc 1.0621
arctic25,dgl,1,701,30.2073,1.0621

epoch:703/50, training loss:0.16199994087219238
Train Acc 1.0649
 Acc 1.0621
arctic25,dgl,1,702,30.2498,1.0621

epoch:704/50, training loss:0.16163083910942078
Train Acc 1.0650
 Acc 1.0594
arctic25,dgl,1,703,30.2923,1.0594

epoch:705/50, training loss:0.16318853199481964
Train Acc 1.0626
 Acc 1.0621
arctic25,dgl,1,704,30.3349,1.0621

epoch:706/50, training loss:0.1626834273338318
Train Acc 1.0650
 Acc 1.0623
new best val f1: 1.0622727451726808
arctic25,dgl,1,705,30.3773,1.0623

epoch:707/50, training loss:0.16164834797382355
Train Acc 1.0652
 Acc 1.0583
arctic25,dgl,1,706,30.4198,1.0583

epoch:708/50, training loss:0.16427142918109894
Train Acc 1.0615
 Acc 1.0621
arctic25,dgl,1,707,30.4623,1.0621

epoch:709/50, training loss:0.1642080843448639
Train Acc 1.0650
 Acc 1.0622
arctic25,dgl,1,708,30.5050,1.0622

epoch:710/50, training loss:0.1636309176683426
Train Acc 1.0651
 Acc 1.0592
arctic25,dgl,1,709,30.5475,1.0592

epoch:711/50, training loss:0.16291387379169464
Train Acc 1.0623
 Acc 1.0623
new best val f1: 1.0622793084889932
arctic25,dgl,1,710,30.5900,1.0623

epoch:712/50, training loss:0.1602945774793625
Train Acc 1.0652
 Acc 1.0622
arctic25,dgl,1,711,30.6325,1.0622

epoch:713/50, training loss:0.16017866134643555
Train Acc 1.0651
 Acc 1.0602
arctic25,dgl,1,712,30.6753,1.0602

epoch:714/50, training loss:0.1609981507062912
Train Acc 1.0635
 Acc 1.0623
arctic25,dgl,1,713,30.7179,1.0623

epoch:715/50, training loss:0.16011874377727509
Train Acc 1.0653
 Acc 1.0623
new best val f1: 1.062318688386868
arctic25,dgl,1,714,30.7604,1.0623

epoch:716/50, training loss:0.15937292575836182
Train Acc 1.0653
 Acc 1.0599
arctic25,dgl,1,715,30.8028,1.0599

epoch:717/50, training loss:0.16118323802947998
Train Acc 1.0630
 Acc 1.0623
arctic25,dgl,1,716,30.8454,1.0623

epoch:718/50, training loss:0.16122007369995117
Train Acc 1.0652
 Acc 1.0624
new best val f1: 1.0623908848663053
arctic25,dgl,1,717,30.8880,1.0624

epoch:719/50, training loss:0.1599196195602417
Train Acc 1.0653
 Acc 1.0581
arctic25,dgl,1,718,30.9305,1.0581

epoch:720/50, training loss:0.16302476823329926
Train Acc 1.0614
 Acc 1.0624
new best val f1: 1.0624237014478677
arctic25,dgl,1,719,30.9730,1.0624

epoch:721/50, training loss:0.16225774586200714
Train Acc 1.0653
 Acc 1.0624
arctic25,dgl,1,720,31.0155,1.0624

epoch:722/50, training loss:0.16183553636074066
Train Acc 1.0653
 Acc 1.0593
arctic25,dgl,1,721,31.0580,1.0593

epoch:723/50, training loss:0.16113834083080292
Train Acc 1.0626
 Acc 1.0626
new best val f1: 1.0625812210393668
arctic25,dgl,1,722,31.1005,1.0626

epoch:724/50, training loss:0.15863607823848724
Train Acc 1.0655
 Acc 1.0626
new best val f1: 1.0626140376209292
arctic25,dgl,1,723,31.1430,1.0626

epoch:725/50, training loss:0.15845206379890442
Train Acc 1.0654
 Acc 1.0603
arctic25,dgl,1,724,31.1855,1.0603

epoch:726/50, training loss:0.1595848649740219
Train Acc 1.0636
 Acc 1.0625
arctic25,dgl,1,725,31.2280,1.0625

epoch:727/50, training loss:0.15924665331840515
Train Acc 1.0653
 Acc 1.0626
arctic25,dgl,1,726,31.2706,1.0626

epoch:728/50, training loss:0.15814034640789032
Train Acc 1.0655
 Acc 1.0593
arctic25,dgl,1,727,31.3131,1.0593

epoch:729/50, training loss:0.16081732511520386
Train Acc 1.0625
 Acc 1.0625
arctic25,dgl,1,728,31.3556,1.0625

epoch:730/50, training loss:0.16110779345035553
Train Acc 1.0653
 Acc 1.0624
arctic25,dgl,1,729,31.3982,1.0624

epoch:731/50, training loss:0.1598944365978241
Train Acc 1.0653
 Acc 1.0586
arctic25,dgl,1,730,31.4406,1.0586

epoch:732/50, training loss:0.1610291600227356
Train Acc 1.0619
 Acc 1.0626
new best val f1: 1.0626206009372416
arctic25,dgl,1,731,31.4832,1.0626

epoch:733/50, training loss:0.15930841863155365
Train Acc 1.0655
 Acc 1.0628
new best val f1: 1.0627584305798035
arctic25,dgl,1,732,31.5257,1.0628

epoch:734/50, training loss:0.15889139473438263
Train Acc 1.0656
 Acc 1.0592
arctic25,dgl,1,733,31.5684,1.0592

epoch:735/50, training loss:0.16017328202724457
Train Acc 1.0624
 Acc 1.0626
arctic25,dgl,1,734,31.6110,1.0626

epoch:736/50, training loss:0.15872591733932495
Train Acc 1.0655
 Acc 1.0627
arctic25,dgl,1,735,31.6535,1.0627

epoch:737/50, training loss:0.15790624916553497
Train Acc 1.0655
 Acc 1.0593
arctic25,dgl,1,736,31.6959,1.0593

epoch:738/50, training loss:0.15957728028297424
Train Acc 1.0625
 Acc 1.0627
arctic25,dgl,1,737,31.7385,1.0627

epoch:739/50, training loss:0.15870273113250732
Train Acc 1.0657
 Acc 1.0628
new best val f1: 1.062837190375553
arctic25,dgl,1,738,31.7810,1.0628

epoch:740/50, training loss:0.157962366938591
Train Acc 1.0658
 Acc 1.0592
arctic25,dgl,1,739,31.8235,1.0592

epoch:741/50, training loss:0.1593167781829834
Train Acc 1.0623
 Acc 1.0627
arctic25,dgl,1,740,31.8661,1.0627

epoch:742/50, training loss:0.15797673165798187
Train Acc 1.0656
 Acc 1.0627
arctic25,dgl,1,741,31.9086,1.0627

epoch:743/50, training loss:0.15747953951358795
Train Acc 1.0656
 Acc 1.0594
arctic25,dgl,1,742,31.9511,1.0594

epoch:744/50, training loss:0.1586819291114807
Train Acc 1.0626
 Acc 1.0629
new best val f1: 1.062896260222365
arctic25,dgl,1,743,31.9936,1.0629

epoch:745/50, training loss:0.15726925432682037
Train Acc 1.0658
 Acc 1.0630
new best val f1: 1.0629618933854899
arctic25,dgl,1,744,32.0361,1.0630

epoch:746/50, training loss:0.15668419003486633
Train Acc 1.0658
 Acc 1.0598
arctic25,dgl,1,745,32.0786,1.0598

epoch:747/50, training loss:0.157613605260849
Train Acc 1.0632
 Acc 1.0629
arctic25,dgl,1,746,32.1211,1.0629

epoch:748/50, training loss:0.1560600847005844
Train Acc 1.0657
 Acc 1.0629
arctic25,dgl,1,747,32.1636,1.0629

epoch:749/50, training loss:0.15563561022281647
Train Acc 1.0657
 Acc 1.0600
arctic25,dgl,1,748,32.2061,1.0600

epoch:750/50, training loss:0.15707892179489136
Train Acc 1.0632
 Acc 1.0629
arctic25,dgl,1,749,32.2486,1.0629

epoch:751/50, training loss:0.15623243153095245
Train Acc 1.0658
 Acc 1.0629
arctic25,dgl,1,750,32.2911,1.0629

epoch:752/50, training loss:0.15543611347675323
Train Acc 1.0659
 Acc 1.0598
arctic25,dgl,1,751,32.3336,1.0598

epoch:753/50, training loss:0.1573445200920105
Train Acc 1.0629
 Acc 1.0630
new best val f1: 1.0630012732833647
arctic25,dgl,1,752,32.3761,1.0630

epoch:754/50, training loss:0.15672704577445984
Train Acc 1.0659
 Acc 1.0629
arctic25,dgl,1,753,32.4186,1.0629

epoch:755/50, training loss:0.15570059418678284
Train Acc 1.0659
 Acc 1.0591
arctic25,dgl,1,754,32.4611,1.0591

epoch:756/50, training loss:0.157666876912117
Train Acc 1.0624
 Acc 1.0629
arctic25,dgl,1,755,32.5036,1.0629

epoch:757/50, training loss:0.15683674812316895
Train Acc 1.0658
 Acc 1.0630
arctic25,dgl,1,756,32.5462,1.0630

epoch:758/50, training loss:0.15613053739070892
Train Acc 1.0659
 Acc 1.0595
arctic25,dgl,1,757,32.5887,1.0595

epoch:759/50, training loss:0.15698286890983582
Train Acc 1.0627
 Acc 1.0631
new best val f1: 1.063145666242239
arctic25,dgl,1,758,32.6312,1.0631

epoch:760/50, training loss:0.15503203868865967
Train Acc 1.0660
 Acc 1.0632
new best val f1: 1.0631587928748638
arctic25,dgl,1,759,32.6738,1.0632

epoch:761/50, training loss:0.15461908280849457
Train Acc 1.0660
 Acc 1.0600
arctic25,dgl,1,760,32.7163,1.0600

epoch:762/50, training loss:0.15575578808784485
Train Acc 1.0634
 Acc 1.0631
arctic25,dgl,1,761,32.7589,1.0631

epoch:763/50, training loss:0.15429404377937317
Train Acc 1.0660
 Acc 1.0632
arctic25,dgl,1,762,32.8014,1.0632

epoch:764/50, training loss:0.15380807220935822
Train Acc 1.0660
 Acc 1.0599
arctic25,dgl,1,763,32.8439,1.0599

epoch:765/50, training loss:0.15574900805950165
Train Acc 1.0631
 Acc 1.0631
arctic25,dgl,1,764,32.8864,1.0631

epoch:766/50, training loss:0.15533660352230072
Train Acc 1.0660
 Acc 1.0632
new best val f1: 1.063217862721676
arctic25,dgl,1,765,32.9289,1.0632

epoch:767/50, training loss:0.15451499819755554
Train Acc 1.0661
 Acc 1.0589
arctic25,dgl,1,766,32.9714,1.0589

epoch:768/50, training loss:0.15666136145591736
Train Acc 1.0621
 Acc 1.0632
arctic25,dgl,1,767,33.0140,1.0632

epoch:769/50, training loss:0.154828280210495
Train Acc 1.0660
 Acc 1.0632
new best val f1: 1.0632375526706135
arctic25,dgl,1,768,33.0565,1.0632

epoch:770/50, training loss:0.15438684821128845
Train Acc 1.0661
 Acc 1.0600
arctic25,dgl,1,769,33.0990,1.0600

epoch:771/50, training loss:0.1548793911933899
Train Acc 1.0631
 Acc 1.0633
new best val f1: 1.0633228757826754
arctic25,dgl,1,770,33.1415,1.0633

epoch:772/50, training loss:0.15311649441719055
Train Acc 1.0662
 Acc 1.0634
new best val f1: 1.0633556923642378
arctic25,dgl,1,771,33.1840,1.0634

epoch:773/50, training loss:0.1525844931602478
Train Acc 1.0662
 Acc 1.0600
arctic25,dgl,1,772,33.2265,1.0600

epoch:774/50, training loss:0.15459004044532776
Train Acc 1.0631
 Acc 1.0632
arctic25,dgl,1,773,33.2691,1.0632

epoch:775/50, training loss:0.1542893946170807
Train Acc 1.0661
 Acc 1.0633
arctic25,dgl,1,774,33.3116,1.0633

epoch:776/50, training loss:0.15344378352165222
Train Acc 1.0662
 Acc 1.0597
arctic25,dgl,1,775,33.3542,1.0597

epoch:777/50, training loss:0.1547364443540573
Train Acc 1.0628
 Acc 1.0634
new best val f1: 1.063401635578425
arctic25,dgl,1,776,33.3967,1.0634

epoch:778/50, training loss:0.1532958596944809
Train Acc 1.0662
 Acc 1.0634
arctic25,dgl,1,777,33.4393,1.0634

epoch:779/50, training loss:0.1526288241147995
Train Acc 1.0662
 Acc 1.0601
arctic25,dgl,1,778,33.4818,1.0601

epoch:780/50, training loss:0.15383945405483246
Train Acc 1.0632
 Acc 1.0634
arctic25,dgl,1,779,33.5243,1.0634

epoch:781/50, training loss:0.15249404311180115
Train Acc 1.0662
 Acc 1.0634
new best val f1: 1.0634344521599874
arctic25,dgl,1,780,33.5668,1.0634

epoch:782/50, training loss:0.15187214314937592
Train Acc 1.0662
 Acc 1.0600
arctic25,dgl,1,781,33.6094,1.0600

epoch:783/50, training loss:0.1537010669708252
Train Acc 1.0631
 Acc 1.0635
new best val f1: 1.0635329019046744
arctic25,dgl,1,782,33.6519,1.0635

epoch:784/50, training loss:0.15291278064250946
Train Acc 1.0662
 Acc 1.0635
arctic25,dgl,1,783,33.6944,1.0635

epoch:785/50, training loss:0.1522057205438614
Train Acc 1.0663
 Acc 1.0600
arctic25,dgl,1,784,33.7369,1.0600

epoch:786/50, training loss:0.15326738357543945
Train Acc 1.0632
 Acc 1.0636
new best val f1: 1.0635919717514866
arctic25,dgl,1,785,33.7794,1.0636

epoch:787/50, training loss:0.15152646601200104
Train Acc 1.0662
 Acc 1.0636
new best val f1: 1.063598535067799
arctic25,dgl,1,786,33.8220,1.0636

epoch:788/50, training loss:0.15110120177268982
Train Acc 1.0663
 Acc 1.0602
arctic25,dgl,1,787,33.8645,1.0602

epoch:789/50, training loss:0.15253625810146332
Train Acc 1.0635
 Acc 1.0636
arctic25,dgl,1,788,33.9070,1.0636

epoch:790/50, training loss:0.1514234095811844
Train Acc 1.0663
 Acc 1.0636
new best val f1: 1.063611661700424
arctic25,dgl,1,789,33.9495,1.0636

epoch:791/50, training loss:0.15067414939403534
Train Acc 1.0663
 Acc 1.0599
arctic25,dgl,1,790,33.9921,1.0599

epoch:792/50, training loss:0.15290260314941406
Train Acc 1.0629
 Acc 1.0635
arctic25,dgl,1,791,34.0346,1.0635

epoch:793/50, training loss:0.1523934155702591
Train Acc 1.0662
 Acc 1.0637
new best val f1: 1.0636576049146114
arctic25,dgl,1,792,34.0772,1.0637

epoch:794/50, training loss:0.15175947546958923
Train Acc 1.0664
 Acc 1.0601
arctic25,dgl,1,793,34.1197,1.0601

epoch:795/50, training loss:0.1522885411977768
Train Acc 1.0631
 Acc 1.0636
arctic25,dgl,1,794,34.1622,1.0636

epoch:796/50, training loss:0.15029729902744293
Train Acc 1.0663
 Acc 1.0636
arctic25,dgl,1,795,34.2047,1.0636

epoch:797/50, training loss:0.15000787377357483
Train Acc 1.0664
 Acc 1.0605
arctic25,dgl,1,796,34.2471,1.0605

epoch:798/50, training loss:0.15124402940273285
Train Acc 1.0637
 Acc 1.0636
arctic25,dgl,1,797,34.2897,1.0636

epoch:799/50, training loss:0.15046074986457825
Train Acc 1.0663
 Acc 1.0637
new best val f1: 1.0636641682309238
arctic25,dgl,1,798,34.3322,1.0637

epoch:800/50, training loss:0.14961908757686615
Train Acc 1.0665
 Acc 1.0598
arctic25,dgl,1,799,34.3747,1.0598

epoch:801/50, training loss:0.15195636451244354
Train Acc 1.0629
 Acc 1.0635
arctic25,dgl,1,800,34.4172,1.0635

epoch:802/50, training loss:0.15208564698696136
Train Acc 1.0662
 Acc 1.0636
arctic25,dgl,1,801,34.4599,1.0636

epoch:803/50, training loss:0.15122312307357788
Train Acc 1.0665
 Acc 1.0603
arctic25,dgl,1,802,34.5024,1.0603

epoch:804/50, training loss:0.15125557780265808
Train Acc 1.0636
 Acc 1.0636
arctic25,dgl,1,803,34.5449,1.0636

epoch:805/50, training loss:0.14917246997356415
Train Acc 1.0663
 Acc 1.0637
new best val f1: 1.0636904214961735
arctic25,dgl,1,804,34.5874,1.0637

epoch:806/50, training loss:0.14868243038654327
Train Acc 1.0663
 Acc 1.0606
arctic25,dgl,1,805,34.6299,1.0606

epoch:807/50, training loss:0.1505204737186432
Train Acc 1.0639
 Acc 1.0637
new best val f1: 1.0637494913429857
arctic25,dgl,1,806,34.6724,1.0637

epoch:808/50, training loss:0.14957445859909058
Train Acc 1.0665
 Acc 1.0638
new best val f1: 1.0638019978734856
arctic25,dgl,1,807,34.7150,1.0638

epoch:809/50, training loss:0.1489323377609253
Train Acc 1.0664
 Acc 1.0602
arctic25,dgl,1,808,34.7575,1.0602

epoch:810/50, training loss:0.15081080794334412
Train Acc 1.0633
 Acc 1.0636
arctic25,dgl,1,809,34.7999,1.0636

epoch:811/50, training loss:0.15051744878292084
Train Acc 1.0666
 Acc 1.0635
arctic25,dgl,1,810,34.8424,1.0635

epoch:812/50, training loss:0.1498384326696396
Train Acc 1.0663
 Acc 1.0600
arctic25,dgl,1,811,34.8850,1.0600

epoch:813/50, training loss:0.15047816932201385
Train Acc 1.0631
 Acc 1.0637
arctic25,dgl,1,812,34.9275,1.0637

epoch:814/50, training loss:0.14918610453605652
Train Acc 1.0666
 Acc 1.0639
new best val f1: 1.0638610677202978
arctic25,dgl,1,813,34.9700,1.0639

epoch:815/50, training loss:0.1482691913843155
Train Acc 1.0666
 Acc 1.0604
arctic25,dgl,1,814,35.0125,1.0604

epoch:816/50, training loss:0.14979852735996246
Train Acc 1.0634
 Acc 1.0638
arctic25,dgl,1,815,35.0551,1.0638

epoch:817/50, training loss:0.14833669364452362
Train Acc 1.0666
 Acc 1.0640
new best val f1: 1.0639726440976096
arctic25,dgl,1,816,35.0976,1.0640

epoch:818/50, training loss:0.14776259660720825
Train Acc 1.0668
 Acc 1.0603
arctic25,dgl,1,817,35.1400,1.0603

epoch:819/50, training loss:0.14970763027668
Train Acc 1.0633
 Acc 1.0638
arctic25,dgl,1,818,35.1826,1.0638

epoch:820/50, training loss:0.148698091506958
Train Acc 1.0666
 Acc 1.0637
arctic25,dgl,1,819,35.2251,1.0637

epoch:821/50, training loss:0.14843595027923584
Train Acc 1.0667
 Acc 1.0602
arctic25,dgl,1,820,35.2676,1.0602

epoch:822/50, training loss:0.1494077891111374
Train Acc 1.0633
 Acc 1.0640
new best val f1: 1.0640317139444218
arctic25,dgl,1,821,35.3101,1.0640

epoch:823/50, training loss:0.14813949167728424
Train Acc 1.0668
 Acc 1.0640
arctic25,dgl,1,822,35.3526,1.0640

epoch:824/50, training loss:0.1475314199924469
Train Acc 1.0669
 Acc 1.0604
arctic25,dgl,1,823,35.3951,1.0604

epoch:825/50, training loss:0.14873391389846802
Train Acc 1.0636
 Acc 1.0639
arctic25,dgl,1,824,35.4376,1.0639

epoch:826/50, training loss:0.1474425047636032
Train Acc 1.0667
 Acc 1.0642
new best val f1: 1.0641629802706711
arctic25,dgl,1,825,35.4802,1.0642

epoch:827/50, training loss:0.14669764041900635
Train Acc 1.0669
 Acc 1.0606
arctic25,dgl,1,826,35.5227,1.0606

epoch:828/50, training loss:0.14844875037670135
Train Acc 1.0638
 Acc 1.0641
arctic25,dgl,1,827,35.5652,1.0641

epoch:829/50, training loss:0.14716951549053192
Train Acc 1.0669
 Acc 1.0640
arctic25,dgl,1,828,35.6077,1.0640

epoch:830/50, training loss:0.1467217355966568
Train Acc 1.0668
 Acc 1.0606
arctic25,dgl,1,829,35.6502,1.0606

epoch:831/50, training loss:0.1478487253189087
Train Acc 1.0638
 Acc 1.0639
arctic25,dgl,1,830,35.6927,1.0639

epoch:832/50, training loss:0.14632995426654816
Train Acc 1.0669
 Acc 1.0642
new best val f1: 1.0642089234848584
arctic25,dgl,1,831,35.7352,1.0642

epoch:833/50, training loss:0.14588221907615662
Train Acc 1.0669
 Acc 1.0613
arctic25,dgl,1,832,35.7777,1.0613

epoch:834/50, training loss:0.14699698984622955
Train Acc 1.0644
 Acc 1.0642
new best val f1: 1.0642220501174833
arctic25,dgl,1,833,35.8202,1.0642

epoch:835/50, training loss:0.14575017988681793
Train Acc 1.0670
 Acc 1.0641
arctic25,dgl,1,834,35.8627,1.0641

epoch:836/50, training loss:0.14507603645324707
Train Acc 1.0669
 Acc 1.0609
arctic25,dgl,1,835,35.9053,1.0609

epoch:837/50, training loss:0.14671674370765686
Train Acc 1.0640
 Acc 1.0642
arctic25,dgl,1,836,35.9478,1.0642

epoch:838/50, training loss:0.1459396481513977
Train Acc 1.0670
 Acc 1.0643
new best val f1: 1.064300809913233
arctic25,dgl,1,837,35.9904,1.0643

epoch:839/50, training loss:0.1450410932302475
Train Acc 1.0671
 Acc 1.0609
arctic25,dgl,1,838,36.0328,1.0609

epoch:840/50, training loss:0.14688658714294434
Train Acc 1.0640
 Acc 1.0642
arctic25,dgl,1,839,36.0754,1.0642

epoch:841/50, training loss:0.14601898193359375
Train Acc 1.0671
 Acc 1.0641
arctic25,dgl,1,840,36.1179,1.0641

epoch:842/50, training loss:0.14524731040000916
Train Acc 1.0670
 Acc 1.0603
arctic25,dgl,1,841,36.1605,1.0603

epoch:843/50, training loss:0.14698222279548645
Train Acc 1.0635
 Acc 1.0643
arctic25,dgl,1,842,36.2030,1.0643

epoch:844/50, training loss:0.14593687653541565
Train Acc 1.0672
 Acc 1.0644
new best val f1: 1.0643664430763575
arctic25,dgl,1,843,36.2455,1.0644

epoch:845/50, training loss:0.14549000561237335
Train Acc 1.0673
 Acc 1.0611
arctic25,dgl,1,844,36.2880,1.0611

epoch:846/50, training loss:0.14613258838653564
Train Acc 1.0642
 Acc 1.0644
new best val f1: 1.0643926963416075
arctic25,dgl,1,845,36.3305,1.0644

epoch:847/50, training loss:0.14427107572555542
Train Acc 1.0672
 Acc 1.0641
arctic25,dgl,1,846,36.3730,1.0641

epoch:848/50, training loss:0.14398032426834106
Train Acc 1.0670
 Acc 1.0610
arctic25,dgl,1,847,36.4155,1.0610

epoch:849/50, training loss:0.14532239735126495
Train Acc 1.0643
 Acc 1.0643
arctic25,dgl,1,848,36.4580,1.0643

epoch:850/50, training loss:0.1443505585193634
Train Acc 1.0672
 Acc 1.0645
new best val f1: 1.0644911460862945
arctic25,dgl,1,849,36.5005,1.0645

epoch:851/50, training loss:0.14357000589370728
Train Acc 1.0674
 Acc 1.0614
arctic25,dgl,1,850,36.5430,1.0614

epoch:852/50, training loss:0.14520196616649628
Train Acc 1.0645
 Acc 1.0644
arctic25,dgl,1,851,36.5854,1.0644

epoch:853/50, training loss:0.14427450299263
Train Acc 1.0672
 Acc 1.0641
arctic25,dgl,1,852,36.6280,1.0641

epoch:854/50, training loss:0.14371781051158905
Train Acc 1.0671
 Acc 1.0605
arctic25,dgl,1,853,36.6705,1.0605

epoch:855/50, training loss:0.14538784325122833
Train Acc 1.0639
 Acc 1.0643
arctic25,dgl,1,854,36.7130,1.0643

epoch:856/50, training loss:0.14427030086517334
Train Acc 1.0672
 Acc 1.0645
new best val f1: 1.0645436526167942
arctic25,dgl,1,855,36.7555,1.0645

epoch:857/50, training loss:0.14388661086559296
Train Acc 1.0674
 Acc 1.0617
arctic25,dgl,1,856,36.7979,1.0617

epoch:858/50, training loss:0.1444942057132721
Train Acc 1.0648
 Acc 1.0646
new best val f1: 1.0646289757288563
arctic25,dgl,1,857,36.8404,1.0646

epoch:859/50, training loss:0.14253190159797668
Train Acc 1.0674
 Acc 1.0641
arctic25,dgl,1,858,36.8830,1.0641

epoch:860/50, training loss:0.14234915375709534
Train Acc 1.0670
 Acc 1.0616
arctic25,dgl,1,859,36.9254,1.0616

epoch:861/50, training loss:0.14363409578800201
Train Acc 1.0647
 Acc 1.0644
arctic25,dgl,1,860,36.9679,1.0644

epoch:862/50, training loss:0.14260275661945343
Train Acc 1.0673
 Acc 1.0646
new best val f1: 1.0646421023614812
arctic25,dgl,1,861,37.0104,1.0646

epoch:863/50, training loss:0.1421164572238922
Train Acc 1.0675
 Acc 1.0614
arctic25,dgl,1,862,37.0529,1.0614

epoch:864/50, training loss:0.14406944811344147
Train Acc 1.0648
 Acc 1.0645
arctic25,dgl,1,863,37.0954,1.0645

epoch:865/50, training loss:0.1437167078256607
Train Acc 1.0674
 Acc 1.0642
arctic25,dgl,1,864,37.1379,1.0642

epoch:866/50, training loss:0.14299409091472626
Train Acc 1.0671
 Acc 1.0599
arctic25,dgl,1,865,37.1804,1.0599

epoch:867/50, training loss:0.14511388540267944
Train Acc 1.0631
 Acc 1.0645
arctic25,dgl,1,866,37.2229,1.0645

epoch:868/50, training loss:0.14319093525409698
Train Acc 1.0674
 Acc 1.0646
arctic25,dgl,1,867,37.2654,1.0646

epoch:869/50, training loss:0.1427907794713974
Train Acc 1.0675
 Acc 1.0612
arctic25,dgl,1,868,37.3080,1.0612

epoch:870/50, training loss:0.14389102160930634
Train Acc 1.0644
 Acc 1.0646
arctic25,dgl,1,869,37.3505,1.0646

epoch:871/50, training loss:0.14305274188518524
Train Acc 1.0675
 Acc 1.0641
arctic25,dgl,1,870,37.3930,1.0641

epoch:872/50, training loss:0.14240504801273346
Train Acc 1.0670
 Acc 1.0598
arctic25,dgl,1,871,37.4354,1.0598

epoch:873/50, training loss:0.144606351852417
Train Acc 1.0631
 Acc 1.0645
arctic25,dgl,1,872,37.4780,1.0645

epoch:874/50, training loss:0.14317066967487335
Train Acc 1.0674
 Acc 1.0647
new best val f1: 1.0647339887898557
arctic25,dgl,1,873,37.5205,1.0647

epoch:875/50, training loss:0.142671138048172
Train Acc 1.0676
 Acc 1.0611
arctic25,dgl,1,874,37.5630,1.0611

epoch:876/50, training loss:0.14377461373806
Train Acc 1.0644
 Acc 1.0647
arctic25,dgl,1,875,37.6055,1.0647

epoch:877/50, training loss:0.14216864109039307
Train Acc 1.0675
 Acc 1.0642
arctic25,dgl,1,876,37.6480,1.0642

epoch:878/50, training loss:0.14200936257839203
Train Acc 1.0673
 Acc 1.0602
arctic25,dgl,1,877,37.6905,1.0602

epoch:879/50, training loss:0.14398443698883057
Train Acc 1.0637
 Acc 1.0638
arctic25,dgl,1,878,37.7331,1.0638

epoch:880/50, training loss:0.1429850310087204
Train Acc 1.0669
 Acc 1.0646
arctic25,dgl,1,879,37.7756,1.0646

epoch:881/50, training loss:0.14146094024181366
Train Acc 1.0676
 Acc 1.0604
arctic25,dgl,1,880,37.8181,1.0604

epoch:882/50, training loss:0.1448378562927246
Train Acc 1.0637
 Acc 1.0650
new best val f1: 1.065003084758667
arctic25,dgl,1,881,37.8607,1.0650

epoch:883/50, training loss:0.14235705137252808
Train Acc 1.0679
 Acc 1.0642
arctic25,dgl,1,882,37.9032,1.0642

epoch:884/50, training loss:0.14279985427856445
Train Acc 1.0672
 Acc 1.0604
arctic25,dgl,1,883,37.9457,1.0604

epoch:885/50, training loss:0.143081396818161
Train Acc 1.0637
 Acc 1.0638
arctic25,dgl,1,884,37.9883,1.0638

epoch:886/50, training loss:0.14066804945468903
Train Acc 1.0669
 Acc 1.0645
arctic25,dgl,1,885,38.0309,1.0645

epoch:887/50, training loss:0.13984836637973785
Train Acc 1.0674
 Acc 1.0613
arctic25,dgl,1,886,38.0733,1.0613

epoch:888/50, training loss:0.14279118180274963
Train Acc 1.0647
 Acc 1.0650
arctic25,dgl,1,887,38.1158,1.0650

epoch:889/50, training loss:0.14249488711357117
Train Acc 1.0679
 Acc 1.0648
arctic25,dgl,1,888,38.1584,1.0648

epoch:890/50, training loss:0.14207994937896729
Train Acc 1.0678
 Acc 1.0606
arctic25,dgl,1,889,38.2009,1.0606

epoch:891/50, training loss:0.14258907735347748
Train Acc 1.0640
 Acc 1.0641
arctic25,dgl,1,890,38.2433,1.0641

epoch:892/50, training loss:0.1402420848608017
Train Acc 1.0672
 Acc 1.0647
arctic25,dgl,1,891,38.2859,1.0647

epoch:893/50, training loss:0.13964246213436127
Train Acc 1.0676
 Acc 1.0617
arctic25,dgl,1,892,38.3284,1.0617

epoch:894/50, training loss:0.14162912964820862
Train Acc 1.0651
 Acc 1.0648
arctic25,dgl,1,893,38.3709,1.0648

epoch:895/50, training loss:0.14075398445129395
Train Acc 1.0678
 Acc 1.0647
arctic25,dgl,1,894,38.4134,1.0647

epoch:896/50, training loss:0.13998670876026154
Train Acc 1.0676
 Acc 1.0609
arctic25,dgl,1,895,38.4559,1.0609

epoch:897/50, training loss:0.14173753559589386
Train Acc 1.0643
 Acc 1.0647
arctic25,dgl,1,896,38.4984,1.0647

epoch:898/50, training loss:0.14082151651382446
Train Acc 1.0677
 Acc 1.0646
arctic25,dgl,1,897,38.5409,1.0646

epoch:899/50, training loss:0.14026910066604614
Train Acc 1.0674
 Acc 1.0612
arctic25,dgl,1,898,38.5834,1.0612

epoch:900/50, training loss:0.14077267050743103
Train Acc 1.0646
 Acc 1.0648
arctic25,dgl,1,899,38.6259,1.0648

epoch:901/50, training loss:0.13904456794261932
Train Acc 1.0678
 Acc 1.0649
arctic25,dgl,1,900,38.6684,1.0649

epoch:902/50, training loss:0.13840262591838837
Train Acc 1.0679
 Acc 1.0615
arctic25,dgl,1,901,38.7109,1.0615

epoch:903/50, training loss:0.14041925966739655
Train Acc 1.0647
 Acc 1.0651
new best val f1: 1.0650949711870414
arctic25,dgl,1,902,38.7535,1.0651

epoch:904/50, training loss:0.13986478745937347
Train Acc 1.0681
 Acc 1.0650
arctic25,dgl,1,903,38.7960,1.0650

epoch:905/50, training loss:0.1392209827899933
Train Acc 1.0679
 Acc 1.0607
arctic25,dgl,1,904,38.8385,1.0607

epoch:906/50, training loss:0.14083892107009888
Train Acc 1.0639
 Acc 1.0646
arctic25,dgl,1,905,38.8810,1.0646

epoch:907/50, training loss:0.13920564949512482
Train Acc 1.0677
 Acc 1.0650
arctic25,dgl,1,906,38.9236,1.0650

epoch:908/50, training loss:0.13889706134796143
Train Acc 1.0680
 Acc 1.0621
arctic25,dgl,1,907,38.9661,1.0621

epoch:909/50, training loss:0.13927944004535675
Train Acc 1.0655
 Acc 1.0651
arctic25,dgl,1,908,39.0086,1.0651

epoch:910/50, training loss:0.13737978041172028
Train Acc 1.0680
 Acc 1.0649
arctic25,dgl,1,909,39.0511,1.0649

epoch:911/50, training loss:0.13709545135498047
Train Acc 1.0679
 Acc 1.0624
arctic25,dgl,1,910,39.0935,1.0624

epoch:912/50, training loss:0.13854098320007324
Train Acc 1.0656
 Acc 1.0649
arctic25,dgl,1,911,39.1360,1.0649

epoch:913/50, training loss:0.13803716003894806
Train Acc 1.0679
 Acc 1.0651
new best val f1: 1.0651277877686036
arctic25,dgl,1,912,39.1785,1.0651

epoch:914/50, training loss:0.13697190582752228
Train Acc 1.0680
 Acc 1.0614
arctic25,dgl,1,913,39.2210,1.0614

epoch:915/50, training loss:0.1395760476589203
Train Acc 1.0649
 Acc 1.0652
new best val f1: 1.0651802942991035
arctic25,dgl,1,914,39.2636,1.0652

epoch:916/50, training loss:0.13954688608646393
Train Acc 1.0681
 Acc 1.0651
arctic25,dgl,1,915,39.3061,1.0651

epoch:917/50, training loss:0.13895170390605927
Train Acc 1.0679
 Acc 1.0617
arctic25,dgl,1,916,39.3485,1.0617

epoch:918/50, training loss:0.13876046240329742
Train Acc 1.0650
 Acc 1.0652
new best val f1: 1.0651999842480409
arctic25,dgl,1,917,39.3910,1.0652

epoch:919/50, training loss:0.1363055557012558
Train Acc 1.0682
 Acc 1.0653
new best val f1: 1.0652524907785406
arctic25,dgl,1,918,39.4335,1.0653

epoch:920/50, training loss:0.13613122701644897
Train Acc 1.0682
 Acc 1.0629
arctic25,dgl,1,919,39.4760,1.0629

epoch:921/50, training loss:0.13705255091190338
Train Acc 1.0662
 Acc 1.0651
arctic25,dgl,1,920,39.5185,1.0651

epoch:922/50, training loss:0.13594505190849304
Train Acc 1.0681
 Acc 1.0654
new best val f1: 1.06535750383954
arctic25,dgl,1,921,39.5610,1.0654

epoch:923/50, training loss:0.135495126247406
Train Acc 1.0683
 Acc 1.0626
arctic25,dgl,1,922,39.6035,1.0626

epoch:924/50, training loss:0.1372305303812027
Train Acc 1.0658
 Acc 1.0651
arctic25,dgl,1,923,39.6460,1.0651

epoch:925/50, training loss:0.1372775286436081
Train Acc 1.0681
 Acc 1.0652
arctic25,dgl,1,924,39.6886,1.0652

epoch:926/50, training loss:0.13613247871398926
Train Acc 1.0681
 Acc 1.0609
arctic25,dgl,1,925,39.7310,1.0609

epoch:927/50, training loss:0.13908648490905762
Train Acc 1.0642
 Acc 1.0653
arctic25,dgl,1,926,39.7736,1.0653

epoch:928/50, training loss:0.13801808655261993
Train Acc 1.0682
 Acc 1.0651
arctic25,dgl,1,927,39.8161,1.0651

epoch:929/50, training loss:0.13758449256420135
Train Acc 1.0681
 Acc 1.0619
arctic25,dgl,1,928,39.8587,1.0619

epoch:930/50, training loss:0.13773584365844727
Train Acc 1.0653
 Acc 1.0654
new best val f1: 1.0654034470537272
arctic25,dgl,1,929,39.9012,1.0654

epoch:931/50, training loss:0.1357833594083786
Train Acc 1.0684
 Acc 1.0651
arctic25,dgl,1,930,39.9437,1.0651

epoch:932/50, training loss:0.13534726202487946
Train Acc 1.0681
 Acc 1.0620
arctic25,dgl,1,931,39.9862,1.0620

epoch:933/50, training loss:0.1370888501405716
Train Acc 1.0654
 Acc 1.0653
arctic25,dgl,1,932,40.0288,1.0653

epoch:934/50, training loss:0.1367575079202652
Train Acc 1.0683
 Acc 1.0654
new best val f1: 1.065442826951602
arctic25,dgl,1,933,40.0712,1.0654

epoch:935/50, training loss:0.1356106847524643
Train Acc 1.0684
 Acc 1.0606
arctic25,dgl,1,934,40.1137,1.0606

epoch:936/50, training loss:0.1387239694595337
Train Acc 1.0640
 Acc 1.0653
arctic25,dgl,1,935,40.1563,1.0653

epoch:937/50, training loss:0.13683122396469116
Train Acc 1.0682
 Acc 1.0654
arctic25,dgl,1,936,40.1988,1.0654

epoch:938/50, training loss:0.13653695583343506
Train Acc 1.0683
 Acc 1.0618
arctic25,dgl,1,937,40.2413,1.0618

epoch:939/50, training loss:0.13704133033752441
Train Acc 1.0652
 Acc 1.0652
arctic25,dgl,1,938,40.2839,1.0652

epoch:940/50, training loss:0.13531233370304108
Train Acc 1.0683
 Acc 1.0654
arctic25,dgl,1,939,40.3264,1.0654

epoch:941/50, training loss:0.1348695009946823
Train Acc 1.0684
 Acc 1.0620
arctic25,dgl,1,940,40.3689,1.0620

epoch:942/50, training loss:0.13673336803913116
Train Acc 1.0654
 Acc 1.0653
arctic25,dgl,1,941,40.4114,1.0653

epoch:943/50, training loss:0.1362735629081726
Train Acc 1.0683
 Acc 1.0653
arctic25,dgl,1,942,40.4539,1.0653

epoch:944/50, training loss:0.1355682611465454
Train Acc 1.0682
 Acc 1.0613
arctic25,dgl,1,943,40.4964,1.0613

epoch:945/50, training loss:0.13725320994853973
Train Acc 1.0647
 Acc 1.0654
arctic25,dgl,1,944,40.5389,1.0654

epoch:946/50, training loss:0.1357048898935318
Train Acc 1.0684
 Acc 1.0653
arctic25,dgl,1,945,40.5815,1.0653

epoch:947/50, training loss:0.13520148396492004
Train Acc 1.0684
 Acc 1.0618
arctic25,dgl,1,946,40.6239,1.0618

epoch:948/50, training loss:0.13637670874595642
Train Acc 1.0652
 Acc 1.0655
new best val f1: 1.0655478400126015
arctic25,dgl,1,947,40.6665,1.0655

epoch:949/50, training loss:0.1352284699678421
Train Acc 1.0685
 Acc 1.0654
arctic25,dgl,1,948,40.7089,1.0654

epoch:950/50, training loss:0.13453926146030426
Train Acc 1.0684
 Acc 1.0614
arctic25,dgl,1,949,40.7514,1.0614

epoch:951/50, training loss:0.1364930272102356
Train Acc 1.0649
 Acc 1.0655
arctic25,dgl,1,950,40.7940,1.0655

epoch:952/50, training loss:0.13588178157806396
Train Acc 1.0685
 Acc 1.0654
arctic25,dgl,1,951,40.8366,1.0654

epoch:953/50, training loss:0.1352342665195465
Train Acc 1.0685
 Acc 1.0615
arctic25,dgl,1,952,40.8791,1.0615

epoch:954/50, training loss:0.13625669479370117
Train Acc 1.0650
 Acc 1.0655
arctic25,dgl,1,953,40.9216,1.0655

epoch:955/50, training loss:0.1347750574350357
Train Acc 1.0685
 Acc 1.0655
arctic25,dgl,1,954,40.9641,1.0655

epoch:956/50, training loss:0.13422316312789917
Train Acc 1.0685
 Acc 1.0617
arctic25,dgl,1,955,41.0065,1.0617

epoch:957/50, training loss:0.13572406768798828
Train Acc 1.0652
 Acc 1.0655
arctic25,dgl,1,956,41.0490,1.0655

epoch:958/50, training loss:0.13458865880966187
Train Acc 1.0685
 Acc 1.0656
new best val f1: 1.0655937832267888
arctic25,dgl,1,957,41.0915,1.0656

epoch:959/50, training loss:0.1339428424835205
Train Acc 1.0686
 Acc 1.0619
arctic25,dgl,1,958,41.1339,1.0619

epoch:960/50, training loss:0.13547135889530182
Train Acc 1.0653
 Acc 1.0656
arctic25,dgl,1,959,41.1765,1.0656

epoch:961/50, training loss:0.13425040245056152
Train Acc 1.0685
 Acc 1.0655
arctic25,dgl,1,960,41.2190,1.0655

epoch:962/50, training loss:0.13373570144176483
Train Acc 1.0686
 Acc 1.0622
arctic25,dgl,1,961,41.2615,1.0622

epoch:963/50, training loss:0.13469824194908142
Train Acc 1.0656
 Acc 1.0655
arctic25,dgl,1,962,41.3040,1.0655

epoch:964/50, training loss:0.1329287737607956
Train Acc 1.0686
 Acc 1.0657
new best val f1: 1.0656594163899136
arctic25,dgl,1,963,41.3465,1.0657

epoch:965/50, training loss:0.13268834352493286
Train Acc 1.0686
 Acc 1.0629
arctic25,dgl,1,964,41.3890,1.0629

epoch:966/50, training loss:0.1338430941104889
Train Acc 1.0660
 Acc 1.0656
arctic25,dgl,1,965,41.4315,1.0656

epoch:967/50, training loss:0.132661834359169
Train Acc 1.0686
 Acc 1.0656
arctic25,dgl,1,966,41.4739,1.0656

epoch:968/50, training loss:0.13221518695354462
Train Acc 1.0686
 Acc 1.0623
arctic25,dgl,1,967,41.5164,1.0623

epoch:969/50, training loss:0.13414818048477173
Train Acc 1.0657
 Acc 1.0656
arctic25,dgl,1,968,41.5590,1.0656

epoch:970/50, training loss:0.1337401419878006
Train Acc 1.0686
 Acc 1.0656
arctic25,dgl,1,969,41.6016,1.0656

epoch:971/50, training loss:0.13281506299972534
Train Acc 1.0687
 Acc 1.0618
arctic25,dgl,1,970,41.6441,1.0618

epoch:972/50, training loss:0.13471092283725739
Train Acc 1.0653
 Acc 1.0656
arctic25,dgl,1,971,41.6866,1.0656

epoch:973/50, training loss:0.1338862031698227
Train Acc 1.0686
 Acc 1.0655
arctic25,dgl,1,972,41.7291,1.0655

epoch:974/50, training loss:0.13316354155540466
Train Acc 1.0685
 Acc 1.0620
arctic25,dgl,1,973,41.7716,1.0620

epoch:975/50, training loss:0.13407151401042938
Train Acc 1.0655
 Acc 1.0656
arctic25,dgl,1,974,41.8141,1.0656

epoch:976/50, training loss:0.13237988948822021
Train Acc 1.0687
 Acc 1.0657
new best val f1: 1.0657184862367257
arctic25,dgl,1,975,41.8567,1.0657

epoch:977/50, training loss:0.13188201189041138
Train Acc 1.0687
 Acc 1.0626
arctic25,dgl,1,976,41.8992,1.0626

epoch:978/50, training loss:0.13328681886196136
Train Acc 1.0660
 Acc 1.0657
arctic25,dgl,1,977,41.9417,1.0657

epoch:979/50, training loss:0.1320788562297821
Train Acc 1.0688
 Acc 1.0656
arctic25,dgl,1,978,41.9842,1.0656

epoch:980/50, training loss:0.13152466714382172
Train Acc 1.0686
 Acc 1.0623
arctic25,dgl,1,979,42.0267,1.0623

epoch:981/50, training loss:0.13326683640480042
Train Acc 1.0657
 Acc 1.0657
arctic25,dgl,1,980,42.0692,1.0657

epoch:982/50, training loss:0.1325361281633377
Train Acc 1.0688
 Acc 1.0657
arctic25,dgl,1,981,42.1118,1.0657

epoch:983/50, training loss:0.131745383143425
Train Acc 1.0688
 Acc 1.0623
arctic25,dgl,1,982,42.1543,1.0623

epoch:984/50, training loss:0.13326618075370789
Train Acc 1.0657
 Acc 1.0658
new best val f1: 1.0657513028182881
arctic25,dgl,1,983,42.1968,1.0658

epoch:985/50, training loss:0.13193832337856293
Train Acc 1.0688
 Acc 1.0657
arctic25,dgl,1,984,42.2393,1.0657

epoch:986/50, training loss:0.13150754570960999
Train Acc 1.0687
 Acc 1.0623
arctic25,dgl,1,985,42.2818,1.0623

epoch:987/50, training loss:0.13282038271427155
Train Acc 1.0658
 Acc 1.0657
arctic25,dgl,1,986,42.3243,1.0657

epoch:988/50, training loss:0.13150769472122192
Train Acc 1.0687
 Acc 1.0658
new best val f1: 1.0657578661346006
arctic25,dgl,1,987,42.3668,1.0658

epoch:989/50, training loss:0.13093465566635132
Train Acc 1.0689
 Acc 1.0624
arctic25,dgl,1,988,42.4093,1.0624

epoch:990/50, training loss:0.13262152671813965
Train Acc 1.0659
 Acc 1.0657
arctic25,dgl,1,989,42.4518,1.0657

epoch:991/50, training loss:0.13168853521347046
Train Acc 1.0687
 Acc 1.0657
arctic25,dgl,1,990,42.4942,1.0657

epoch:992/50, training loss:0.13092711567878723
Train Acc 1.0687
 Acc 1.0620
arctic25,dgl,1,991,42.5368,1.0620

epoch:993/50, training loss:0.13280828297138214
Train Acc 1.0654
 Acc 1.0656
arctic25,dgl,1,992,42.5793,1.0656

epoch:994/50, training loss:0.1320149451494217
Train Acc 1.0688
 Acc 1.0658
new best val f1: 1.065764429450913
arctic25,dgl,1,993,42.6218,1.0658

epoch:995/50, training loss:0.13134117424488068
Train Acc 1.0689
 Acc 1.0623
arctic25,dgl,1,994,42.6643,1.0623

epoch:996/50, training loss:0.13247711956501007
Train Acc 1.0657
 Acc 1.0659
new best val f1: 1.0658891324608497
arctic25,dgl,1,995,42.7068,1.0659

epoch:997/50, training loss:0.13108514249324799
Train Acc 1.0689
 Acc 1.0657
arctic25,dgl,1,996,42.7493,1.0657

epoch:998/50, training loss:0.13043975830078125
Train Acc 1.0686
 Acc 1.0619
arctic25,dgl,1,997,42.7917,1.0619

epoch:999/50, training loss:0.13245265185832977
Train Acc 1.0653
 Acc 1.0657
arctic25,dgl,1,998,42.8342,1.0657

epoch:1000/50, training loss:0.1320471465587616
Train Acc 1.0689
 Acc 1.0658
arctic25,dgl,1,999,42.8767,1.0658

epoch:1001/50, training loss:0.1312350034713745
Train Acc 1.0690
 Acc 1.0621
arctic25,dgl,1,1000,42.9193,1.0621

epoch:1002/50, training loss:0.13238206505775452
Train Acc 1.0656
 Acc 1.0658
arctic25,dgl,1,1001,42.9619,1.0658

epoch:1003/50, training loss:0.13073959946632385
Train Acc 1.0688
 Acc 1.0656
arctic25,dgl,1,1002,43.0044,1.0656

epoch:1004/50, training loss:0.13034425675868988
Train Acc 1.0686
 Acc 1.0621
arctic25,dgl,1,1003,43.0468,1.0621

epoch:1005/50, training loss:0.13162778317928314
Train Acc 1.0658
 Acc 1.0658
arctic25,dgl,1,1004,43.0893,1.0658

epoch:1006/50, training loss:0.13037548959255219
Train Acc 1.0690
 Acc 1.0659
new best val f1: 1.0659285123587245
arctic25,dgl,1,1005,43.1318,1.0659

epoch:1007/50, training loss:0.12974798679351807
Train Acc 1.0690
 Acc 1.0623
arctic25,dgl,1,1006,43.1743,1.0623

epoch:1008/50, training loss:0.13170082867145538
Train Acc 1.0657
 Acc 1.0657
arctic25,dgl,1,1007,43.2169,1.0657

epoch:1009/50, training loss:0.1311495453119278
Train Acc 1.0689
 Acc 1.0656
arctic25,dgl,1,1008,43.2594,1.0656

epoch:1010/50, training loss:0.1304958313703537
Train Acc 1.0685
 Acc 1.0617
arctic25,dgl,1,1009,43.3019,1.0617

epoch:1011/50, training loss:0.13184385001659393
Train Acc 1.0653
 Acc 1.0658
arctic25,dgl,1,1010,43.3445,1.0658

epoch:1012/50, training loss:0.1301213502883911
Train Acc 1.0690
 Acc 1.0660
new best val f1: 1.0659678922565994
arctic25,dgl,1,1011,43.3870,1.0660

epoch:1013/50, training loss:0.12964819371700287
Train Acc 1.0691
 Acc 1.0628
arctic25,dgl,1,1012,43.4295,1.0628

epoch:1014/50, training loss:0.1306890994310379
Train Acc 1.0663
 Acc 1.0658
arctic25,dgl,1,1013,43.4720,1.0658

epoch:1015/50, training loss:0.12937988340854645
Train Acc 1.0689
 Acc 1.0657
arctic25,dgl,1,1014,43.5146,1.0657

epoch:1016/50, training loss:0.12881329655647278
Train Acc 1.0686
 Acc 1.0620
arctic25,dgl,1,1015,43.5570,1.0620

epoch:1017/50, training loss:0.13092659413814545
Train Acc 1.0657
 Acc 1.0658
arctic25,dgl,1,1016,43.5996,1.0658

epoch:1018/50, training loss:0.13052372634410858
Train Acc 1.0689
 Acc 1.0659
arctic25,dgl,1,1017,43.6421,1.0659

epoch:1019/50, training loss:0.12975968420505524
Train Acc 1.0691
 Acc 1.0626
arctic25,dgl,1,1018,43.6846,1.0626

epoch:1020/50, training loss:0.13059960305690765
Train Acc 1.0662
 Acc 1.0659
arctic25,dgl,1,1019,43.7271,1.0659

epoch:1021/50, training loss:0.12890572845935822
Train Acc 1.0689
 Acc 1.0656
arctic25,dgl,1,1020,43.7696,1.0656

epoch:1022/50, training loss:0.12853379547595978
Train Acc 1.0687
 Acc 1.0619
arctic25,dgl,1,1021,43.8121,1.0619

epoch:1023/50, training loss:0.1307312697172165
Train Acc 1.0654
 Acc 1.0657
arctic25,dgl,1,1022,43.8546,1.0657

epoch:1024/50, training loss:0.1306854784488678
Train Acc 1.0688
 Acc 1.0660
new best val f1: 1.0659810188892243
arctic25,dgl,1,1023,43.8971,1.0660

epoch:1025/50, training loss:0.12976913154125214
Train Acc 1.0692
 Acc 1.0620
arctic25,dgl,1,1024,43.9396,1.0620

epoch:1026/50, training loss:0.13134998083114624
Train Acc 1.0656
 Acc 1.0658
arctic25,dgl,1,1025,43.9821,1.0658

epoch:1027/50, training loss:0.12957370281219482
Train Acc 1.0688
 Acc 1.0655
arctic25,dgl,1,1026,44.0246,1.0655

epoch:1028/50, training loss:0.12917231023311615
Train Acc 1.0686
 Acc 1.0613
arctic25,dgl,1,1027,44.0671,1.0613

epoch:1029/50, training loss:0.1310795247554779
Train Acc 1.0651
 Acc 1.0657
arctic25,dgl,1,1028,44.1097,1.0657

epoch:1030/50, training loss:0.12961888313293457
Train Acc 1.0687
 Acc 1.0660
new best val f1: 1.0660072721544742
arctic25,dgl,1,1029,44.1522,1.0660

epoch:1031/50, training loss:0.1287384331226349
Train Acc 1.0691
 Acc 1.0624
arctic25,dgl,1,1030,44.1947,1.0624

epoch:1032/50, training loss:0.13040301203727722
Train Acc 1.0660
 Acc 1.0659
arctic25,dgl,1,1031,44.2372,1.0659

epoch:1033/50, training loss:0.1288936585187912
Train Acc 1.0691
 Acc 1.0656
arctic25,dgl,1,1032,44.2796,1.0656

epoch:1034/50, training loss:0.1286325305700302
Train Acc 1.0686
 Acc 1.0621
arctic25,dgl,1,1033,44.3221,1.0621

epoch:1035/50, training loss:0.12972696125507355
Train Acc 1.0656
 Acc 1.0657
arctic25,dgl,1,1034,44.3647,1.0657

epoch:1036/50, training loss:0.1283763200044632
Train Acc 1.0689
 Acc 1.0662
new best val f1: 1.0661516651133485
arctic25,dgl,1,1035,44.4072,1.0662

epoch:1037/50, training loss:0.12763676047325134
Train Acc 1.0693
 Acc 1.0625
arctic25,dgl,1,1036,44.4497,1.0625

epoch:1038/50, training loss:0.129827082157135
Train Acc 1.0660
 Acc 1.0661
arctic25,dgl,1,1037,44.4922,1.0661

epoch:1039/50, training loss:0.12910203635692596
Train Acc 1.0692
 Acc 1.0658
arctic25,dgl,1,1038,44.5347,1.0658

epoch:1040/50, training loss:0.1286967247724533
Train Acc 1.0689
 Acc 1.0619
arctic25,dgl,1,1039,44.5772,1.0619

epoch:1041/50, training loss:0.1295669674873352
Train Acc 1.0655
 Acc 1.0659
arctic25,dgl,1,1040,44.6198,1.0659

epoch:1042/50, training loss:0.12781208753585815
Train Acc 1.0691
 Acc 1.0661
arctic25,dgl,1,1041,44.6623,1.0661

epoch:1043/50, training loss:0.12737378478050232
Train Acc 1.0694
 Acc 1.0627
arctic25,dgl,1,1042,44.7048,1.0627

epoch:1044/50, training loss:0.1289263814687729
Train Acc 1.0662
 Acc 1.0661
arctic25,dgl,1,1043,44.7473,1.0661

epoch:1045/50, training loss:0.127807155251503
Train Acc 1.0692
 Acc 1.0660
arctic25,dgl,1,1044,44.7898,1.0660

epoch:1046/50, training loss:0.12736530601978302
Train Acc 1.0692
 Acc 1.0621
arctic25,dgl,1,1045,44.8323,1.0621

epoch:1047/50, training loss:0.12900570034980774
Train Acc 1.0657
 Acc 1.0659
arctic25,dgl,1,1046,44.8748,1.0659

epoch:1048/50, training loss:0.12804777920246124
Train Acc 1.0691
 Acc 1.0661
arctic25,dgl,1,1047,44.9173,1.0661

epoch:1049/50, training loss:0.12752719223499298
Train Acc 1.0694
 Acc 1.0630
arctic25,dgl,1,1048,44.9598,1.0630

epoch:1050/50, training loss:0.1281960904598236
Train Acc 1.0665
 Acc 1.0662
new best val f1: 1.066243551541723
arctic25,dgl,1,1049,45.0023,1.0662

epoch:1051/50, training loss:0.12631696462631226
Train Acc 1.0692
 Acc 1.0661
arctic25,dgl,1,1050,45.0448,1.0661

epoch:1052/50, training loss:0.1260259747505188
Train Acc 1.0693
 Acc 1.0631
arctic25,dgl,1,1051,45.0874,1.0631

epoch:1053/50, training loss:0.12741200625896454
Train Acc 1.0666
 Acc 1.0659
arctic25,dgl,1,1052,45.1299,1.0659

epoch:1054/50, training loss:0.12635397911071777
Train Acc 1.0691
 Acc 1.0661
arctic25,dgl,1,1053,45.1725,1.0661

epoch:1055/50, training loss:0.1258413940668106
Train Acc 1.0693
 Acc 1.0628
arctic25,dgl,1,1054,45.2149,1.0628

epoch:1056/50, training loss:0.12805244326591492
Train Acc 1.0663
 Acc 1.0663
new best val f1: 1.0662763681232854
arctic25,dgl,1,1055,45.2574,1.0663

epoch:1057/50, training loss:0.12778247892856598
Train Acc 1.0692
 Acc 1.0660
arctic25,dgl,1,1056,45.3000,1.0660

epoch:1058/50, training loss:0.12699593603610992
Train Acc 1.0692
 Acc 1.0622
arctic25,dgl,1,1057,45.3425,1.0622

epoch:1059/50, training loss:0.12817618250846863
Train Acc 1.0660
 Acc 1.0660
arctic25,dgl,1,1058,45.3850,1.0660

epoch:1060/50, training loss:0.12644965946674347
Train Acc 1.0691
 Acc 1.0662
arctic25,dgl,1,1059,45.4275,1.0662

epoch:1061/50, training loss:0.12607073783874512
Train Acc 1.0694
 Acc 1.0633
arctic25,dgl,1,1060,45.4700,1.0633

epoch:1062/50, training loss:0.12733370065689087
Train Acc 1.0669
 Acc 1.0663
arctic25,dgl,1,1061,45.5125,1.0663

epoch:1063/50, training loss:0.1257428526878357
Train Acc 1.0693
 Acc 1.0661
arctic25,dgl,1,1062,45.5550,1.0661

epoch:1064/50, training loss:0.12524721026420593
Train Acc 1.0690
 Acc 1.0626
arctic25,dgl,1,1063,45.5974,1.0626

epoch:1065/50, training loss:0.12729458510875702
Train Acc 1.0662
 Acc 1.0661
arctic25,dgl,1,1064,45.6399,1.0661

epoch:1066/50, training loss:0.12655390799045563
Train Acc 1.0693
 Acc 1.0664
new best val f1: 1.0663551279190349
arctic25,dgl,1,1065,45.6825,1.0664

epoch:1067/50, training loss:0.1256648749113083
Train Acc 1.0695
 Acc 1.0628
arctic25,dgl,1,1066,45.7249,1.0628

epoch:1068/50, training loss:0.12759128212928772
Train Acc 1.0663
 Acc 1.0663
arctic25,dgl,1,1067,45.7673,1.0663

epoch:1069/50, training loss:0.1261814832687378
Train Acc 1.0694
 Acc 1.0661
arctic25,dgl,1,1068,45.8098,1.0661

epoch:1070/50, training loss:0.12573830783367157
Train Acc 1.0691
 Acc 1.0627
arctic25,dgl,1,1069,45.8524,1.0627

epoch:1071/50, training loss:0.126910001039505
Train Acc 1.0662
 Acc 1.0662
arctic25,dgl,1,1070,45.8949,1.0662

epoch:1072/50, training loss:0.12510867416858673
Train Acc 1.0694
 Acc 1.0664
new best val f1: 1.0663879445005973
arctic25,dgl,1,1071,45.9375,1.0664

epoch:1073/50, training loss:0.12477575987577438
Train Acc 1.0696
 Acc 1.0634
arctic25,dgl,1,1072,45.9799,1.0634

epoch:1074/50, training loss:0.12638965249061584
Train Acc 1.0669
 Acc 1.0663
arctic25,dgl,1,1073,46.0224,1.0663

epoch:1075/50, training loss:0.1253918707370758
Train Acc 1.0694
 Acc 1.0662
arctic25,dgl,1,1074,46.0650,1.0662

epoch:1076/50, training loss:0.1247205063700676
Train Acc 1.0692
 Acc 1.0624
arctic25,dgl,1,1075,46.1074,1.0624

epoch:1077/50, training loss:0.12687107920646667
Train Acc 1.0661
 Acc 1.0661
arctic25,dgl,1,1076,46.1500,1.0661

epoch:1078/50, training loss:0.12607243657112122
Train Acc 1.0693
 Acc 1.0663
arctic25,dgl,1,1077,46.1925,1.0663

epoch:1079/50, training loss:0.12564021348953247
Train Acc 1.0695
 Acc 1.0632
arctic25,dgl,1,1078,46.2350,1.0632

epoch:1080/50, training loss:0.12636609375476837
Train Acc 1.0668
 Acc 1.0664
new best val f1: 1.0664141977658472
arctic25,dgl,1,1079,46.2775,1.0664

epoch:1081/50, training loss:0.12441669404506683
Train Acc 1.0695
 Acc 1.0661
arctic25,dgl,1,1080,46.3200,1.0661

epoch:1082/50, training loss:0.12413512170314789
Train Acc 1.0692
 Acc 1.0631
arctic25,dgl,1,1081,46.3625,1.0631

epoch:1083/50, training loss:0.1256125420331955
Train Acc 1.0667
 Acc 1.0661
arctic25,dgl,1,1082,46.4050,1.0661

epoch:1084/50, training loss:0.12474609166383743
Train Acc 1.0693
 Acc 1.0663
arctic25,dgl,1,1083,46.4474,1.0663

epoch:1085/50, training loss:0.12386768311262131
Train Acc 1.0695
 Acc 1.0628
arctic25,dgl,1,1084,46.4899,1.0628

epoch:1086/50, training loss:0.12655898928642273
Train Acc 1.0665
 Acc 1.0664
arctic25,dgl,1,1085,46.5325,1.0664

epoch:1087/50, training loss:0.12637577950954437
Train Acc 1.0695
 Acc 1.0661
arctic25,dgl,1,1086,46.5750,1.0661

epoch:1088/50, training loss:0.12577205896377563
Train Acc 1.0692
 Acc 1.0624
arctic25,dgl,1,1087,46.6175,1.0624

epoch:1089/50, training loss:0.1262451708316803
Train Acc 1.0661
 Acc 1.0663
arctic25,dgl,1,1088,46.6600,1.0663

epoch:1090/50, training loss:0.1241111159324646
Train Acc 1.0694
 Acc 1.0666
new best val f1: 1.0665782806736588
arctic25,dgl,1,1089,46.7025,1.0666

epoch:1091/50, training loss:0.12352216988801956
Train Acc 1.0696
 Acc 1.0633
arctic25,dgl,1,1090,46.7450,1.0633

epoch:1092/50, training loss:0.1256159245967865
Train Acc 1.0669
 Acc 1.0665
arctic25,dgl,1,1091,46.7875,1.0665

epoch:1093/50, training loss:0.12463288009166718
Train Acc 1.0695
 Acc 1.0663
arctic25,dgl,1,1092,46.8301,1.0663

epoch:1094/50, training loss:0.12399926781654358
Train Acc 1.0693
 Acc 1.0625
arctic25,dgl,1,1093,46.8726,1.0625

epoch:1095/50, training loss:0.12580329179763794
Train Acc 1.0662
 Acc 1.0663
arctic25,dgl,1,1094,46.9150,1.0663

epoch:1096/50, training loss:0.1245039850473404
Train Acc 1.0694
 Acc 1.0666
arctic25,dgl,1,1095,46.9576,1.0666

epoch:1097/50, training loss:0.12404045462608337
Train Acc 1.0697
 Acc 1.0632
arctic25,dgl,1,1096,47.0001,1.0632

epoch:1098/50, training loss:0.12543250620365143
Train Acc 1.0667
 Acc 1.0665
arctic25,dgl,1,1097,47.0426,1.0665

epoch:1099/50, training loss:0.12390932440757751
Train Acc 1.0696
 Acc 1.0663
arctic25,dgl,1,1098,47.0852,1.0663

epoch:1100/50, training loss:0.12352873384952545
Train Acc 1.0694
 Acc 1.0630
arctic25,dgl,1,1099,47.1277,1.0630

epoch:1101/50, training loss:0.12493208050727844
Train Acc 1.0665
 Acc 1.0664
arctic25,dgl,1,1100,47.1703,1.0664

epoch:1102/50, training loss:0.12375874072313309
Train Acc 1.0695
 Acc 1.0665
arctic25,dgl,1,1101,47.2127,1.0665

epoch:1103/50, training loss:0.12324168533086777
Train Acc 1.0697
 Acc 1.0631
arctic25,dgl,1,1102,47.2552,1.0631

epoch:1104/50, training loss:0.12487965077161789
Train Acc 1.0666
 Acc 1.0665
arctic25,dgl,1,1103,47.2977,1.0665

epoch:1105/50, training loss:0.12375970184803009
Train Acc 1.0696
 Acc 1.0664
arctic25,dgl,1,1104,47.3403,1.0664

epoch:1106/50, training loss:0.12326804548501968
Train Acc 1.0695
 Acc 1.0631
arctic25,dgl,1,1105,47.3828,1.0631

epoch:1107/50, training loss:0.12453401833772659
Train Acc 1.0666
 Acc 1.0664
arctic25,dgl,1,1106,47.4253,1.0664

epoch:1108/50, training loss:0.12325169146060944
Train Acc 1.0696
 Acc 1.0665
arctic25,dgl,1,1107,47.4678,1.0665

epoch:1109/50, training loss:0.12265153229236603
Train Acc 1.0696
 Acc 1.0634
arctic25,dgl,1,1108,47.5103,1.0634

epoch:1110/50, training loss:0.12436999380588531
Train Acc 1.0668
 Acc 1.0665
arctic25,dgl,1,1109,47.5528,1.0665

epoch:1111/50, training loss:0.12355304509401321
Train Acc 1.0697
 Acc 1.0664
arctic25,dgl,1,1110,47.5953,1.0664

epoch:1112/50, training loss:0.1228608787059784
Train Acc 1.0695
 Acc 1.0625
arctic25,dgl,1,1111,47.6377,1.0625

epoch:1113/50, training loss:0.1247515007853508
Train Acc 1.0662
 Acc 1.0664
arctic25,dgl,1,1112,47.6803,1.0664

epoch:1114/50, training loss:0.12366160750389099
Train Acc 1.0695
 Acc 1.0666
new best val f1: 1.0665914073062837
arctic25,dgl,1,1113,47.7229,1.0666

epoch:1115/50, training loss:0.12309722602367401
Train Acc 1.0697
 Acc 1.0631
arctic25,dgl,1,1114,47.7654,1.0631

epoch:1116/50, training loss:0.1244824156165123
Train Acc 1.0667
 Acc 1.0666
arctic25,dgl,1,1115,47.8080,1.0666

epoch:1117/50, training loss:0.12327833473682404
Train Acc 1.0697
 Acc 1.0664
arctic25,dgl,1,1116,47.8505,1.0664

epoch:1118/50, training loss:0.12271000444889069
Train Acc 1.0694
 Acc 1.0621
arctic25,dgl,1,1117,47.8931,1.0621

epoch:1119/50, training loss:0.12485919892787933
Train Acc 1.0659
 Acc 1.0665
arctic25,dgl,1,1118,47.9357,1.0665

epoch:1120/50, training loss:0.12317778915166855
Train Acc 1.0695
 Acc 1.0666
new best val f1: 1.0666439138367834
arctic25,dgl,1,1119,47.9782,1.0666

epoch:1121/50, training loss:0.1225418895483017
Train Acc 1.0698
 Acc 1.0633
arctic25,dgl,1,1120,48.0208,1.0633

epoch:1122/50, training loss:0.12398340553045273
Train Acc 1.0668
 Acc 1.0665
arctic25,dgl,1,1121,48.0633,1.0665

epoch:1123/50, training loss:0.12252897769212723
Train Acc 1.0697
 Acc 1.0664
arctic25,dgl,1,1122,48.1059,1.0664

epoch:1124/50, training loss:0.12191329151391983
Train Acc 1.0693
 Acc 1.0627
arctic25,dgl,1,1123,48.1484,1.0627

epoch:1125/50, training loss:0.12399070709943771
Train Acc 1.0664
 Acc 1.0666
arctic25,dgl,1,1124,48.1909,1.0666

epoch:1126/50, training loss:0.12334564328193665
Train Acc 1.0696
 Acc 1.0668
new best val f1: 1.0667686168467203
arctic25,dgl,1,1125,48.2334,1.0668

epoch:1127/50, training loss:0.12260042876005173
Train Acc 1.0698
 Acc 1.0631
arctic25,dgl,1,1126,48.2758,1.0631

epoch:1128/50, training loss:0.12397520244121552
Train Acc 1.0666
 Acc 1.0666
arctic25,dgl,1,1127,48.3183,1.0666

epoch:1129/50, training loss:0.12229566276073456
Train Acc 1.0697
 Acc 1.0664
arctic25,dgl,1,1128,48.3608,1.0664

epoch:1130/50, training loss:0.12193430215120316
Train Acc 1.0694
 Acc 1.0629
arctic25,dgl,1,1129,48.4033,1.0629

epoch:1131/50, training loss:0.12337226420640945
Train Acc 1.0666
 Acc 1.0667
arctic25,dgl,1,1130,48.4458,1.0667

epoch:1132/50, training loss:0.12218057364225388
Train Acc 1.0697
 Acc 1.0668
new best val f1: 1.0667751801630327
arctic25,dgl,1,1131,48.4883,1.0668

epoch:1133/50, training loss:0.12151104211807251
Train Acc 1.0699
 Acc 1.0633
arctic25,dgl,1,1132,48.5307,1.0633

epoch:1134/50, training loss:0.12312883138656616
Train Acc 1.0669
 Acc 1.0666
arctic25,dgl,1,1133,48.5732,1.0666

epoch:1135/50, training loss:0.12183669954538345
Train Acc 1.0697
 Acc 1.0666
arctic25,dgl,1,1134,48.6158,1.0666

epoch:1136/50, training loss:0.12145878374576569
Train Acc 1.0696
 Acc 1.0638
arctic25,dgl,1,1135,48.6583,1.0638

epoch:1137/50, training loss:0.1220240369439125
Train Acc 1.0672
 Acc 1.0667
arctic25,dgl,1,1136,48.7008,1.0667

epoch:1138/50, training loss:0.1201033815741539
Train Acc 1.0698
 Acc 1.0668
arctic25,dgl,1,1137,48.7433,1.0668

epoch:1139/50, training loss:0.11998920887708664
Train Acc 1.0699
 Acc 1.0646
arctic25,dgl,1,1138,48.7858,1.0646

epoch:1140/50, training loss:0.12110789120197296
Train Acc 1.0679
 Acc 1.0667
arctic25,dgl,1,1139,48.8283,1.0667

epoch:1141/50, training loss:0.12028731405735016
Train Acc 1.0698
 Acc 1.0667
arctic25,dgl,1,1140,48.8708,1.0667

epoch:1142/50, training loss:0.11953546851873398
Train Acc 1.0699
 Acc 1.0639
arctic25,dgl,1,1141,48.9133,1.0639

epoch:1143/50, training loss:0.12160477042198181
Train Acc 1.0673
 Acc 1.0666
arctic25,dgl,1,1142,48.9559,1.0666

epoch:1144/50, training loss:0.1214507669210434
Train Acc 1.0697
 Acc 1.0667
arctic25,dgl,1,1143,48.9984,1.0667

epoch:1145/50, training loss:0.1205962747335434
Train Acc 1.0698
 Acc 1.0630
arctic25,dgl,1,1144,49.0409,1.0630

epoch:1146/50, training loss:0.1226198598742485
Train Acc 1.0667
 Acc 1.0668
arctic25,dgl,1,1145,49.0834,1.0668

epoch:1147/50, training loss:0.12185224890708923
Train Acc 1.0699
 Acc 1.0667
arctic25,dgl,1,1146,49.1259,1.0667

epoch:1148/50, training loss:0.12148343771696091
Train Acc 1.0698
 Acc 1.0634
arctic25,dgl,1,1147,49.1684,1.0634

epoch:1149/50, training loss:0.12191450595855713
Train Acc 1.0669
 Acc 1.0667
arctic25,dgl,1,1148,49.2109,1.0667

epoch:1150/50, training loss:0.12011276185512543
Train Acc 1.0698
 Acc 1.0668
arctic25,dgl,1,1149,49.2534,1.0668

epoch:1151/50, training loss:0.11964785307645798
Train Acc 1.0698
 Acc 1.0641
arctic25,dgl,1,1150,49.2959,1.0641

epoch:1152/50, training loss:0.12118791043758392
Train Acc 1.0676
 Acc 1.0668
new best val f1: 1.0667817434793452
arctic25,dgl,1,1151,49.3384,1.0668

epoch:1153/50, training loss:0.12062759697437286
Train Acc 1.0699
 Acc 1.0667
arctic25,dgl,1,1152,49.3810,1.0667

epoch:1154/50, training loss:0.11966852098703384
Train Acc 1.0697
 Acc 1.0629
arctic25,dgl,1,1153,49.4235,1.0629

epoch:1155/50, training loss:0.12208113074302673
Train Acc 1.0667
 Acc 1.0667
arctic25,dgl,1,1154,49.4660,1.0667

epoch:1156/50, training loss:0.12174928933382034
Train Acc 1.0698
 Acc 1.0668
new best val f1: 1.0668079967445951
arctic25,dgl,1,1155,49.5085,1.0668

epoch:1157/50, training loss:0.12116384506225586
Train Acc 1.0700
 Acc 1.0638
arctic25,dgl,1,1156,49.5510,1.0638

epoch:1158/50, training loss:0.1213642880320549
Train Acc 1.0674
 Acc 1.0668
new best val f1: 1.06684737664247
arctic25,dgl,1,1157,49.5935,1.0668

epoch:1159/50, training loss:0.11895669251680374
Train Acc 1.0700
 Acc 1.0667
arctic25,dgl,1,1158,49.6360,1.0667

epoch:1160/50, training loss:0.11893419176340103
Train Acc 1.0697
 Acc 1.0646
arctic25,dgl,1,1159,49.6785,1.0646

epoch:1161/50, training loss:0.11977327615022659
Train Acc 1.0680
 Acc 1.0668
arctic25,dgl,1,1160,49.7210,1.0668

epoch:1162/50, training loss:0.1186174526810646
Train Acc 1.0699
 Acc 1.0669
new best val f1: 1.066945826387157
arctic25,dgl,1,1161,49.7635,1.0669

epoch:1163/50, training loss:0.11810823529958725
Train Acc 1.0701
 Acc 1.0650
arctic25,dgl,1,1162,49.8060,1.0650

epoch:1164/50, training loss:0.1196541115641594
Train Acc 1.0682
 Acc 1.0668
arctic25,dgl,1,1163,49.8485,1.0668

epoch:1165/50, training loss:0.11925286054611206
Train Acc 1.0700
 Acc 1.0667
arctic25,dgl,1,1164,49.8910,1.0667

epoch:1166/50, training loss:0.11821403354406357
Train Acc 1.0699
 Acc 1.0635
arctic25,dgl,1,1165,49.9335,1.0635

epoch:1167/50, training loss:0.12072867155075073
Train Acc 1.0671
 Acc 1.0668
arctic25,dgl,1,1166,49.9760,1.0668

epoch:1168/50, training loss:0.1211186945438385
Train Acc 1.0699
 Acc 1.0669
arctic25,dgl,1,1167,50.0184,1.0669

epoch:1169/50, training loss:0.12015656381845474
Train Acc 1.0701
 Acc 1.0625
arctic25,dgl,1,1168,50.0610,1.0625

epoch:1170/50, training loss:0.12213661521673203
Train Acc 1.0662
 Acc 1.0669
arctic25,dgl,1,1169,50.1036,1.0669

epoch:1171/50, training loss:0.11928463727235794
Train Acc 1.0701
 Acc 1.0667
arctic25,dgl,1,1170,50.1461,1.0667

epoch:1172/50, training loss:0.11919968575239182
Train Acc 1.0699
 Acc 1.0638
arctic25,dgl,1,1171,50.1886,1.0638

epoch:1173/50, training loss:0.12003257870674133
Train Acc 1.0673
 Acc 1.0669
arctic25,dgl,1,1172,50.2310,1.0669

epoch:1174/50, training loss:0.1188543289899826
Train Acc 1.0701
 Acc 1.0670
new best val f1: 1.0669917696013442
arctic25,dgl,1,1173,50.2736,1.0670

epoch:1175/50, training loss:0.11807777732610703
Train Acc 1.0702
 Acc 1.0636
arctic25,dgl,1,1174,50.3160,1.0636

epoch:1176/50, training loss:0.12039348483085632
Train Acc 1.0672
 Acc 1.0669
arctic25,dgl,1,1175,50.3585,1.0669

epoch:1177/50, training loss:0.12039946019649506
Train Acc 1.0700
 Acc 1.0668
arctic25,dgl,1,1176,50.4011,1.0668

epoch:1178/50, training loss:0.11961258947849274
Train Acc 1.0700
 Acc 1.0625
arctic25,dgl,1,1177,50.4435,1.0625

epoch:1179/50, training loss:0.12140195071697235
Train Acc 1.0662
 Acc 1.0669
arctic25,dgl,1,1178,50.4861,1.0669

epoch:1180/50, training loss:0.11893846094608307
Train Acc 1.0701
 Acc 1.0670
new best val f1: 1.0670114595502815
arctic25,dgl,1,1179,50.5286,1.0670

epoch:1181/50, training loss:0.11865636706352234
Train Acc 1.0701
 Acc 1.0637
arctic25,dgl,1,1180,50.5711,1.0637

epoch:1182/50, training loss:0.11992533504962921
Train Acc 1.0673
 Acc 1.0670
new best val f1: 1.067044276131844
arctic25,dgl,1,1181,50.6136,1.0670

epoch:1183/50, training loss:0.11897115409374237
Train Acc 1.0701
 Acc 1.0670
arctic25,dgl,1,1182,50.6562,1.0670

epoch:1184/50, training loss:0.11813123524188995
Train Acc 1.0701
 Acc 1.0631
arctic25,dgl,1,1183,50.6987,1.0631

epoch:1185/50, training loss:0.12057657539844513
Train Acc 1.0668
 Acc 1.0669
arctic25,dgl,1,1184,50.7412,1.0669

epoch:1186/50, training loss:0.11980447918176651
Train Acc 1.0700
 Acc 1.0669
arctic25,dgl,1,1185,50.7837,1.0669

epoch:1187/50, training loss:0.1192687451839447
Train Acc 1.0700
 Acc 1.0638
arctic25,dgl,1,1186,50.8262,1.0638

epoch:1188/50, training loss:0.11961198598146439
Train Acc 1.0674
 Acc 1.0671
new best val f1: 1.0670967826623436
arctic25,dgl,1,1187,50.8687,1.0671

epoch:1189/50, training loss:0.11766215413808823
Train Acc 1.0702
 Acc 1.0670
arctic25,dgl,1,1188,50.9113,1.0670

epoch:1190/50, training loss:0.11732155084609985
Train Acc 1.0701
 Acc 1.0641
arctic25,dgl,1,1189,50.9538,1.0641

epoch:1191/50, training loss:0.1189340204000473
Train Acc 1.0676
 Acc 1.0670
arctic25,dgl,1,1190,50.9963,1.0670

epoch:1192/50, training loss:0.11833351850509644
Train Acc 1.0700
 Acc 1.0669
arctic25,dgl,1,1191,51.0388,1.0669

epoch:1193/50, training loss:0.11755521595478058
Train Acc 1.0701
 Acc 1.0635
arctic25,dgl,1,1192,51.0818,1.0635

epoch:1194/50, training loss:0.11975706368684769
Train Acc 1.0672
 Acc 1.0670
arctic25,dgl,1,1193,51.1243,1.0670

epoch:1195/50, training loss:0.11919873207807541
Train Acc 1.0702
 Acc 1.0668
arctic25,dgl,1,1194,51.1668,1.0668

epoch:1196/50, training loss:0.11856168508529663
Train Acc 1.0700
 Acc 1.0636
arctic25,dgl,1,1195,51.2093,1.0636

epoch:1197/50, training loss:0.11922968924045563
Train Acc 1.0672
 Acc 1.0670
arctic25,dgl,1,1196,51.2518,1.0670

epoch:1198/50, training loss:0.11720657348632812
Train Acc 1.0700
 Acc 1.0671
new best val f1: 1.0671361625602185
arctic25,dgl,1,1197,51.2943,1.0671

epoch:1199/50, training loss:0.11698569357395172
Train Acc 1.0703
 Acc 1.0644
arctic25,dgl,1,1198,51.3368,1.0644

epoch:1200/50, training loss:0.11849207431077957
Train Acc 1.0680
 Acc 1.0670
arctic25,dgl,1,1199,51.3793,1.0670

epoch:1201/50, training loss:0.11757960915565491
Train Acc 1.0703
 Acc 1.0669
arctic25,dgl,1,1200,51.4219,1.0669

epoch:1202/50, training loss:0.1168186217546463
Train Acc 1.0700
 Acc 1.0631
arctic25,dgl,1,1201,51.4644,1.0631

epoch:1203/50, training loss:0.1194818913936615
Train Acc 1.0669
 Acc 1.0670
arctic25,dgl,1,1202,51.5070,1.0670

epoch:1204/50, training loss:0.11939527839422226
Train Acc 1.0700
 Acc 1.0671
arctic25,dgl,1,1203,51.5495,1.0671

epoch:1205/50, training loss:0.11888295412063599
Train Acc 1.0703
 Acc 1.0639
arctic25,dgl,1,1204,51.5920,1.0639

epoch:1206/50, training loss:0.11873848736286163
Train Acc 1.0676
 Acc 1.0671
arctic25,dgl,1,1205,51.6345,1.0671

epoch:1207/50, training loss:0.11645834147930145
Train Acc 1.0702
 Acc 1.0669
arctic25,dgl,1,1206,51.6771,1.0669

epoch:1208/50, training loss:0.1162552461028099
Train Acc 1.0701
 Acc 1.0643
arctic25,dgl,1,1207,51.7196,1.0643

epoch:1209/50, training loss:0.11781452596187592
Train Acc 1.0678
 Acc 1.0672
new best val f1: 1.0671689791417807
arctic25,dgl,1,1208,51.7621,1.0672

epoch:1210/50, training loss:0.11752140522003174
Train Acc 1.0703
 Acc 1.0672
new best val f1: 1.067214922355968
arctic25,dgl,1,1209,51.8046,1.0672

epoch:1211/50, training loss:0.11642424017190933
Train Acc 1.0704
 Acc 1.0628
arctic25,dgl,1,1210,51.8471,1.0628

epoch:1212/50, training loss:0.11969291418790817
Train Acc 1.0667
 Acc 1.0670
arctic25,dgl,1,1211,51.8896,1.0670

epoch:1213/50, training loss:0.11865060031414032
Train Acc 1.0701
 Acc 1.0669
arctic25,dgl,1,1212,51.9322,1.0669

epoch:1214/50, training loss:0.11829190701246262
Train Acc 1.0701
 Acc 1.0637
arctic25,dgl,1,1213,51.9747,1.0637

epoch:1215/50, training loss:0.11831878125667572
Train Acc 1.0673
 Acc 1.0672
arctic25,dgl,1,1214,52.0172,1.0672

epoch:1216/50, training loss:0.11614828556776047
Train Acc 1.0703
 Acc 1.0672
arctic25,dgl,1,1215,52.0597,1.0672

epoch:1217/50, training loss:0.11592893302440643
Train Acc 1.0704
 Acc 1.0646
arctic25,dgl,1,1216,52.1022,1.0646

epoch:1218/50, training loss:0.11741095781326294
Train Acc 1.0680
 Acc 1.0671
arctic25,dgl,1,1217,52.1448,1.0671

epoch:1219/50, training loss:0.11682386696338654
Train Acc 1.0704
 Acc 1.0670
arctic25,dgl,1,1218,52.1873,1.0670

epoch:1220/50, training loss:0.11595384776592255
Train Acc 1.0701
 Acc 1.0635
arctic25,dgl,1,1219,52.2297,1.0635

epoch:1221/50, training loss:0.11837907880544662
Train Acc 1.0671
 Acc 1.0671
arctic25,dgl,1,1220,52.2724,1.0671

epoch:1222/50, training loss:0.11821398884057999
Train Acc 1.0701
 Acc 1.0671
arctic25,dgl,1,1221,52.3150,1.0671

epoch:1223/50, training loss:0.11762752383947372
Train Acc 1.0703
 Acc 1.0641
arctic25,dgl,1,1222,52.3575,1.0641

epoch:1224/50, training loss:0.11767087131738663
Train Acc 1.0678
 Acc 1.0672
arctic25,dgl,1,1223,52.4000,1.0672

epoch:1225/50, training loss:0.11552228033542633
Train Acc 1.0703
 Acc 1.0669
arctic25,dgl,1,1224,52.4426,1.0669

epoch:1226/50, training loss:0.11553944647312164
Train Acc 1.0700
 Acc 1.0648
arctic25,dgl,1,1225,52.4851,1.0648

epoch:1227/50, training loss:0.11640893667936325
Train Acc 1.0681
 Acc 1.0671
arctic25,dgl,1,1226,52.5276,1.0671

epoch:1228/50, training loss:0.11525904387235641
Train Acc 1.0702
 Acc 1.0673
new best val f1: 1.0672805555190927
arctic25,dgl,1,1227,52.5701,1.0673

epoch:1229/50, training loss:0.1147284060716629
Train Acc 1.0704
 Acc 1.0651
arctic25,dgl,1,1228,52.6127,1.0651

epoch:1230/50, training loss:0.11646580696105957
Train Acc 1.0684
 Acc 1.0670
arctic25,dgl,1,1229,52.6552,1.0670

epoch:1231/50, training loss:0.11622054129838943
Train Acc 1.0703
 Acc 1.0669
arctic25,dgl,1,1230,52.6977,1.0669

epoch:1232/50, training loss:0.11520419269800186
Train Acc 1.0700
 Acc 1.0632
arctic25,dgl,1,1231,52.7402,1.0632

epoch:1233/50, training loss:0.11795046925544739
Train Acc 1.0670
 Acc 1.0671
arctic25,dgl,1,1232,52.7827,1.0671

epoch:1234/50, training loss:0.11795175820589066
Train Acc 1.0703
 Acc 1.0673
arctic25,dgl,1,1233,52.8252,1.0673

epoch:1235/50, training loss:0.11727549135684967
Train Acc 1.0704
 Acc 1.0640
arctic25,dgl,1,1234,52.8677,1.0640

epoch:1236/50, training loss:0.11741995811462402
Train Acc 1.0676
 Acc 1.0671
arctic25,dgl,1,1235,52.9103,1.0671

epoch:1237/50, training loss:0.11532748490571976
Train Acc 1.0703
 Acc 1.0669
arctic25,dgl,1,1236,52.9528,1.0669

epoch:1238/50, training loss:0.11486396193504333
Train Acc 1.0700
 Acc 1.0639
arctic25,dgl,1,1237,52.9953,1.0639

epoch:1239/50, training loss:0.11685137450695038
Train Acc 1.0674
 Acc 1.0673
arctic25,dgl,1,1238,53.0378,1.0673

epoch:1240/50, training loss:0.11690515279769897
Train Acc 1.0704
 Acc 1.0673
new best val f1: 1.0673461886822173
arctic25,dgl,1,1239,53.0803,1.0673

epoch:1241/50, training loss:0.1157270148396492
Train Acc 1.0704
 Acc 1.0628
arctic25,dgl,1,1240,53.1228,1.0628

epoch:1242/50, training loss:0.11829124391078949
Train Acc 1.0666
 Acc 1.0671
arctic25,dgl,1,1241,53.1654,1.0671

epoch:1243/50, training loss:0.11607535928487778
Train Acc 1.0703
 Acc 1.0670
arctic25,dgl,1,1242,53.2079,1.0670

epoch:1244/50, training loss:0.11581481248140335
Train Acc 1.0702
 Acc 1.0638
arctic25,dgl,1,1243,53.2504,1.0638

epoch:1245/50, training loss:0.11672349274158478
Train Acc 1.0674
 Acc 1.0673
arctic25,dgl,1,1244,53.2930,1.0673

epoch:1246/50, training loss:0.11555702239274979
Train Acc 1.0705
 Acc 1.0674
new best val f1: 1.0673921318964046
arctic25,dgl,1,1245,53.3354,1.0674

epoch:1247/50, training loss:0.1148822084069252
Train Acc 1.0705
 Acc 1.0633
arctic25,dgl,1,1246,53.3779,1.0633

epoch:1248/50, training loss:0.11722689867019653
Train Acc 1.0672
 Acc 1.0671
arctic25,dgl,1,1247,53.4205,1.0671

epoch:1249/50, training loss:0.11659442633390427
Train Acc 1.0702
 Acc 1.0672
arctic25,dgl,1,1248,53.4630,1.0672

epoch:1250/50, training loss:0.1162489578127861
Train Acc 1.0703
 Acc 1.0638
arctic25,dgl,1,1249,53.5056,1.0638

epoch:1251/50, training loss:0.11663777381181717
Train Acc 1.0675
 Acc 1.0673
arctic25,dgl,1,1250,53.5481,1.0673

epoch:1252/50, training loss:0.1148788332939148
Train Acc 1.0704
 Acc 1.0673
arctic25,dgl,1,1251,53.5906,1.0673

epoch:1253/50, training loss:0.1142519861459732
Train Acc 1.0704
 Acc 1.0639
arctic25,dgl,1,1252,53.6332,1.0639

epoch:1254/50, training loss:0.11615363508462906
Train Acc 1.0677
 Acc 1.0672
arctic25,dgl,1,1253,53.6757,1.0672

epoch:1255/50, training loss:0.11592784523963928
Train Acc 1.0703
 Acc 1.0674
arctic25,dgl,1,1254,53.7183,1.0674

epoch:1256/50, training loss:0.11478197574615479
Train Acc 1.0706
 Acc 1.0632
arctic25,dgl,1,1255,53.7608,1.0632

epoch:1257/50, training loss:0.11734602600336075
Train Acc 1.0670
 Acc 1.0673
arctic25,dgl,1,1256,53.8034,1.0673

epoch:1258/50, training loss:0.11547395586967468
Train Acc 1.0705
 Acc 1.0671
arctic25,dgl,1,1257,53.8459,1.0671

epoch:1259/50, training loss:0.11518875509500504
Train Acc 1.0703
 Acc 1.0639
arctic25,dgl,1,1258,53.8885,1.0639

epoch:1260/50, training loss:0.1159861609339714
Train Acc 1.0676
 Acc 1.0674
new best val f1: 1.067398695212717
arctic25,dgl,1,1259,53.9310,1.0674

epoch:1261/50, training loss:0.1144731193780899
Train Acc 1.0705
 Acc 1.0675
new best val f1: 1.0674643283758418
arctic25,dgl,1,1260,53.9735,1.0675

epoch:1262/50, training loss:0.11390797048807144
Train Acc 1.0706
 Acc 1.0643
arctic25,dgl,1,1261,54.0159,1.0643

epoch:1263/50, training loss:0.11573890596628189
Train Acc 1.0679
 Acc 1.0673
arctic25,dgl,1,1262,54.0585,1.0673

epoch:1264/50, training loss:0.11501764506101608
Train Acc 1.0704
 Acc 1.0673
arctic25,dgl,1,1263,54.1010,1.0673

epoch:1265/50, training loss:0.11428704112768173
Train Acc 1.0704
 Acc 1.0638
arctic25,dgl,1,1264,54.1435,1.0638

epoch:1266/50, training loss:0.11613231897354126
Train Acc 1.0675
 Acc 1.0674
arctic25,dgl,1,1265,54.1860,1.0674

epoch:1267/50, training loss:0.11535543203353882
Train Acc 1.0705
 Acc 1.0674
arctic25,dgl,1,1266,54.2286,1.0674

epoch:1268/50, training loss:0.11463981866836548
Train Acc 1.0705
 Acc 1.0634
arctic25,dgl,1,1267,54.2711,1.0634

epoch:1269/50, training loss:0.11624139547348022
Train Acc 1.0674
 Acc 1.0673
arctic25,dgl,1,1268,54.3136,1.0673

epoch:1270/50, training loss:0.114934541285038
Train Acc 1.0703
 Acc 1.0675
arctic25,dgl,1,1269,54.3562,1.0675

epoch:1271/50, training loss:0.11431652307510376
Train Acc 1.0706
 Acc 1.0640
arctic25,dgl,1,1270,54.3987,1.0640

epoch:1272/50, training loss:0.11566747725009918
Train Acc 1.0677
 Acc 1.0674
arctic25,dgl,1,1271,54.4412,1.0674

epoch:1273/50, training loss:0.11431023478507996
Train Acc 1.0706
 Acc 1.0672
arctic25,dgl,1,1272,54.4837,1.0672

epoch:1274/50, training loss:0.11361109465360641
Train Acc 1.0704
 Acc 1.0639
arctic25,dgl,1,1273,54.5262,1.0639

epoch:1275/50, training loss:0.11551139503717422
Train Acc 1.0676
 Acc 1.0674
arctic25,dgl,1,1274,54.5687,1.0674

epoch:1276/50, training loss:0.1148374155163765
Train Acc 1.0705
 Acc 1.0675
arctic25,dgl,1,1275,54.6113,1.0675

epoch:1277/50, training loss:0.11397816240787506
Train Acc 1.0705
 Acc 1.0639
arctic25,dgl,1,1276,54.6538,1.0639

epoch:1278/50, training loss:0.11565902829170227
Train Acc 1.0675
 Acc 1.0673
arctic25,dgl,1,1277,54.6963,1.0673

epoch:1279/50, training loss:0.11455190926790237
Train Acc 1.0705
 Acc 1.0671
arctic25,dgl,1,1278,54.7388,1.0671

epoch:1280/50, training loss:0.11390550434589386
Train Acc 1.0703
 Acc 1.0638
arctic25,dgl,1,1279,54.7814,1.0638

epoch:1281/50, training loss:0.11524603515863419
Train Acc 1.0675
 Acc 1.0674
arctic25,dgl,1,1280,54.8239,1.0674

epoch:1282/50, training loss:0.11427948623895645
Train Acc 1.0706
 Acc 1.0674
arctic25,dgl,1,1281,54.8664,1.0674

epoch:1283/50, training loss:0.11367037147283554
Train Acc 1.0704
 Acc 1.0640
arctic25,dgl,1,1282,54.9089,1.0640

epoch:1284/50, training loss:0.11507080495357513
Train Acc 1.0676
 Acc 1.0673
arctic25,dgl,1,1283,54.9516,1.0673

epoch:1285/50, training loss:0.11411301046609879
Train Acc 1.0705
 Acc 1.0673
arctic25,dgl,1,1284,54.9942,1.0673

epoch:1286/50, training loss:0.11326410621404648
Train Acc 1.0705
 Acc 1.0640
arctic25,dgl,1,1285,55.0366,1.0640

epoch:1287/50, training loss:0.1148497611284256
Train Acc 1.0677
 Acc 1.0674
arctic25,dgl,1,1286,55.0791,1.0674

epoch:1288/50, training loss:0.11395809054374695
Train Acc 1.0707
 Acc 1.0676
new best val f1: 1.0676021580184036
arctic25,dgl,1,1287,55.1216,1.0676

epoch:1289/50, training loss:0.11337705701589584
Train Acc 1.0706
 Acc 1.0641
arctic25,dgl,1,1288,55.1641,1.0641

epoch:1290/50, training loss:0.114532470703125
Train Acc 1.0677
 Acc 1.0673
arctic25,dgl,1,1289,55.2065,1.0673

epoch:1291/50, training loss:0.11326926201581955
Train Acc 1.0705
 Acc 1.0675
arctic25,dgl,1,1290,55.2490,1.0675

epoch:1292/50, training loss:0.11276260018348694
Train Acc 1.0706
 Acc 1.0646
arctic25,dgl,1,1291,55.2914,1.0646

epoch:1293/50, training loss:0.11418884992599487
Train Acc 1.0681
 Acc 1.0675
arctic25,dgl,1,1292,55.3340,1.0675

epoch:1294/50, training loss:0.11316942423582077
Train Acc 1.0706
 Acc 1.0673
arctic25,dgl,1,1293,55.3765,1.0673

epoch:1295/50, training loss:0.1125890463590622
Train Acc 1.0705
 Acc 1.0639
arctic25,dgl,1,1294,55.4190,1.0639

epoch:1296/50, training loss:0.11455029249191284
Train Acc 1.0675
 Acc 1.0673
arctic25,dgl,1,1295,55.4615,1.0673

epoch:1297/50, training loss:0.11427046358585358
Train Acc 1.0706
 Acc 1.0674
arctic25,dgl,1,1296,55.5040,1.0674

epoch:1298/50, training loss:0.1134481206536293
Train Acc 1.0705
 Acc 1.0637
arctic25,dgl,1,1297,55.5465,1.0637

epoch:1299/50, training loss:0.11503127217292786
Train Acc 1.0674
 Acc 1.0673
arctic25,dgl,1,1298,55.5891,1.0673

epoch:1300/50, training loss:0.11382883042097092
Train Acc 1.0706
 Acc 1.0671
arctic25,dgl,1,1299,55.6316,1.0671

epoch:1301/50, training loss:0.11310294270515442
Train Acc 1.0704
 Acc 1.0638
arctic25,dgl,1,1300,55.6741,1.0638

epoch:1302/50, training loss:0.11433631181716919
Train Acc 1.0674
 Acc 1.0675
arctic25,dgl,1,1301,55.7166,1.0675

epoch:1303/50, training loss:0.11312776803970337
Train Acc 1.0707
 Acc 1.0676
arctic25,dgl,1,1302,55.7591,1.0676

epoch:1304/50, training loss:0.1124156266450882
Train Acc 1.0707
 Acc 1.0640
arctic25,dgl,1,1303,55.8016,1.0640

epoch:1305/50, training loss:0.11435003578662872
Train Acc 1.0676
 Acc 1.0674
arctic25,dgl,1,1304,55.8440,1.0674

epoch:1306/50, training loss:0.11348243057727814
Train Acc 1.0705
 Acc 1.0671
arctic25,dgl,1,1305,55.8865,1.0671

epoch:1307/50, training loss:0.11316943913698196
Train Acc 1.0704
 Acc 1.0639
arctic25,dgl,1,1306,55.9291,1.0639

epoch:1308/50, training loss:0.11418253183364868
Train Acc 1.0675
 Acc 1.0674
arctic25,dgl,1,1307,55.9716,1.0674

epoch:1309/50, training loss:0.11258015036582947
Train Acc 1.0706
 Acc 1.0676
arctic25,dgl,1,1308,56.0141,1.0676

epoch:1310/50, training loss:0.1123943105340004
Train Acc 1.0706
 Acc 1.0644
arctic25,dgl,1,1309,56.0565,1.0644

epoch:1311/50, training loss:0.11336517333984375
Train Acc 1.0680
 Acc 1.0669
arctic25,dgl,1,1310,56.0991,1.0669

epoch:1312/50, training loss:0.11271387338638306
Train Acc 1.0701
 Acc 1.0672
arctic25,dgl,1,1311,56.1416,1.0672

epoch:1313/50, training loss:0.11178148537874222
Train Acc 1.0703
 Acc 1.0644
arctic25,dgl,1,1312,56.1841,1.0644

epoch:1314/50, training loss:0.11390022933483124
Train Acc 1.0680
 Acc 1.0675
arctic25,dgl,1,1313,56.2266,1.0675

epoch:1315/50, training loss:0.11356880515813828
Train Acc 1.0706
 Acc 1.0673
arctic25,dgl,1,1314,56.2691,1.0673

epoch:1316/50, training loss:0.11305553466081619
Train Acc 1.0704
 Acc 1.0640
arctic25,dgl,1,1315,56.3117,1.0640

epoch:1317/50, training loss:0.11349528282880783
Train Acc 1.0677
 Acc 1.0671
arctic25,dgl,1,1316,56.3542,1.0671

epoch:1318/50, training loss:0.11205212771892548
Train Acc 1.0704
 Acc 1.0672
arctic25,dgl,1,1317,56.3967,1.0672

epoch:1319/50, training loss:0.1116713136434555
Train Acc 1.0703
 Acc 1.0652
arctic25,dgl,1,1318,56.4392,1.0652

epoch:1320/50, training loss:0.11286287754774094
Train Acc 1.0686
 Acc 1.0677
new best val f1: 1.067707171079403
arctic25,dgl,1,1319,56.4817,1.0677

epoch:1321/50, training loss:0.11149659007787704
Train Acc 1.0707
 Acc 1.0674
arctic25,dgl,1,1320,56.5242,1.0674

epoch:1322/50, training loss:0.1107424721121788
Train Acc 1.0705
 Acc 1.0645
arctic25,dgl,1,1321,56.5667,1.0645

epoch:1323/50, training loss:0.11271362006664276
Train Acc 1.0680
 Acc 1.0674
arctic25,dgl,1,1322,56.6092,1.0674

epoch:1324/50, training loss:0.11149150878190994
Train Acc 1.0705
 Acc 1.0679
new best val f1: 1.0678581273545897
arctic25,dgl,1,1323,56.6517,1.0679

epoch:1325/50, training loss:0.11082706600427628
Train Acc 1.0709
 Acc 1.0648
arctic25,dgl,1,1324,56.6942,1.0648

epoch:1326/50, training loss:0.11284507811069489
Train Acc 1.0684
 Acc 1.0676
arctic25,dgl,1,1325,56.7368,1.0676

epoch:1327/50, training loss:0.11177434772253036
Train Acc 1.0708
 Acc 1.0673
arctic25,dgl,1,1326,56.7793,1.0673

epoch:1328/50, training loss:0.11134703457355499
Train Acc 1.0705
 Acc 1.0639
arctic25,dgl,1,1327,56.8217,1.0639

epoch:1329/50, training loss:0.11313455551862717
Train Acc 1.0676
 Acc 1.0675
arctic25,dgl,1,1328,56.8643,1.0675

epoch:1330/50, training loss:0.11243651807308197
Train Acc 1.0708
 Acc 1.0678
arctic25,dgl,1,1329,56.9068,1.0678

epoch:1331/50, training loss:0.11183430999517441
Train Acc 1.0709
 Acc 1.0642
arctic25,dgl,1,1330,56.9493,1.0642

epoch:1332/50, training loss:0.11315812170505524
Train Acc 1.0679
 Acc 1.0676
arctic25,dgl,1,1331,56.9918,1.0676

epoch:1333/50, training loss:0.11202432215213776
Train Acc 1.0709
 Acc 1.0673
arctic25,dgl,1,1332,57.0344,1.0673

epoch:1334/50, training loss:0.11159148067235947
Train Acc 1.0705
 Acc 1.0642
arctic25,dgl,1,1333,57.0769,1.0642

epoch:1335/50, training loss:0.11256639659404755
Train Acc 1.0679
 Acc 1.0677
arctic25,dgl,1,1334,57.1194,1.0677

epoch:1336/50, training loss:0.11157447844743729
Train Acc 1.0709
 Acc 1.0677
arctic25,dgl,1,1335,57.1621,1.0677

epoch:1337/50, training loss:0.11088971793651581
Train Acc 1.0707
 Acc 1.0643
arctic25,dgl,1,1336,57.2046,1.0643

epoch:1338/50, training loss:0.11246620863676071
Train Acc 1.0680
 Acc 1.0676
arctic25,dgl,1,1337,57.2471,1.0676

epoch:1339/50, training loss:0.11145579069852829
Train Acc 1.0708
 Acc 1.0677
arctic25,dgl,1,1338,57.2897,1.0677

epoch:1340/50, training loss:0.11055043339729309
Train Acc 1.0709
 Acc 1.0640
arctic25,dgl,1,1339,57.3321,1.0640

epoch:1341/50, training loss:0.11276436597108841
Train Acc 1.0677
 Acc 1.0677
arctic25,dgl,1,1340,57.3746,1.0677

epoch:1342/50, training loss:0.11215436458587646
Train Acc 1.0708
 Acc 1.0677
arctic25,dgl,1,1341,57.4172,1.0677

epoch:1343/50, training loss:0.11161388456821442
Train Acc 1.0709
 Acc 1.0641
arctic25,dgl,1,1342,57.4597,1.0641

epoch:1344/50, training loss:0.1124451532959938
Train Acc 1.0678
 Acc 1.0677
arctic25,dgl,1,1343,57.5022,1.0677

epoch:1345/50, training loss:0.11073935776948929
Train Acc 1.0709
 Acc 1.0678
arctic25,dgl,1,1344,57.5447,1.0678

epoch:1346/50, training loss:0.11038481444120407
Train Acc 1.0710
 Acc 1.0644
arctic25,dgl,1,1345,57.5872,1.0644

epoch:1347/50, training loss:0.11212792992591858
Train Acc 1.0680
 Acc 1.0677
arctic25,dgl,1,1346,57.6297,1.0677

epoch:1348/50, training loss:0.11147285252809525
Train Acc 1.0708
 Acc 1.0676
arctic25,dgl,1,1347,57.6723,1.0676

epoch:1349/50, training loss:0.11092080175876617
Train Acc 1.0708
 Acc 1.0642
arctic25,dgl,1,1348,57.7148,1.0642

epoch:1350/50, training loss:0.11221206188201904
Train Acc 1.0679
 Acc 1.0677
arctic25,dgl,1,1349,57.7574,1.0677

epoch:1351/50, training loss:0.11101873219013214
Train Acc 1.0709
 Acc 1.0678
arctic25,dgl,1,1350,57.7998,1.0678

epoch:1352/50, training loss:0.11044957488775253
Train Acc 1.0709
 Acc 1.0638
arctic25,dgl,1,1351,57.8423,1.0638

epoch:1353/50, training loss:0.1125679761171341
Train Acc 1.0676
 Acc 1.0676
arctic25,dgl,1,1352,57.8849,1.0676

epoch:1354/50, training loss:0.11131645739078522
Train Acc 1.0709
 Acc 1.0678
arctic25,dgl,1,1353,57.9274,1.0678

epoch:1355/50, training loss:0.11087324470281601
Train Acc 1.0709
 Acc 1.0647
arctic25,dgl,1,1354,57.9699,1.0647

epoch:1356/50, training loss:0.1115468367934227
Train Acc 1.0684
 Acc 1.0678
arctic25,dgl,1,1355,58.0125,1.0678

epoch:1357/50, training loss:0.10971043258905411
Train Acc 1.0710
 Acc 1.0677
arctic25,dgl,1,1356,58.0550,1.0677

epoch:1358/50, training loss:0.10938329249620438
Train Acc 1.0708
 Acc 1.0651
arctic25,dgl,1,1357,58.0975,1.0651

epoch:1359/50, training loss:0.1107289046049118
Train Acc 1.0686
 Acc 1.0675
arctic25,dgl,1,1358,58.1400,1.0675

epoch:1360/50, training loss:0.10980521887540817
Train Acc 1.0708
 Acc 1.0678
arctic25,dgl,1,1359,58.1826,1.0678

epoch:1361/50, training loss:0.10905182361602783
Train Acc 1.0710
 Acc 1.0649
arctic25,dgl,1,1360,58.2250,1.0649

epoch:1362/50, training loss:0.11113195866346359
Train Acc 1.0685
 Acc 1.0678
arctic25,dgl,1,1361,58.2676,1.0678

epoch:1363/50, training loss:0.11060483753681183
Train Acc 1.0709
 Acc 1.0676
arctic25,dgl,1,1362,58.3101,1.0676

epoch:1364/50, training loss:0.11001458764076233
Train Acc 1.0708
 Acc 1.0642
arctic25,dgl,1,1363,58.3526,1.0642

epoch:1365/50, training loss:0.11139778792858124
Train Acc 1.0679
 Acc 1.0675
arctic25,dgl,1,1364,58.3951,1.0675

epoch:1366/50, training loss:0.1102411150932312
Train Acc 1.0708
 Acc 1.0677
arctic25,dgl,1,1365,58.4377,1.0677

epoch:1367/50, training loss:0.10968628525733948
Train Acc 1.0709
 Acc 1.0649
arctic25,dgl,1,1366,58.4802,1.0649

epoch:1368/50, training loss:0.11098039895296097
Train Acc 1.0684
 Acc 1.0678
arctic25,dgl,1,1367,58.5228,1.0678

epoch:1369/50, training loss:0.10969825834035873
Train Acc 1.0710
 Acc 1.0676
arctic25,dgl,1,1368,58.5653,1.0676

epoch:1370/50, training loss:0.10920627415180206
Train Acc 1.0707
 Acc 1.0645
arctic25,dgl,1,1369,58.6078,1.0645

epoch:1371/50, training loss:0.11077268421649933
Train Acc 1.0681
 Acc 1.0676
arctic25,dgl,1,1370,58.6503,1.0676

epoch:1372/50, training loss:0.10978875309228897
Train Acc 1.0709
 Acc 1.0679
new best val f1: 1.0678843806198397
arctic25,dgl,1,1371,58.6928,1.0679

epoch:1373/50, training loss:0.1090354323387146
Train Acc 1.0711
 Acc 1.0646
arctic25,dgl,1,1372,58.7354,1.0646

epoch:1374/50, training loss:0.110992930829525
Train Acc 1.0683
 Acc 1.0679
new best val f1: 1.0679237605177143
arctic25,dgl,1,1373,58.7779,1.0679

epoch:1375/50, training loss:0.10998749732971191
Train Acc 1.0710
 Acc 1.0677
arctic25,dgl,1,1374,58.8205,1.0677

epoch:1376/50, training loss:0.1096966415643692
Train Acc 1.0708
 Acc 1.0639
arctic25,dgl,1,1375,58.8630,1.0639

epoch:1377/50, training loss:0.11124581098556519
Train Acc 1.0677
 Acc 1.0676
arctic25,dgl,1,1376,58.9055,1.0676

epoch:1378/50, training loss:0.11007135361433029
Train Acc 1.0709
 Acc 1.0679
arctic25,dgl,1,1377,58.9480,1.0679

epoch:1379/50, training loss:0.1096789687871933
Train Acc 1.0711
 Acc 1.0645
arctic25,dgl,1,1378,58.9904,1.0645

epoch:1380/50, training loss:0.11095668375492096
Train Acc 1.0682
 Acc 1.0679
new best val f1: 1.0679368871503394
arctic25,dgl,1,1379,59.0329,1.0679

epoch:1381/50, training loss:0.10955189168453217
Train Acc 1.0710
 Acc 1.0677
arctic25,dgl,1,1380,59.0755,1.0677

epoch:1382/50, training loss:0.10927028954029083
Train Acc 1.0709
 Acc 1.0642
arctic25,dgl,1,1381,59.1180,1.0642

epoch:1383/50, training loss:0.11087875813245773
Train Acc 1.0678
 Acc 1.0676
arctic25,dgl,1,1382,59.1605,1.0676

epoch:1384/50, training loss:0.10996337980031967
Train Acc 1.0709
 Acc 1.0679
arctic25,dgl,1,1383,59.2030,1.0679

epoch:1385/50, training loss:0.10944425314664841
Train Acc 1.0712
 Acc 1.0645
arctic25,dgl,1,1384,59.2454,1.0645

epoch:1386/50, training loss:0.11073359102010727
Train Acc 1.0682
 Acc 1.0679
arctic25,dgl,1,1385,59.2880,1.0679

epoch:1387/50, training loss:0.10928806662559509
Train Acc 1.0710
 Acc 1.0677
arctic25,dgl,1,1386,59.3305,1.0677

epoch:1388/50, training loss:0.10886570811271667
Train Acc 1.0708
 Acc 1.0643
arctic25,dgl,1,1387,59.3730,1.0643

epoch:1389/50, training loss:0.11045047640800476
Train Acc 1.0680
 Acc 1.0677
arctic25,dgl,1,1388,59.4155,1.0677

epoch:1390/50, training loss:0.10929787158966064
Train Acc 1.0710
 Acc 1.0679
new best val f1: 1.0679434504666518
arctic25,dgl,1,1389,59.4580,1.0679

epoch:1391/50, training loss:0.10861620306968689
Train Acc 1.0712
 Acc 1.0648
arctic25,dgl,1,1390,59.5006,1.0648

epoch:1392/50, training loss:0.11019276827573776
Train Acc 1.0685
 Acc 1.0680
new best val f1: 1.0680090836297764
arctic25,dgl,1,1391,59.5430,1.0680

epoch:1393/50, training loss:0.10884097218513489
Train Acc 1.0711
 Acc 1.0678
arctic25,dgl,1,1392,59.5856,1.0678

epoch:1394/50, training loss:0.10850170999765396
Train Acc 1.0708
 Acc 1.0643
arctic25,dgl,1,1393,59.6281,1.0643

epoch:1395/50, training loss:0.11037159711122513
Train Acc 1.0680
 Acc 1.0677
arctic25,dgl,1,1394,59.6706,1.0677

epoch:1396/50, training loss:0.1095055490732193
Train Acc 1.0710
 Acc 1.0680
arctic25,dgl,1,1395,59.7131,1.0680

epoch:1397/50, training loss:0.10888530313968658
Train Acc 1.0712
 Acc 1.0646
arctic25,dgl,1,1396,59.7556,1.0646

epoch:1398/50, training loss:0.11023709923028946
Train Acc 1.0683
 Acc 1.0680
new best val f1: 1.0680156469460889
arctic25,dgl,1,1397,59.7981,1.0680

epoch:1399/50, training loss:0.10853728652000427
Train Acc 1.0711
 Acc 1.0677
arctic25,dgl,1,1398,59.8407,1.0677

epoch:1400/50, training loss:0.10822935402393341
Train Acc 1.0708
 Acc 1.0646
arctic25,dgl,1,1399,59.8832,1.0646

epoch:1401/50, training loss:0.1096838116645813
Train Acc 1.0682
 Acc 1.0676
arctic25,dgl,1,1400,59.9258,1.0676

epoch:1402/50, training loss:0.10851481556892395
Train Acc 1.0710
 Acc 1.0680
arctic25,dgl,1,1401,59.9683,1.0680

epoch:1403/50, training loss:0.10783004760742188
Train Acc 1.0712
 Acc 1.0650
arctic25,dgl,1,1402,60.0108,1.0650

epoch:1404/50, training loss:0.10970348119735718
Train Acc 1.0686
 Acc 1.0680
new best val f1: 1.068028773578714
arctic25,dgl,1,1403,60.0534,1.0680

epoch:1405/50, training loss:0.10852188616991043
Train Acc 1.0711
 Acc 1.0677
arctic25,dgl,1,1404,60.0959,1.0677

epoch:1406/50, training loss:0.10821114480495453
Train Acc 1.0708
 Acc 1.0643
arctic25,dgl,1,1405,60.1384,1.0643

epoch:1407/50, training loss:0.10991255193948746
Train Acc 1.0679
 Acc 1.0676
arctic25,dgl,1,1406,60.1810,1.0676

epoch:1408/50, training loss:0.10892347991466522
Train Acc 1.0709
 Acc 1.0680
arctic25,dgl,1,1407,60.2235,1.0680

epoch:1409/50, training loss:0.10831022262573242
Train Acc 1.0712
 Acc 1.0647
arctic25,dgl,1,1408,60.2660,1.0647

epoch:1410/50, training loss:0.10979245603084564
Train Acc 1.0684
 Acc 1.0680
arctic25,dgl,1,1409,60.3086,1.0680

epoch:1411/50, training loss:0.10832223296165466
Train Acc 1.0712
 Acc 1.0677
arctic25,dgl,1,1410,60.3511,1.0677

epoch:1412/50, training loss:0.10797243565320969
Train Acc 1.0708
 Acc 1.0641
arctic25,dgl,1,1411,60.3935,1.0641

epoch:1413/50, training loss:0.10994383692741394
Train Acc 1.0677
 Acc 1.0677
arctic25,dgl,1,1412,60.4360,1.0677

epoch:1414/50, training loss:0.10927640646696091
Train Acc 1.0709
 Acc 1.0680
arctic25,dgl,1,1413,60.4786,1.0680

epoch:1415/50, training loss:0.10846561938524246
Train Acc 1.0712
 Acc 1.0646
arctic25,dgl,1,1414,60.5211,1.0646

epoch:1416/50, training loss:0.1097872406244278
Train Acc 1.0684
 Acc 1.0681
new best val f1: 1.0680681534765886
arctic25,dgl,1,1415,60.5636,1.0681

epoch:1417/50, training loss:0.10804549604654312
Train Acc 1.0712
 Acc 1.0678
arctic25,dgl,1,1416,60.6062,1.0678

epoch:1418/50, training loss:0.1076255738735199
Train Acc 1.0708
 Acc 1.0643
arctic25,dgl,1,1417,60.6487,1.0643

epoch:1419/50, training loss:0.10951701551675797
Train Acc 1.0679
 Acc 1.0677
arctic25,dgl,1,1418,60.6913,1.0677

epoch:1420/50, training loss:0.10880441963672638
Train Acc 1.0709
 Acc 1.0680
arctic25,dgl,1,1419,60.7338,1.0680

epoch:1421/50, training loss:0.1077817976474762
Train Acc 1.0712
 Acc 1.0641
arctic25,dgl,1,1420,60.7763,1.0641

epoch:1422/50, training loss:0.11029215902090073
Train Acc 1.0679
 Acc 1.0681
new best val f1: 1.0681403499560258
arctic25,dgl,1,1421,60.8188,1.0681

epoch:1423/50, training loss:0.10853224992752075
Train Acc 1.0713
 Acc 1.0679
arctic25,dgl,1,1422,60.8613,1.0679

epoch:1424/50, training loss:0.10807514190673828
Train Acc 1.0710
 Acc 1.0644
arctic25,dgl,1,1423,60.9038,1.0644

epoch:1425/50, training loss:0.10915250331163406
Train Acc 1.0681
 Acc 1.0678
arctic25,dgl,1,1424,60.9464,1.0678

epoch:1426/50, training loss:0.10752382129430771
Train Acc 1.0710
 Acc 1.0682
new best val f1: 1.0681534765886507
arctic25,dgl,1,1425,60.9889,1.0682

epoch:1427/50, training loss:0.10671021789312363
Train Acc 1.0713
 Acc 1.0646
arctic25,dgl,1,1426,61.0318,1.0646

epoch:1428/50, training loss:0.10946767777204514
Train Acc 1.0684
 Acc 1.0682
new best val f1: 1.0681928564865255
arctic25,dgl,1,1427,61.0744,1.0682

epoch:1429/50, training loss:0.10892920196056366
Train Acc 1.0713
 Acc 1.0680
arctic25,dgl,1,1428,61.1169,1.0680

epoch:1430/50, training loss:0.10827343910932541
Train Acc 1.0711
 Acc 1.0638
arctic25,dgl,1,1429,61.1594,1.0638

epoch:1431/50, training loss:0.10976811498403549
Train Acc 1.0676
 Acc 1.0680
arctic25,dgl,1,1430,61.2019,1.0680

epoch:1432/50, training loss:0.10736843198537827
Train Acc 1.0711
 Acc 1.0682
new best val f1: 1.0682059831191504
arctic25,dgl,1,1431,61.2444,1.0682

epoch:1433/50, training loss:0.1071145310997963
Train Acc 1.0713
 Acc 1.0655
arctic25,dgl,1,1432,61.2869,1.0655

epoch:1434/50, training loss:0.10798873007297516
Train Acc 1.0690
 Acc 1.0681
arctic25,dgl,1,1433,61.3294,1.0681

epoch:1435/50, training loss:0.10630631446838379
Train Acc 1.0714
 Acc 1.0680
arctic25,dgl,1,1434,61.3720,1.0680

epoch:1436/50, training loss:0.1061490848660469
Train Acc 1.0711
 Acc 1.0654
arctic25,dgl,1,1435,61.4144,1.0654

epoch:1437/50, training loss:0.10759972780942917
Train Acc 1.0689
 Acc 1.0681
arctic25,dgl,1,1436,61.4569,1.0681

epoch:1438/50, training loss:0.10704898089170456
Train Acc 1.0713
 Acc 1.0682
arctic25,dgl,1,1437,61.4994,1.0682

epoch:1439/50, training loss:0.10628283768892288
Train Acc 1.0713
 Acc 1.0646
arctic25,dgl,1,1438,61.5419,1.0646

epoch:1440/50, training loss:0.10877533257007599
Train Acc 1.0683
 Acc 1.0679
arctic25,dgl,1,1439,61.5845,1.0679

epoch:1441/50, training loss:0.10876572132110596
Train Acc 1.0713
 Acc 1.0679
arctic25,dgl,1,1440,61.6270,1.0679

epoch:1442/50, training loss:0.10817357152700424
Train Acc 1.0711
 Acc 1.0650
arctic25,dgl,1,1441,61.6695,1.0650

epoch:1443/50, training loss:0.10798916965723038
Train Acc 1.0686
 Acc 1.0681
arctic25,dgl,1,1442,61.7120,1.0681

epoch:1444/50, training loss:0.10598954558372498
Train Acc 1.0714
 Acc 1.0679
arctic25,dgl,1,1443,61.7545,1.0679

epoch:1445/50, training loss:0.10575603693723679
Train Acc 1.0711
 Acc 1.0659
arctic25,dgl,1,1444,61.7970,1.0659

epoch:1446/50, training loss:0.1068129763007164
Train Acc 1.0694
 Acc 1.0681
arctic25,dgl,1,1445,61.8395,1.0681

epoch:1447/50, training loss:0.1060493141412735
Train Acc 1.0713
 Acc 1.0680
arctic25,dgl,1,1446,61.8820,1.0680

epoch:1448/50, training loss:0.10515082627534866
Train Acc 1.0713
 Acc 1.0655
arctic25,dgl,1,1447,61.9246,1.0655

epoch:1449/50, training loss:0.10717417299747467
Train Acc 1.0689
 Acc 1.0681
arctic25,dgl,1,1448,61.9671,1.0681

epoch:1450/50, training loss:0.10706520825624466
Train Acc 1.0713
 Acc 1.0682
new best val f1: 1.0682125464354628
arctic25,dgl,1,1449,62.0097,1.0682

epoch:1451/50, training loss:0.10608941316604614
Train Acc 1.0714
 Acc 1.0644
arctic25,dgl,1,1450,62.0522,1.0644

epoch:1452/50, training loss:0.10840755701065063
Train Acc 1.0681
 Acc 1.0680
arctic25,dgl,1,1451,62.0947,1.0680

epoch:1453/50, training loss:0.10775415599346161
Train Acc 1.0713
 Acc 1.0681
arctic25,dgl,1,1452,62.1372,1.0681

epoch:1454/50, training loss:0.10717789828777313
Train Acc 1.0713
 Acc 1.0649
arctic25,dgl,1,1453,62.1798,1.0649

epoch:1455/50, training loss:0.1077193170785904
Train Acc 1.0686
 Acc 1.0682
arctic25,dgl,1,1454,62.2223,1.0682

epoch:1456/50, training loss:0.10585930198431015
Train Acc 1.0714
 Acc 1.0682
arctic25,dgl,1,1455,62.2649,1.0682

epoch:1457/50, training loss:0.10567541420459747
Train Acc 1.0714
 Acc 1.0657
arctic25,dgl,1,1456,62.3074,1.0657

epoch:1458/50, training loss:0.1066872626543045
Train Acc 1.0692
 Acc 1.0681
arctic25,dgl,1,1457,62.3499,1.0681

epoch:1459/50, training loss:0.1056462973356247
Train Acc 1.0713
 Acc 1.0682
arctic25,dgl,1,1458,62.3924,1.0682

epoch:1460/50, training loss:0.10490140318870544
Train Acc 1.0714
 Acc 1.0656
arctic25,dgl,1,1459,62.4349,1.0656

epoch:1461/50, training loss:0.10682214051485062
Train Acc 1.0690
 Acc 1.0682
new best val f1: 1.0682191097517755
arctic25,dgl,1,1460,62.4774,1.0682

epoch:1462/50, training loss:0.10673493146896362
Train Acc 1.0714
 Acc 1.0683
new best val f1: 1.06825848964965
arctic25,dgl,1,1461,62.5200,1.0683

epoch:1463/50, training loss:0.1055067628622055
Train Acc 1.0714
 Acc 1.0643
arctic25,dgl,1,1462,62.5625,1.0643

epoch:1464/50, training loss:0.10805509984493256
Train Acc 1.0681
 Acc 1.0681
arctic25,dgl,1,1463,62.6050,1.0681

epoch:1465/50, training loss:0.10721378028392792
Train Acc 1.0714
 Acc 1.0682
arctic25,dgl,1,1464,62.6476,1.0682

epoch:1466/50, training loss:0.1067999005317688
Train Acc 1.0715
 Acc 1.0650
arctic25,dgl,1,1465,62.6900,1.0650

epoch:1467/50, training loss:0.10710957646369934
Train Acc 1.0687
 Acc 1.0682
arctic25,dgl,1,1466,62.7325,1.0682

epoch:1468/50, training loss:0.10511390119791031
Train Acc 1.0714
 Acc 1.0682
arctic25,dgl,1,1467,62.7751,1.0682

epoch:1469/50, training loss:0.10493363440036774
Train Acc 1.0715
 Acc 1.0660
arctic25,dgl,1,1468,62.8176,1.0660

epoch:1470/50, training loss:0.10612428933382034
Train Acc 1.0693
 Acc 1.0682
arctic25,dgl,1,1469,62.8601,1.0682

epoch:1471/50, training loss:0.1052103117108345
Train Acc 1.0715
 Acc 1.0682
arctic25,dgl,1,1470,62.9027,1.0682

epoch:1472/50, training loss:0.10451891273260117
Train Acc 1.0715
 Acc 1.0657
arctic25,dgl,1,1471,62.9451,1.0657

epoch:1473/50, training loss:0.10619474947452545
Train Acc 1.0692
 Acc 1.0682
arctic25,dgl,1,1472,62.9876,1.0682

epoch:1474/50, training loss:0.10553980618715286
Train Acc 1.0714
 Acc 1.0682
arctic25,dgl,1,1473,63.0302,1.0682

epoch:1475/50, training loss:0.1047944501042366
Train Acc 1.0714
 Acc 1.0650
arctic25,dgl,1,1474,63.0727,1.0650

epoch:1476/50, training loss:0.10683716833591461
Train Acc 1.0687
 Acc 1.0683
new best val f1: 1.0682913062312125
arctic25,dgl,1,1475,63.1152,1.0683

epoch:1477/50, training loss:0.10638963431119919
Train Acc 1.0715
 Acc 1.0682
arctic25,dgl,1,1476,63.1577,1.0682

epoch:1478/50, training loss:0.10555817931890488
Train Acc 1.0715
 Acc 1.0648
arctic25,dgl,1,1477,63.2002,1.0648

epoch:1479/50, training loss:0.1070440486073494
Train Acc 1.0686
 Acc 1.0682
arctic25,dgl,1,1478,63.2439,1.0682

epoch:1480/50, training loss:0.10570240020751953
Train Acc 1.0714
 Acc 1.0682
arctic25,dgl,1,1479,63.2865,1.0682

epoch:1481/50, training loss:0.1053454577922821
Train Acc 1.0714
 Acc 1.0651
arctic25,dgl,1,1480,63.3290,1.0651

epoch:1482/50, training loss:0.10659336298704147
Train Acc 1.0688
 Acc 1.0683
arctic25,dgl,1,1481,63.3716,1.0683

epoch:1483/50, training loss:0.1053348258137703
Train Acc 1.0715
 Acc 1.0682
arctic25,dgl,1,1482,63.4141,1.0682

epoch:1484/50, training loss:0.10472208261489868
Train Acc 1.0714
 Acc 1.0651
arctic25,dgl,1,1483,63.4566,1.0651

epoch:1485/50, training loss:0.10645733773708344
Train Acc 1.0688
 Acc 1.0683
arctic25,dgl,1,1484,63.4991,1.0683

epoch:1486/50, training loss:0.10563073307275772
Train Acc 1.0715
 Acc 1.0683
arctic25,dgl,1,1485,63.5416,1.0683

epoch:1487/50, training loss:0.10485927760601044
Train Acc 1.0715
 Acc 1.0648
arctic25,dgl,1,1486,63.5842,1.0648

epoch:1488/50, training loss:0.10681083798408508
Train Acc 1.0686
 Acc 1.0682
arctic25,dgl,1,1487,63.6267,1.0682

epoch:1489/50, training loss:0.10598668456077576
Train Acc 1.0715
 Acc 1.0682
arctic25,dgl,1,1488,63.6692,1.0682

epoch:1490/50, training loss:0.1053326278924942
Train Acc 1.0714
 Acc 1.0648
arctic25,dgl,1,1489,63.7117,1.0648

epoch:1491/50, training loss:0.10660690814256668
Train Acc 1.0687
 Acc 1.0683
new best val f1: 1.0683044328638374
arctic25,dgl,1,1490,63.7543,1.0683

epoch:1492/50, training loss:0.10520342737436295
Train Acc 1.0715
 Acc 1.0683
arctic25,dgl,1,1491,63.7967,1.0683

epoch:1493/50, training loss:0.1046486496925354
Train Acc 1.0715
 Acc 1.0652
arctic25,dgl,1,1492,63.8393,1.0652

epoch:1494/50, training loss:0.10607871413230896
Train Acc 1.0688
 Acc 1.0683
arctic25,dgl,1,1493,63.8818,1.0683

epoch:1495/50, training loss:0.1048283651471138
Train Acc 1.0715
 Acc 1.0682
arctic25,dgl,1,1494,63.9243,1.0682

epoch:1496/50, training loss:0.10439657419919968
Train Acc 1.0715
 Acc 1.0651
arctic25,dgl,1,1495,63.9668,1.0651

epoch:1497/50, training loss:0.10612767189741135
Train Acc 1.0688
 Acc 1.0683
arctic25,dgl,1,1496,64.0094,1.0683

epoch:1498/50, training loss:0.10522407293319702
Train Acc 1.0716
 Acc 1.0683
new best val f1: 1.068324122812775
arctic25,dgl,1,1497,64.0519,1.0683

epoch:1499/50, training loss:0.10453912615776062
Train Acc 1.0715
 Acc 1.0650
arctic25,dgl,1,1498,64.0944,1.0650

epoch:1500/50, training loss:0.10625013709068298
Train Acc 1.0687
 Acc 1.0684
new best val f1: 1.068356939394337
arctic25,dgl,1,1499,64.1370,1.0684

epoch:1501/50, training loss:0.10519208014011383
Train Acc 1.0715
 Acc 1.0683
arctic25,dgl,1,1500,64.1795,1.0683

epoch:1502/50, training loss:0.10450085252523422
Train Acc 1.0715
 Acc 1.0649
arctic25,dgl,1,1501,64.2221,1.0649

epoch:1503/50, training loss:0.10615704208612442
Train Acc 1.0687
 Acc 1.0683
arctic25,dgl,1,1502,64.2645,1.0683

epoch:1504/50, training loss:0.10495644062757492
Train Acc 1.0715
 Acc 1.0683
arctic25,dgl,1,1503,64.3070,1.0683

epoch:1505/50, training loss:0.10434982180595398
Train Acc 1.0715
 Acc 1.0653
arctic25,dgl,1,1504,64.3495,1.0653

epoch:1506/50, training loss:0.10573814064264297
Train Acc 1.0689
 Acc 1.0684
new best val f1: 1.0684225725574619
arctic25,dgl,1,1505,64.3920,1.0684

epoch:1507/50, training loss:0.10428451001644135
Train Acc 1.0716
 Acc 1.0683
arctic25,dgl,1,1506,64.4390,1.0683

epoch:1508/50, training loss:0.10384950786828995
Train Acc 1.0714
 Acc 1.0656
arctic25,dgl,1,1507,64.4815,1.0656

epoch:1509/50, training loss:0.10527381300926208
Train Acc 1.0690
 Acc 1.0682
arctic25,dgl,1,1508,64.5240,1.0682

epoch:1510/50, training loss:0.10406351089477539
Train Acc 1.0715
 Acc 1.0683
arctic25,dgl,1,1509,64.5665,1.0683

epoch:1511/50, training loss:0.10374625027179718
Train Acc 1.0715
 Acc 1.0656
arctic25,dgl,1,1510,64.6090,1.0656

epoch:1512/50, training loss:0.10531120747327805
Train Acc 1.0691
 Acc 1.0684
arctic25,dgl,1,1511,64.6516,1.0684

epoch:1513/50, training loss:0.10448836535215378
Train Acc 1.0716
 Acc 1.0682
arctic25,dgl,1,1512,64.6941,1.0682

epoch:1514/50, training loss:0.10404840856790543
Train Acc 1.0714
 Acc 1.0649
arctic25,dgl,1,1513,64.7366,1.0649

epoch:1515/50, training loss:0.10563794523477554
Train Acc 1.0686
 Acc 1.0682
arctic25,dgl,1,1514,64.7791,1.0682

epoch:1516/50, training loss:0.10490675270557404
Train Acc 1.0715
 Acc 1.0682
arctic25,dgl,1,1515,64.8216,1.0682

epoch:1517/50, training loss:0.10421718657016754
Train Acc 1.0715
 Acc 1.0653
arctic25,dgl,1,1516,64.8641,1.0653

epoch:1518/50, training loss:0.10537410527467728
Train Acc 1.0689
 Acc 1.0684
new best val f1: 1.0684291358737743
arctic25,dgl,1,1517,64.9067,1.0684

epoch:1519/50, training loss:0.104103222489357
Train Acc 1.0716
 Acc 1.0682
arctic25,dgl,1,1518,64.9492,1.0682

epoch:1520/50, training loss:0.10360163450241089
Train Acc 1.0713
 Acc 1.0656
arctic25,dgl,1,1519,64.9917,1.0656

epoch:1521/50, training loss:0.10475610941648483
Train Acc 1.0690
 Acc 1.0681
arctic25,dgl,1,1520,65.0343,1.0681

epoch:1522/50, training loss:0.10349960625171661
Train Acc 1.0714
 Acc 1.0682
arctic25,dgl,1,1521,65.0768,1.0682

epoch:1523/50, training loss:0.10293584316968918
Train Acc 1.0716
 Acc 1.0662
arctic25,dgl,1,1522,65.1192,1.0662

epoch:1524/50, training loss:0.10439033061265945
Train Acc 1.0695
 Acc 1.0684
new best val f1: 1.0684356991900867
arctic25,dgl,1,1523,65.1618,1.0684

epoch:1525/50, training loss:0.10351842641830444
Train Acc 1.0717
 Acc 1.0683
arctic25,dgl,1,1524,65.2043,1.0683

epoch:1526/50, training loss:0.10276935994625092
Train Acc 1.0715
 Acc 1.0654
arctic25,dgl,1,1525,65.2468,1.0654

epoch:1527/50, training loss:0.10473576188087463
Train Acc 1.0689
 Acc 1.0683
arctic25,dgl,1,1526,65.2893,1.0683

epoch:1528/50, training loss:0.10429123044013977
Train Acc 1.0716
 Acc 1.0684
arctic25,dgl,1,1527,65.3319,1.0684

epoch:1529/50, training loss:0.10337857156991959
Train Acc 1.0716
 Acc 1.0645
arctic25,dgl,1,1528,65.3744,1.0645

epoch:1530/50, training loss:0.10580100864171982
Train Acc 1.0685
 Acc 1.0685
new best val f1: 1.0684750790879616
arctic25,dgl,1,1529,65.4170,1.0685

epoch:1531/50, training loss:0.10487188398838043
Train Acc 1.0717
 Acc 1.0683
arctic25,dgl,1,1530,65.4595,1.0683

epoch:1532/50, training loss:0.10454913228750229
Train Acc 1.0715
 Acc 1.0651
arctic25,dgl,1,1531,65.5020,1.0651

epoch:1533/50, training loss:0.10485653579235077
Train Acc 1.0688
 Acc 1.0685
arctic25,dgl,1,1532,65.5446,1.0685

epoch:1534/50, training loss:0.102995365858078
Train Acc 1.0716
 Acc 1.0684
arctic25,dgl,1,1533,65.5872,1.0684

epoch:1535/50, training loss:0.1025778278708458
Train Acc 1.0717
 Acc 1.0659
arctic25,dgl,1,1534,65.6296,1.0659

epoch:1536/50, training loss:0.10421540588140488
Train Acc 1.0693
 Acc 1.0684
arctic25,dgl,1,1535,65.6722,1.0684

epoch:1537/50, training loss:0.10374601930379868
Train Acc 1.0717
 Acc 1.0683
arctic25,dgl,1,1536,65.7146,1.0683

epoch:1538/50, training loss:0.10270802676677704
Train Acc 1.0716
 Acc 1.0645
arctic25,dgl,1,1537,65.7571,1.0645

epoch:1539/50, training loss:0.10555899888277054
Train Acc 1.0683
 Acc 1.0686
new best val f1: 1.0685538388837112
arctic25,dgl,1,1538,65.7997,1.0686

epoch:1540/50, training loss:0.10493006557226181
Train Acc 1.0717
 Acc 1.0685
arctic25,dgl,1,1539,65.8422,1.0685

epoch:1541/50, training loss:0.10440070927143097
Train Acc 1.0717
 Acc 1.0649
arctic25,dgl,1,1540,65.8847,1.0649

epoch:1542/50, training loss:0.10496324300765991
Train Acc 1.0688
 Acc 1.0684
arctic25,dgl,1,1541,65.9272,1.0684

epoch:1543/50, training loss:0.10335244238376617
Train Acc 1.0716
 Acc 1.0684
arctic25,dgl,1,1542,65.9697,1.0684

epoch:1544/50, training loss:0.10268441587686539
Train Acc 1.0715
 Acc 1.0645
arctic25,dgl,1,1543,66.0122,1.0645

epoch:1545/50, training loss:0.10528998076915741
Train Acc 1.0684
 Acc 1.0685
arctic25,dgl,1,1544,66.0548,1.0685

epoch:1546/50, training loss:0.10494484007358551
Train Acc 1.0717
 Acc 1.0685
arctic25,dgl,1,1545,66.0973,1.0685

epoch:1547/50, training loss:0.1043684184551239
Train Acc 1.0717
 Acc 1.0653
arctic25,dgl,1,1546,66.1399,1.0653

epoch:1548/50, training loss:0.1044149249792099
Train Acc 1.0690
 Acc 1.0684
arctic25,dgl,1,1547,66.1823,1.0684

epoch:1549/50, training loss:0.10216188430786133
Train Acc 1.0717
 Acc 1.0683
arctic25,dgl,1,1548,66.2249,1.0683

epoch:1550/50, training loss:0.10206273198127747
Train Acc 1.0715
 Acc 1.0662
arctic25,dgl,1,1549,66.2674,1.0662

epoch:1551/50, training loss:0.1032416969537735
Train Acc 1.0696
 Acc 1.0684
arctic25,dgl,1,1550,66.3099,1.0684

epoch:1552/50, training loss:0.1025233194231987
Train Acc 1.0716
 Acc 1.0684
arctic25,dgl,1,1551,66.3525,1.0684

epoch:1553/50, training loss:0.10164810717105865
Train Acc 1.0717
 Acc 1.0654
arctic25,dgl,1,1552,66.3950,1.0654

epoch:1554/50, training loss:0.10421697795391083
Train Acc 1.0690
 Acc 1.0684
arctic25,dgl,1,1553,66.4375,1.0684

epoch:1555/50, training loss:0.10466857999563217
Train Acc 1.0717
 Acc 1.0684
arctic25,dgl,1,1554,66.4800,1.0684

epoch:1556/50, training loss:0.10381905734539032
Train Acc 1.0715
 Acc 1.0645
arctic25,dgl,1,1555,66.5225,1.0645

epoch:1557/50, training loss:0.10493995249271393
Train Acc 1.0683
 Acc 1.0683
arctic25,dgl,1,1556,66.5651,1.0683

epoch:1558/50, training loss:0.10266906768083572
Train Acc 1.0716
 Acc 1.0683
arctic25,dgl,1,1557,66.6077,1.0683

epoch:1559/50, training loss:0.10247499495744705
Train Acc 1.0717
 Acc 1.0657
arctic25,dgl,1,1558,66.6502,1.0657

epoch:1560/50, training loss:0.1036403626203537
Train Acc 1.0693
 Acc 1.0686
arctic25,dgl,1,1559,66.6926,1.0686

epoch:1561/50, training loss:0.10277898609638214
Train Acc 1.0717
 Acc 1.0684
arctic25,dgl,1,1560,66.7352,1.0684

epoch:1562/50, training loss:0.10200084745883942
Train Acc 1.0715
 Acc 1.0648
arctic25,dgl,1,1561,66.7776,1.0648

epoch:1563/50, training loss:0.1043054386973381
Train Acc 1.0687
 Acc 1.0684
arctic25,dgl,1,1562,66.8201,1.0684

epoch:1564/50, training loss:0.10428176820278168
Train Acc 1.0717
 Acc 1.0684
arctic25,dgl,1,1563,66.8626,1.0684

epoch:1565/50, training loss:0.10357154160737991
Train Acc 1.0717
 Acc 1.0649
arctic25,dgl,1,1564,66.9051,1.0649

epoch:1566/50, training loss:0.10451341420412064
Train Acc 1.0687
 Acc 1.0686
new best val f1: 1.0685604022000237
arctic25,dgl,1,1565,66.9477,1.0686

epoch:1567/50, training loss:0.10306549072265625
Train Acc 1.0718
 Acc 1.0684
arctic25,dgl,1,1566,66.9903,1.0684

epoch:1568/50, training loss:0.1024734154343605
Train Acc 1.0715
 Acc 1.0646
arctic25,dgl,1,1567,67.0327,1.0646

epoch:1569/50, training loss:0.10438428819179535
Train Acc 1.0685
 Acc 1.0684
arctic25,dgl,1,1568,67.0753,1.0684

epoch:1570/50, training loss:0.10360771417617798
Train Acc 1.0717
 Acc 1.0685
arctic25,dgl,1,1569,67.1178,1.0685

epoch:1571/50, training loss:0.10293451696634293
Train Acc 1.0718
 Acc 1.0655
arctic25,dgl,1,1570,67.1603,1.0655

epoch:1572/50, training loss:0.10376739501953125
Train Acc 1.0691
 Acc 1.0686
new best val f1: 1.0685866554652734
arctic25,dgl,1,1571,67.2028,1.0686

epoch:1573/50, training loss:0.10174935311079025
Train Acc 1.0718
 Acc 1.0684
arctic25,dgl,1,1572,67.2454,1.0684

epoch:1574/50, training loss:0.10151296108961105
Train Acc 1.0715
 Acc 1.0659
arctic25,dgl,1,1573,67.2879,1.0659

epoch:1575/50, training loss:0.10293301939964294
Train Acc 1.0693
 Acc 1.0684
arctic25,dgl,1,1574,67.3304,1.0684

epoch:1576/50, training loss:0.10192281007766724
Train Acc 1.0716
 Acc 1.0686
arctic25,dgl,1,1575,67.3733,1.0686

epoch:1577/50, training loss:0.10122168064117432
Train Acc 1.0718
 Acc 1.0656
arctic25,dgl,1,1576,67.4158,1.0656

epoch:1578/50, training loss:0.10340096801519394
Train Acc 1.0692
 Acc 1.0687
new best val f1: 1.0686719785773355
arctic25,dgl,1,1577,67.4583,1.0687

epoch:1579/50, training loss:0.10290119051933289
Train Acc 1.0718
 Acc 1.0685
arctic25,dgl,1,1578,67.5009,1.0685

epoch:1580/50, training loss:0.10225024074316025
Train Acc 1.0717
 Acc 1.0648
arctic25,dgl,1,1579,67.5434,1.0648

epoch:1581/50, training loss:0.10380944609642029
Train Acc 1.0687
 Acc 1.0685
arctic25,dgl,1,1580,67.5859,1.0685

epoch:1582/50, training loss:0.1025228276848793
Train Acc 1.0717
 Acc 1.0686
arctic25,dgl,1,1581,67.6284,1.0686

epoch:1583/50, training loss:0.10209882259368896
Train Acc 1.0718
 Acc 1.0655
arctic25,dgl,1,1582,67.6709,1.0655

epoch:1584/50, training loss:0.10323555767536163
Train Acc 1.0692
 Acc 1.0687
arctic25,dgl,1,1583,67.7134,1.0687

epoch:1585/50, training loss:0.1017480418086052
Train Acc 1.0718
 Acc 1.0685
arctic25,dgl,1,1584,67.7559,1.0685

epoch:1586/50, training loss:0.10138282924890518
Train Acc 1.0716
 Acc 1.0651
arctic25,dgl,1,1585,67.7984,1.0651

epoch:1587/50, training loss:0.10329378396272659
Train Acc 1.0690
 Acc 1.0685
arctic25,dgl,1,1586,67.8409,1.0685

epoch:1588/50, training loss:0.10284046083688736
Train Acc 1.0718
 Acc 1.0686
arctic25,dgl,1,1587,67.8835,1.0686

epoch:1589/50, training loss:0.1021670326590538
Train Acc 1.0718
 Acc 1.0655
arctic25,dgl,1,1588,67.9259,1.0655

epoch:1590/50, training loss:0.10325449705123901
Train Acc 1.0692
 Acc 1.0686
arctic25,dgl,1,1589,67.9684,1.0686

epoch:1591/50, training loss:0.10157784074544907
Train Acc 1.0718
 Acc 1.0684
arctic25,dgl,1,1590,68.0109,1.0684

epoch:1592/50, training loss:0.1013292595744133
Train Acc 1.0716
 Acc 1.0653
arctic25,dgl,1,1591,68.0534,1.0653

epoch:1593/50, training loss:0.10296770185232162
Train Acc 1.0690
 Acc 1.0685
arctic25,dgl,1,1592,68.0959,1.0685

epoch:1594/50, training loss:0.10208477079868317
Train Acc 1.0718
 Acc 1.0686
arctic25,dgl,1,1593,68.1384,1.0686

epoch:1595/50, training loss:0.10148168355226517
Train Acc 1.0718
 Acc 1.0653
arctic25,dgl,1,1594,68.1809,1.0653

epoch:1596/50, training loss:0.10323647409677505
Train Acc 1.0692
 Acc 1.0686
arctic25,dgl,1,1595,68.2234,1.0686

epoch:1597/50, training loss:0.10217951983213425
Train Acc 1.0718
 Acc 1.0685
arctic25,dgl,1,1596,68.2658,1.0685

epoch:1598/50, training loss:0.10164730250835419
Train Acc 1.0714
 Acc 1.0648
arctic25,dgl,1,1597,68.3083,1.0648

epoch:1599/50, training loss:0.10336680710315704
Train Acc 1.0687
 Acc 1.0684
arctic25,dgl,1,1598,68.3509,1.0684

epoch:1600/50, training loss:0.1024160236120224
Train Acc 1.0717
 Acc 1.0684
arctic25,dgl,1,1599,68.3934,1.0684

epoch:1601/50, training loss:0.10170236229896545
Train Acc 1.0717
 Acc 1.0653
arctic25,dgl,1,1600,68.4360,1.0653

epoch:1602/50, training loss:0.10314850509166718
Train Acc 1.0691
 Acc 1.0687
arctic25,dgl,1,1601,68.4786,1.0687

epoch:1603/50, training loss:0.10183346271514893
Train Acc 1.0719
 Acc 1.0684
arctic25,dgl,1,1602,68.5211,1.0684

epoch:1604/50, training loss:0.10132776945829391
Train Acc 1.0714
 Acc 1.0650
arctic25,dgl,1,1603,68.5636,1.0650

epoch:1605/50, training loss:0.10295535624027252
Train Acc 1.0688
 Acc 1.0684
arctic25,dgl,1,1604,68.6062,1.0684

epoch:1606/50, training loss:0.10191401094198227
Train Acc 1.0717
 Acc 1.0686
arctic25,dgl,1,1605,68.6486,1.0686

epoch:1607/50, training loss:0.1012258231639862
Train Acc 1.0719
 Acc 1.0651
arctic25,dgl,1,1606,68.6911,1.0651

epoch:1608/50, training loss:0.10319508612155914
Train Acc 1.0691
 Acc 1.0686
arctic25,dgl,1,1607,68.7337,1.0686

epoch:1609/50, training loss:0.10223262757062912
Train Acc 1.0719
 Acc 1.0684
arctic25,dgl,1,1608,68.7762,1.0684

epoch:1610/50, training loss:0.10177560895681381
Train Acc 1.0716
 Acc 1.0650
arctic25,dgl,1,1609,68.8187,1.0650

epoch:1611/50, training loss:0.1027650460600853
Train Acc 1.0688
 Acc 1.0685
arctic25,dgl,1,1610,68.8613,1.0685

epoch:1612/50, training loss:0.1012803390622139
Train Acc 1.0717
 Acc 1.0686
arctic25,dgl,1,1611,68.9038,1.0686

epoch:1613/50, training loss:0.10071054846048355
Train Acc 1.0719
 Acc 1.0658
arctic25,dgl,1,1612,68.9463,1.0658

epoch:1614/50, training loss:0.10220484435558319
Train Acc 1.0694
 Acc 1.0687
new best val f1: 1.068704795158898
arctic25,dgl,1,1613,68.9889,1.0687

epoch:1615/50, training loss:0.10102328658103943
Train Acc 1.0719
 Acc 1.0685
arctic25,dgl,1,1614,69.0314,1.0685

epoch:1616/50, training loss:0.10066618770360947
Train Acc 1.0717
 Acc 1.0654
arctic25,dgl,1,1615,69.0739,1.0654

epoch:1617/50, training loss:0.10212172567844391
Train Acc 1.0691
 Acc 1.0686
arctic25,dgl,1,1616,69.1165,1.0686

epoch:1618/50, training loss:0.10110655426979065
Train Acc 1.0718
 Acc 1.0687
arctic25,dgl,1,1617,69.1590,1.0687

epoch:1619/50, training loss:0.1005479097366333
Train Acc 1.0720
 Acc 1.0654
arctic25,dgl,1,1618,69.2015,1.0654

epoch:1620/50, training loss:0.10239303112030029
Train Acc 1.0692
 Acc 1.0687
new best val f1: 1.0687376117404601
arctic25,dgl,1,1619,69.2490,1.0687

epoch:1621/50, training loss:0.10160345584154129
Train Acc 1.0720
 Acc 1.0685
arctic25,dgl,1,1620,69.2926,1.0685

epoch:1622/50, training loss:0.10106701403856277
Train Acc 1.0717
 Acc 1.0651
arctic25,dgl,1,1621,69.3373,1.0651

epoch:1623/50, training loss:0.10243014991283417
Train Acc 1.0689
 Acc 1.0686
arctic25,dgl,1,1622,69.3798,1.0686

epoch:1624/50, training loss:0.1013178899884224
Train Acc 1.0720
 Acc 1.0686
arctic25,dgl,1,1623,69.4223,1.0686

epoch:1625/50, training loss:0.10075991600751877
Train Acc 1.0720
 Acc 1.0654
arctic25,dgl,1,1624,69.4648,1.0654

epoch:1626/50, training loss:0.10216626524925232
Train Acc 1.0691
 Acc 1.0686
arctic25,dgl,1,1625,69.5074,1.0686

epoch:1627/50, training loss:0.10097574442625046
Train Acc 1.0720
 Acc 1.0685
arctic25,dgl,1,1626,69.5538,1.0685

epoch:1628/50, training loss:0.10044591873884201
Train Acc 1.0718
 Acc 1.0654
arctic25,dgl,1,1627,69.5958,1.0654

epoch:1629/50, training loss:0.10194022953510284
Train Acc 1.0691
 Acc 1.0687
arctic25,dgl,1,1628,69.6378,1.0687

epoch:1630/50, training loss:0.10087598115205765
Train Acc 1.0719
 Acc 1.0687
arctic25,dgl,1,1629,69.6797,1.0687

epoch:1631/50, training loss:0.10032651573419571
Train Acc 1.0719
 Acc 1.0654
arctic25,dgl,1,1630,69.7226,1.0654

epoch:1632/50, training loss:0.10202367603778839
Train Acc 1.0691
 Acc 1.0687
arctic25,dgl,1,1631,69.7652,1.0687

epoch:1633/50, training loss:0.10125605762004852
Train Acc 1.0720
 Acc 1.0686
arctic25,dgl,1,1632,69.8079,1.0686

epoch:1634/50, training loss:0.10047127306461334
Train Acc 1.0718
 Acc 1.0653
arctic25,dgl,1,1633,69.8505,1.0653

epoch:1635/50, training loss:0.10183680057525635
Train Acc 1.0691
 Acc 1.0687
arctic25,dgl,1,1634,69.8931,1.0687

epoch:1636/50, training loss:0.10034941881895065
Train Acc 1.0719
 Acc 1.0687
arctic25,dgl,1,1635,69.9357,1.0687

epoch:1637/50, training loss:0.10007796436548233
Train Acc 1.0720
 Acc 1.0660
arctic25,dgl,1,1636,69.9783,1.0660

epoch:1638/50, training loss:0.10120189934968948
Train Acc 1.0696
 Acc 1.0688
new best val f1: 1.06876386500571
arctic25,dgl,1,1637,70.0209,1.0688

epoch:1639/50, training loss:0.09990427643060684
Train Acc 1.0720
 Acc 1.0686
arctic25,dgl,1,1638,70.0635,1.0686

epoch:1640/50, training loss:0.09945487976074219
Train Acc 1.0719
 Acc 1.0656
arctic25,dgl,1,1639,70.1061,1.0656

epoch:1641/50, training loss:0.10134343802928925
Train Acc 1.0692
 Acc 1.0687
arctic25,dgl,1,1640,70.1486,1.0687

epoch:1642/50, training loss:0.10064732283353806
Train Acc 1.0719
 Acc 1.0687
arctic25,dgl,1,1641,70.1913,1.0687

epoch:1643/50, training loss:0.10018257051706314
Train Acc 1.0721
 Acc 1.0657
arctic25,dgl,1,1642,70.2338,1.0657

epoch:1644/50, training loss:0.10148313641548157
Train Acc 1.0693
 Acc 1.0688
new best val f1: 1.068776991638335
arctic25,dgl,1,1643,70.2764,1.0688

epoch:1645/50, training loss:0.09993097186088562
Train Acc 1.0720
 Acc 1.0687
arctic25,dgl,1,1644,70.3190,1.0687

epoch:1646/50, training loss:0.09970426559448242
Train Acc 1.0719
 Acc 1.0658
arctic25,dgl,1,1645,70.3616,1.0658

epoch:1647/50, training loss:0.10094251483678818
Train Acc 1.0694
 Acc 1.0686
arctic25,dgl,1,1646,70.4042,1.0686

epoch:1648/50, training loss:0.09984280169010162
Train Acc 1.0719
 Acc 1.0687
arctic25,dgl,1,1647,70.4467,1.0687

epoch:1649/50, training loss:0.09938503056764603
Train Acc 1.0721
 Acc 1.0658
arctic25,dgl,1,1648,70.4893,1.0658

epoch:1650/50, training loss:0.10118715465068817
Train Acc 1.0695
 Acc 1.0687
arctic25,dgl,1,1649,70.5319,1.0687

epoch:1651/50, training loss:0.10049576312303543
Train Acc 1.0720
 Acc 1.0687
arctic25,dgl,1,1650,70.5744,1.0687

epoch:1652/50, training loss:0.10001083463430405
Train Acc 1.0718
 Acc 1.0654
arctic25,dgl,1,1651,70.6170,1.0654

epoch:1653/50, training loss:0.10131221264600754
Train Acc 1.0692
 Acc 1.0686
arctic25,dgl,1,1652,70.6596,1.0686

epoch:1654/50, training loss:0.1001785397529602
Train Acc 1.0720
 Acc 1.0686
arctic25,dgl,1,1653,70.7023,1.0686

epoch:1655/50, training loss:0.09970591962337494
Train Acc 1.0720
 Acc 1.0654
arctic25,dgl,1,1654,70.7450,1.0654

epoch:1656/50, training loss:0.10129772871732712
Train Acc 1.0694
 Acc 1.0689
new best val f1: 1.068901694648272
arctic25,dgl,1,1655,70.7878,1.0689

epoch:1657/50, training loss:0.10059525072574615
Train Acc 1.0721
 Acc 1.0687
arctic25,dgl,1,1656,70.8303,1.0687

epoch:1658/50, training loss:0.10007793456315994
Train Acc 1.0719
 Acc 1.0651
arctic25,dgl,1,1657,70.8729,1.0651

epoch:1659/50, training loss:0.10142382234334946
Train Acc 1.0690
 Acc 1.0687
arctic25,dgl,1,1658,70.9156,1.0687

epoch:1660/50, training loss:0.10030460357666016
Train Acc 1.0720
 Acc 1.0687
arctic25,dgl,1,1659,70.9582,1.0687

epoch:1661/50, training loss:0.09971105307340622
Train Acc 1.0721
 Acc 1.0653
arctic25,dgl,1,1660,71.0008,1.0653

epoch:1662/50, training loss:0.10137731581926346
Train Acc 1.0692
 Acc 1.0689
arctic25,dgl,1,1661,71.0433,1.0689

epoch:1663/50, training loss:0.10061104595661163
Train Acc 1.0721
 Acc 1.0687
arctic25,dgl,1,1662,71.0860,1.0687

epoch:1664/50, training loss:0.09978542476892471
Train Acc 1.0719
 Acc 1.0648
arctic25,dgl,1,1663,71.1286,1.0648

epoch:1665/50, training loss:0.10176896303892136
Train Acc 1.0687
 Acc 1.0688
arctic25,dgl,1,1664,71.1712,1.0688

epoch:1666/50, training loss:0.10033568739891052
Train Acc 1.0720
 Acc 1.0688
arctic25,dgl,1,1665,71.2138,1.0688

epoch:1667/50, training loss:0.09965645521879196
Train Acc 1.0720
 Acc 1.0654
arctic25,dgl,1,1666,71.2564,1.0654

epoch:1668/50, training loss:0.10130053013563156
Train Acc 1.0692
 Acc 1.0689
arctic25,dgl,1,1667,71.2990,1.0689

epoch:1669/50, training loss:0.10012687742710114
Train Acc 1.0721
 Acc 1.0687
arctic25,dgl,1,1668,71.3416,1.0687

epoch:1670/50, training loss:0.09947299212217331
Train Acc 1.0719
 Acc 1.0650
arctic25,dgl,1,1669,71.3842,1.0650

epoch:1671/50, training loss:0.10146476328372955
Train Acc 1.0688
 Acc 1.0688
arctic25,dgl,1,1670,71.4268,1.0688

epoch:1672/50, training loss:0.10034114867448807
Train Acc 1.0721
 Acc 1.0688
arctic25,dgl,1,1671,71.4694,1.0688

epoch:1673/50, training loss:0.09988632053136826
Train Acc 1.0721
 Acc 1.0656
arctic25,dgl,1,1672,71.5120,1.0656

epoch:1674/50, training loss:0.10084503144025803
Train Acc 1.0695
 Acc 1.0689
arctic25,dgl,1,1673,71.5546,1.0689

epoch:1675/50, training loss:0.09897588938474655
Train Acc 1.0721
 Acc 1.0687
arctic25,dgl,1,1674,71.5972,1.0687

epoch:1676/50, training loss:0.09869489818811417
Train Acc 1.0718
 Acc 1.0657
arctic25,dgl,1,1675,71.6398,1.0657

epoch:1677/50, training loss:0.10049103945493698
Train Acc 1.0692
 Acc 1.0687
arctic25,dgl,1,1676,71.6824,1.0687

epoch:1678/50, training loss:0.09977853298187256
Train Acc 1.0720
 Acc 1.0688
arctic25,dgl,1,1677,71.7250,1.0688

epoch:1679/50, training loss:0.09913373738527298
Train Acc 1.0721
 Acc 1.0656
arctic25,dgl,1,1678,71.7676,1.0656

epoch:1680/50, training loss:0.10076585412025452
Train Acc 1.0695
 Acc 1.0688
arctic25,dgl,1,1679,71.8102,1.0688

epoch:1681/50, training loss:0.09981729090213776
Train Acc 1.0722
 Acc 1.0686
arctic25,dgl,1,1680,71.8528,1.0686

epoch:1682/50, training loss:0.09947739541530609
Train Acc 1.0717
 Acc 1.0656
arctic25,dgl,1,1681,71.8955,1.0656

epoch:1683/50, training loss:0.10036009550094604
Train Acc 1.0692
 Acc 1.0685
arctic25,dgl,1,1682,71.9381,1.0685

epoch:1684/50, training loss:0.09937374293804169
Train Acc 1.0719
 Acc 1.0686
arctic25,dgl,1,1683,71.9807,1.0686

epoch:1685/50, training loss:0.09871046245098114
Train Acc 1.0720
 Acc 1.0659
arctic25,dgl,1,1684,72.0232,1.0659

epoch:1686/50, training loss:0.10043755918741226
Train Acc 1.0696
 Acc 1.0688
arctic25,dgl,1,1685,72.0658,1.0688

epoch:1687/50, training loss:0.09978385269641876
Train Acc 1.0721
 Acc 1.0687
arctic25,dgl,1,1686,72.1085,1.0687

epoch:1688/50, training loss:0.09885133057832718
Train Acc 1.0718
 Acc 1.0654
arctic25,dgl,1,1687,72.1511,1.0654

epoch:1689/50, training loss:0.10042546689510345
Train Acc 1.0691
 Acc 1.0686
arctic25,dgl,1,1688,72.1937,1.0686

epoch:1690/50, training loss:0.09940160810947418
Train Acc 1.0720
 Acc 1.0688
arctic25,dgl,1,1689,72.2363,1.0688

epoch:1691/50, training loss:0.09864667803049088
Train Acc 1.0721
 Acc 1.0659
arctic25,dgl,1,1690,72.2789,1.0659

epoch:1692/50, training loss:0.10024394094944
Train Acc 1.0696
 Acc 1.0689
new best val f1: 1.0689345112298343
arctic25,dgl,1,1691,72.3215,1.0689

epoch:1693/50, training loss:0.09898541867733002
Train Acc 1.0722
 Acc 1.0687
arctic25,dgl,1,1692,72.3642,1.0687

epoch:1694/50, training loss:0.09849950671195984
Train Acc 1.0720
 Acc 1.0658
arctic25,dgl,1,1693,72.4067,1.0658

epoch:1695/50, training loss:0.09983796626329422
Train Acc 1.0695
 Acc 1.0688
arctic25,dgl,1,1694,72.4493,1.0688

epoch:1696/50, training loss:0.09867678582668304
Train Acc 1.0721
 Acc 1.0689
arctic25,dgl,1,1695,72.4919,1.0689

epoch:1697/50, training loss:0.09798237681388855
Train Acc 1.0721
 Acc 1.0662
arctic25,dgl,1,1696,72.5345,1.0662

epoch:1698/50, training loss:0.09963729977607727
Train Acc 1.0697
 Acc 1.0689
arctic25,dgl,1,1697,72.5771,1.0689

epoch:1699/50, training loss:0.09870981425046921
Train Acc 1.0722
 Acc 1.0687
arctic25,dgl,1,1698,72.6197,1.0687

epoch:1700/50, training loss:0.09810786694288254
Train Acc 1.0720
 Acc 1.0657
arctic25,dgl,1,1699,72.6623,1.0657

epoch:1701/50, training loss:0.09974972903728485
Train Acc 1.0695
 Acc 1.0689
new best val f1: 1.0689410745461467
arctic25,dgl,1,1700,72.7050,1.0689

epoch:1702/50, training loss:0.09877902269363403
Train Acc 1.0723
 Acc 1.0690
new best val f1: 1.0690329609745213
arctic25,dgl,1,1701,72.7476,1.0690

epoch:1703/50, training loss:0.09835265576839447
Train Acc 1.0723
 Acc 1.0658
arctic25,dgl,1,1702,72.7902,1.0658

epoch:1704/50, training loss:0.09971881657838821
Train Acc 1.0695
 Acc 1.0688
arctic25,dgl,1,1703,72.8328,1.0688

epoch:1705/50, training loss:0.09868879616260529
Train Acc 1.0722
 Acc 1.0687
arctic25,dgl,1,1704,72.8754,1.0687

epoch:1706/50, training loss:0.09808903932571411
Train Acc 1.0721
 Acc 1.0658
arctic25,dgl,1,1705,72.9180,1.0658

epoch:1707/50, training loss:0.09959948807954788
Train Acc 1.0695
 Acc 1.0690
arctic25,dgl,1,1706,72.9606,1.0690

epoch:1708/50, training loss:0.09873225539922714
Train Acc 1.0723
 Acc 1.0690
arctic25,dgl,1,1707,73.0031,1.0690

epoch:1709/50, training loss:0.09813479334115982
Train Acc 1.0722
 Acc 1.0659
arctic25,dgl,1,1708,73.0457,1.0659

epoch:1710/50, training loss:0.09947700053453445
Train Acc 1.0695
 Acc 1.0688
arctic25,dgl,1,1709,73.0882,1.0688

epoch:1711/50, training loss:0.09843148291110992
Train Acc 1.0722
 Acc 1.0688
arctic25,dgl,1,1710,73.1308,1.0688

epoch:1712/50, training loss:0.0977846309542656
Train Acc 1.0722
 Acc 1.0658
arctic25,dgl,1,1711,73.1733,1.0658

epoch:1713/50, training loss:0.09947475045919418
Train Acc 1.0696
 Acc 1.0690
new best val f1: 1.0690460876071461
arctic25,dgl,1,1712,73.2159,1.0690

epoch:1714/50, training loss:0.09883619099855423
Train Acc 1.0723
 Acc 1.0690
arctic25,dgl,1,1713,73.2584,1.0690

epoch:1715/50, training loss:0.09796781092882156
Train Acc 1.0722
 Acc 1.0654
arctic25,dgl,1,1714,73.3010,1.0654

epoch:1716/50, training loss:0.09976304322481155
Train Acc 1.0692
 Acc 1.0689
arctic25,dgl,1,1715,73.3435,1.0689

epoch:1717/50, training loss:0.0987711027264595
Train Acc 1.0722
 Acc 1.0689
arctic25,dgl,1,1716,73.3861,1.0689

epoch:1718/50, training loss:0.09818606078624725
Train Acc 1.0722
 Acc 1.0658
arctic25,dgl,1,1717,73.4286,1.0658

epoch:1719/50, training loss:0.099378302693367
Train Acc 1.0696
 Acc 1.0691
new best val f1: 1.069085467505021
arctic25,dgl,1,1718,73.4711,1.0691

epoch:1720/50, training loss:0.09785950928926468
Train Acc 1.0723
 Acc 1.0690
arctic25,dgl,1,1719,73.5137,1.0690

epoch:1721/50, training loss:0.09744691103696823
Train Acc 1.0722
 Acc 1.0661
arctic25,dgl,1,1720,73.5562,1.0661

epoch:1722/50, training loss:0.09891358762979507
Train Acc 1.0697
 Acc 1.0689
arctic25,dgl,1,1721,73.5987,1.0689

epoch:1723/50, training loss:0.09794224798679352
Train Acc 1.0723
 Acc 1.0689
arctic25,dgl,1,1722,73.6413,1.0689

epoch:1724/50, training loss:0.09735392779111862
Train Acc 1.0722
 Acc 1.0660
arctic25,dgl,1,1723,73.6838,1.0660

epoch:1725/50, training loss:0.09893908351659775
Train Acc 1.0697
 Acc 1.0690
arctic25,dgl,1,1724,73.7264,1.0690

epoch:1726/50, training loss:0.0979561060667038
Train Acc 1.0724
 Acc 1.0689
arctic25,dgl,1,1725,73.7694,1.0689

epoch:1727/50, training loss:0.09747553616762161
Train Acc 1.0722
 Acc 1.0660
arctic25,dgl,1,1726,73.8120,1.0660

epoch:1728/50, training loss:0.09889046847820282
Train Acc 1.0696
 Acc 1.0690
arctic25,dgl,1,1727,73.8545,1.0690

epoch:1729/50, training loss:0.09792669862508774
Train Acc 1.0723
 Acc 1.0688
arctic25,dgl,1,1728,73.8971,1.0688

epoch:1730/50, training loss:0.0974317416548729
Train Acc 1.0722
 Acc 1.0662
arctic25,dgl,1,1729,73.9397,1.0662

epoch:1731/50, training loss:0.09864962100982666
Train Acc 1.0698
 Acc 1.0690
arctic25,dgl,1,1730,73.9822,1.0690

epoch:1732/50, training loss:0.09748780727386475
Train Acc 1.0724
 Acc 1.0689
arctic25,dgl,1,1731,74.0248,1.0689

epoch:1733/50, training loss:0.09699974209070206
Train Acc 1.0721
 Acc 1.0661
arctic25,dgl,1,1732,74.0673,1.0661

epoch:1734/50, training loss:0.09854654222726822
Train Acc 1.0697
 Acc 1.0690
arctic25,dgl,1,1733,74.1098,1.0690

epoch:1735/50, training loss:0.09771101176738739
Train Acc 1.0723
 Acc 1.0689
arctic25,dgl,1,1734,74.1524,1.0689

epoch:1736/50, training loss:0.09709346294403076
Train Acc 1.0723
 Acc 1.0662
arctic25,dgl,1,1735,74.1949,1.0662

epoch:1737/50, training loss:0.09850820899009705
Train Acc 1.0697
 Acc 1.0691
new best val f1: 1.0690920308213334
arctic25,dgl,1,1736,74.2374,1.0691

epoch:1738/50, training loss:0.09727288037538528
Train Acc 1.0724
 Acc 1.0691
arctic25,dgl,1,1737,74.2800,1.0691

epoch:1739/50, training loss:0.09682052582502365
Train Acc 1.0723
 Acc 1.0664
arctic25,dgl,1,1738,74.3225,1.0664

epoch:1740/50, training loss:0.09815408289432526
Train Acc 1.0699
 Acc 1.0690
arctic25,dgl,1,1739,74.3651,1.0690

epoch:1741/50, training loss:0.09716873615980148
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1740,74.4077,1.0690

epoch:1742/50, training loss:0.09645279496908188
Train Acc 1.0724
 Acc 1.0661
arctic25,dgl,1,1741,74.4502,1.0661

epoch:1743/50, training loss:0.09843898564577103
Train Acc 1.0698
 Acc 1.0692
new best val f1: 1.0691642273007704
arctic25,dgl,1,1742,74.4927,1.0692

epoch:1744/50, training loss:0.09796224534511566
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1743,74.5353,1.0690

epoch:1745/50, training loss:0.09719190746545792
Train Acc 1.0723
 Acc 1.0656
arctic25,dgl,1,1744,74.5778,1.0656

epoch:1746/50, training loss:0.09879132360219955
Train Acc 1.0695
 Acc 1.0690
arctic25,dgl,1,1745,74.6204,1.0690

epoch:1747/50, training loss:0.09772118180990219
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1746,74.6628,1.0690

epoch:1748/50, training loss:0.09720128029584885
Train Acc 1.0724
 Acc 1.0659
arctic25,dgl,1,1747,74.7054,1.0659

epoch:1749/50, training loss:0.09852351248264313
Train Acc 1.0697
 Acc 1.0692
new best val f1: 1.069170790617083
arctic25,dgl,1,1748,74.7479,1.0692

epoch:1750/50, training loss:0.09741842746734619
Train Acc 1.0725
 Acc 1.0690
arctic25,dgl,1,1749,74.7905,1.0690

epoch:1751/50, training loss:0.09687774628400803
Train Acc 1.0723
 Acc 1.0658
arctic25,dgl,1,1750,74.8330,1.0658

epoch:1752/50, training loss:0.09845639765262604
Train Acc 1.0696
 Acc 1.0690
arctic25,dgl,1,1751,74.8755,1.0690

epoch:1753/50, training loss:0.09748592227697372
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1752,74.9180,1.0690

epoch:1754/50, training loss:0.09699267148971558
Train Acc 1.0724
 Acc 1.0661
arctic25,dgl,1,1753,74.9606,1.0661

epoch:1755/50, training loss:0.09819909185171127
Train Acc 1.0698
 Acc 1.0691
arctic25,dgl,1,1754,75.0031,1.0691

epoch:1756/50, training loss:0.09690885990858078
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1755,75.0456,1.0690

epoch:1757/50, training loss:0.09642119705677032
Train Acc 1.0723
 Acc 1.0664
arctic25,dgl,1,1756,75.0881,1.0664

epoch:1758/50, training loss:0.09777376055717468
Train Acc 1.0699
 Acc 1.0691
arctic25,dgl,1,1757,75.1307,1.0691

epoch:1759/50, training loss:0.09672395884990692
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1758,75.1733,1.0690

epoch:1760/50, training loss:0.09624921530485153
Train Acc 1.0724
 Acc 1.0665
arctic25,dgl,1,1759,75.2158,1.0665

epoch:1761/50, training loss:0.09766541421413422
Train Acc 1.0701
 Acc 1.0692
arctic25,dgl,1,1760,75.2584,1.0692

epoch:1762/50, training loss:0.09678152948617935
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1761,75.3009,1.0690

epoch:1763/50, training loss:0.09614652395248413
Train Acc 1.0723
 Acc 1.0662
arctic25,dgl,1,1762,75.3434,1.0662

epoch:1764/50, training loss:0.09785537421703339
Train Acc 1.0697
 Acc 1.0691
arctic25,dgl,1,1763,75.3861,1.0691

epoch:1765/50, training loss:0.09723270684480667
Train Acc 1.0724
 Acc 1.0690
arctic25,dgl,1,1764,75.4286,1.0690

epoch:1766/50, training loss:0.09652365744113922
Train Acc 1.0724
 Acc 1.0661
arctic25,dgl,1,1765,75.4711,1.0661

epoch:1767/50, training loss:0.0979161411523819
Train Acc 1.0698
 Acc 1.0692
arctic25,dgl,1,1766,75.5137,1.0692

epoch:1768/50, training loss:0.09671308845281601
Train Acc 1.0725
 Acc 1.0689
arctic25,dgl,1,1767,75.5563,1.0689

epoch:1769/50, training loss:0.09637033939361572
Train Acc 1.0722
 Acc 1.0665
arctic25,dgl,1,1768,75.5988,1.0665

epoch:1770/50, training loss:0.09743066877126694
Train Acc 1.0700
 Acc 1.0691
arctic25,dgl,1,1769,75.6414,1.0691

epoch:1771/50, training loss:0.09637857228517532
Train Acc 1.0725
 Acc 1.0690
arctic25,dgl,1,1770,75.6839,1.0690

epoch:1772/50, training loss:0.0957259014248848
Train Acc 1.0723
 Acc 1.0665
arctic25,dgl,1,1771,75.7264,1.0665

epoch:1773/50, training loss:0.09730366617441177
Train Acc 1.0701
 Acc 1.0692
new best val f1: 1.0692036071986453
arctic25,dgl,1,1772,75.7690,1.0692

epoch:1774/50, training loss:0.09650181978940964
Train Acc 1.0725
 Acc 1.0691
arctic25,dgl,1,1773,75.8116,1.0691

epoch:1775/50, training loss:0.09583447873592377
Train Acc 1.0724
 Acc 1.0661
arctic25,dgl,1,1774,75.8542,1.0661

epoch:1776/50, training loss:0.09760673344135284
Train Acc 1.0698
 Acc 1.0691
arctic25,dgl,1,1775,75.8979,1.0691

epoch:1777/50, training loss:0.09666779637336731
Train Acc 1.0725
 Acc 1.0692
new best val f1: 1.0692495504128325
arctic25,dgl,1,1776,75.9404,1.0692

epoch:1778/50, training loss:0.09617230296134949
Train Acc 1.0725
 Acc 1.0662
arctic25,dgl,1,1777,75.9829,1.0662

epoch:1779/50, training loss:0.09760941565036774
Train Acc 1.0698
 Acc 1.0692
arctic25,dgl,1,1778,76.0255,1.0692

epoch:1780/50, training loss:0.09636245667934418
Train Acc 1.0725
 Acc 1.0691
arctic25,dgl,1,1779,76.0681,1.0691

epoch:1781/50, training loss:0.09602024406194687
Train Acc 1.0724
 Acc 1.0660
arctic25,dgl,1,1780,76.1106,1.0660

epoch:1782/50, training loss:0.09755219519138336
Train Acc 1.0698
 Acc 1.0691
arctic25,dgl,1,1781,76.1532,1.0691

epoch:1783/50, training loss:0.0965985506772995
Train Acc 1.0724
 Acc 1.0693
new best val f1: 1.06926924036177
arctic25,dgl,1,1782,76.1958,1.0693

epoch:1784/50, training loss:0.09622672200202942
Train Acc 1.0725
 Acc 1.0663
arctic25,dgl,1,1783,76.2383,1.0663

epoch:1785/50, training loss:0.09731468558311462
Train Acc 1.0699
 Acc 1.0691
arctic25,dgl,1,1784,76.2809,1.0691

epoch:1786/50, training loss:0.09592548757791519
Train Acc 1.0725
 Acc 1.0691
arctic25,dgl,1,1785,76.3234,1.0691

epoch:1787/50, training loss:0.09561062604188919
Train Acc 1.0725
 Acc 1.0667
arctic25,dgl,1,1786,76.3660,1.0667

epoch:1788/50, training loss:0.0967763289809227
Train Acc 1.0702
 Acc 1.0692
arctic25,dgl,1,1787,76.4085,1.0692

epoch:1789/50, training loss:0.09566307067871094
Train Acc 1.0725
 Acc 1.0693
arctic25,dgl,1,1788,76.4510,1.0693

epoch:1790/50, training loss:0.0952606052160263
Train Acc 1.0725
 Acc 1.0667
arctic25,dgl,1,1789,76.4936,1.0667

epoch:1791/50, training loss:0.0968402698636055
Train Acc 1.0702
 Acc 1.0692
arctic25,dgl,1,1790,76.5362,1.0692

epoch:1792/50, training loss:0.09613888710737228
Train Acc 1.0726
 Acc 1.0691
arctic25,dgl,1,1791,76.5788,1.0691

epoch:1793/50, training loss:0.09550609439611435
Train Acc 1.0725
 Acc 1.0660
arctic25,dgl,1,1792,76.6214,1.0660

epoch:1794/50, training loss:0.09729152172803879
Train Acc 1.0698
 Acc 1.0692
arctic25,dgl,1,1793,76.6639,1.0692

epoch:1795/50, training loss:0.09658317267894745
Train Acc 1.0726
 Acc 1.0692
arctic25,dgl,1,1794,76.7064,1.0692

epoch:1796/50, training loss:0.09607850015163422
Train Acc 1.0725
 Acc 1.0658
arctic25,dgl,1,1795,76.7490,1.0658

epoch:1797/50, training loss:0.09742330759763718
Train Acc 1.0697
 Acc 1.0693
arctic25,dgl,1,1796,76.7916,1.0693

epoch:1798/50, training loss:0.09634712338447571
Train Acc 1.0725
 Acc 1.0692
arctic25,dgl,1,1797,76.8341,1.0692

epoch:1799/50, training loss:0.09572603553533554
Train Acc 1.0725
 Acc 1.0659
arctic25,dgl,1,1798,76.8767,1.0659

epoch:1800/50, training loss:0.09734870493412018
Train Acc 1.0697
 Acc 1.0693
arctic25,dgl,1,1799,76.9192,1.0693

epoch:1801/50, training loss:0.09646299481391907
Train Acc 1.0726
 Acc 1.0692
arctic25,dgl,1,1800,76.9618,1.0692

epoch:1802/50, training loss:0.09574970602989197
Train Acc 1.0725
 Acc 1.0658
arctic25,dgl,1,1801,77.0043,1.0658

epoch:1803/50, training loss:0.0974154844880104
Train Acc 1.0697
 Acc 1.0692
arctic25,dgl,1,1802,77.0469,1.0692

epoch:1804/50, training loss:0.0964847207069397
Train Acc 1.0725
 Acc 1.0692
arctic25,dgl,1,1803,77.0895,1.0692

epoch:1805/50, training loss:0.0957886204123497
Train Acc 1.0725
 Acc 1.0656
arctic25,dgl,1,1804,77.1320,1.0656

epoch:1806/50, training loss:0.09758224338293076
Train Acc 1.0696
 Acc 1.0693
arctic25,dgl,1,1805,77.1745,1.0693

epoch:1807/50, training loss:0.09653785824775696
Train Acc 1.0726
 Acc 1.0692
arctic25,dgl,1,1806,77.2170,1.0692

epoch:1808/50, training loss:0.09592124819755554
Train Acc 1.0726
 Acc 1.0660
arctic25,dgl,1,1807,77.2596,1.0660

epoch:1809/50, training loss:0.09729255735874176
Train Acc 1.0697
 Acc 1.0693
new best val f1: 1.0692889303107074
arctic25,dgl,1,1808,77.3022,1.0693

epoch:1810/50, training loss:0.09573093801736832
Train Acc 1.0726
 Acc 1.0693
new best val f1: 1.0693283102085822
arctic25,dgl,1,1809,77.3448,1.0693

epoch:1811/50, training loss:0.09526028484106064
Train Acc 1.0726
 Acc 1.0664
arctic25,dgl,1,1810,77.3873,1.0664

epoch:1812/50, training loss:0.09668144583702087
Train Acc 1.0700
 Acc 1.0692
arctic25,dgl,1,1811,77.4299,1.0692

epoch:1813/50, training loss:0.09519313275814056
Train Acc 1.0726
 Acc 1.0692
arctic25,dgl,1,1812,77.4724,1.0692

epoch:1814/50, training loss:0.09495807439088821
Train Acc 1.0726
 Acc 1.0666
arctic25,dgl,1,1813,77.5150,1.0666

epoch:1815/50, training loss:0.0964144915342331
Train Acc 1.0701
 Acc 1.0693
arctic25,dgl,1,1814,77.5575,1.0693

epoch:1816/50, training loss:0.09546102583408356
Train Acc 1.0726
 Acc 1.0693
arctic25,dgl,1,1815,77.6000,1.0693

epoch:1817/50, training loss:0.09495222568511963
Train Acc 1.0726
 Acc 1.0665
arctic25,dgl,1,1816,77.6425,1.0665

epoch:1818/50, training loss:0.09638144075870514
Train Acc 1.0701
 Acc 1.0692
arctic25,dgl,1,1817,77.6851,1.0692

epoch:1819/50, training loss:0.09537911415100098
Train Acc 1.0725
 Acc 1.0692
arctic25,dgl,1,1818,77.7277,1.0692

epoch:1820/50, training loss:0.09498564153909683
Train Acc 1.0725
 Acc 1.0666
arctic25,dgl,1,1819,77.7702,1.0666

epoch:1821/50, training loss:0.09630411118268967
Train Acc 1.0701
 Acc 1.0693
arctic25,dgl,1,1820,77.8127,1.0693

epoch:1822/50, training loss:0.09544649720191956
Train Acc 1.0726
 Acc 1.0691
arctic25,dgl,1,1821,77.8553,1.0691

epoch:1823/50, training loss:0.09479058533906937
Train Acc 1.0724
 Acc 1.0662
arctic25,dgl,1,1822,77.8978,1.0662

epoch:1824/50, training loss:0.09647990763187408
Train Acc 1.0699
 Acc 1.0692
arctic25,dgl,1,1823,77.9405,1.0692

epoch:1825/50, training loss:0.09601764380931854
Train Acc 1.0726
 Acc 1.0690
arctic25,dgl,1,1824,77.9831,1.0690

epoch:1826/50, training loss:0.09533065557479858
Train Acc 1.0724
 Acc 1.0659
arctic25,dgl,1,1825,78.0256,1.0659

epoch:1827/50, training loss:0.09688804298639297
Train Acc 1.0697
 Acc 1.0693
arctic25,dgl,1,1826,78.0682,1.0693

epoch:1828/50, training loss:0.09605935961008072
Train Acc 1.0728
 Acc 1.0691
arctic25,dgl,1,1827,78.1107,1.0691

epoch:1829/50, training loss:0.09538619965314865
Train Acc 1.0724
 Acc 1.0661
arctic25,dgl,1,1828,78.1533,1.0661

epoch:1830/50, training loss:0.09646434336900711
Train Acc 1.0697
 Acc 1.0693
arctic25,dgl,1,1829,78.1958,1.0693

epoch:1831/50, training loss:0.09509918093681335
Train Acc 1.0727
 Acc 1.0693
arctic25,dgl,1,1830,78.2384,1.0693

epoch:1832/50, training loss:0.09451821446418762
Train Acc 1.0726
 Acc 1.0664
arctic25,dgl,1,1831,78.2809,1.0664

epoch:1833/50, training loss:0.0962032824754715
Train Acc 1.0701
 Acc 1.0693
arctic25,dgl,1,1832,78.3235,1.0693

epoch:1834/50, training loss:0.09550576657056808
Train Acc 1.0727
 Acc 1.0692
arctic25,dgl,1,1833,78.3660,1.0692

epoch:1835/50, training loss:0.09484867006540298
Train Acc 1.0726
 Acc 1.0658
arctic25,dgl,1,1834,78.4085,1.0658

epoch:1836/50, training loss:0.09665966033935547
Train Acc 1.0697
 Acc 1.0693
arctic25,dgl,1,1835,78.4511,1.0693

epoch:1837/50, training loss:0.09571723639965057
Train Acc 1.0726
 Acc 1.0694
new best val f1: 1.0694201966369568
arctic25,dgl,1,1836,78.4935,1.0694

epoch:1838/50, training loss:0.09535173326730728
Train Acc 1.0727
 Acc 1.0660
arctic25,dgl,1,1837,78.5361,1.0660

epoch:1839/50, training loss:0.09638828784227371
Train Acc 1.0699
 Acc 1.0693
arctic25,dgl,1,1838,78.5787,1.0693

epoch:1840/50, training loss:0.09475568681955338
Train Acc 1.0726
 Acc 1.0693
arctic25,dgl,1,1839,78.6213,1.0693

epoch:1841/50, training loss:0.09453605115413666
Train Acc 1.0727
 Acc 1.0662
arctic25,dgl,1,1840,78.6638,1.0662

epoch:1842/50, training loss:0.0961027443408966
Train Acc 1.0700
 Acc 1.0693
arctic25,dgl,1,1841,78.7063,1.0693

epoch:1843/50, training loss:0.09528598189353943
Train Acc 1.0725
 Acc 1.0693
arctic25,dgl,1,1842,78.7488,1.0693

epoch:1844/50, training loss:0.09478899091482162
Train Acc 1.0729
 Acc 1.0663
arctic25,dgl,1,1843,78.7913,1.0663

epoch:1845/50, training loss:0.09593094885349274
Train Acc 1.0702
 Acc 1.0692
arctic25,dgl,1,1844,78.8339,1.0692

epoch:1846/50, training loss:0.09461493045091629
Train Acc 1.0726
 Acc 1.0693
arctic25,dgl,1,1845,78.8764,1.0693

epoch:1847/50, training loss:0.09426329284906387
Train Acc 1.0727
 Acc 1.0667
arctic25,dgl,1,1846,78.9190,1.0667

epoch:1848/50, training loss:0.09555839747190475
Train Acc 1.0703
 Acc 1.0694
arctic25,dgl,1,1847,78.9615,1.0694

epoch:1849/50, training loss:0.09432381391525269
Train Acc 1.0726
 Acc 1.0693
arctic25,dgl,1,1848,79.0040,1.0693

epoch:1850/50, training loss:0.09396011382341385
Train Acc 1.0728
 Acc 1.0667
arctic25,dgl,1,1849,79.0465,1.0667

epoch:1851/50, training loss:0.09548205882310867
Train Acc 1.0705
 Acc 1.0693
arctic25,dgl,1,1850,79.0891,1.0693

epoch:1852/50, training loss:0.09458423405885696
Train Acc 1.0727
 Acc 1.0693
arctic25,dgl,1,1851,79.1317,1.0693

epoch:1853/50, training loss:0.09405685216188431
Train Acc 1.0726
 Acc 1.0665
arctic25,dgl,1,1852,79.1743,1.0665

epoch:1854/50, training loss:0.0956554114818573
Train Acc 1.0702
 Acc 1.0694
arctic25,dgl,1,1853,79.2169,1.0694

epoch:1855/50, training loss:0.09461004287004471
Train Acc 1.0727
 Acc 1.0693
arctic25,dgl,1,1854,79.2595,1.0693

epoch:1856/50, training loss:0.09411803632974625
Train Acc 1.0727
 Acc 1.0666
arctic25,dgl,1,1855,79.3020,1.0666

epoch:1857/50, training loss:0.0954914391040802
Train Acc 1.0704
 Acc 1.0695
new best val f1: 1.0694530132185192
arctic25,dgl,1,1856,79.3446,1.0695

epoch:1858/50, training loss:0.09419660270214081
Train Acc 1.0728
 Acc 1.0693
arctic25,dgl,1,1857,79.3872,1.0693

epoch:1859/50, training loss:0.09383956342935562
Train Acc 1.0725
 Acc 1.0666
arctic25,dgl,1,1858,79.4298,1.0666

epoch:1860/50, training loss:0.0953911542892456
Train Acc 1.0702
 Acc 1.0694
arctic25,dgl,1,1859,79.4723,1.0694

epoch:1861/50, training loss:0.09453961998224258
Train Acc 1.0728
 Acc 1.0693
arctic25,dgl,1,1860,79.5148,1.0693

epoch:1862/50, training loss:0.09399141371250153
Train Acc 1.0727
 Acc 1.0668
arctic25,dgl,1,1861,79.5573,1.0668

epoch:1863/50, training loss:0.09514888375997543
Train Acc 1.0705
 Acc 1.0694
arctic25,dgl,1,1862,79.5999,1.0694

epoch:1864/50, training loss:0.09396065026521683
Train Acc 1.0729
 Acc 1.0693
arctic25,dgl,1,1863,79.6425,1.0693

epoch:1865/50, training loss:0.0935719758272171
Train Acc 1.0725
 Acc 1.0669
arctic25,dgl,1,1864,79.6850,1.0669

epoch:1866/50, training loss:0.09495019912719727
Train Acc 1.0703
 Acc 1.0693
arctic25,dgl,1,1865,79.7275,1.0693

epoch:1867/50, training loss:0.09406863152980804
Train Acc 1.0728
 Acc 1.0693
arctic25,dgl,1,1866,79.7701,1.0693

epoch:1868/50, training loss:0.09341730922460556
Train Acc 1.0728
 Acc 1.0669
arctic25,dgl,1,1867,79.8126,1.0669

epoch:1869/50, training loss:0.09491521865129471
Train Acc 1.0706
 Acc 1.0694
arctic25,dgl,1,1868,79.8551,1.0694

epoch:1870/50, training loss:0.09396309405565262
Train Acc 1.0728
 Acc 1.0694
arctic25,dgl,1,1869,79.8977,1.0694

epoch:1871/50, training loss:0.09348270297050476
Train Acc 1.0725
 Acc 1.0668
arctic25,dgl,1,1870,79.9402,1.0668

epoch:1872/50, training loss:0.09478005021810532
Train Acc 1.0703
 Acc 1.0693
arctic25,dgl,1,1871,79.9828,1.0693

epoch:1873/50, training loss:0.09361999481916428
Train Acc 1.0728
 Acc 1.0694
arctic25,dgl,1,1872,80.0253,1.0694

epoch:1874/50, training loss:0.09319327026605606
Train Acc 1.0728
 Acc 1.0669
arctic25,dgl,1,1873,80.0678,1.0669

epoch:1875/50, training loss:0.09474924951791763
Train Acc 1.0706
 Acc 1.0695
new best val f1: 1.069479266483769
arctic25,dgl,1,1874,80.1104,1.0695

epoch:1876/50, training loss:0.09401781857013702
Train Acc 1.0728
 Acc 1.0694
arctic25,dgl,1,1875,80.1529,1.0694

epoch:1877/50, training loss:0.09343289583921432
Train Acc 1.0726
 Acc 1.0662
arctic25,dgl,1,1876,80.1954,1.0662

epoch:1878/50, training loss:0.09526819735765457
Train Acc 1.0700
 Acc 1.0693
arctic25,dgl,1,1877,80.2380,1.0693

epoch:1879/50, training loss:0.09458620101213455
Train Acc 1.0728
 Acc 1.0694
arctic25,dgl,1,1878,80.2806,1.0694

epoch:1880/50, training loss:0.0941135361790657
Train Acc 1.0729
 Acc 1.0667
arctic25,dgl,1,1879,80.3231,1.0667

epoch:1881/50, training loss:0.09492739289999008
Train Acc 1.0705
 Acc 1.0694
arctic25,dgl,1,1880,80.3657,1.0694

epoch:1882/50, training loss:0.09311044216156006
Train Acc 1.0728
 Acc 1.0693
arctic25,dgl,1,1881,80.4082,1.0693

epoch:1883/50, training loss:0.09306756407022476
Train Acc 1.0727
 Acc 1.0675
arctic25,dgl,1,1882,80.4507,1.0675

epoch:1884/50, training loss:0.09371119737625122
Train Acc 1.0711
 Acc 1.0693
arctic25,dgl,1,1883,80.4932,1.0693

epoch:1885/50, training loss:0.09237104654312134
Train Acc 1.0728
 Acc 1.0694
arctic25,dgl,1,1884,80.5358,1.0694

epoch:1886/50, training loss:0.09221764653921127
Train Acc 1.0729
 Acc 1.0679
arctic25,dgl,1,1885,80.5783,1.0679

epoch:1887/50, training loss:0.09322778135538101
Train Acc 1.0716
 Acc 1.0694
arctic25,dgl,1,1886,80.6208,1.0694

epoch:1888/50, training loss:0.09258715808391571
Train Acc 1.0728
 Acc 1.0694
arctic25,dgl,1,1887,80.6633,1.0694

epoch:1889/50, training loss:0.09203901141881943
Train Acc 1.0728
 Acc 1.0675
arctic25,dgl,1,1888,80.7059,1.0675

epoch:1890/50, training loss:0.09372550249099731
Train Acc 1.0709
 Acc 1.0694
arctic25,dgl,1,1889,80.7485,1.0694

epoch:1891/50, training loss:0.09360680729150772
Train Acc 1.0729
 Acc 1.0694
arctic25,dgl,1,1890,80.7910,1.0694

epoch:1892/50, training loss:0.09268593788146973
Train Acc 1.0729
 Acc 1.0661
arctic25,dgl,1,1891,80.8335,1.0661

epoch:1893/50, training loss:0.09519685059785843
Train Acc 1.0702
 Acc 1.0695
new best val f1: 1.0694923931163938
arctic25,dgl,1,1892,80.8761,1.0695

epoch:1894/50, training loss:0.09512670338153839
Train Acc 1.0729
 Acc 1.0693
arctic25,dgl,1,1893,80.9186,1.0693

epoch:1895/50, training loss:0.09463334083557129
Train Acc 1.0726
 Acc 1.0665
arctic25,dgl,1,1894,80.9611,1.0665

epoch:1896/50, training loss:0.09472516179084778
Train Acc 1.0702
 Acc 1.0694
arctic25,dgl,1,1895,81.0037,1.0694

epoch:1897/50, training loss:0.09287796914577484
Train Acc 1.0730
 Acc 1.0693
arctic25,dgl,1,1896,81.0462,1.0693

epoch:1898/50, training loss:0.09247896075248718
Train Acc 1.0728
 Acc 1.0673
arctic25,dgl,1,1897,81.0888,1.0673

epoch:1899/50, training loss:0.0937458798289299
Train Acc 1.0710
 Acc 1.0694
arctic25,dgl,1,1898,81.1313,1.0694

epoch:1900/50, training loss:0.09309988468885422
Train Acc 1.0730
 Acc 1.0693
arctic25,dgl,1,1899,81.1739,1.0693

epoch:1901/50, training loss:0.09236583858728409
Train Acc 1.0726
 Acc 1.0665
arctic25,dgl,1,1900,81.2165,1.0665

epoch:1902/50, training loss:0.09457497298717499
Train Acc 1.0702
 Acc 1.0694
arctic25,dgl,1,1901,81.2591,1.0694

epoch:1903/50, training loss:0.09442860633134842
Train Acc 1.0729
 Acc 1.0693
arctic25,dgl,1,1902,81.3016,1.0693

epoch:1904/50, training loss:0.09375344961881638
Train Acc 1.0729
 Acc 1.0665
arctic25,dgl,1,1903,81.3441,1.0665

epoch:1905/50, training loss:0.09455130249261856
Train Acc 1.0705
 Acc 1.0695
arctic25,dgl,1,1904,81.3867,1.0695

epoch:1906/50, training loss:0.09280907362699509
Train Acc 1.0730
 Acc 1.0694
arctic25,dgl,1,1905,81.4292,1.0694

epoch:1907/50, training loss:0.09251389652490616
Train Acc 1.0726
 Acc 1.0671
arctic25,dgl,1,1906,81.4718,1.0671

epoch:1908/50, training loss:0.09372244030237198
Train Acc 1.0707
 Acc 1.0694
arctic25,dgl,1,1907,81.5144,1.0694

epoch:1909/50, training loss:0.09268379956483841
Train Acc 1.0729
 Acc 1.0694
arctic25,dgl,1,1908,81.5570,1.0694

epoch:1910/50, training loss:0.09204177558422089
Train Acc 1.0730
 Acc 1.0669
arctic25,dgl,1,1909,81.5995,1.0669

epoch:1911/50, training loss:0.09399288892745972
Train Acc 1.0707
 Acc 1.0696
new best val f1: 1.0695514629632061
arctic25,dgl,1,1910,81.6419,1.0696

epoch:1912/50, training loss:0.09351585805416107
Train Acc 1.0729
 Acc 1.0695
arctic25,dgl,1,1911,81.6845,1.0695

epoch:1913/50, training loss:0.09281042218208313
Train Acc 1.0728
 Acc 1.0659
arctic25,dgl,1,1912,81.7270,1.0659

epoch:1914/50, training loss:0.09504318237304688
Train Acc 1.0698
 Acc 1.0693
arctic25,dgl,1,1913,81.7696,1.0693

epoch:1915/50, training loss:0.09385224431753159
Train Acc 1.0729
 Acc 1.0694
arctic25,dgl,1,1914,81.8121,1.0694

epoch:1916/50, training loss:0.09361325204372406
Train Acc 1.0730
 Acc 1.0668
arctic25,dgl,1,1915,81.8546,1.0668

epoch:1917/50, training loss:0.09416034817695618
Train Acc 1.0706
 Acc 1.0696
new best val f1: 1.0695842795447683
arctic25,dgl,1,1916,81.8972,1.0696

epoch:1918/50, training loss:0.09235139191150665
Train Acc 1.0730
 Acc 1.0695
arctic25,dgl,1,1917,81.9397,1.0695

epoch:1919/50, training loss:0.09207135438919067
Train Acc 1.0728
 Acc 1.0671
arctic25,dgl,1,1918,81.9822,1.0671

epoch:1920/50, training loss:0.0935298502445221
Train Acc 1.0707
 Acc 1.0694
arctic25,dgl,1,1919,82.0248,1.0694

epoch:1921/50, training loss:0.09285961836576462
Train Acc 1.0730
 Acc 1.0695
arctic25,dgl,1,1920,82.0673,1.0695

epoch:1922/50, training loss:0.09204776585102081
Train Acc 1.0731
 Acc 1.0667
arctic25,dgl,1,1921,82.1098,1.0667

epoch:1923/50, training loss:0.09413041174411774
Train Acc 1.0706
 Acc 1.0695
arctic25,dgl,1,1922,82.1524,1.0695

epoch:1924/50, training loss:0.09343962371349335
Train Acc 1.0730
 Acc 1.0695
arctic25,dgl,1,1923,82.1949,1.0695

epoch:1925/50, training loss:0.09279893338680267
Train Acc 1.0728
 Acc 1.0664
arctic25,dgl,1,1924,82.2375,1.0664

epoch:1926/50, training loss:0.09431330859661102
Train Acc 1.0701
 Acc 1.0695
arctic25,dgl,1,1925,82.2800,1.0695

epoch:1927/50, training loss:0.09286709874868393
Train Acc 1.0730
 Acc 1.0695
arctic25,dgl,1,1926,82.3226,1.0695

epoch:1928/50, training loss:0.09241975098848343
Train Acc 1.0730
 Acc 1.0668
arctic25,dgl,1,1927,82.3650,1.0668

epoch:1929/50, training loss:0.09386706352233887
Train Acc 1.0707
 Acc 1.0696
new best val f1: 1.0696105328100183
arctic25,dgl,1,1928,82.4076,1.0696

epoch:1930/50, training loss:0.09254593402147293
Train Acc 1.0731
 Acc 1.0695
arctic25,dgl,1,1929,82.4502,1.0695

epoch:1931/50, training loss:0.09191876649856567
Train Acc 1.0727
 Acc 1.0668
arctic25,dgl,1,1930,82.4926,1.0668

epoch:1932/50, training loss:0.09369749575853348
Train Acc 1.0704
 Acc 1.0694
arctic25,dgl,1,1931,82.5352,1.0694

epoch:1933/50, training loss:0.0929841473698616
Train Acc 1.0730
 Acc 1.0694
arctic25,dgl,1,1932,82.5778,1.0694

epoch:1934/50, training loss:0.09237772971391678
Train Acc 1.0730
 Acc 1.0661
arctic25,dgl,1,1933,82.6203,1.0661

epoch:1935/50, training loss:0.09457635879516602
Train Acc 1.0702
 Acc 1.0695
arctic25,dgl,1,1934,82.6629,1.0695

epoch:1936/50, training loss:0.09420952200889587
Train Acc 1.0730
 Acc 1.0693
arctic25,dgl,1,1935,82.7055,1.0693

epoch:1937/50, training loss:0.09386298805475235
Train Acc 1.0725
 Acc 1.0661
arctic25,dgl,1,1936,82.7480,1.0661

epoch:1938/50, training loss:0.09442726522684097
Train Acc 1.0698
 Acc 1.0693
arctic25,dgl,1,1937,82.7906,1.0693

epoch:1939/50, training loss:0.09288907051086426
Train Acc 1.0729
 Acc 1.0694
arctic25,dgl,1,1938,82.8332,1.0694

epoch:1940/50, training loss:0.0923730805516243
Train Acc 1.0728
 Acc 1.0664
arctic25,dgl,1,1939,82.8757,1.0664

epoch:1941/50, training loss:0.09412351250648499
Train Acc 1.0703
 Acc 1.0694
arctic25,dgl,1,1940,82.9183,1.0694

epoch:1942/50, training loss:0.09365089982748032
Train Acc 1.0729
 Acc 1.0694
arctic25,dgl,1,1941,82.9608,1.0694

epoch:1943/50, training loss:0.09309414029121399
Train Acc 1.0726
 Acc 1.0661
arctic25,dgl,1,1942,83.0034,1.0661

epoch:1944/50, training loss:0.09427190572023392
Train Acc 1.0697
 Acc 1.0692
arctic25,dgl,1,1943,83.0460,1.0692

epoch:1945/50, training loss:0.09296437352895737
Train Acc 1.0729
 Acc 1.0694
arctic25,dgl,1,1944,83.0885,1.0694

epoch:1946/50, training loss:0.09249820560216904
Train Acc 1.0729
 Acc 1.0669
arctic25,dgl,1,1945,83.1311,1.0669

epoch:1947/50, training loss:0.0936267152428627
Train Acc 1.0707
 Acc 1.0694
arctic25,dgl,1,1946,83.1736,1.0694

epoch:1948/50, training loss:0.0924391970038414
Train Acc 1.0730
 Acc 1.0694
arctic25,dgl,1,1947,83.2162,1.0694

epoch:1949/50, training loss:0.09181385487318039
Train Acc 1.0727
 Acc 1.0670
arctic25,dgl,1,1948,83.2587,1.0670

epoch:1950/50, training loss:0.09312854707241058
Train Acc 1.0704
 Acc 1.0693
arctic25,dgl,1,1949,83.3013,1.0693

epoch:1951/50, training loss:0.09213193506002426
Train Acc 1.0730
 Acc 1.0696
arctic25,dgl,1,1950,83.3439,1.0696

epoch:1952/50, training loss:0.09134407341480255
Train Acc 1.0731
 Acc 1.0673
arctic25,dgl,1,1951,83.3863,1.0673

epoch:1953/50, training loss:0.09307420253753662
Train Acc 1.0710
 Acc 1.0696
arctic25,dgl,1,1952,83.4289,1.0696

epoch:1954/50, training loss:0.09218081086874008
Train Acc 1.0731
 Acc 1.0694
arctic25,dgl,1,1953,83.4715,1.0694

epoch:1955/50, training loss:0.09156215190887451
Train Acc 1.0728
 Acc 1.0664
arctic25,dgl,1,1954,83.5140,1.0664

epoch:1956/50, training loss:0.09359198063611984
Train Acc 1.0701
 Acc 1.0694
arctic25,dgl,1,1955,83.5565,1.0694

epoch:1957/50, training loss:0.0930904820561409
Train Acc 1.0730
 Acc 1.0695
arctic25,dgl,1,1956,83.5990,1.0695

epoch:1958/50, training loss:0.0925016701221466
Train Acc 1.0732
 Acc 1.0663
arctic25,dgl,1,1957,83.6416,1.0663

epoch:1959/50, training loss:0.09390615671873093
Train Acc 1.0703
 Acc 1.0696
new best val f1: 1.069636786075268
arctic25,dgl,1,1958,83.6842,1.0696

epoch:1960/50, training loss:0.09254288673400879
Train Acc 1.0732
 Acc 1.0694
arctic25,dgl,1,1959,83.7267,1.0694

epoch:1961/50, training loss:0.09219186007976532
Train Acc 1.0730
 Acc 1.0665
arctic25,dgl,1,1960,83.7692,1.0665

epoch:1962/50, training loss:0.09331108629703522
Train Acc 1.0702
 Acc 1.0696
arctic25,dgl,1,1961,83.8118,1.0696

epoch:1963/50, training loss:0.09198447316884995
Train Acc 1.0731
 Acc 1.0696
arctic25,dgl,1,1962,83.8544,1.0696

epoch:1964/50, training loss:0.09143422544002533
Train Acc 1.0732
 Acc 1.0671
arctic25,dgl,1,1963,83.8969,1.0671

epoch:1965/50, training loss:0.09279883652925491
Train Acc 1.0709
 Acc 1.0696
arctic25,dgl,1,1964,83.9395,1.0696

epoch:1966/50, training loss:0.09159838408231735
Train Acc 1.0732
 Acc 1.0695
arctic25,dgl,1,1965,83.9820,1.0695

epoch:1967/50, training loss:0.09123070538043976
Train Acc 1.0730
 Acc 1.0667
arctic25,dgl,1,1966,84.0245,1.0667

epoch:1968/50, training loss:0.09301058948040009
Train Acc 1.0704
 Acc 1.0695
arctic25,dgl,1,1967,84.0670,1.0695

epoch:1969/50, training loss:0.09239228814840317
Train Acc 1.0731
 Acc 1.0697
new best val f1: 1.0696564760242055
arctic25,dgl,1,1968,84.1095,1.0697

epoch:1970/50, training loss:0.09172507375478745
Train Acc 1.0732
 Acc 1.0664
arctic25,dgl,1,1969,84.1521,1.0664

epoch:1971/50, training loss:0.09343966096639633
Train Acc 1.0704
 Acc 1.0696
arctic25,dgl,1,1970,84.1947,1.0696

epoch:1972/50, training loss:0.09242241829633713
Train Acc 1.0731
 Acc 1.0696
arctic25,dgl,1,1971,84.2372,1.0696

epoch:1973/50, training loss:0.09201011061668396
Train Acc 1.0731
 Acc 1.0665
arctic25,dgl,1,1972,84.2798,1.0665

epoch:1974/50, training loss:0.0931357815861702
Train Acc 1.0702
 Acc 1.0694
arctic25,dgl,1,1973,84.3223,1.0694

epoch:1975/50, training loss:0.091808021068573
Train Acc 1.0730
 Acc 1.0696
arctic25,dgl,1,1974,84.3648,1.0696

epoch:1976/50, training loss:0.09140168130397797
Train Acc 1.0732
 Acc 1.0668
arctic25,dgl,1,1975,84.4073,1.0668

epoch:1977/50, training loss:0.09278585016727448
Train Acc 1.0707
 Acc 1.0696
arctic25,dgl,1,1976,84.4498,1.0696

epoch:1978/50, training loss:0.09176076948642731
Train Acc 1.0731
 Acc 1.0697
arctic25,dgl,1,1977,84.4924,1.0697

epoch:1979/50, training loss:0.09127556532621384
Train Acc 1.0731
 Acc 1.0664
arctic25,dgl,1,1978,84.5349,1.0664

epoch:1980/50, training loss:0.09316890686750412
Train Acc 1.0703
 Acc 1.0695
arctic25,dgl,1,1979,84.5775,1.0695

epoch:1981/50, training loss:0.0926675945520401
Train Acc 1.0731
 Acc 1.0696
arctic25,dgl,1,1980,84.6200,1.0696

epoch:1982/50, training loss:0.09223667532205582
Train Acc 1.0732
 Acc 1.0664
arctic25,dgl,1,1981,84.6625,1.0664

epoch:1983/50, training loss:0.09321051836013794
Train Acc 1.0704
 Acc 1.0697
new best val f1: 1.0696958559220804
arctic25,dgl,1,1982,84.7051,1.0697

epoch:1984/50, training loss:0.09168191999197006
Train Acc 1.0732
 Acc 1.0696
arctic25,dgl,1,1983,84.7477,1.0696

epoch:1985/50, training loss:0.09146372973918915
Train Acc 1.0731
 Acc 1.0667
arctic25,dgl,1,1984,84.7902,1.0667

epoch:1986/50, training loss:0.09264446794986725
Train Acc 1.0705
 Acc 1.0695
arctic25,dgl,1,1985,84.8327,1.0695

epoch:1987/50, training loss:0.09161108732223511
Train Acc 1.0731
 Acc 1.0697
arctic25,dgl,1,1986,84.8753,1.0697

epoch:1988/50, training loss:0.09116599708795547
Train Acc 1.0732
 Acc 1.0669
arctic25,dgl,1,1987,84.9178,1.0669

epoch:1989/50, training loss:0.09252635389566422
Train Acc 1.0708
 Acc 1.0696
arctic25,dgl,1,1988,84.9603,1.0696

epoch:1990/50, training loss:0.09145848453044891
Train Acc 1.0732
 Acc 1.0696
arctic25,dgl,1,1989,85.0029,1.0696

epoch:1991/50, training loss:0.091038279235363
Train Acc 1.0731
 Acc 1.0668
arctic25,dgl,1,1990,85.0454,1.0668

epoch:1992/50, training loss:0.09247825294733047
Train Acc 1.0706
 Acc 1.0696
arctic25,dgl,1,1991,85.0879,1.0696

epoch:1993/50, training loss:0.09152324497699738
Train Acc 1.0732
 Acc 1.0696
arctic25,dgl,1,1992,85.1305,1.0696

epoch:1994/50, training loss:0.09103868901729584
Train Acc 1.0732
 Acc 1.0666
arctic25,dgl,1,1993,85.1730,1.0666

epoch:1995/50, training loss:0.09263021498918533
Train Acc 1.0707
 Acc 1.0697
new best val f1: 1.0697417991362677
arctic25,dgl,1,1994,85.2156,1.0697

epoch:1996/50, training loss:0.0917864739894867
Train Acc 1.0733
 Acc 1.0695
arctic25,dgl,1,1995,85.2581,1.0695

epoch:1997/50, training loss:0.09135604649782181
Train Acc 1.0730
 Acc 1.0666
arctic25,dgl,1,1996,85.3006,1.0666

epoch:1998/50, training loss:0.09242846816778183
Train Acc 1.0705
 Acc 1.0696
arctic25,dgl,1,1997,85.3432,1.0696

epoch:1999/50, training loss:0.09123623371124268
Train Acc 1.0732
 Acc 1.0697
arctic25,dgl,1,1998,85.3857,1.0697

epoch:2000/50, training loss:0.09065600484609604
Train Acc 1.0731
 Acc 1.0671
arctic25,dgl,1,1999,85.4283,1.0671

training using time 198.56924748420715
Traceback (most recent call last):
  File "dgl/train_full_load.py", line 340, in <module>
    main(args)
  File "dgl/train_full_load.py", line 317, in main
    model, g, g.ndata['label'], test_mask, False)
TypeError: cannot unpack non-iterable float object
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2122136) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2122136 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     dgl/train_full_load.py FAILED     
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:19:36
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2122136)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

Namespace(csv='full_new.csv', dataset='oral', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
Inited proc group
Max label: tensor(24.)
----Data statistics------'
    #Nodes 1000000
    #Edges 21734972
    #Classes/Labels (multi binary labels) 32
    #Train samples 333333
    #Val samples 0
    #Test samples 333334
Running on: 0
GCN(
  (layers): ModuleList(
    (0): GraphConv(in=32, out=128, normalization=both, activation=<function relu at 0x14dae92c3f80>)
    (1): GraphConv(in=128, out=32, normalization=both, activation=None)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
Namespace(csv='full_new.csv', dataset='oral', gpu=0, log_dir='test', lr=0.001, n_epochs=50, n_hidden=128, online=False)
epoch:1/50, training loss:10.451058387756348
Train Acc 0.0104
 Acc 0.0125
new best val f1: 0.012512199559988751
oral,dgl,1,0,0.3653,0.0125

epoch:2/50, training loss:8.368183135986328
Train Acc 0.0124
 Acc 0.0643
new best val f1: 0.0643375845698311
oral,dgl,1,1,0.4078,0.0643

epoch:3/50, training loss:6.722717761993408
Train Acc 0.0643
 Acc 0.1075
new best val f1: 0.107461995269052
oral,dgl,1,2,0.4501,0.1075

epoch:4/50, training loss:5.524725437164307
Train Acc 0.1074
 Acc 0.1546
new best val f1: 0.15464906621673036
oral,dgl,1,3,0.4923,0.1546

epoch:5/50, training loss:4.618772983551025
Train Acc 0.1536
 Acc 0.2793
new best val f1: 0.2792847335946934
oral,dgl,1,4,0.5345,0.2793

epoch:6/50, training loss:4.031582355499268
Train Acc 0.2789
 Acc 0.4640
new best val f1: 0.46400344068946126
oral,dgl,1,5,0.5765,0.4640

epoch:7/50, training loss:3.793393850326538
Train Acc 0.4628
 Acc 0.5183
new best val f1: 0.5182935503614378
oral,dgl,1,6,0.6187,0.5183

epoch:8/50, training loss:3.699613094329834
Train Acc 0.5164
 Acc 0.4993
oral,dgl,1,7,0.6610,0.4993

epoch:9/50, training loss:3.5758321285247803
Train Acc 0.4982
 Acc 0.4509
oral,dgl,1,8,0.7032,0.4509

epoch:10/50, training loss:3.4116671085357666
Train Acc 0.4503
 Acc 0.3912
oral,dgl,1,9,0.7455,0.3912

epoch:11/50, training loss:3.2404897212982178
Train Acc 0.3913
 Acc 0.3486
oral,dgl,1,10,0.7876,0.3486

epoch:12/50, training loss:3.0863959789276123
Train Acc 0.3488
 Acc 0.3243
oral,dgl,1,11,0.8298,0.3243

epoch:13/50, training loss:2.966346263885498
Train Acc 0.3261
 Acc 0.3432
oral,dgl,1,12,0.8719,0.3432

epoch:14/50, training loss:2.87648606300354
Train Acc 0.3445
 Acc 0.3950
oral,dgl,1,13,0.9141,0.3950

epoch:15/50, training loss:2.7725417613983154
Train Acc 0.3974
 Acc 0.4813
oral,dgl,1,14,0.9563,0.4813

epoch:16/50, training loss:2.6430575847625732
Train Acc 0.4830
 Acc 0.5562
new best val f1: 0.5561907597637834
oral,dgl,1,15,0.9984,0.5562

epoch:17/50, training loss:2.536757230758667
Train Acc 0.5571
 Acc 0.5935
new best val f1: 0.5935156237076737
oral,dgl,1,16,1.0406,0.5935

epoch:18/50, training loss:2.450657606124878
Train Acc 0.5941
 Acc 0.6096
new best val f1: 0.6096471639124609
oral,dgl,1,17,1.0826,0.6096

epoch:19/50, training loss:2.368009328842163
Train Acc 0.6104
 Acc 0.6104
new best val f1: 0.6104345524622434
oral,dgl,1,18,1.1247,0.6104

epoch:20/50, training loss:2.3071541786193848
Train Acc 0.6116
 Acc 0.6056
oral,dgl,1,19,1.1667,0.6056

epoch:21/50, training loss:2.2626185417175293
Train Acc 0.6076
 Acc 0.6158
new best val f1: 0.6157907796139149
oral,dgl,1,20,1.2089,0.6158

epoch:22/50, training loss:2.203256368637085
Train Acc 0.6177
 Acc 0.6363
new best val f1: 0.6362860403950176
oral,dgl,1,21,1.2510,0.6363

epoch:23/50, training loss:2.1410019397735596
Train Acc 0.6377
 Acc 0.6450
new best val f1: 0.6449572395083784
oral,dgl,1,22,1.2930,0.6450

epoch:24/50, training loss:2.1088366508483887
Train Acc 0.6463
 Acc 0.6457
new best val f1: 0.6456553024663788
oral,dgl,1,23,1.3351,0.6457

epoch:25/50, training loss:2.078596591949463
Train Acc 0.6473
 Acc 0.6475
new best val f1: 0.6475212148280483
oral,dgl,1,24,1.3772,0.6475

epoch:26/50, training loss:2.0323452949523926
Train Acc 0.6493
 Acc 0.6529
new best val f1: 0.6529303756637388
oral,dgl,1,25,1.4192,0.6529

epoch:27/50, training loss:1.9913899898529053
Train Acc 0.6550
 Acc 0.6714
new best val f1: 0.6713843812548591
oral,dgl,1,26,1.4615,0.6714

epoch:28/50, training loss:1.952529788017273
Train Acc 0.6729
 Acc 0.6875
new best val f1: 0.6875225381701487
oral,dgl,1,27,1.5036,0.6875

epoch:29/50, training loss:1.9200797080993652
Train Acc 0.6885
 Acc 0.6977
new best val f1: 0.697656030304534
oral,dgl,1,28,1.5457,0.6977

epoch:30/50, training loss:1.8878304958343506
Train Acc 0.6981
 Acc 0.7005
new best val f1: 0.7005342993730667
oral,dgl,1,29,1.5877,0.7005

epoch:31/50, training loss:1.8555693626403809
Train Acc 0.7014
 Acc 0.6981
oral,dgl,1,30,1.6299,0.6981

epoch:32/50, training loss:1.8323866128921509
Train Acc 0.6984
 Acc 0.7046
new best val f1: 0.7045704927795147
oral,dgl,1,31,1.6720,0.7046

epoch:33/50, training loss:1.8054046630859375
Train Acc 0.7052
 Acc 0.7176
new best val f1: 0.717588870692935
oral,dgl,1,32,1.7141,0.7176

epoch:34/50, training loss:1.7756203413009644
Train Acc 0.7183
 Acc 0.7261
new best val f1: 0.7260913436884853
oral,dgl,1,33,1.7562,0.7261

epoch:35/50, training loss:1.7511855363845825
Train Acc 0.7271
 Acc 0.7306
new best val f1: 0.7306072486063554
oral,dgl,1,34,1.7983,0.7306

epoch:36/50, training loss:1.727287769317627
Train Acc 0.7319
 Acc 0.7334
new best val f1: 0.7334028087936083
oral,dgl,1,35,1.8403,0.7334

epoch:37/50, training loss:1.705114722251892
Train Acc 0.7353
 Acc 0.7371
new best val f1: 0.7371379418721983
oral,dgl,1,36,1.8823,0.7371

epoch:38/50, training loss:1.6823567152023315
Train Acc 0.7390
 Acc 0.7402
new best val f1: 0.7401948621242949
oral,dgl,1,37,1.9245,0.7402

epoch:39/50, training loss:1.6618571281433105
Train Acc 0.7420
 Acc 0.7431
new best val f1: 0.7430797479033299
oral,dgl,1,38,1.9666,0.7431

epoch:40/50, training loss:1.6420612335205078
Train Acc 0.7446
 Acc 0.7475
new best val f1: 0.7474831687426596
oral,dgl,1,39,2.0086,0.7475

epoch:41/50, training loss:1.6192280054092407
Train Acc 0.7496
 Acc 0.7531
new best val f1: 0.7531172977354308
oral,dgl,1,40,2.0508,0.7531

epoch:42/50, training loss:1.5973979234695435
Train Acc 0.7558
 Acc 0.7580
new best val f1: 0.7579772715994243
oral,dgl,1,41,2.0928,0.7580

epoch:43/50, training loss:1.5762053728103638
Train Acc 0.7605
 Acc 0.7610
new best val f1: 0.7610209584305163
oral,dgl,1,42,2.1349,0.7610

epoch:44/50, training loss:1.557726502418518
Train Acc 0.7632
 Acc 0.7648
new best val f1: 0.7648421087456371
oral,dgl,1,43,2.1768,0.7648

epoch:45/50, training loss:1.5390164852142334
Train Acc 0.7669
 Acc 0.7696
new best val f1: 0.7696359155046069
oral,dgl,1,44,2.2190,0.7696

epoch:46/50, training loss:1.5212141275405884
Train Acc 0.7718
 Acc 0.7732
new best val f1: 0.7731659305576233
oral,dgl,1,45,2.2610,0.7732

epoch:47/50, training loss:1.504747748374939
Train Acc 0.7749
 Acc 0.7741
new best val f1: 0.7740691115411973
oral,dgl,1,46,2.3031,0.7741

epoch:48/50, training loss:1.4889055490493774
Train Acc 0.7759
 Acc 0.7777
new best val f1: 0.7776619853439862
oral,dgl,1,47,2.3451,0.7777

epoch:49/50, training loss:1.4732505083084106
Train Acc 0.7795
 Acc 0.7839
new best val f1: 0.78388830992672
oral,dgl,1,48,2.3871,0.7839

epoch:50/50, training loss:1.4575283527374268
Train Acc 0.7854
 Acc 0.7877
new best val f1: 0.7877458521496038
oral,dgl,1,49,2.4292,0.7877

epoch:51/50, training loss:1.441955327987671
Train Acc 0.7893
 Acc 0.7894
new best val f1: 0.7893834879989413
oral,dgl,1,50,2.4713,0.7894

epoch:52/50, training loss:1.426592469215393
Train Acc 0.7909
 Acc 0.7926
new best val f1: 0.7925925925925926
oral,dgl,1,51,2.5134,0.7926

epoch:53/50, training loss:1.4127161502838135
Train Acc 0.7939
 Acc 0.7979
new best val f1: 0.7978661108629844
oral,dgl,1,52,2.5555,0.7979

epoch:54/50, training loss:1.4000710248947144
Train Acc 0.7993
 Acc 0.8007
new best val f1: 0.800744379931517
oral,dgl,1,53,2.5976,0.8007

epoch:55/50, training loss:1.387897253036499
Train Acc 0.8022
 Acc 0.8016
new best val f1: 0.8016210940730816
oral,dgl,1,54,2.6397,0.8016

epoch:56/50, training loss:1.3759633302688599
Train Acc 0.8033
 Acc 0.8065
new best val f1: 0.8065009180685822
oral,dgl,1,55,2.6818,0.8065

epoch:57/50, training loss:1.3642646074295044
Train Acc 0.8080
 Acc 0.8110
new best val f1: 0.8109903561444428
oral,dgl,1,56,2.7240,0.8110

epoch:58/50, training loss:1.3536045551300049
Train Acc 0.8127
 Acc 0.8122
new best val f1: 0.8122012141663771
oral,dgl,1,57,2.7659,0.8122

epoch:59/50, training loss:1.343247652053833
Train Acc 0.8142
 Acc 0.8134
new best val f1: 0.813402147122558
oral,dgl,1,58,2.8080,0.8134

epoch:60/50, training loss:1.333315372467041
Train Acc 0.8155
 Acc 0.8173
new best val f1: 0.8172563809901907
oral,dgl,1,59,2.8502,0.8173

epoch:61/50, training loss:1.3235619068145752
Train Acc 0.8191
 Acc 0.8203
new best val f1: 0.8202835260450267
oral,dgl,1,60,2.8922,0.8203

epoch:62/50, training loss:1.3140708208084106
Train Acc 0.8220
 Acc 0.8208
new best val f1: 0.820802937819463
oral,dgl,1,61,2.9342,0.8208

epoch:63/50, training loss:1.3046422004699707
Train Acc 0.8226
 Acc 0.8236
new best val f1: 0.8236315815592278
oral,dgl,1,62,2.9763,0.8236

epoch:64/50, training loss:1.2955000400543213
Train Acc 0.8254
 Acc 0.8265
new best val f1: 0.8265065422725092
oral,dgl,1,63,3.0184,0.8265

epoch:65/50, training loss:1.2867252826690674
Train Acc 0.8286
 Acc 0.8280
new best val f1: 0.8280151522670505
oral,dgl,1,64,3.0604,0.8280

epoch:66/50, training loss:1.2783180475234985
Train Acc 0.8302
 Acc 0.8291
new best val f1: 0.8291002927894398
oral,dgl,1,65,3.1025,0.8291

epoch:67/50, training loss:1.2701698541641235
Train Acc 0.8313
 Acc 0.8317
new best val f1: 0.8316741931748631
oral,dgl,1,66,3.1446,0.8317

epoch:68/50, training loss:1.2622755765914917
Train Acc 0.8335
 Acc 0.8341
new best val f1: 0.8341422261922485
oral,dgl,1,67,3.1865,0.8341

epoch:69/50, training loss:1.254563570022583
Train Acc 0.8362
 Acc 0.8348
new best val f1: 0.8348138223082394
oral,dgl,1,68,3.2286,0.8348

epoch:70/50, training loss:1.246964931488037
Train Acc 0.8368
 Acc 0.8363
new best val f1: 0.8362893487502688
oral,dgl,1,69,3.2706,0.8363

epoch:71/50, training loss:1.2395073175430298
Train Acc 0.8382
 Acc 0.8392
new best val f1: 0.8392106264370668
oral,dgl,1,70,3.3125,0.8392

epoch:72/50, training loss:1.232200264930725
Train Acc 0.8411
 Acc 0.8401
new best val f1: 0.8400939572891337
oral,dgl,1,71,3.3546,0.8401

epoch:73/50, training loss:1.2250514030456543
Train Acc 0.8421
 Acc 0.8412
new best val f1: 0.841232031495542
oral,dgl,1,72,3.3968,0.8412

epoch:74/50, training loss:1.2181990146636963
Train Acc 0.8434
 Acc 0.8447
new best val f1: 0.8446594875357716
oral,dgl,1,73,3.4387,0.8447

epoch:75/50, training loss:1.2115988731384277
Train Acc 0.8471
 Acc 0.8446
oral,dgl,1,74,3.4808,0.8446

epoch:76/50, training loss:1.2050800323486328
Train Acc 0.8468
 Acc 0.8455
new best val f1: 0.8455163515458289
oral,dgl,1,75,3.5227,0.8455

epoch:77/50, training loss:1.1988413333892822
Train Acc 0.8476
 Acc 0.8478
new best val f1: 0.8478222751559062
oral,dgl,1,76,3.5648,0.8478

epoch:78/50, training loss:1.1929385662078857
Train Acc 0.8499
 Acc 0.8481
new best val f1: 0.8481067937075083
oral,dgl,1,77,3.6068,0.8481

epoch:79/50, training loss:1.1871258020401
Train Acc 0.8502
 Acc 0.8483
new best val f1: 0.8483185284435842
oral,dgl,1,78,3.6487,0.8483

epoch:80/50, training loss:1.181498646736145
Train Acc 0.8506
 Acc 0.8517
new best val f1: 0.851712900931302
oral,dgl,1,79,3.6908,0.8517

epoch:81/50, training loss:1.1758710145950317
Train Acc 0.8537
 Acc 0.8518
new best val f1: 0.8517856847468281
oral,dgl,1,80,3.7327,0.8518

epoch:82/50, training loss:1.1702781915664673
Train Acc 0.8539
 Acc 0.8528
new best val f1: 0.8527881163879377
oral,dgl,1,81,3.7748,0.8528

epoch:83/50, training loss:1.164908766746521
Train Acc 0.8550
 Acc 0.8551
new best val f1: 0.8550609564455032
oral,dgl,1,82,3.8169,0.8551

epoch:84/50, training loss:1.1596778631210327
Train Acc 0.8574
 Acc 0.8553
new best val f1: 0.855289232957835
oral,dgl,1,83,3.8589,0.8553

epoch:85/50, training loss:1.1544595956802368
Train Acc 0.8577
 Acc 0.8558
new best val f1: 0.8557788695350107
oral,dgl,1,84,3.9009,0.8558

epoch:86/50, training loss:1.1493409872055054
Train Acc 0.8581
 Acc 0.8582
new best val f1: 0.8581807354473724
oral,dgl,1,85,3.9429,0.8582

epoch:87/50, training loss:1.144377589225769
Train Acc 0.8601
 Acc 0.8575
oral,dgl,1,86,3.9850,0.8575

epoch:88/50, training loss:1.1394575834274292
Train Acc 0.8597
 Acc 0.8591
new best val f1: 0.8590706830099416
oral,dgl,1,87,4.0271,0.8591

epoch:89/50, training loss:1.1346890926361084
Train Acc 0.8612
 Acc 0.8596
new best val f1: 0.8596231783368898
oral,dgl,1,88,4.0693,0.8596

epoch:90/50, training loss:1.130074143409729
Train Acc 0.8619
 Acc 0.8598
new best val f1: 0.8598316047177146
oral,dgl,1,89,4.1114,0.8598

epoch:91/50, training loss:1.125588059425354
Train Acc 0.8622
 Acc 0.8638
new best val f1: 0.8638016310191389
oral,dgl,1,90,4.1535,0.8638

epoch:92/50, training loss:1.1212177276611328
Train Acc 0.8656
 Acc 0.8605
oral,dgl,1,91,4.1954,0.8605

epoch:93/50, training loss:1.116928219795227
Train Acc 0.8626
 Acc 0.8663
new best val f1: 0.8663325227862968
oral,dgl,1,92,4.2376,0.8663

epoch:94/50, training loss:1.1124497652053833
Train Acc 0.8681
 Acc 0.8644
oral,dgl,1,93,4.2798,0.8644

epoch:95/50, training loss:1.107939600944519
Train Acc 0.8664
 Acc 0.8649
oral,dgl,1,94,4.3220,0.8649

epoch:96/50, training loss:1.1036951541900635
Train Acc 0.8670
 Acc 0.8682
new best val f1: 0.8681521181744496
oral,dgl,1,95,4.3640,0.8682

epoch:97/50, training loss:1.0996544361114502
Train Acc 0.8701
 Acc 0.8658
oral,dgl,1,96,4.4062,0.8658

epoch:98/50, training loss:1.0956758260726929
Train Acc 0.8678
 Acc 0.8683
new best val f1: 0.8683307693580137
oral,dgl,1,97,4.4484,0.8683

epoch:99/50, training loss:1.0917824506759644
Train Acc 0.8704
 Acc 0.8693
new best val f1: 0.8693431260648768
oral,dgl,1,98,4.4903,0.8693

epoch:100/50, training loss:1.0879863500595093
Train Acc 0.8713
 Acc 0.8681
oral,dgl,1,99,4.5325,0.8681

epoch:101/50, training loss:1.0842655897140503
Train Acc 0.8700
 Acc 0.8719
new best val f1: 0.8719468016475609
oral,dgl,1,100,4.5746,0.8719

epoch:102/50, training loss:1.0806118249893188
Train Acc 0.8738
 Acc 0.8696
oral,dgl,1,101,4.6166,0.8696

epoch:103/50, training loss:1.076920509338379
Train Acc 0.8715
 Acc 0.8727
new best val f1: 0.8727308818420922
oral,dgl,1,102,4.6587,0.8727

epoch:104/50, training loss:1.0732557773590088
Train Acc 0.8744
 Acc 0.8722
oral,dgl,1,103,4.7007,0.8722

epoch:105/50, training loss:1.0696704387664795
Train Acc 0.8739
 Acc 0.8727
new best val f1: 0.8727474236183481
oral,dgl,1,104,4.7428,0.8727

epoch:106/50, training loss:1.066184401512146
Train Acc 0.8743
 Acc 0.8744
new best val f1: 0.8743883678229368
oral,dgl,1,105,4.7848,0.8744

epoch:107/50, training loss:1.0627745389938354
Train Acc 0.8758
 Acc 0.8740
oral,dgl,1,106,4.8269,0.8740

epoch:108/50, training loss:1.059417724609375
Train Acc 0.8755
 Acc 0.8756
new best val f1: 0.8756422344631366
oral,dgl,1,107,4.8690,0.8756

epoch:109/50, training loss:1.0560884475708008
Train Acc 0.8771
 Acc 0.8757
new best val f1: 0.8757447934759235
oral,dgl,1,108,4.9111,0.8757

epoch:110/50, training loss:1.0528043508529663
Train Acc 0.8772
 Acc 0.8766
new best val f1: 0.8766082741964832
oral,dgl,1,109,4.9532,0.8766

epoch:111/50, training loss:1.049569010734558
Train Acc 0.8780
 Acc 0.8775
new best val f1: 0.8775048384695548
oral,dgl,1,110,4.9953,0.8775

epoch:112/50, training loss:1.0463669300079346
Train Acc 0.8790
 Acc 0.8769
oral,dgl,1,111,5.0375,0.8769

epoch:113/50, training loss:1.0432252883911133
Train Acc 0.8784
 Acc 0.8792
new best val f1: 0.8792218748449209
oral,dgl,1,112,5.0795,0.8792

epoch:114/50, training loss:1.0401374101638794
Train Acc 0.8807
 Acc 0.8776
oral,dgl,1,113,5.1214,0.8776

epoch:115/50, training loss:1.0370802879333496
Train Acc 0.8791
 Acc 0.8799
new best val f1: 0.8799232461581724
oral,dgl,1,114,5.1636,0.8799

epoch:116/50, training loss:1.0340092182159424
Train Acc 0.8814
 Acc 0.8796
oral,dgl,1,115,5.2057,0.8796

epoch:117/50, training loss:1.0309789180755615
Train Acc 0.8813
 Acc 0.8798
oral,dgl,1,116,5.2476,0.8798

epoch:118/50, training loss:1.0280416011810303
Train Acc 0.8814
 Acc 0.8810
new best val f1: 0.8810183117463153
oral,dgl,1,117,5.2897,0.8810

epoch:119/50, training loss:1.0251485109329224
Train Acc 0.8827
 Acc 0.8801
oral,dgl,1,118,5.3318,0.8801

epoch:120/50, training loss:1.0223172903060913
Train Acc 0.8817
 Acc 0.8825
new best val f1: 0.8825401551618612
oral,dgl,1,119,5.3737,0.8825

epoch:121/50, training loss:1.0195623636245728
Train Acc 0.8842
 Acc 0.8795
oral,dgl,1,120,5.4158,0.8795

epoch:122/50, training loss:1.0169073343276978
Train Acc 0.8812
 Acc 0.8843
new best val f1: 0.8842935834449903
oral,dgl,1,121,5.4579,0.8843

epoch:123/50, training loss:1.014304518699646
Train Acc 0.8859
 Acc 0.8794
oral,dgl,1,122,5.5000,0.8794

epoch:124/50, training loss:1.0117535591125488
Train Acc 0.8810
 Acc 0.8852
new best val f1: 0.8852099978495691
oral,dgl,1,123,5.5421,0.8852

epoch:125/50, training loss:1.0091173648834229
Train Acc 0.8868
 Acc 0.8806
oral,dgl,1,124,5.5843,0.8806

epoch:126/50, training loss:1.0064404010772705
Train Acc 0.8821
 Acc 0.8847
oral,dgl,1,125,5.6264,0.8847

epoch:127/50, training loss:1.0037156343460083
Train Acc 0.8861
 Acc 0.8833
oral,dgl,1,126,5.6686,0.8833

epoch:128/50, training loss:1.0011022090911865
Train Acc 0.8847
 Acc 0.8824
oral,dgl,1,127,5.7107,0.8824

epoch:129/50, training loss:0.9987279772758484
Train Acc 0.8841
 Acc 0.8860
new best val f1: 0.8860172365308587
oral,dgl,1,128,5.7528,0.8860

epoch:130/50, training loss:0.99651700258255
Train Acc 0.8875
 Acc 0.8816
oral,dgl,1,129,5.7949,0.8816

epoch:131/50, training loss:0.9942643642425537
Train Acc 0.8831
 Acc 0.8867
new best val f1: 0.886678907581096
oral,dgl,1,130,5.8369,0.8867

epoch:132/50, training loss:0.9918455481529236
Train Acc 0.8884
 Acc 0.8835
oral,dgl,1,131,5.8789,0.8835

epoch:133/50, training loss:0.9893266558647156
Train Acc 0.8851
 Acc 0.8852
oral,dgl,1,132,5.9211,0.8852

epoch:134/50, training loss:0.986941933631897
Train Acc 0.8869
 Acc 0.8866
oral,dgl,1,133,5.9633,0.8866

epoch:135/50, training loss:0.9847825765609741
Train Acc 0.8883
 Acc 0.8835
oral,dgl,1,134,6.0052,0.8835

epoch:136/50, training loss:0.9827725887298584
Train Acc 0.8852
 Acc 0.8883
new best val f1: 0.8883000016541777
oral,dgl,1,135,6.0473,0.8883

epoch:137/50, training loss:0.9806912541389465
Train Acc 0.8901
 Acc 0.8840
oral,dgl,1,136,6.0894,0.8840

epoch:138/50, training loss:0.9784204959869385
Train Acc 0.8858
 Acc 0.8875
oral,dgl,1,137,6.1313,0.8875

epoch:139/50, training loss:0.9760509729385376
Train Acc 0.8894
 Acc 0.8869
oral,dgl,1,138,6.1734,0.8869

epoch:140/50, training loss:0.9738711714744568
Train Acc 0.8888
 Acc 0.8857
oral,dgl,1,139,6.2155,0.8857

epoch:141/50, training loss:0.971885085105896
Train Acc 0.8874
 Acc 0.8892
new best val f1: 0.8891998742825005
oral,dgl,1,140,6.2575,0.8892

epoch:142/50, training loss:0.9699516892433167
Train Acc 0.8909
 Acc 0.8856
oral,dgl,1,141,6.2996,0.8856

epoch:143/50, training loss:0.9679052233695984
Train Acc 0.8873
 Acc 0.8889
oral,dgl,1,142,6.3417,0.8889

epoch:144/50, training loss:0.9657188653945923
Train Acc 0.8906
 Acc 0.8880
oral,dgl,1,143,6.3836,0.8880

epoch:145/50, training loss:0.9636486768722534
Train Acc 0.8899
 Acc 0.8874
oral,dgl,1,144,6.4259,0.8874

epoch:146/50, training loss:0.9617283940315247
Train Acc 0.8891
 Acc 0.8903
new best val f1: 0.8903313317784064
oral,dgl,1,145,6.4679,0.8903

epoch:147/50, training loss:0.9599038362503052
Train Acc 0.8920
 Acc 0.8869
oral,dgl,1,146,6.5098,0.8869

epoch:148/50, training loss:0.95799320936203
Train Acc 0.8886
 Acc 0.8906
new best val f1: 0.8905662250012406
oral,dgl,1,147,6.5519,0.8906

epoch:149/50, training loss:0.9559672474861145
Train Acc 0.8922
 Acc 0.8887
oral,dgl,1,148,6.5940,0.8887

epoch:150/50, training loss:0.9539421796798706
Train Acc 0.8903
 Acc 0.8896
oral,dgl,1,149,6.6359,0.8896

epoch:151/50, training loss:0.9520212411880493
Train Acc 0.8911
 Acc 0.8908
new best val f1: 0.8908308934213356
oral,dgl,1,150,6.6781,0.8908

epoch:152/50, training loss:0.9501932859420776
Train Acc 0.8924
 Acc 0.8888
oral,dgl,1,151,6.7202,0.8888

epoch:153/50, training loss:0.9484428763389587
Train Acc 0.8903
 Acc 0.8923
new best val f1: 0.8923494284816303
oral,dgl,1,152,6.7622,0.8923

epoch:154/50, training loss:0.9467073678970337
Train Acc 0.8941
 Acc 0.8884
oral,dgl,1,153,6.8043,0.8884

epoch:155/50, training loss:0.9449546337127686
Train Acc 0.8900
 Acc 0.8929
new best val f1: 0.8928754569665691
oral,dgl,1,154,6.8464,0.8929

epoch:156/50, training loss:0.9430885314941406
Train Acc 0.8945
 Acc 0.8900
oral,dgl,1,155,6.8885,0.8900

epoch:157/50, training loss:0.9411453008651733
Train Acc 0.8914
 Acc 0.8918
oral,dgl,1,156,6.9305,0.8918

epoch:158/50, training loss:0.9392553567886353
Train Acc 0.8933
 Acc 0.8923
oral,dgl,1,157,6.9727,0.8923

epoch:159/50, training loss:0.9375342130661011
Train Acc 0.8937
 Acc 0.8905
oral,dgl,1,158,7.0148,0.8905

epoch:160/50, training loss:0.9359256625175476
Train Acc 0.8918
 Acc 0.8940
new best val f1: 0.8940168395282285
oral,dgl,1,159,7.0568,0.8940

epoch:161/50, training loss:0.9343461394309998
Train Acc 0.8956
 Acc 0.8903
oral,dgl,1,160,7.0987,0.8903

epoch:162/50, training loss:0.9326335191726685
Train Acc 0.8917
 Acc 0.8936
oral,dgl,1,161,7.1407,0.8936

epoch:163/50, training loss:0.9307777285575867
Train Acc 0.8952
 Acc 0.8926
oral,dgl,1,162,7.1829,0.8926

epoch:164/50, training loss:0.9289965033531189
Train Acc 0.8939
 Acc 0.8922
oral,dgl,1,163,7.2248,0.8922

epoch:165/50, training loss:0.9273788332939148
Train Acc 0.8935
 Acc 0.8942
new best val f1: 0.8942285742643045
oral,dgl,1,164,7.2669,0.8942

epoch:166/50, training loss:0.9258205890655518
Train Acc 0.8958
 Acc 0.8918
oral,dgl,1,165,7.3089,0.8918

epoch:167/50, training loss:0.9242222905158997
Train Acc 0.8933
 Acc 0.8945
new best val f1: 0.8945097844606554
oral,dgl,1,166,7.3508,0.8945

epoch:168/50, training loss:0.9225568771362305
Train Acc 0.8960
 Acc 0.8931
oral,dgl,1,167,7.3929,0.8931

epoch:169/50, training loss:0.9208918809890747
Train Acc 0.8945
 Acc 0.8940
oral,dgl,1,168,7.4349,0.8940

epoch:170/50, training loss:0.9192718863487244
Train Acc 0.8953
 Acc 0.8945
oral,dgl,1,169,7.4771,0.8945

epoch:171/50, training loss:0.9177235960960388
Train Acc 0.8960
 Acc 0.8929
oral,dgl,1,170,7.5192,0.8929

epoch:172/50, training loss:0.9162210822105408
Train Acc 0.8947
 Acc 0.8956
new best val f1: 0.8956081584040494
oral,dgl,1,171,7.5613,0.8956

epoch:173/50, training loss:0.9147235751152039
Train Acc 0.8972
 Acc 0.8929
oral,dgl,1,172,7.6032,0.8929

epoch:174/50, training loss:0.9131860733032227
Train Acc 0.8948
 Acc 0.8958
new best val f1: 0.8957868095876135
oral,dgl,1,173,7.6453,0.8958

epoch:175/50, training loss:0.9116364121437073
Train Acc 0.8975
 Acc 0.8934
oral,dgl,1,174,7.6874,0.8934

epoch:176/50, training loss:0.9101111888885498
Train Acc 0.8954
 Acc 0.8960
new best val f1: 0.8959522273501729
oral,dgl,1,175,7.7294,0.8960

epoch:177/50, training loss:0.9086003303527832
Train Acc 0.8976
 Acc 0.8938
oral,dgl,1,176,7.7716,0.8938

epoch:178/50, training loss:0.907095730304718
Train Acc 0.8958
 Acc 0.8957
oral,dgl,1,177,7.8136,0.8957

epoch:179/50, training loss:0.9055666327476501
Train Acc 0.8975
 Acc 0.8952
oral,dgl,1,178,7.8558,0.8952

epoch:180/50, training loss:0.9040706753730774
Train Acc 0.8971
 Acc 0.8948
oral,dgl,1,179,7.8979,0.8948

epoch:181/50, training loss:0.9026377201080322
Train Acc 0.8969
 Acc 0.8964
new best val f1: 0.8964484806378509
oral,dgl,1,180,7.9400,0.8964

epoch:182/50, training loss:0.9012140035629272
Train Acc 0.8983
 Acc 0.8947
oral,dgl,1,181,7.9820,0.8947

epoch:183/50, training loss:0.8998087048530579
Train Acc 0.8966
 Acc 0.8971
new best val f1: 0.8971399268853489
oral,dgl,1,182,8.0241,0.8971

epoch:184/50, training loss:0.8984190821647644
Train Acc 0.8988
 Acc 0.8947
oral,dgl,1,183,8.0662,0.8947

epoch:185/50, training loss:0.897061288356781
Train Acc 0.8965
 Acc 0.8982
new best val f1: 0.898238300828743
oral,dgl,1,184,8.1083,0.8982

epoch:186/50, training loss:0.8957638740539551
Train Acc 0.8998
 Acc 0.8942
oral,dgl,1,185,8.1504,0.8942

epoch:187/50, training loss:0.8944713473320007
Train Acc 0.8960
 Acc 0.8986
new best val f1: 0.8986055282616248
oral,dgl,1,186,8.1924,0.8986

epoch:188/50, training loss:0.8930587768554688
Train Acc 0.9001
 Acc 0.8950
oral,dgl,1,187,8.2345,0.8950

epoch:189/50, training loss:0.8916389346122742
Train Acc 0.8967
 Acc 0.8986
new best val f1: 0.898628686748383
oral,dgl,1,188,8.2766,0.8986

epoch:190/50, training loss:0.8902025818824768
Train Acc 0.9001
 Acc 0.8959
oral,dgl,1,189,8.3186,0.8959

epoch:191/50, training loss:0.8888036608695984
Train Acc 0.8977
 Acc 0.8983
oral,dgl,1,190,8.3608,0.8983

epoch:192/50, training loss:0.8874227404594421
Train Acc 0.9000
 Acc 0.8969
oral,dgl,1,191,8.4029,0.8969

epoch:193/50, training loss:0.8860518932342529
Train Acc 0.8987
 Acc 0.8975
oral,dgl,1,192,8.4448,0.8975

epoch:194/50, training loss:0.8847154974937439
Train Acc 0.8993
 Acc 0.8983
oral,dgl,1,193,8.4870,0.8983

epoch:195/50, training loss:0.8834428787231445
Train Acc 0.9000
 Acc 0.8966
oral,dgl,1,194,8.5290,0.8966

epoch:196/50, training loss:0.882209300994873
Train Acc 0.8985
 Acc 0.8995
new best val f1: 0.8994557755611797
oral,dgl,1,195,8.5711,0.8995

epoch:197/50, training loss:0.8809747695922852
Train Acc 0.9011
 Acc 0.8963
oral,dgl,1,196,8.6132,0.8963

epoch:198/50, training loss:0.8797692656517029
Train Acc 0.8983
 Acc 0.9003
new best val f1: 0.9003192562817396
oral,dgl,1,197,8.6552,0.9003

epoch:199/50, training loss:0.8786352276802063
Train Acc 0.9021
 Acc 0.8954
oral,dgl,1,198,8.6974,0.8954

epoch:200/50, training loss:0.8775877952575684
Train Acc 0.8975
 Acc 0.9014
new best val f1: 0.9013878550278729
oral,dgl,1,199,8.7395,0.9014

epoch:201/50, training loss:0.8765246868133545
Train Acc 0.9032
 Acc 0.8950
oral,dgl,1,200,8.7817,0.8950

epoch:202/50, training loss:0.8754129409790039
Train Acc 0.8970
 Acc 0.9016
new best val f1: 0.9016492150927167
oral,dgl,1,201,8.8237,0.9016

epoch:203/50, training loss:0.8741313219070435
Train Acc 0.9036
 Acc 0.8958
oral,dgl,1,202,8.8657,0.8958

epoch:204/50, training loss:0.8728046417236328
Train Acc 0.8977
 Acc 0.9013
oral,dgl,1,203,8.9078,0.9013

epoch:205/50, training loss:0.8712880611419678
Train Acc 0.9030
 Acc 0.8980
oral,dgl,1,204,8.9499,0.8980

epoch:206/50, training loss:0.8698107600212097
Train Acc 0.8999
 Acc 0.8995
oral,dgl,1,205,8.9920,0.8995

epoch:207/50, training loss:0.8685063719749451
Train Acc 0.9013
 Acc 0.9000
oral,dgl,1,206,9.0341,0.9000

epoch:208/50, training loss:0.8673509955406189
Train Acc 0.9020
 Acc 0.8985
oral,dgl,1,207,9.0761,0.8985

epoch:209/50, training loss:0.8662198185920715
Train Acc 0.9005
 Acc 0.9003
oral,dgl,1,208,9.1181,0.9003

epoch:210/50, training loss:0.8650270700454712
Train Acc 0.9024
 Acc 0.8993
oral,dgl,1,209,9.1603,0.8993

epoch:211/50, training loss:0.8638370037078857
Train Acc 0.9012
 Acc 0.9000
oral,dgl,1,210,9.2024,0.9000

epoch:212/50, training loss:0.862669825553894
Train Acc 0.9018
 Acc 0.9002
oral,dgl,1,211,9.2445,0.9002

epoch:213/50, training loss:0.8615243434906006
Train Acc 0.9022
 Acc 0.8996
oral,dgl,1,212,9.2865,0.8996

epoch:214/50, training loss:0.8604030013084412
Train Acc 0.9015
 Acc 0.9011
oral,dgl,1,213,9.3287,0.9011

epoch:215/50, training loss:0.8592996001243591
Train Acc 0.9031
 Acc 0.8993
oral,dgl,1,214,9.3708,0.8993

epoch:216/50, training loss:0.8582097291946411
Train Acc 0.9013
 Acc 0.9016
oral,dgl,1,215,9.4130,0.9016

epoch:217/50, training loss:0.8571045994758606
Train Acc 0.9036
 Acc 0.8996
oral,dgl,1,216,9.4549,0.8996

epoch:218/50, training loss:0.8559662699699402
Train Acc 0.9016
 Acc 0.9016
oral,dgl,1,217,9.4968,0.9016

epoch:219/50, training loss:0.8548133373260498
Train Acc 0.9036
 Acc 0.9005
oral,dgl,1,218,9.5390,0.9005

epoch:220/50, training loss:0.8536824584007263
Train Acc 0.9026
 Acc 0.9010
oral,dgl,1,219,9.5811,0.9010

epoch:221/50, training loss:0.8525769710540771
Train Acc 0.9031
 Acc 0.9018
new best val f1: 0.9017716242370105
oral,dgl,1,220,9.6231,0.9018

epoch:222/50, training loss:0.8515276312828064
Train Acc 0.9038
 Acc 0.8994
oral,dgl,1,221,9.6651,0.8994

epoch:223/50, training loss:0.8506163358688354
Train Acc 0.9016
 Acc 0.9038
new best val f1: 0.9037632540982251
oral,dgl,1,222,9.7072,0.9038

epoch:224/50, training loss:0.8498121500015259
Train Acc 0.9054
 Acc 0.8982
oral,dgl,1,223,9.7493,0.8982

epoch:225/50, training loss:0.8489518165588379
Train Acc 0.9003
 Acc 0.9043
new best val f1: 0.9043322912014292
oral,dgl,1,224,9.7914,0.9043

epoch:226/50, training loss:0.8479791879653931
Train Acc 0.9061
 Acc 0.8982
oral,dgl,1,225,9.8334,0.8982

epoch:227/50, training loss:0.8468937873840332
Train Acc 0.9003
 Acc 0.9040
oral,dgl,1,226,9.8755,0.9040

epoch:228/50, training loss:0.8455639481544495
Train Acc 0.9059
 Acc 0.9001
oral,dgl,1,227,9.9177,0.9001

epoch:229/50, training loss:0.8442733287811279
Train Acc 0.9022
 Acc 0.9030
oral,dgl,1,228,9.9598,0.9030

epoch:230/50, training loss:0.8431156873703003
Train Acc 0.9050
 Acc 0.9018
oral,dgl,1,229,10.0018,0.9018

epoch:231/50, training loss:0.8420239686965942
Train Acc 0.9039
 Acc 0.9015
oral,dgl,1,230,10.0438,0.9015

epoch:232/50, training loss:0.8410186171531677
Train Acc 0.9038
 Acc 0.9035
oral,dgl,1,231,10.0858,0.9035

epoch:233/50, training loss:0.8401355743408203
Train Acc 0.9057
 Acc 0.9003
oral,dgl,1,232,10.1280,0.9003

epoch:234/50, training loss:0.8392441868782043
Train Acc 0.9026
 Acc 0.9042
oral,dgl,1,233,10.1699,0.9042

epoch:235/50, training loss:0.8382851481437683
Train Acc 0.9063
 Acc 0.9006
oral,dgl,1,234,10.2121,0.9006

epoch:236/50, training loss:0.8372578024864197
Train Acc 0.9027
 Acc 0.9037
oral,dgl,1,235,10.2541,0.9037

epoch:237/50, training loss:0.8361246585845947
Train Acc 0.9060
 Acc 0.9019
oral,dgl,1,236,10.2962,0.9019

epoch:238/50, training loss:0.835043728351593
Train Acc 0.9043
 Acc 0.9029
oral,dgl,1,237,10.3383,0.9029

epoch:239/50, training loss:0.8340488076210022
Train Acc 0.9053
 Acc 0.9032
oral,dgl,1,238,10.3803,0.9032

epoch:240/50, training loss:0.8330690264701843
Train Acc 0.9056
 Acc 0.9022
oral,dgl,1,239,10.4223,0.9022

epoch:241/50, training loss:0.8321435451507568
Train Acc 0.9045
 Acc 0.9040
oral,dgl,1,240,10.4645,0.9040

epoch:242/50, training loss:0.8312300443649292
Train Acc 0.9063
 Acc 0.9016
oral,dgl,1,241,10.5065,0.9016

epoch:243/50, training loss:0.8303291201591492
Train Acc 0.9040
 Acc 0.9047
new best val f1: 0.9046697434370503
oral,dgl,1,242,10.5485,0.9047

epoch:244/50, training loss:0.8294652104377747
Train Acc 0.9071
 Acc 0.9014
oral,dgl,1,243,10.5906,0.9014

epoch:245/50, training loss:0.828579843044281
Train Acc 0.9036
 Acc 0.9051
new best val f1: 0.9051494549484723
oral,dgl,1,244,10.6327,0.9051

epoch:246/50, training loss:0.8276405334472656
Train Acc 0.9075
 Acc 0.9015
oral,dgl,1,245,10.6748,0.9015

epoch:247/50, training loss:0.8266914486885071
Train Acc 0.9038
 Acc 0.9051
oral,dgl,1,246,10.7167,0.9051

epoch:248/50, training loss:0.825695812702179
Train Acc 0.9075
 Acc 0.9025
oral,dgl,1,247,10.7589,0.9025

epoch:249/50, training loss:0.8246772885322571
Train Acc 0.9049
 Acc 0.9046
oral,dgl,1,248,10.8010,0.9046

epoch:250/50, training loss:0.8236879706382751
Train Acc 0.9069
 Acc 0.9034
oral,dgl,1,249,10.8430,0.9034

epoch:251/50, training loss:0.8227277398109436
Train Acc 0.9058
 Acc 0.9041
oral,dgl,1,250,10.8850,0.9041

epoch:252/50, training loss:0.82181715965271
Train Acc 0.9064
 Acc 0.9040
oral,dgl,1,251,10.9270,0.9040

epoch:253/50, training loss:0.8209176063537598
Train Acc 0.9062
 Acc 0.9041
oral,dgl,1,252,10.9691,0.9041

epoch:254/50, training loss:0.8200134038925171
Train Acc 0.9065
 Acc 0.9044
oral,dgl,1,253,11.0111,0.9044

epoch:255/50, training loss:0.8191191554069519
Train Acc 0.9068
 Acc 0.9040
oral,dgl,1,254,11.0531,0.9040

epoch:256/50, training loss:0.8182513117790222
Train Acc 0.9064
 Acc 0.9046
oral,dgl,1,255,11.0953,0.9046

epoch:257/50, training loss:0.8173635005950928
Train Acc 0.9070
 Acc 0.9046
oral,dgl,1,256,11.1374,0.9046

epoch:258/50, training loss:0.8164646029472351
Train Acc 0.9070
 Acc 0.9040
oral,dgl,1,257,11.1793,0.9040

epoch:259/50, training loss:0.815601646900177
Train Acc 0.9064
 Acc 0.9056
new best val f1: 0.9055927745521314
oral,dgl,1,258,11.2215,0.9056

epoch:260/50, training loss:0.8148118853569031
Train Acc 0.9081
 Acc 0.9030
oral,dgl,1,259,11.2636,0.9030

epoch:261/50, training loss:0.8140829801559448
Train Acc 0.9055
 Acc 0.9066
new best val f1: 0.9065621226407291
oral,dgl,1,260,11.3057,0.9066

epoch:262/50, training loss:0.8133479952812195
Train Acc 0.9090
 Acc 0.9024
oral,dgl,1,261,11.3476,0.9024

epoch:263/50, training loss:0.8125993013381958
Train Acc 0.9047
 Acc 0.9070
new best val f1: 0.907002133889137
oral,dgl,1,262,11.3897,0.9070

epoch:264/50, training loss:0.8117716312408447
Train Acc 0.9094
 Acc 0.9025
oral,dgl,1,263,11.4318,0.9025

epoch:265/50, training loss:0.8109201192855835
Train Acc 0.9048
 Acc 0.9070
new best val f1: 0.9070120589548906
oral,dgl,1,264,11.4739,0.9070

epoch:266/50, training loss:0.8100222945213318
Train Acc 0.9094
 Acc 0.9030
oral,dgl,1,265,11.5161,0.9030

epoch:267/50, training loss:0.8091409206390381
Train Acc 0.9052
 Acc 0.9069
oral,dgl,1,266,11.5582,0.9069

epoch:268/50, training loss:0.8082089424133301
Train Acc 0.9094
 Acc 0.9039
oral,dgl,1,267,11.6003,0.9039

epoch:269/50, training loss:0.807275652885437
Train Acc 0.9062
 Acc 0.9065
oral,dgl,1,268,11.6423,0.9065

epoch:270/50, training loss:0.8063702583312988
Train Acc 0.9090
 Acc 0.9048
oral,dgl,1,269,11.6844,0.9048

epoch:271/50, training loss:0.8055006265640259
Train Acc 0.9070
 Acc 0.9062
oral,dgl,1,270,11.7265,0.9062

epoch:272/50, training loss:0.8046441078186035
Train Acc 0.9087
 Acc 0.9050
oral,dgl,1,271,11.7685,0.9050

epoch:273/50, training loss:0.8038315773010254
Train Acc 0.9073
 Acc 0.9067
oral,dgl,1,272,11.8106,0.9067

epoch:274/50, training loss:0.8030720949172974
Train Acc 0.9090
 Acc 0.9045
oral,dgl,1,273,11.8525,0.9045

epoch:275/50, training loss:0.8023519515991211
Train Acc 0.9067
 Acc 0.9070
new best val f1: 0.9070352174416488
oral,dgl,1,274,11.8946,0.9070

epoch:276/50, training loss:0.8015836477279663
Train Acc 0.9094
 Acc 0.9045
oral,dgl,1,275,11.9367,0.9045

epoch:277/50, training loss:0.8007842302322388
Train Acc 0.9068
 Acc 0.9073
new best val f1: 0.9072998858617438
oral,dgl,1,276,11.9789,0.9073

epoch:278/50, training loss:0.7999772429466248
Train Acc 0.9098
 Acc 0.9046
oral,dgl,1,277,12.0208,0.9046

epoch:279/50, training loss:0.7992032170295715
Train Acc 0.9068
 Acc 0.9075
new best val f1: 0.9075447041503316
oral,dgl,1,278,12.0629,0.9075

epoch:280/50, training loss:0.7984499335289001
Train Acc 0.9100
 Acc 0.9045
oral,dgl,1,279,12.1050,0.9045

epoch:281/50, training loss:0.7977396845817566
Train Acc 0.9066
 Acc 0.9081
new best val f1: 0.9080608075695168
oral,dgl,1,280,12.1471,0.9081

epoch:282/50, training loss:0.7970467209815979
Train Acc 0.9104
 Acc 0.9040
oral,dgl,1,281,12.1891,0.9040

epoch:283/50, training loss:0.7963535189628601
Train Acc 0.9062
 Acc 0.9085
new best val f1: 0.9084975104626735
oral,dgl,1,282,12.2311,0.9085

epoch:284/50, training loss:0.7956218719482422
Train Acc 0.9108
 Acc 0.9039
oral,dgl,1,283,12.2733,0.9039

epoch:285/50, training loss:0.7949260473251343
Train Acc 0.9060
 Acc 0.9089
new best val f1: 0.9088878963823135
oral,dgl,1,284,12.3154,0.9089

epoch:286/50, training loss:0.7942816615104675
Train Acc 0.9111
 Acc 0.9030
oral,dgl,1,285,12.3574,0.9030

epoch:287/50, training loss:0.7938455939292908
Train Acc 0.9052
 Acc 0.9098
new best val f1: 0.9097679188791292
oral,dgl,1,286,12.3995,0.9098

epoch:288/50, training loss:0.7935418486595154
Train Acc 0.9121
 Acc 0.9017
oral,dgl,1,287,12.4414,0.9017

epoch:289/50, training loss:0.7933066487312317
Train Acc 0.9040
 Acc 0.9101
new best val f1: 0.9101483797330158
oral,dgl,1,288,12.4836,0.9101

epoch:290/50, training loss:0.7925771474838257
Train Acc 0.9126
 Acc 0.9024
oral,dgl,1,289,12.5257,0.9024

epoch:291/50, training loss:0.7914926409721375
Train Acc 0.9046
 Acc 0.9095
oral,dgl,1,290,12.5678,0.9095

epoch:292/50, training loss:0.7899754047393799
Train Acc 0.9116
 Acc 0.9055
oral,dgl,1,291,12.6098,0.9055

epoch:293/50, training loss:0.7886740565299988
Train Acc 0.9076
 Acc 0.9072
oral,dgl,1,292,12.6519,0.9072

epoch:294/50, training loss:0.7877461314201355
Train Acc 0.9095
 Acc 0.9082
oral,dgl,1,293,12.6940,0.9082

epoch:295/50, training loss:0.7871202826499939
Train Acc 0.9105
 Acc 0.9054
oral,dgl,1,294,12.7360,0.9054

epoch:296/50, training loss:0.7865722179412842
Train Acc 0.9074
 Acc 0.9091
oral,dgl,1,295,12.7781,0.9091

epoch:297/50, training loss:0.7859700918197632
Train Acc 0.9113
 Acc 0.9053
oral,dgl,1,296,12.8202,0.9053

epoch:298/50, training loss:0.7852379083633423
Train Acc 0.9074
 Acc 0.9083
oral,dgl,1,297,12.8623,0.9083

epoch:299/50, training loss:0.7843154072761536
Train Acc 0.9107
 Acc 0.9072
oral,dgl,1,298,12.9043,0.9072

epoch:300/50, training loss:0.783447802066803
Train Acc 0.9095
 Acc 0.9070
oral,dgl,1,299,12.9464,0.9070

epoch:301/50, training loss:0.7827200293540955
Train Acc 0.9093
 Acc 0.9084
oral,dgl,1,300,12.9885,0.9084

epoch:302/50, training loss:0.7820951342582703
Train Acc 0.9108
 Acc 0.9063
oral,dgl,1,301,13.0305,0.9063

epoch:303/50, training loss:0.7815068960189819
Train Acc 0.9084
 Acc 0.9089
oral,dgl,1,302,13.0725,0.9089

epoch:304/50, training loss:0.7808219790458679
Train Acc 0.9111
 Acc 0.9069
oral,dgl,1,303,13.1146,0.9069

epoch:305/50, training loss:0.780034601688385
Train Acc 0.9089
 Acc 0.9084
oral,dgl,1,304,13.1567,0.9084

epoch:306/50, training loss:0.7792574167251587
Train Acc 0.9106
 Acc 0.9078
oral,dgl,1,305,13.1987,0.9078

epoch:307/50, training loss:0.7785629630088806
Train Acc 0.9101
 Acc 0.9079
oral,dgl,1,306,13.2409,0.9079

epoch:308/50, training loss:0.7779171466827393
Train Acc 0.9102
 Acc 0.9082
oral,dgl,1,307,13.2830,0.9082

epoch:309/50, training loss:0.7772679924964905
Train Acc 0.9105
 Acc 0.9078
oral,dgl,1,308,13.3250,0.9078

epoch:310/50, training loss:0.7765665054321289
Train Acc 0.9100
 Acc 0.9082
oral,dgl,1,309,13.3670,0.9082

epoch:311/50, training loss:0.775849461555481
Train Acc 0.9104
 Acc 0.9085
oral,dgl,1,310,13.4091,0.9085

epoch:312/50, training loss:0.7751793265342712
Train Acc 0.9108
 Acc 0.9074
oral,dgl,1,311,13.4512,0.9074

epoch:313/50, training loss:0.7746108770370483
Train Acc 0.9097
 Acc 0.9092
oral,dgl,1,312,13.4933,0.9092

epoch:314/50, training loss:0.77405846118927
Train Acc 0.9113
 Acc 0.9071
oral,dgl,1,313,13.5355,0.9071

epoch:315/50, training loss:0.7733953595161438
Train Acc 0.9093
 Acc 0.9092
oral,dgl,1,314,13.5776,0.9092

epoch:316/50, training loss:0.7726734280586243
Train Acc 0.9114
 Acc 0.9075
oral,dgl,1,315,13.6197,0.9075

epoch:317/50, training loss:0.7719599008560181
Train Acc 0.9097
 Acc 0.9093
oral,dgl,1,316,13.6616,0.9093

epoch:318/50, training loss:0.7712749242782593
Train Acc 0.9116
 Acc 0.9079
oral,dgl,1,317,13.7038,0.9079

epoch:319/50, training loss:0.7706051468849182
Train Acc 0.9102
 Acc 0.9091
oral,dgl,1,318,13.7459,0.9091

epoch:320/50, training loss:0.7699305415153503
Train Acc 0.9114
 Acc 0.9086
oral,dgl,1,319,13.7879,0.9086

epoch:321/50, training loss:0.7692537903785706
Train Acc 0.9108
 Acc 0.9082
oral,dgl,1,320,13.8299,0.9082

epoch:322/50, training loss:0.7686166167259216
Train Acc 0.9105
 Acc 0.9096
oral,dgl,1,321,13.8720,0.9096

epoch:323/50, training loss:0.7680515050888062
Train Acc 0.9119
 Acc 0.9073
oral,dgl,1,322,13.9140,0.9073

epoch:324/50, training loss:0.7675442695617676
Train Acc 0.9095
 Acc 0.9105
new best val f1: 0.9104593651266273
oral,dgl,1,323,13.9560,0.9105

epoch:325/50, training loss:0.7670345902442932
Train Acc 0.9127
 Acc 0.9067
oral,dgl,1,324,13.9981,0.9067

epoch:326/50, training loss:0.7665303349494934
Train Acc 0.9086
 Acc 0.9111
new best val f1: 0.9110879526243528
oral,dgl,1,325,14.0401,0.9111

epoch:327/50, training loss:0.7660699486732483
Train Acc 0.9132
 Acc 0.9057
oral,dgl,1,326,14.0821,0.9057

epoch:328/50, training loss:0.7656629085540771
Train Acc 0.9079
 Acc 0.9114
new best val f1: 0.9113658544654525
oral,dgl,1,327,14.1243,0.9114

epoch:329/50, training loss:0.7651323080062866
Train Acc 0.9136
 Acc 0.9059
oral,dgl,1,328,14.1664,0.9059

epoch:330/50, training loss:0.764487087726593
Train Acc 0.9081
 Acc 0.9109
oral,dgl,1,329,14.2086,0.9109

epoch:331/50, training loss:0.763529360294342
Train Acc 0.9129
 Acc 0.9078
oral,dgl,1,330,14.2507,0.9078

epoch:332/50, training loss:0.7625653147697449
Train Acc 0.9098
 Acc 0.9094
oral,dgl,1,331,14.2929,0.9094

epoch:333/50, training loss:0.7617250680923462
Train Acc 0.9116
 Acc 0.9099
oral,dgl,1,332,14.3350,0.9099

epoch:334/50, training loss:0.7610969543457031
Train Acc 0.9120
 Acc 0.9079
oral,dgl,1,333,14.3771,0.9079

epoch:335/50, training loss:0.7606337666511536
Train Acc 0.9101
 Acc 0.9110
oral,dgl,1,334,14.4190,0.9110

epoch:336/50, training loss:0.7602145075798035
Train Acc 0.9131
 Acc 0.9068
oral,dgl,1,335,14.4611,0.9068

epoch:337/50, training loss:0.7597948312759399
Train Acc 0.9090
 Acc 0.9114
new best val f1: 0.9114187881494715
oral,dgl,1,336,14.5032,0.9114

epoch:338/50, training loss:0.7592771053314209
Train Acc 0.9135
 Acc 0.9066
oral,dgl,1,337,14.5452,0.9066

epoch:339/50, training loss:0.7586954236030579
Train Acc 0.9089
 Acc 0.9115
new best val f1: 0.9115081137412535
oral,dgl,1,338,14.5872,0.9115

epoch:340/50, training loss:0.7579846978187561
Train Acc 0.9135
 Acc 0.9075
oral,dgl,1,339,14.6293,0.9075

epoch:341/50, training loss:0.7572351098060608
Train Acc 0.9098
 Acc 0.9109
oral,dgl,1,340,14.6715,0.9109

epoch:342/50, training loss:0.7564505934715271
Train Acc 0.9130
 Acc 0.9090
oral,dgl,1,341,14.7134,0.9090

epoch:343/50, training loss:0.7557321786880493
Train Acc 0.9112
 Acc 0.9100
oral,dgl,1,342,14.7555,0.9100

epoch:344/50, training loss:0.7550910115242004
Train Acc 0.9120
 Acc 0.9101
oral,dgl,1,343,14.7975,0.9101

epoch:345/50, training loss:0.7545105218887329
Train Acc 0.9121
 Acc 0.9089
oral,dgl,1,344,14.8396,0.9089

epoch:346/50, training loss:0.7539777755737305
Train Acc 0.9110
 Acc 0.9110
oral,dgl,1,345,14.8816,0.9110

epoch:347/50, training loss:0.753507673740387
Train Acc 0.9130
 Acc 0.9080
oral,dgl,1,346,14.9238,0.9080

epoch:348/50, training loss:0.7530952095985413
Train Acc 0.9100
 Acc 0.9116
new best val f1: 0.9116205978197939
oral,dgl,1,347,14.9659,0.9116

epoch:349/50, training loss:0.7526488304138184
Train Acc 0.9137
 Acc 0.9073
oral,dgl,1,348,15.0079,0.9073

epoch:350/50, training loss:0.752172589302063
Train Acc 0.9094
 Acc 0.9120
new best val f1: 0.9119712834764196
oral,dgl,1,349,15.0501,0.9120

epoch:351/50, training loss:0.7515437602996826
Train Acc 0.9139
 Acc 0.9079
oral,dgl,1,350,15.0923,0.9079

epoch:352/50, training loss:0.7508416175842285
Train Acc 0.9099
 Acc 0.9114
oral,dgl,1,351,15.1343,0.9114

epoch:353/50, training loss:0.7500956654548645
Train Acc 0.9135
 Acc 0.9088
oral,dgl,1,352,15.1763,0.9088

epoch:354/50, training loss:0.7494453191757202
Train Acc 0.9111
 Acc 0.9113
oral,dgl,1,353,15.2183,0.9113

epoch:355/50, training loss:0.7488574385643005
Train Acc 0.9134
 Acc 0.9090
oral,dgl,1,354,15.2606,0.9090

epoch:356/50, training loss:0.748311460018158
Train Acc 0.9112
 Acc 0.9113
oral,dgl,1,355,15.3027,0.9113

epoch:357/50, training loss:0.7477599382400513
Train Acc 0.9134
 Acc 0.9092
oral,dgl,1,356,15.3448,0.9092

epoch:358/50, training loss:0.7472239136695862
Train Acc 0.9114
 Acc 0.9113
oral,dgl,1,357,15.3868,0.9113

epoch:359/50, training loss:0.7466902732849121
Train Acc 0.9133
 Acc 0.9094
oral,dgl,1,358,15.4289,0.9094

epoch:360/50, training loss:0.7461628913879395
Train Acc 0.9115
 Acc 0.9113
oral,dgl,1,359,15.4710,0.9113

epoch:361/50, training loss:0.7456231713294983
Train Acc 0.9134
 Acc 0.9093
oral,dgl,1,360,15.5130,0.9093

epoch:362/50, training loss:0.7451114654541016
Train Acc 0.9115
 Acc 0.9116
oral,dgl,1,361,15.5551,0.9116

epoch:363/50, training loss:0.7445829510688782
Train Acc 0.9137
 Acc 0.9091
oral,dgl,1,362,15.5972,0.9091

epoch:364/50, training loss:0.7440840005874634
Train Acc 0.9114
 Acc 0.9118
oral,dgl,1,363,15.6394,0.9118

epoch:365/50, training loss:0.7435478568077087
Train Acc 0.9138
 Acc 0.9090
oral,dgl,1,364,15.6815,0.9090

epoch:366/50, training loss:0.7429839372634888
Train Acc 0.9114
 Acc 0.9117
oral,dgl,1,365,15.7236,0.9117

epoch:367/50, training loss:0.7423547506332397
Train Acc 0.9138
 Acc 0.9096
oral,dgl,1,366,15.7657,0.9096

epoch:368/50, training loss:0.741698682308197
Train Acc 0.9117
 Acc 0.9108
oral,dgl,1,367,15.8079,0.9108

epoch:369/50, training loss:0.7410582304000854
Train Acc 0.9131
 Acc 0.9109
oral,dgl,1,368,15.8498,0.9109

epoch:370/50, training loss:0.7405162453651428
Train Acc 0.9130
 Acc 0.9099
oral,dgl,1,369,15.8920,0.9099

epoch:371/50, training loss:0.7400413155555725
Train Acc 0.9121
 Acc 0.9115
oral,dgl,1,370,15.9341,0.9115

epoch:372/50, training loss:0.7395464181900024
Train Acc 0.9136
 Acc 0.9097
oral,dgl,1,371,15.9761,0.9097

epoch:373/50, training loss:0.7390366196632385
Train Acc 0.9119
 Acc 0.9116
oral,dgl,1,372,16.0182,0.9116

epoch:374/50, training loss:0.7385480403900146
Train Acc 0.9137
 Acc 0.9098
oral,dgl,1,373,16.0602,0.9098

epoch:375/50, training loss:0.7381061315536499
Train Acc 0.9120
 Acc 0.9116
oral,dgl,1,374,16.1022,0.9116

epoch:376/50, training loss:0.7377194762229919
Train Acc 0.9138
 Acc 0.9095
oral,dgl,1,375,16.1444,0.9095

epoch:377/50, training loss:0.7375067472457886
Train Acc 0.9118
 Acc 0.9122
new best val f1: 0.912189634922998
oral,dgl,1,376,16.1865,0.9122

epoch:378/50, training loss:0.7372598052024841
Train Acc 0.9145
 Acc 0.9085
oral,dgl,1,377,16.2285,0.9085

epoch:379/50, training loss:0.7371810078620911
Train Acc 0.9108
 Acc 0.9130
new best val f1: 0.9129637900517757
oral,dgl,1,378,16.2705,0.9130

epoch:380/50, training loss:0.7365990281105042
Train Acc 0.9151
 Acc 0.9082
oral,dgl,1,379,16.3127,0.9082

epoch:381/50, training loss:0.7359272241592407
Train Acc 0.9107
 Acc 0.9132
new best val f1: 0.9131722164326005
oral,dgl,1,380,16.3547,0.9132

epoch:382/50, training loss:0.7350715398788452
Train Acc 0.9153
 Acc 0.9085
oral,dgl,1,381,16.3968,0.9085

epoch:383/50, training loss:0.7345139980316162
Train Acc 0.9109
 Acc 0.9126
oral,dgl,1,382,16.4389,0.9126

epoch:384/50, training loss:0.734157145023346
Train Acc 0.9147
 Acc 0.9092
oral,dgl,1,383,16.4808,0.9092

epoch:385/50, training loss:0.7338462471961975
Train Acc 0.9115
 Acc 0.9119
oral,dgl,1,384,16.5230,0.9119

epoch:386/50, training loss:0.7331758737564087
Train Acc 0.9141
 Acc 0.9103
oral,dgl,1,385,16.5651,0.9103

epoch:387/50, training loss:0.7321744561195374
Train Acc 0.9125
 Acc 0.9115
oral,dgl,1,386,16.6072,0.9115

epoch:388/50, training loss:0.7313476800918579
Train Acc 0.9139
 Acc 0.9115
oral,dgl,1,387,16.6492,0.9115

epoch:389/50, training loss:0.7310754656791687
Train Acc 0.9137
 Acc 0.9098
oral,dgl,1,388,16.6912,0.9098

epoch:390/50, training loss:0.7310830950737
Train Acc 0.9121
 Acc 0.9124
oral,dgl,1,389,16.7334,0.9124

epoch:391/50, training loss:0.7307780981063843
Train Acc 0.9147
 Acc 0.9091
oral,dgl,1,390,16.7755,0.9091

epoch:392/50, training loss:0.7301012873649597
Train Acc 0.9116
 Acc 0.9131
oral,dgl,1,391,16.8174,0.9131

epoch:393/50, training loss:0.7294370532035828
Train Acc 0.9153
 Acc 0.9092
oral,dgl,1,392,16.8596,0.9092

epoch:394/50, training loss:0.7291640639305115
Train Acc 0.9118
 Acc 0.9129
oral,dgl,1,393,16.9016,0.9129

epoch:395/50, training loss:0.7288693189620972
Train Acc 0.9152
 Acc 0.9098
oral,dgl,1,394,16.9437,0.9098

epoch:396/50, training loss:0.7283561825752258
Train Acc 0.9123
 Acc 0.9123
oral,dgl,1,395,16.9857,0.9123

epoch:397/50, training loss:0.7274061441421509
Train Acc 0.9147
 Acc 0.9107
oral,dgl,1,396,17.0279,0.9107

epoch:398/50, training loss:0.726639449596405
Train Acc 0.9133
 Acc 0.9120
oral,dgl,1,397,17.0701,0.9120

epoch:399/50, training loss:0.7261996865272522
Train Acc 0.9143
 Acc 0.9110
oral,dgl,1,398,17.1123,0.9110

epoch:400/50, training loss:0.7259213924407959
Train Acc 0.9136
 Acc 0.9114
oral,dgl,1,399,17.1545,0.9114

epoch:401/50, training loss:0.7255550622940063
Train Acc 0.9138
 Acc 0.9118
oral,dgl,1,400,17.1967,0.9118

epoch:402/50, training loss:0.7249330878257751
Train Acc 0.9143
 Acc 0.9106
oral,dgl,1,401,17.2389,0.9106

epoch:403/50, training loss:0.7243571281433105
Train Acc 0.9134
 Acc 0.9125
oral,dgl,1,402,17.2810,0.9125

epoch:404/50, training loss:0.7239106893539429
Train Acc 0.9147
 Acc 0.9107
oral,dgl,1,403,17.3229,0.9107

epoch:405/50, training loss:0.7235611081123352
Train Acc 0.9132
 Acc 0.9126
oral,dgl,1,404,17.3651,0.9126

epoch:406/50, training loss:0.7231464385986328
Train Acc 0.9149
 Acc 0.9105
oral,dgl,1,405,17.4072,0.9105

epoch:407/50, training loss:0.7226813435554504
Train Acc 0.9129
 Acc 0.9129
oral,dgl,1,406,17.4492,0.9129

epoch:408/50, training loss:0.7222434282302856
Train Acc 0.9153
 Acc 0.9100
oral,dgl,1,407,17.4913,0.9100

epoch:409/50, training loss:0.7219164371490479
Train Acc 0.9127
 Acc 0.9134
new best val f1: 0.9134335764974443
oral,dgl,1,408,17.5335,0.9134

epoch:410/50, training loss:0.7216101884841919
Train Acc 0.9158
 Acc 0.9097
oral,dgl,1,409,17.5756,0.9097

epoch:411/50, training loss:0.7212575078010559
Train Acc 0.9122
 Acc 0.9136
new best val f1: 0.9136353861677667
oral,dgl,1,410,17.6176,0.9136

epoch:412/50, training loss:0.720789909362793
Train Acc 0.9159
 Acc 0.9097
oral,dgl,1,411,17.6598,0.9097

epoch:413/50, training loss:0.7202791571617126
Train Acc 0.9122
 Acc 0.9133
oral,dgl,1,412,17.7019,0.9133

epoch:414/50, training loss:0.7196940183639526
Train Acc 0.9158
 Acc 0.9107
oral,dgl,1,413,17.7439,0.9107

epoch:415/50, training loss:0.7191105484962463
Train Acc 0.9133
 Acc 0.9127
oral,dgl,1,414,17.7860,0.9127

epoch:416/50, training loss:0.7185416221618652
Train Acc 0.9152
 Acc 0.9115
oral,dgl,1,415,17.8281,0.9115

epoch:417/50, training loss:0.7180343866348267
Train Acc 0.9142
 Acc 0.9121
oral,dgl,1,416,17.8703,0.9121

epoch:418/50, training loss:0.7175805568695068
Train Acc 0.9148
 Acc 0.9114
oral,dgl,1,417,17.9123,0.9114

epoch:419/50, training loss:0.7171847224235535
Train Acc 0.9142
 Acc 0.9127
oral,dgl,1,418,17.9545,0.9127

epoch:420/50, training loss:0.7168614268302917
Train Acc 0.9154
 Acc 0.9106
oral,dgl,1,419,17.9967,0.9106

epoch:421/50, training loss:0.7166136503219604
Train Acc 0.9135
 Acc 0.9137
new best val f1: 0.913658544654525
oral,dgl,1,420,18.0388,0.9137

epoch:422/50, training loss:0.716423511505127
Train Acc 0.9161
 Acc 0.9098
oral,dgl,1,421,18.0808,0.9098

epoch:423/50, training loss:0.7163048982620239
Train Acc 0.9123
 Acc 0.9142
new best val f1: 0.914207731626222
oral,dgl,1,422,18.1229,0.9142

epoch:424/50, training loss:0.7160499095916748
Train Acc 0.9169
 Acc 0.9092
oral,dgl,1,423,18.1651,0.9092

epoch:425/50, training loss:0.7156811356544495
Train Acc 0.9118
 Acc 0.9142
oral,dgl,1,424,18.2072,0.9142

epoch:426/50, training loss:0.7151350378990173
Train Acc 0.9168
 Acc 0.9093
oral,dgl,1,425,18.2492,0.9093

epoch:427/50, training loss:0.7147007584571838
Train Acc 0.9120
 Acc 0.9142
oral,dgl,1,426,18.2913,0.9142

epoch:428/50, training loss:0.714370846748352
Train Acc 0.9166
 Acc 0.9091
oral,dgl,1,427,18.3334,0.9091

epoch:429/50, training loss:0.7141919732093811
Train Acc 0.9120
 Acc 0.9138
oral,dgl,1,428,18.3753,0.9138

epoch:430/50, training loss:0.7135953903198242
Train Acc 0.9163
 Acc 0.9105
oral,dgl,1,429,18.4175,0.9105

epoch:431/50, training loss:0.7127768993377686
Train Acc 0.9132
 Acc 0.9132
oral,dgl,1,430,18.4597,0.9132

epoch:432/50, training loss:0.7118756771087646
Train Acc 0.9157
 Acc 0.9123
oral,dgl,1,431,18.5017,0.9123

epoch:433/50, training loss:0.7113068699836731
Train Acc 0.9150
 Acc 0.9117
oral,dgl,1,432,18.5437,0.9117

epoch:434/50, training loss:0.7110871076583862
Train Acc 0.9145
 Acc 0.9134
oral,dgl,1,433,18.5858,0.9134

epoch:435/50, training loss:0.7110361456871033
Train Acc 0.9158
 Acc 0.9106
oral,dgl,1,434,18.6280,0.9106

epoch:436/50, training loss:0.7108764052391052
Train Acc 0.9132
 Acc 0.9141
oral,dgl,1,435,18.6700,0.9141

epoch:437/50, training loss:0.710392951965332
Train Acc 0.9165
 Acc 0.9103
oral,dgl,1,436,18.7121,0.9103

epoch:438/50, training loss:0.7099427580833435
Train Acc 0.9129
 Acc 0.9144
new best val f1: 0.9144326997833028
oral,dgl,1,437,18.7541,0.9144

epoch:439/50, training loss:0.7097072005271912
Train Acc 0.9169
 Acc 0.9100
oral,dgl,1,438,18.7962,0.9100

epoch:440/50, training loss:0.7097364664077759
Train Acc 0.9125
 Acc 0.9146
new best val f1: 0.9146378178088763
oral,dgl,1,439,18.8384,0.9146

epoch:441/50, training loss:0.7095155119895935
Train Acc 0.9171
 Acc 0.9101
oral,dgl,1,440,18.8804,0.9101

epoch:442/50, training loss:0.7091587781906128
Train Acc 0.9126
 Acc 0.9147
new best val f1: 0.9146808264271418
oral,dgl,1,441,18.9225,0.9147

epoch:443/50, training loss:0.7083315253257751
Train Acc 0.9172
 Acc 0.9105
oral,dgl,1,442,18.9646,0.9105

epoch:444/50, training loss:0.7076190710067749
Train Acc 0.9131
 Acc 0.9140
oral,dgl,1,443,19.0067,0.9140

epoch:445/50, training loss:0.7070220708847046
Train Acc 0.9166
 Acc 0.9115
oral,dgl,1,444,19.0487,0.9115

epoch:446/50, training loss:0.706524670124054
Train Acc 0.9142
 Acc 0.9132
oral,dgl,1,445,19.0908,0.9132

epoch:447/50, training loss:0.7060588598251343
Train Acc 0.9156
 Acc 0.9132
oral,dgl,1,446,19.1329,0.9132

epoch:448/50, training loss:0.705601692199707
Train Acc 0.9157
 Acc 0.9116
oral,dgl,1,447,19.1750,0.9116

epoch:449/50, training loss:0.705293595790863
Train Acc 0.9143
 Acc 0.9144
oral,dgl,1,448,19.2170,0.9144

epoch:450/50, training loss:0.7051796913146973
Train Acc 0.9168
 Acc 0.9099
oral,dgl,1,449,19.2591,0.9099

epoch:451/50, training loss:0.705325186252594
Train Acc 0.9126
 Acc 0.9152
new best val f1: 0.915200238201578
oral,dgl,1,450,19.3013,0.9152

epoch:452/50, training loss:0.7055096626281738
Train Acc 0.9176
 Acc 0.9083
oral,dgl,1,451,19.3434,0.9083

epoch:453/50, training loss:0.7057216167449951
Train Acc 0.9111
 Acc 0.9155
new best val f1: 0.9154582899111706
oral,dgl,1,452,19.3855,0.9155

epoch:454/50, training loss:0.7051652073860168
Train Acc 0.9181
 Acc 0.9091
oral,dgl,1,453,19.4274,0.9091

epoch:455/50, training loss:0.7043909430503845
Train Acc 0.9119
 Acc 0.9150
oral,dgl,1,454,19.4696,0.9150

epoch:456/50, training loss:0.7034717202186584
Train Acc 0.9175
 Acc 0.9118
oral,dgl,1,455,19.5117,0.9118

epoch:457/50, training loss:0.7029261589050293
Train Acc 0.9143
 Acc 0.9131
oral,dgl,1,456,19.5539,0.9131

epoch:458/50, training loss:0.7024001479148865
Train Acc 0.9155
 Acc 0.9139
oral,dgl,1,457,19.5960,0.9139

epoch:459/50, training loss:0.7019267678260803
Train Acc 0.9164
 Acc 0.9115
oral,dgl,1,458,19.6380,0.9115

epoch:460/50, training loss:0.701462984085083
Train Acc 0.9141
 Acc 0.9148
oral,dgl,1,459,19.6802,0.9148

epoch:461/50, training loss:0.7011996507644653
Train Acc 0.9172
 Acc 0.9117
oral,dgl,1,460,19.7223,0.9117

epoch:462/50, training loss:0.7010884881019592
Train Acc 0.9144
 Acc 0.9137
oral,dgl,1,461,19.7642,0.9137

epoch:463/50, training loss:0.70057612657547
Train Acc 0.9162
 Acc 0.9133
oral,dgl,1,462,19.8063,0.9133

epoch:464/50, training loss:0.6999485492706299
Train Acc 0.9158
 Acc 0.9125
oral,dgl,1,463,19.8485,0.9125

epoch:465/50, training loss:0.6994263529777527
Train Acc 0.9152
 Acc 0.9142
oral,dgl,1,464,19.8907,0.9142

epoch:466/50, training loss:0.6991623044013977
Train Acc 0.9167
 Acc 0.9121
oral,dgl,1,465,19.9328,0.9121

epoch:467/50, training loss:0.6991013884544373
Train Acc 0.9147
 Acc 0.9143
oral,dgl,1,466,19.9748,0.9143

epoch:468/50, training loss:0.6989079117774963
Train Acc 0.9168
 Acc 0.9124
oral,dgl,1,467,20.0169,0.9124

epoch:469/50, training loss:0.6985480785369873
Train Acc 0.9149
 Acc 0.9141
oral,dgl,1,468,20.0590,0.9141

epoch:470/50, training loss:0.6979182958602905
Train Acc 0.9165
 Acc 0.9129
oral,dgl,1,469,20.1011,0.9129

epoch:471/50, training loss:0.6973403096199036
Train Acc 0.9154
 Acc 0.9136
oral,dgl,1,470,20.1432,0.9136

epoch:472/50, training loss:0.6969673037528992
Train Acc 0.9160
 Acc 0.9135
oral,dgl,1,471,20.1853,0.9135

epoch:473/50, training loss:0.6967920660972595
Train Acc 0.9159
 Acc 0.9133
oral,dgl,1,472,20.2273,0.9133

epoch:474/50, training loss:0.6966460347175598
Train Acc 0.9158
 Acc 0.9138
oral,dgl,1,473,20.2694,0.9138

epoch:475/50, training loss:0.6962955594062805
Train Acc 0.9164
 Acc 0.9127
oral,dgl,1,474,20.3115,0.9127

epoch:476/50, training loss:0.6958269476890564
Train Acc 0.9154
 Acc 0.9142
oral,dgl,1,475,20.3536,0.9142

epoch:477/50, training loss:0.6953715682029724
Train Acc 0.9167
 Acc 0.9127
oral,dgl,1,476,20.3957,0.9127

epoch:478/50, training loss:0.6950636506080627
Train Acc 0.9153
 Acc 0.9143
oral,dgl,1,477,20.4379,0.9143

epoch:479/50, training loss:0.6947700381278992
Train Acc 0.9166
 Acc 0.9132
oral,dgl,1,478,20.4799,0.9132

epoch:480/50, training loss:0.6944064497947693
Train Acc 0.9157
 Acc 0.9136
oral,dgl,1,479,20.5220,0.9136

epoch:481/50, training loss:0.6940146684646606
Train Acc 0.9160
 Acc 0.9142
oral,dgl,1,480,20.5664,0.9142

epoch:482/50, training loss:0.6937122941017151
Train Acc 0.9167
 Acc 0.9123
oral,dgl,1,481,20.6087,0.9123

epoch:483/50, training loss:0.693538248538971
Train Acc 0.9150
 Acc 0.9151
oral,dgl,1,482,20.6508,0.9151

epoch:484/50, training loss:0.6934025883674622
Train Acc 0.9175
 Acc 0.9116
oral,dgl,1,483,20.6928,0.9116

epoch:485/50, training loss:0.6933027505874634
Train Acc 0.9141
 Acc 0.9157
new best val f1: 0.9156832580682513
oral,dgl,1,484,20.7349,0.9157

epoch:486/50, training loss:0.6931514143943787
Train Acc 0.9181
 Acc 0.9110
oral,dgl,1,485,20.7770,0.9110

epoch:487/50, training loss:0.6929816603660583
Train Acc 0.9136
 Acc 0.9158
new best val f1: 0.9158155922782989
oral,dgl,1,486,20.8190,0.9158

epoch:488/50, training loss:0.6926725506782532
Train Acc 0.9183
 Acc 0.9114
oral,dgl,1,487,20.8611,0.9114

epoch:489/50, training loss:0.6922515034675598
Train Acc 0.9140
 Acc 0.9154
oral,dgl,1,488,20.9033,0.9154

epoch:490/50, training loss:0.6916042566299438
Train Acc 0.9178
 Acc 0.9130
oral,dgl,1,489,20.9454,0.9130

epoch:491/50, training loss:0.6909766793251038
Train Acc 0.9155
 Acc 0.9143
oral,dgl,1,490,20.9875,0.9143

epoch:492/50, training loss:0.6904891133308411
Train Acc 0.9165
 Acc 0.9143
oral,dgl,1,491,21.0297,0.9143

epoch:493/50, training loss:0.6901921033859253
Train Acc 0.9166
 Acc 0.9131
oral,dgl,1,492,21.0719,0.9131

epoch:494/50, training loss:0.6900441646575928
Train Acc 0.9156
 Acc 0.9150
oral,dgl,1,493,21.1140,0.9150

epoch:495/50, training loss:0.6898947954177856
Train Acc 0.9175
 Acc 0.9126
oral,dgl,1,494,21.1560,0.9126

epoch:496/50, training loss:0.6897172331809998
Train Acc 0.9152
 Acc 0.9152
oral,dgl,1,495,21.1982,0.9152

epoch:497/50, training loss:0.689418613910675
Train Acc 0.9177
 Acc 0.9128
oral,dgl,1,496,21.2403,0.9128

epoch:498/50, training loss:0.6891516447067261
Train Acc 0.9153
 Acc 0.9153
oral,dgl,1,497,21.2824,0.9153

epoch:499/50, training loss:0.6888394355773926
Train Acc 0.9178
 Acc 0.9128
oral,dgl,1,498,21.3244,0.9128

epoch:500/50, training loss:0.6885013580322266
Train Acc 0.9153
 Acc 0.9152
oral,dgl,1,499,21.3665,0.9152

epoch:501/50, training loss:0.6880176663398743
Train Acc 0.9176
 Acc 0.9134
oral,dgl,1,500,21.4087,0.9134

epoch:502/50, training loss:0.6875650882720947
Train Acc 0.9158
 Acc 0.9149
oral,dgl,1,501,21.4507,0.9149

epoch:503/50, training loss:0.6872328519821167
Train Acc 0.9173
 Acc 0.9138
oral,dgl,1,502,21.4928,0.9138

epoch:504/50, training loss:0.6870319247245789
Train Acc 0.9163
 Acc 0.9145
oral,dgl,1,503,21.5348,0.9145

epoch:505/50, training loss:0.6868548393249512
Train Acc 0.9170
 Acc 0.9141
oral,dgl,1,504,21.5770,0.9141

epoch:506/50, training loss:0.6866217851638794
Train Acc 0.9166
 Acc 0.9143
oral,dgl,1,505,21.6191,0.9143

epoch:507/50, training loss:0.6863493323326111
Train Acc 0.9166
 Acc 0.9145
oral,dgl,1,506,21.6613,0.9145

epoch:508/50, training loss:0.6859388947486877
Train Acc 0.9170
 Acc 0.9141
oral,dgl,1,507,21.7034,0.9141

epoch:509/50, training loss:0.6855377554893494
Train Acc 0.9164
 Acc 0.9150
oral,dgl,1,508,21.7454,0.9150

epoch:510/50, training loss:0.685183584690094
Train Acc 0.9174
 Acc 0.9138
oral,dgl,1,509,21.7875,0.9138

epoch:511/50, training loss:0.6849067211151123
Train Acc 0.9161
 Acc 0.9153
oral,dgl,1,510,21.8296,0.9153

epoch:512/50, training loss:0.6846665143966675
Train Acc 0.9177
 Acc 0.9137
oral,dgl,1,511,21.8719,0.9137

epoch:513/50, training loss:0.6844084858894348
Train Acc 0.9162
 Acc 0.9152
oral,dgl,1,512,21.9140,0.9152

epoch:514/50, training loss:0.6841158866882324
Train Acc 0.9176
 Acc 0.9140
oral,dgl,1,513,21.9559,0.9140

epoch:515/50, training loss:0.6837679743766785
Train Acc 0.9164
 Acc 0.9150
oral,dgl,1,514,21.9979,0.9150

epoch:516/50, training loss:0.6834187507629395
Train Acc 0.9173
 Acc 0.9148
oral,dgl,1,515,22.0401,0.9148

epoch:517/50, training loss:0.6831204891204834
Train Acc 0.9170
 Acc 0.9144
oral,dgl,1,516,22.0823,0.9144

epoch:518/50, training loss:0.6828767657279968
Train Acc 0.9166
 Acc 0.9153
oral,dgl,1,517,22.1244,0.9153

epoch:519/50, training loss:0.6826587319374084
Train Acc 0.9176
 Acc 0.9137
oral,dgl,1,518,22.1665,0.9137

epoch:520/50, training loss:0.6824918985366821
Train Acc 0.9162
 Acc 0.9159
new best val f1: 0.9159446181330951
oral,dgl,1,519,22.2085,0.9159

epoch:521/50, training loss:0.6824347376823425
Train Acc 0.9184
 Acc 0.9127
oral,dgl,1,520,22.2507,0.9127

epoch:522/50, training loss:0.6825408935546875
Train Acc 0.9150
 Acc 0.9168
new best val f1: 0.9167717069458918
oral,dgl,1,521,22.2929,0.9168

epoch:523/50, training loss:0.6827121376991272
Train Acc 0.9193
 Acc 0.9108
oral,dgl,1,522,22.3349,0.9108

epoch:524/50, training loss:0.6831356287002563
Train Acc 0.9133
 Acc 0.9172
new best val f1: 0.9171786346417878
oral,dgl,1,523,22.3769,0.9172

epoch:525/50, training loss:0.6834227442741394
Train Acc 0.9201
 Acc 0.9101
oral,dgl,1,524,22.4191,0.9101

epoch:526/50, training loss:0.683940052986145
Train Acc 0.9126
 Acc 0.9174
new best val f1: 0.9173506691148495
oral,dgl,1,525,22.4612,0.9174

epoch:527/50, training loss:0.6840311288833618
Train Acc 0.9204
 Acc 0.9110
oral,dgl,1,526,22.5033,0.9110

epoch:528/50, training loss:0.6844161152839661
Train Acc 0.9135
 Acc 0.9163
oral,dgl,1,527,22.5453,0.9163

epoch:529/50, training loss:0.6825203895568848
Train Acc 0.9192
 Acc 0.9148
oral,dgl,1,528,22.5874,0.9148

epoch:530/50, training loss:0.6811323761940002
Train Acc 0.9170
 Acc 0.9136
oral,dgl,1,529,22.6295,0.9136

epoch:531/50, training loss:0.6800215840339661
Train Acc 0.9161
 Acc 0.9170
oral,dgl,1,530,22.6717,0.9170

epoch:532/50, training loss:0.6802871823310852
Train Acc 0.9197
 Acc 0.9118
oral,dgl,1,531,22.7137,0.9118

epoch:533/50, training loss:0.6813709735870361
Train Acc 0.9141
 Acc 0.9171
oral,dgl,1,532,22.7558,0.9171

epoch:534/50, training loss:0.6813574433326721
Train Acc 0.9201
 Acc 0.9128
oral,dgl,1,533,22.7979,0.9128

epoch:535/50, training loss:0.680591344833374
Train Acc 0.9151
 Acc 0.9159
oral,dgl,1,534,22.8400,0.9159

epoch:536/50, training loss:0.6785817742347717
Train Acc 0.9185
 Acc 0.9157
oral,dgl,1,535,22.8820,0.9157

epoch:537/50, training loss:0.6778875589370728
Train Acc 0.9182
 Acc 0.9136
oral,dgl,1,536,22.9241,0.9136

epoch:538/50, training loss:0.6785956025123596
Train Acc 0.9158
 Acc 0.9170
oral,dgl,1,537,22.9662,0.9170

epoch:539/50, training loss:0.6791262626647949
Train Acc 0.9197
 Acc 0.9128
oral,dgl,1,538,23.0083,0.9128

epoch:540/50, training loss:0.6789098978042603
Train Acc 0.9153
 Acc 0.9170
oral,dgl,1,539,23.0504,0.9170

epoch:541/50, training loss:0.6775861978530884
Train Acc 0.9197
 Acc 0.9143
oral,dgl,1,540,23.0923,0.9143

epoch:542/50, training loss:0.6769338250160217
Train Acc 0.9167
 Acc 0.9154
oral,dgl,1,541,23.1345,0.9154

epoch:543/50, training loss:0.6771318912506104
Train Acc 0.9176
 Acc 0.9162
oral,dgl,1,542,23.1766,0.9162

epoch:544/50, training loss:0.6770811080932617
Train Acc 0.9190
 Acc 0.9136
oral,dgl,1,543,23.2187,0.9136

epoch:545/50, training loss:0.6768208146095276
Train Acc 0.9162
 Acc 0.9170
oral,dgl,1,544,23.2609,0.9170

epoch:546/50, training loss:0.6763163805007935
Train Acc 0.9198
 Acc 0.9136
oral,dgl,1,545,23.3029,0.9136

epoch:547/50, training loss:0.6761186718940735
Train Acc 0.9162
 Acc 0.9162
oral,dgl,1,546,23.3450,0.9162

epoch:548/50, training loss:0.675918698310852
Train Acc 0.9184
 Acc 0.9155
oral,dgl,1,547,23.3871,0.9155

epoch:549/50, training loss:0.6754426956176758
Train Acc 0.9181
 Acc 0.9149
oral,dgl,1,548,23.4292,0.9149

epoch:550/50, training loss:0.6749919652938843
Train Acc 0.9170
 Acc 0.9168
oral,dgl,1,549,23.4713,0.9168

epoch:551/50, training loss:0.6747706532478333
Train Acc 0.9194
 Acc 0.9140
oral,dgl,1,550,23.5135,0.9140

epoch:552/50, training loss:0.6748470067977905
Train Acc 0.9165
 Acc 0.9166
oral,dgl,1,551,23.5556,0.9166

epoch:553/50, training loss:0.6746645569801331
Train Acc 0.9190
 Acc 0.9147
oral,dgl,1,552,23.5977,0.9147

epoch:554/50, training loss:0.6741676330566406
Train Acc 0.9172
 Acc 0.9163
oral,dgl,1,553,23.6399,0.9163

epoch:555/50, training loss:0.6735915541648865
Train Acc 0.9187
 Acc 0.9158
oral,dgl,1,554,23.6820,0.9158

epoch:556/50, training loss:0.6732951998710632
Train Acc 0.9181
 Acc 0.9153
oral,dgl,1,555,23.7240,0.9153

epoch:557/50, training loss:0.6732452511787415
Train Acc 0.9178
 Acc 0.9166
oral,dgl,1,556,23.7662,0.9166

epoch:558/50, training loss:0.6731740236282349
Train Acc 0.9190
 Acc 0.9147
oral,dgl,1,557,23.8083,0.9147

epoch:559/50, training loss:0.6729463934898376
Train Acc 0.9171
 Acc 0.9169
oral,dgl,1,558,23.8504,0.9169

epoch:560/50, training loss:0.6725324988365173
Train Acc 0.9193
 Acc 0.9155
oral,dgl,1,559,23.8925,0.9155

epoch:561/50, training loss:0.6722093224525452
Train Acc 0.9178
 Acc 0.9157
oral,dgl,1,560,23.9347,0.9157

epoch:562/50, training loss:0.6721150279045105
Train Acc 0.9181
 Acc 0.9166
oral,dgl,1,561,23.9768,0.9166

epoch:563/50, training loss:0.6720828413963318
Train Acc 0.9190
 Acc 0.9149
oral,dgl,1,562,24.0189,0.9149

epoch:564/50, training loss:0.6718698740005493
Train Acc 0.9174
 Acc 0.9171
oral,dgl,1,563,24.0609,0.9171

epoch:565/50, training loss:0.6715446710586548
Train Acc 0.9197
 Acc 0.9148
oral,dgl,1,564,24.1030,0.9148

epoch:566/50, training loss:0.6713463664054871
Train Acc 0.9171
 Acc 0.9169
oral,dgl,1,565,24.1452,0.9169

epoch:567/50, training loss:0.6711778044700623
Train Acc 0.9196
 Acc 0.9157
oral,dgl,1,566,24.1873,0.9157

epoch:568/50, training loss:0.6709680557250977
Train Acc 0.9179
 Acc 0.9165
oral,dgl,1,567,24.2293,0.9165

epoch:569/50, training loss:0.6705172657966614
Train Acc 0.9191
 Acc 0.9160
oral,dgl,1,568,24.2714,0.9160

epoch:570/50, training loss:0.6701317429542542
Train Acc 0.9184
 Acc 0.9162
oral,dgl,1,569,24.3135,0.9162

epoch:571/50, training loss:0.6698863506317139
Train Acc 0.9185
 Acc 0.9165
oral,dgl,1,570,24.3557,0.9165

epoch:572/50, training loss:0.6697462201118469
Train Acc 0.9191
 Acc 0.9160
oral,dgl,1,571,24.3976,0.9160

epoch:573/50, training loss:0.6696031093597412
Train Acc 0.9184
 Acc 0.9166
oral,dgl,1,572,24.4399,0.9166

epoch:574/50, training loss:0.6693615317344666
Train Acc 0.9193
 Acc 0.9160
oral,dgl,1,573,24.4820,0.9160

epoch:575/50, training loss:0.6690531373023987
Train Acc 0.9184
 Acc 0.9168
oral,dgl,1,574,24.5242,0.9168

epoch:576/50, training loss:0.6688057780265808
Train Acc 0.9191
 Acc 0.9160
oral,dgl,1,575,24.5662,0.9160

epoch:577/50, training loss:0.6686468124389648
Train Acc 0.9185
 Acc 0.9168
oral,dgl,1,576,24.6083,0.9168

epoch:578/50, training loss:0.6684805154800415
Train Acc 0.9190
 Acc 0.9163
oral,dgl,1,577,24.6504,0.9163

epoch:579/50, training loss:0.6682458519935608
Train Acc 0.9189
 Acc 0.9160
oral,dgl,1,578,24.6923,0.9160

epoch:580/50, training loss:0.668005108833313
Train Acc 0.9184
 Acc 0.9173
oral,dgl,1,579,24.7344,0.9173

epoch:581/50, training loss:0.6678420901298523
Train Acc 0.9198
 Acc 0.9152
oral,dgl,1,580,24.7767,0.9152

epoch:582/50, training loss:0.667789876461029
Train Acc 0.9177
 Acc 0.9175
new best val f1: 0.9175458620746696
oral,dgl,1,581,24.8187,0.9175

epoch:583/50, training loss:0.667748212814331
Train Acc 0.9201
 Acc 0.9152
oral,dgl,1,582,24.8607,0.9152

epoch:584/50, training loss:0.6675150990486145
Train Acc 0.9178
 Acc 0.9172
oral,dgl,1,583,24.9029,0.9172

epoch:585/50, training loss:0.6670493483543396
Train Acc 0.9198
 Acc 0.9164
oral,dgl,1,584,24.9450,0.9164

epoch:586/50, training loss:0.6666691303253174
Train Acc 0.9189
 Acc 0.9164
oral,dgl,1,585,24.9871,0.9164

epoch:587/50, training loss:0.666476845741272
Train Acc 0.9190
 Acc 0.9173
oral,dgl,1,586,25.0292,0.9173

epoch:588/50, training loss:0.6664177775382996
Train Acc 0.9199
 Acc 0.9157
oral,dgl,1,587,25.0713,0.9157

epoch:589/50, training loss:0.6663236618041992
Train Acc 0.9184
 Acc 0.9173
oral,dgl,1,588,25.1134,0.9173

epoch:590/50, training loss:0.6660654544830322
Train Acc 0.9200
 Acc 0.9159
oral,dgl,1,589,25.1555,0.9159

epoch:591/50, training loss:0.6657381057739258
Train Acc 0.9185
 Acc 0.9175
oral,dgl,1,590,25.1977,0.9175

epoch:592/50, training loss:0.665523111820221
Train Acc 0.9200
 Acc 0.9162
oral,dgl,1,591,25.2398,0.9162

epoch:593/50, training loss:0.6654351353645325
Train Acc 0.9185
 Acc 0.9172
oral,dgl,1,592,25.2817,0.9172

epoch:594/50, training loss:0.665320098400116
Train Acc 0.9198
 Acc 0.9163
oral,dgl,1,593,25.3236,0.9163

epoch:595/50, training loss:0.6651249527931213
Train Acc 0.9187
 Acc 0.9174
oral,dgl,1,594,25.3658,0.9174

epoch:596/50, training loss:0.6647428274154663
Train Acc 0.9200
 Acc 0.9165
oral,dgl,1,595,25.4079,0.9165

epoch:597/50, training loss:0.6644219756126404
Train Acc 0.9191
 Acc 0.9171
oral,dgl,1,596,25.4500,0.9171

epoch:598/50, training loss:0.664227306842804
Train Acc 0.9198
 Acc 0.9168
oral,dgl,1,597,25.4920,0.9168

epoch:599/50, training loss:0.6641136407852173
Train Acc 0.9194
 Acc 0.9169
oral,dgl,1,598,25.5342,0.9169

epoch:600/50, training loss:0.6640413403511047
Train Acc 0.9194
 Acc 0.9172
oral,dgl,1,599,25.5762,0.9172

epoch:601/50, training loss:0.6638917326927185
Train Acc 0.9199
 Acc 0.9164
oral,dgl,1,600,25.6183,0.9164

epoch:602/50, training loss:0.6637025475502014
Train Acc 0.9189
 Acc 0.9180
new best val f1: 0.9179726399020727
oral,dgl,1,601,25.6604,0.9180

epoch:603/50, training loss:0.6634939312934875
Train Acc 0.9207
 Acc 0.9156
oral,dgl,1,602,25.7025,0.9156

epoch:604/50, training loss:0.663413941860199
Train Acc 0.9181
 Acc 0.9185
new best val f1: 0.9185052850975137
oral,dgl,1,603,25.7446,0.9185

epoch:605/50, training loss:0.6634455919265747
Train Acc 0.9211
 Acc 0.9148
oral,dgl,1,604,25.7866,0.9148

epoch:606/50, training loss:0.6635324358940125
Train Acc 0.9172
 Acc 0.9185
new best val f1: 0.9185383686500257
oral,dgl,1,605,25.8287,0.9185

epoch:607/50, training loss:0.6633471846580505
Train Acc 0.9212
 Acc 0.9153
oral,dgl,1,606,25.8709,0.9153

epoch:608/50, training loss:0.6629028916358948
Train Acc 0.9177
 Acc 0.9182
oral,dgl,1,607,25.9130,0.9182

epoch:609/50, training loss:0.6623192429542542
Train Acc 0.9209
 Acc 0.9167
oral,dgl,1,608,25.9549,0.9167

epoch:610/50, training loss:0.6619196534156799
Train Acc 0.9191
 Acc 0.9173
oral,dgl,1,609,25.9970,0.9173

epoch:611/50, training loss:0.6616920828819275
Train Acc 0.9200
 Acc 0.9177
oral,dgl,1,610,26.0390,0.9177

epoch:612/50, training loss:0.661573588848114
Train Acc 0.9205
 Acc 0.9163
oral,dgl,1,611,26.0811,0.9163

epoch:613/50, training loss:0.6614705920219421
Train Acc 0.9190
 Acc 0.9182
oral,dgl,1,612,26.1232,0.9182

epoch:614/50, training loss:0.6612696647644043
Train Acc 0.9209
 Acc 0.9166
oral,dgl,1,613,26.1654,0.9166

epoch:615/50, training loss:0.6609951257705688
Train Acc 0.9191
 Acc 0.9178
oral,dgl,1,614,26.2075,0.9178

epoch:616/50, training loss:0.6607123613357544
Train Acc 0.9207
 Acc 0.9173
oral,dgl,1,615,26.2496,0.9173

epoch:617/50, training loss:0.6605006456375122
Train Acc 0.9199
 Acc 0.9175
oral,dgl,1,616,26.2917,0.9175

epoch:618/50, training loss:0.6603392958641052
Train Acc 0.9201
 Acc 0.9176
oral,dgl,1,617,26.3338,0.9176

epoch:619/50, training loss:0.660173773765564
Train Acc 0.9204
 Acc 0.9172
oral,dgl,1,618,26.3758,0.9172

epoch:620/50, training loss:0.6599816083908081
Train Acc 0.9199
 Acc 0.9178
oral,dgl,1,619,26.4179,0.9178

epoch:621/50, training loss:0.6597721576690674
Train Acc 0.9206
 Acc 0.9169
oral,dgl,1,620,26.4600,0.9169

epoch:622/50, training loss:0.6596275568008423
Train Acc 0.9195
 Acc 0.9185
oral,dgl,1,621,26.5020,0.9185

epoch:623/50, training loss:0.6596075892448425
Train Acc 0.9212
 Acc 0.9161
oral,dgl,1,622,26.5442,0.9161

epoch:624/50, training loss:0.6597161293029785
Train Acc 0.9186
 Acc 0.9189
new best val f1: 0.9189055960829073
oral,dgl,1,623,26.5863,0.9189

epoch:625/50, training loss:0.6598690152168274
Train Acc 0.9218
 Acc 0.9154
oral,dgl,1,624,26.6285,0.9154

epoch:626/50, training loss:0.660072922706604
Train Acc 0.9179
 Acc 0.9193
new best val f1: 0.9193323739103105
oral,dgl,1,625,26.6706,0.9193

epoch:627/50, training loss:0.659966766834259
Train Acc 0.9221
 Acc 0.9152
oral,dgl,1,626,26.7127,0.9152

epoch:628/50, training loss:0.6596981287002563
Train Acc 0.9178
 Acc 0.9192
oral,dgl,1,627,26.7547,0.9192

epoch:629/50, training loss:0.6591564416885376
Train Acc 0.9220
 Acc 0.9162
oral,dgl,1,628,26.7969,0.9162

epoch:630/50, training loss:0.6585895419120789
Train Acc 0.9187
 Acc 0.9185
oral,dgl,1,629,26.8390,0.9185

epoch:631/50, training loss:0.6580812931060791
Train Acc 0.9212
 Acc 0.9176
oral,dgl,1,630,26.8811,0.9176

epoch:632/50, training loss:0.6578159928321838
Train Acc 0.9204
 Acc 0.9172
oral,dgl,1,631,26.9231,0.9172

epoch:633/50, training loss:0.6578261852264404
Train Acc 0.9200
 Acc 0.9190
oral,dgl,1,632,26.9653,0.9190

epoch:634/50, training loss:0.6579745411872864
Train Acc 0.9218
 Acc 0.9163
oral,dgl,1,633,27.0073,0.9163

epoch:635/50, training loss:0.6581445336341858
Train Acc 0.9188
 Acc 0.9193
oral,dgl,1,634,27.0495,0.9193

epoch:636/50, training loss:0.6579002141952515
Train Acc 0.9221
 Acc 0.9164
oral,dgl,1,635,27.0914,0.9164

epoch:637/50, training loss:0.6575422286987305
Train Acc 0.9189
 Acc 0.9191
oral,dgl,1,636,27.1335,0.9191

epoch:638/50, training loss:0.657113790512085
Train Acc 0.9220
 Acc 0.9169
oral,dgl,1,637,27.1756,0.9169

epoch:639/50, training loss:0.6567800045013428
Train Acc 0.9194
 Acc 0.9190
oral,dgl,1,638,27.2177,0.9190

epoch:640/50, training loss:0.6565213799476624
Train Acc 0.9217
 Acc 0.9169
oral,dgl,1,639,27.2597,0.9169

epoch:641/50, training loss:0.656358540058136
Train Acc 0.9195
 Acc 0.9188
oral,dgl,1,640,27.3019,0.9188

epoch:642/50, training loss:0.6561886668205261
Train Acc 0.9217
 Acc 0.9172
oral,dgl,1,641,27.3440,0.9172

epoch:643/50, training loss:0.6560040712356567
Train Acc 0.9199
 Acc 0.9186
oral,dgl,1,642,27.3861,0.9186

epoch:644/50, training loss:0.6557604074478149
Train Acc 0.9215
 Acc 0.9177
oral,dgl,1,643,27.4283,0.9177

epoch:645/50, training loss:0.6554880142211914
Train Acc 0.9205
 Acc 0.9181
oral,dgl,1,644,27.4704,0.9181

epoch:646/50, training loss:0.6552292108535767
Train Acc 0.9212
 Acc 0.9183
oral,dgl,1,645,27.5124,0.9183

epoch:647/50, training loss:0.6550136208534241
Train Acc 0.9212
 Acc 0.9180
oral,dgl,1,646,27.5545,0.9180

epoch:648/50, training loss:0.6548989415168762
Train Acc 0.9206
 Acc 0.9186
oral,dgl,1,647,27.5968,0.9186

epoch:649/50, training loss:0.6548782587051392
Train Acc 0.9215
 Acc 0.9177
oral,dgl,1,648,27.6387,0.9177

epoch:650/50, training loss:0.6548317074775696
Train Acc 0.9204
 Acc 0.9186
oral,dgl,1,649,27.6808,0.9186

epoch:651/50, training loss:0.6547147035598755
Train Acc 0.9215
 Acc 0.9180
oral,dgl,1,650,27.7228,0.9180

epoch:652/50, training loss:0.6544961333274841
Train Acc 0.9208
 Acc 0.9181
oral,dgl,1,651,27.7648,0.9181

epoch:653/50, training loss:0.654277503490448
Train Acc 0.9210
 Acc 0.9188
oral,dgl,1,652,27.8069,0.9188

epoch:654/50, training loss:0.6541035175323486
Train Acc 0.9219
 Acc 0.9176
oral,dgl,1,653,27.8491,0.9176

epoch:655/50, training loss:0.653971791267395
Train Acc 0.9203
 Acc 0.9192
oral,dgl,1,654,27.8912,0.9192

epoch:656/50, training loss:0.6537684798240662
Train Acc 0.9221
 Acc 0.9177
oral,dgl,1,655,27.9332,0.9177

epoch:657/50, training loss:0.6535601019859314
Train Acc 0.9205
 Acc 0.9193
oral,dgl,1,656,27.9752,0.9193

epoch:658/50, training loss:0.653342604637146
Train Acc 0.9221
 Acc 0.9176
oral,dgl,1,657,28.0174,0.9176

epoch:659/50, training loss:0.6531428694725037
Train Acc 0.9205
 Acc 0.9192
oral,dgl,1,658,28.0595,0.9192

epoch:660/50, training loss:0.6529258489608765
Train Acc 0.9221
 Acc 0.9180
oral,dgl,1,659,28.1017,0.9180

epoch:661/50, training loss:0.6527034044265747
Train Acc 0.9208
 Acc 0.9187
oral,dgl,1,660,28.1436,0.9187

epoch:662/50, training loss:0.6524885296821594
Train Acc 0.9218
 Acc 0.9186
oral,dgl,1,661,28.1858,0.9186

epoch:663/50, training loss:0.6523123979568481
Train Acc 0.9216
 Acc 0.9182
oral,dgl,1,662,28.2279,0.9182

epoch:664/50, training loss:0.6521808505058289
Train Acc 0.9213
 Acc 0.9190
oral,dgl,1,663,28.2700,0.9190

epoch:665/50, training loss:0.6520452499389648
Train Acc 0.9220
 Acc 0.9182
oral,dgl,1,664,28.3121,0.9182

epoch:666/50, training loss:0.6519069075584412
Train Acc 0.9210
 Acc 0.9194
new best val f1: 0.9193919243048319
oral,dgl,1,665,28.3541,0.9194

epoch:667/50, training loss:0.6517821550369263
Train Acc 0.9222
 Acc 0.9177
oral,dgl,1,666,28.3961,0.9177

epoch:668/50, training loss:0.6517145037651062
Train Acc 0.9205
 Acc 0.9198
new best val f1: 0.9197823102244719
oral,dgl,1,667,28.4382,0.9198

epoch:669/50, training loss:0.6516868472099304
Train Acc 0.9226
 Acc 0.9176
oral,dgl,1,668,28.4803,0.9176

epoch:670/50, training loss:0.6516093611717224
Train Acc 0.9201
 Acc 0.9198
new best val f1: 0.9197889269349743
oral,dgl,1,669,28.5223,0.9198

epoch:671/50, training loss:0.6514119505882263
Train Acc 0.9227
 Acc 0.9177
oral,dgl,1,670,28.5645,0.9177

epoch:672/50, training loss:0.651261031627655
Train Acc 0.9202
 Acc 0.9196
oral,dgl,1,671,28.6066,0.9196

epoch:673/50, training loss:0.6511367559432983
Train Acc 0.9226
 Acc 0.9180
oral,dgl,1,672,28.6487,0.9180

epoch:674/50, training loss:0.6510605812072754
Train Acc 0.9205
 Acc 0.9192
oral,dgl,1,673,28.6907,0.9192

epoch:675/50, training loss:0.6508656740188599
Train Acc 0.9222
 Acc 0.9186
oral,dgl,1,674,28.7327,0.9186

epoch:676/50, training loss:0.6506004929542542
Train Acc 0.9214
 Acc 0.9184
oral,dgl,1,675,28.7748,0.9184

epoch:677/50, training loss:0.6504210829734802
Train Acc 0.9217
 Acc 0.9198
new best val f1: 0.9197955436454767
oral,dgl,1,676,28.8169,0.9198

epoch:678/50, training loss:0.6503013372421265
Train Acc 0.9226
 Acc 0.9176
oral,dgl,1,677,28.8591,0.9176

epoch:679/50, training loss:0.6503179669380188
Train Acc 0.9204
 Acc 0.9205
new best val f1: 0.9205167650902354
oral,dgl,1,678,28.9011,0.9205

epoch:680/50, training loss:0.6503071188926697
Train Acc 0.9232
 Acc 0.9170
oral,dgl,1,679,28.9433,0.9170

epoch:681/50, training loss:0.6503769755363464
Train Acc 0.9196
 Acc 0.9206
new best val f1: 0.9206226324582734
oral,dgl,1,680,28.9854,0.9206

epoch:682/50, training loss:0.6504707336425781
Train Acc 0.9234
 Acc 0.9162
oral,dgl,1,681,29.0274,0.9162

epoch:683/50, training loss:0.6507068872451782
Train Acc 0.9189
 Acc 0.9206
oral,dgl,1,682,29.0695,0.9206

epoch:684/50, training loss:0.650560200214386
Train Acc 0.9232
 Acc 0.9163
oral,dgl,1,683,29.1116,0.9163

epoch:685/50, training loss:0.6503720283508301
Train Acc 0.9190
 Acc 0.9206
oral,dgl,1,684,29.1537,0.9206

epoch:686/50, training loss:0.6497745513916016
Train Acc 0.9232
 Acc 0.9176
oral,dgl,1,685,29.1958,0.9176

epoch:687/50, training loss:0.6491720676422119
Train Acc 0.9202
 Acc 0.9201
oral,dgl,1,686,29.2378,0.9201

epoch:688/50, training loss:0.6485262513160706
Train Acc 0.9228
 Acc 0.9189
oral,dgl,1,687,29.2799,0.9189

epoch:689/50, training loss:0.6481293439865112
Train Acc 0.9220
 Acc 0.9189
oral,dgl,1,688,29.3221,0.9189

epoch:690/50, training loss:0.6480639576911926
Train Acc 0.9217
 Acc 0.9201
oral,dgl,1,689,29.3641,0.9201

epoch:691/50, training loss:0.6481994986534119
Train Acc 0.9230
 Acc 0.9178
oral,dgl,1,690,29.4062,0.9178

epoch:692/50, training loss:0.6484217047691345
Train Acc 0.9203
 Acc 0.9206
oral,dgl,1,691,29.4483,0.9206

epoch:693/50, training loss:0.6484791040420532
Train Acc 0.9233
 Acc 0.9175
oral,dgl,1,692,29.4904,0.9175

epoch:694/50, training loss:0.6483795642852783
Train Acc 0.9201
 Acc 0.9204
oral,dgl,1,693,29.5324,0.9204

epoch:695/50, training loss:0.6477888822555542
Train Acc 0.9233
 Acc 0.9186
oral,dgl,1,694,29.5745,0.9186

epoch:696/50, training loss:0.6471949219703674
Train Acc 0.9214
 Acc 0.9193
oral,dgl,1,695,29.6166,0.9193

epoch:697/50, training loss:0.646881103515625
Train Acc 0.9223
 Acc 0.9201
oral,dgl,1,696,29.6586,0.9201

epoch:698/50, training loss:0.6468410491943359
Train Acc 0.9230
 Acc 0.9184
oral,dgl,1,697,29.7009,0.9184

epoch:699/50, training loss:0.6469196677207947
Train Acc 0.9211
 Acc 0.9205
oral,dgl,1,698,29.7430,0.9205

epoch:700/50, training loss:0.6469290852546692
Train Acc 0.9234
 Acc 0.9180
oral,dgl,1,699,29.7852,0.9180

epoch:701/50, training loss:0.6468821167945862
Train Acc 0.9207
 Acc 0.9207
new best val f1: 0.9206821828527947
oral,dgl,1,700,29.8271,0.9207

epoch:702/50, training loss:0.6466837525367737
Train Acc 0.9236
 Acc 0.9182
oral,dgl,1,701,29.8693,0.9182

epoch:703/50, training loss:0.6464647650718689
Train Acc 0.9210
 Acc 0.9207
oral,dgl,1,702,29.9115,0.9207

epoch:704/50, training loss:0.646200954914093
Train Acc 0.9236
 Acc 0.9187
oral,dgl,1,703,29.9535,0.9187

epoch:705/50, training loss:0.6459181904792786
Train Acc 0.9218
 Acc 0.9202
oral,dgl,1,704,29.9956,0.9202

epoch:706/50, training loss:0.6456594467163086
Train Acc 0.9232
 Acc 0.9196
oral,dgl,1,705,30.0375,0.9196

epoch:707/50, training loss:0.6454914212226868
Train Acc 0.9226
 Acc 0.9194
oral,dgl,1,706,30.0797,0.9194

epoch:708/50, training loss:0.645366907119751
Train Acc 0.9225
 Acc 0.9200
oral,dgl,1,707,30.1218,0.9200

epoch:709/50, training loss:0.6452521085739136
Train Acc 0.9230
 Acc 0.9193
oral,dgl,1,708,30.1639,0.9193

epoch:710/50, training loss:0.6450883746147156
Train Acc 0.9223
 Acc 0.9199
oral,dgl,1,709,30.2060,0.9199

epoch:711/50, training loss:0.6449057459831238
Train Acc 0.9230
 Acc 0.9195
oral,dgl,1,710,30.2480,0.9195

epoch:712/50, training loss:0.6447367668151855
Train Acc 0.9225
 Acc 0.9201
oral,dgl,1,711,30.2901,0.9201

epoch:713/50, training loss:0.6446006894111633
Train Acc 0.9232
 Acc 0.9196
oral,dgl,1,712,30.3324,0.9196

epoch:714/50, training loss:0.6444966793060303
Train Acc 0.9227
 Acc 0.9199
oral,dgl,1,713,30.3744,0.9199

epoch:715/50, training loss:0.6444154977798462
Train Acc 0.9230
 Acc 0.9198
oral,dgl,1,714,30.4164,0.9198

epoch:716/50, training loss:0.6442912220954895
Train Acc 0.9228
 Acc 0.9199
oral,dgl,1,715,30.4586,0.9199

epoch:717/50, training loss:0.6441554427146912
Train Acc 0.9229
 Acc 0.9202
oral,dgl,1,716,30.5007,0.9202

epoch:718/50, training loss:0.6439729928970337
Train Acc 0.9232
 Acc 0.9197
oral,dgl,1,717,30.5428,0.9197

epoch:719/50, training loss:0.6438308358192444
Train Acc 0.9228
 Acc 0.9203
oral,dgl,1,718,30.5847,0.9203

epoch:720/50, training loss:0.6436702609062195
Train Acc 0.9233
 Acc 0.9198
oral,dgl,1,719,30.6269,0.9198

epoch:721/50, training loss:0.6435003876686096
Train Acc 0.9228
 Acc 0.9204
oral,dgl,1,720,30.6691,0.9204

epoch:722/50, training loss:0.6432934403419495
Train Acc 0.9234
 Acc 0.9197
oral,dgl,1,721,30.7110,0.9197

epoch:723/50, training loss:0.6431202292442322
Train Acc 0.9227
 Acc 0.9206
oral,dgl,1,722,30.7531,0.9206

epoch:724/50, training loss:0.642979085445404
Train Acc 0.9238
 Acc 0.9192
oral,dgl,1,723,30.7952,0.9192

epoch:725/50, training loss:0.6428669691085815
Train Acc 0.9223
 Acc 0.9209
new best val f1: 0.9208873008783683
oral,dgl,1,724,30.8373,0.9209

epoch:726/50, training loss:0.6427589654922485
Train Acc 0.9240
 Acc 0.9190
oral,dgl,1,725,30.8793,0.9190

epoch:727/50, training loss:0.6427094340324402
Train Acc 0.9220
 Acc 0.9212
new best val f1: 0.921165202719468
oral,dgl,1,726,30.9215,0.9212

epoch:728/50, training loss:0.6427034139633179
Train Acc 0.9242
 Acc 0.9185
oral,dgl,1,727,30.9636,0.9185

epoch:729/50, training loss:0.642772912979126
Train Acc 0.9213
 Acc 0.9216
new best val f1: 0.9215622053496104
oral,dgl,1,728,31.0056,0.9216

epoch:730/50, training loss:0.6427958011627197
Train Acc 0.9245
 Acc 0.9184
oral,dgl,1,729,31.0476,0.9184

epoch:731/50, training loss:0.642753541469574
Train Acc 0.9212
 Acc 0.9217
new best val f1: 0.9216713810728996
oral,dgl,1,730,31.0898,0.9217

epoch:732/50, training loss:0.6426568031311035
Train Acc 0.9245
 Acc 0.9188
oral,dgl,1,731,31.1318,0.9188

epoch:733/50, training loss:0.6427062153816223
Train Acc 0.9217
 Acc 0.9214
oral,dgl,1,732,31.1739,0.9214

epoch:734/50, training loss:0.6426090598106384
Train Acc 0.9244
 Acc 0.9192
oral,dgl,1,733,31.2159,0.9192

epoch:735/50, training loss:0.6427492499351501
Train Acc 0.9223
 Acc 0.9210
oral,dgl,1,734,31.2581,0.9210

epoch:736/50, training loss:0.6423878073692322
Train Acc 0.9239
 Acc 0.9200
oral,dgl,1,735,31.3002,0.9200

epoch:737/50, training loss:0.642350435256958
Train Acc 0.9231
 Acc 0.9199
oral,dgl,1,736,31.3423,0.9199

epoch:738/50, training loss:0.6418880224227905
Train Acc 0.9229
 Acc 0.9211
oral,dgl,1,737,31.3843,0.9211

epoch:739/50, training loss:0.6414921283721924
Train Acc 0.9241
 Acc 0.9193
oral,dgl,1,738,31.4264,0.9193

epoch:740/50, training loss:0.6411810517311096
Train Acc 0.9218
 Acc 0.9218
new best val f1: 0.9217706317304352
oral,dgl,1,739,31.4685,0.9218

epoch:741/50, training loss:0.6411415338516235
Train Acc 0.9248
 Acc 0.9186
oral,dgl,1,740,31.5105,0.9186

epoch:742/50, training loss:0.6414993405342102
Train Acc 0.9212
 Acc 0.9219
new best val f1: 0.9218864241642267
oral,dgl,1,741,31.5526,0.9219

epoch:743/50, training loss:0.6417734622955322
Train Acc 0.9249
 Acc 0.9189
oral,dgl,1,742,31.5948,0.9189

epoch:744/50, training loss:0.64205002784729
Train Acc 0.9217
 Acc 0.9214
oral,dgl,1,743,31.6369,0.9214

epoch:745/50, training loss:0.6411945223808289
Train Acc 0.9245
 Acc 0.9201
oral,dgl,1,744,31.6790,0.9201

epoch:746/50, training loss:0.6404843330383301
Train Acc 0.9233
 Acc 0.9204
oral,dgl,1,745,31.7210,0.9204

epoch:747/50, training loss:0.6397725939750671
Train Acc 0.9234
 Acc 0.9215
oral,dgl,1,746,31.7631,0.9215

epoch:748/50, training loss:0.6395596861839294
Train Acc 0.9246
 Acc 0.9194
oral,dgl,1,747,31.8053,0.9194

epoch:749/50, training loss:0.6397265791893005
Train Acc 0.9222
 Acc 0.9220
new best val f1: 0.9219856748217624
oral,dgl,1,748,31.8473,0.9220

epoch:750/50, training loss:0.6399871110916138
Train Acc 0.9251
 Acc 0.9189
oral,dgl,1,749,31.8893,0.9189

epoch:751/50, training loss:0.6402407288551331
Train Acc 0.9216
 Acc 0.9222
new best val f1: 0.922190792847336
oral,dgl,1,750,31.9315,0.9222

epoch:752/50, training loss:0.640047550201416
Train Acc 0.9252
 Acc 0.9192
oral,dgl,1,751,31.9736,0.9192

epoch:753/50, training loss:0.6396666169166565
Train Acc 0.9220
 Acc 0.9219
oral,dgl,1,752,32.0157,0.9219

epoch:754/50, training loss:0.6390336751937866
Train Acc 0.9249
 Acc 0.9200
oral,dgl,1,753,32.0578,0.9200

epoch:755/50, training loss:0.6385205984115601
Train Acc 0.9230
 Acc 0.9213
oral,dgl,1,754,32.1000,0.9213

epoch:756/50, training loss:0.638185977935791
Train Acc 0.9245
 Acc 0.9207
oral,dgl,1,755,32.1419,0.9207

epoch:757/50, training loss:0.6380265355110168
Train Acc 0.9238
 Acc 0.9210
oral,dgl,1,756,32.1840,0.9210

epoch:758/50, training loss:0.6380070447921753
Train Acc 0.9242
 Acc 0.9212
oral,dgl,1,757,32.2261,0.9212

epoch:759/50, training loss:0.6380041241645813
Train Acc 0.9242
 Acc 0.9208
oral,dgl,1,758,32.2681,0.9208

epoch:760/50, training loss:0.6379631161689758
Train Acc 0.9241
 Acc 0.9210
oral,dgl,1,759,32.3101,0.9210

epoch:761/50, training loss:0.637712299823761
Train Acc 0.9242
 Acc 0.9212
oral,dgl,1,760,32.3522,0.9212

epoch:762/50, training loss:0.637414276599884
Train Acc 0.9245
 Acc 0.9206
oral,dgl,1,761,32.3942,0.9206

epoch:763/50, training loss:0.6371769905090332
Train Acc 0.9237
 Acc 0.9217
oral,dgl,1,762,32.4363,0.9217

epoch:764/50, training loss:0.6370320916175842
Train Acc 0.9248
 Acc 0.9205
oral,dgl,1,763,32.4784,0.9205

epoch:765/50, training loss:0.6369450092315674
Train Acc 0.9235
 Acc 0.9214
oral,dgl,1,764,32.5206,0.9214

epoch:766/50, training loss:0.6368039846420288
Train Acc 0.9248
 Acc 0.9210
oral,dgl,1,765,32.5627,0.9210

epoch:767/50, training loss:0.6366209387779236
Train Acc 0.9242
 Acc 0.9211
oral,dgl,1,766,32.6046,0.9211

epoch:768/50, training loss:0.6364061832427979
Train Acc 0.9243
 Acc 0.9214
oral,dgl,1,767,32.6467,0.9214

epoch:769/50, training loss:0.6362317204475403
Train Acc 0.9246
 Acc 0.9209
oral,dgl,1,768,32.6889,0.9209

epoch:770/50, training loss:0.6360524892807007
Train Acc 0.9240
 Acc 0.9216
oral,dgl,1,769,32.7310,0.9216

epoch:771/50, training loss:0.6358922123908997
Train Acc 0.9250
 Acc 0.9207
oral,dgl,1,770,32.7729,0.9207

epoch:772/50, training loss:0.6357869505882263
Train Acc 0.9238
 Acc 0.9219
oral,dgl,1,771,32.8151,0.9219

epoch:773/50, training loss:0.6357449889183044
Train Acc 0.9252
 Acc 0.9200
oral,dgl,1,772,32.8572,0.9200

epoch:774/50, training loss:0.635896623134613
Train Acc 0.9228
 Acc 0.9225
new best val f1: 0.9224885448199428
oral,dgl,1,773,32.8992,0.9225

epoch:775/50, training loss:0.6362131237983704
Train Acc 0.9256
 Acc 0.9189
oral,dgl,1,774,32.9412,0.9189

epoch:776/50, training loss:0.6366921067237854
Train Acc 0.9216
 Acc 0.9228
new best val f1: 0.9227896051478007
oral,dgl,1,775,32.9834,0.9228

epoch:777/50, training loss:0.6369388103485107
Train Acc 0.9259
 Acc 0.9182
oral,dgl,1,776,33.0255,0.9182

epoch:778/50, training loss:0.6370715498924255
Train Acc 0.9208
 Acc 0.9227
oral,dgl,1,777,33.0676,0.9227

epoch:779/50, training loss:0.636468231678009
Train Acc 0.9258
 Acc 0.9193
oral,dgl,1,778,33.1096,0.9193

epoch:780/50, training loss:0.6357623934745789
Train Acc 0.9220
 Acc 0.9225
oral,dgl,1,779,33.1516,0.9225

epoch:781/50, training loss:0.6349349617958069
Train Acc 0.9254
 Acc 0.9207
oral,dgl,1,780,33.1938,0.9207

epoch:782/50, training loss:0.6343488693237305
Train Acc 0.9239
 Acc 0.9216
oral,dgl,1,781,33.2360,0.9216

epoch:783/50, training loss:0.6340212225914001
Train Acc 0.9250
 Acc 0.9219
oral,dgl,1,782,33.2781,0.9219

epoch:784/50, training loss:0.6339153051376343
Train Acc 0.9253
 Acc 0.9210
oral,dgl,1,783,33.3200,0.9210

epoch:785/50, training loss:0.6338945627212524
Train Acc 0.9241
 Acc 0.9222
oral,dgl,1,784,33.3621,0.9222

epoch:786/50, training loss:0.6337839365005493
Train Acc 0.9255
 Acc 0.9210
oral,dgl,1,785,33.4042,0.9210

epoch:787/50, training loss:0.6335954666137695
Train Acc 0.9241
 Acc 0.9220
oral,dgl,1,786,33.4463,0.9220

epoch:788/50, training loss:0.6333767175674438
Train Acc 0.9254
 Acc 0.9215
oral,dgl,1,787,33.4884,0.9215

epoch:789/50, training loss:0.6331768035888672
Train Acc 0.9247
 Acc 0.9216
oral,dgl,1,788,33.5303,0.9216

epoch:790/50, training loss:0.6330234408378601
Train Acc 0.9250
 Acc 0.9220
oral,dgl,1,789,33.5725,0.9220

epoch:791/50, training loss:0.6329103708267212
Train Acc 0.9253
 Acc 0.9212
oral,dgl,1,790,33.6146,0.9212

epoch:792/50, training loss:0.6328337788581848
Train Acc 0.9244
 Acc 0.9224
oral,dgl,1,791,33.6567,0.9224

epoch:793/50, training loss:0.6327788233757019
Train Acc 0.9256
 Acc 0.9212
oral,dgl,1,792,33.6987,0.9212

epoch:794/50, training loss:0.6326724886894226
Train Acc 0.9243
 Acc 0.9224
oral,dgl,1,793,33.7408,0.9224

epoch:795/50, training loss:0.632495105266571
Train Acc 0.9257
 Acc 0.9213
oral,dgl,1,794,33.7829,0.9213

epoch:796/50, training loss:0.6323218941688538
Train Acc 0.9243
 Acc 0.9224
oral,dgl,1,795,33.8249,0.9224

epoch:797/50, training loss:0.6321665048599243
Train Acc 0.9257
 Acc 0.9215
oral,dgl,1,796,33.8669,0.9215

epoch:798/50, training loss:0.6320348381996155
Train Acc 0.9246
 Acc 0.9224
oral,dgl,1,797,33.9090,0.9224

epoch:799/50, training loss:0.63190096616745
Train Acc 0.9256
 Acc 0.9216
oral,dgl,1,798,33.9511,0.9216

epoch:800/50, training loss:0.6317592263221741
Train Acc 0.9248
 Acc 0.9220
oral,dgl,1,799,33.9932,0.9220

epoch:801/50, training loss:0.6316179037094116
Train Acc 0.9253
 Acc 0.9222
oral,dgl,1,800,34.0352,0.9222

epoch:802/50, training loss:0.6315386295318604
Train Acc 0.9255
 Acc 0.9214
oral,dgl,1,801,34.0774,0.9214

epoch:803/50, training loss:0.6315538287162781
Train Acc 0.9245
 Acc 0.9227
oral,dgl,1,802,34.1195,0.9227

epoch:804/50, training loss:0.6316213011741638
Train Acc 0.9260
 Acc 0.9208
oral,dgl,1,803,34.1617,0.9208

epoch:805/50, training loss:0.631714403629303
Train Acc 0.9238
 Acc 0.9231
new best val f1: 0.9231237490281706
oral,dgl,1,804,34.2037,0.9231

epoch:806/50, training loss:0.6316319108009338
Train Acc 0.9263
 Acc 0.9209
oral,dgl,1,805,34.2458,0.9209

epoch:807/50, training loss:0.6315146088600159
Train Acc 0.9238
 Acc 0.9232
new best val f1: 0.9231535242254313
oral,dgl,1,806,34.2879,0.9232

epoch:808/50, training loss:0.631263792514801
Train Acc 0.9263
 Acc 0.9212
oral,dgl,1,807,34.3300,0.9212

epoch:809/50, training loss:0.6309322118759155
Train Acc 0.9243
 Acc 0.9227
oral,dgl,1,808,34.3720,0.9227

epoch:810/50, training loss:0.6305019855499268
Train Acc 0.9260
 Acc 0.9221
oral,dgl,1,809,34.4141,0.9221

epoch:811/50, training loss:0.6301910281181335
Train Acc 0.9252
 Acc 0.9223
oral,dgl,1,810,34.4563,0.9223

epoch:812/50, training loss:0.6300436854362488
Train Acc 0.9254
 Acc 0.9226
oral,dgl,1,811,34.4983,0.9226

epoch:813/50, training loss:0.6300134062767029
Train Acc 0.9258
 Acc 0.9220
oral,dgl,1,812,34.5402,0.9220

epoch:814/50, training loss:0.6299874186515808
Train Acc 0.9250
 Acc 0.9227
oral,dgl,1,813,34.5824,0.9227

epoch:815/50, training loss:0.6299046874046326
Train Acc 0.9259
 Acc 0.9219
oral,dgl,1,814,34.6245,0.9219

epoch:816/50, training loss:0.6297776103019714
Train Acc 0.9250
 Acc 0.9226
oral,dgl,1,815,34.6665,0.9226

epoch:817/50, training loss:0.6295480728149414
Train Acc 0.9259
 Acc 0.9224
oral,dgl,1,816,34.7086,0.9224

epoch:818/50, training loss:0.6293147802352905
Train Acc 0.9255
 Acc 0.9226
oral,dgl,1,817,34.7507,0.9226

epoch:819/50, training loss:0.6291356682777405
Train Acc 0.9255
 Acc 0.9228
oral,dgl,1,818,34.7928,0.9228

epoch:820/50, training loss:0.6290367841720581
Train Acc 0.9258
 Acc 0.9219
oral,dgl,1,819,34.8348,0.9219

epoch:821/50, training loss:0.6290386319160461
Train Acc 0.9249
 Acc 0.9233
new best val f1: 0.9233487171852514
oral,dgl,1,820,34.8769,0.9233

epoch:822/50, training loss:0.6291251182556152
Train Acc 0.9264
 Acc 0.9214
oral,dgl,1,821,34.9191,0.9214

epoch:823/50, training loss:0.6292808055877686
Train Acc 0.9242
 Acc 0.9236
new best val f1: 0.9236233106710998
oral,dgl,1,822,34.9612,0.9236

epoch:824/50, training loss:0.629408597946167
Train Acc 0.9268
 Acc 0.9208
oral,dgl,1,823,35.0032,0.9208

epoch:825/50, training loss:0.6295643448829651
Train Acc 0.9236
 Acc 0.9238
new best val f1: 0.9237523365258962
oral,dgl,1,824,35.0453,0.9238

epoch:826/50, training loss:0.6296040415763855
Train Acc 0.9269
 Acc 0.9204
oral,dgl,1,825,35.0873,0.9204

epoch:827/50, training loss:0.6297985911369324
Train Acc 0.9231
 Acc 0.9239
new best val f1: 0.9239309877094602
oral,dgl,1,826,35.1293,0.9239

epoch:828/50, training loss:0.6298862099647522
Train Acc 0.9270
 Acc 0.9200
oral,dgl,1,827,35.1714,0.9200

epoch:829/50, training loss:0.6299760937690735
Train Acc 0.9227
 Acc 0.9239
oral,dgl,1,828,35.2133,0.9239

epoch:830/50, training loss:0.6296254992485046
Train Acc 0.9270
 Acc 0.9205
oral,dgl,1,829,35.2554,0.9205

epoch:831/50, training loss:0.6292098164558411
Train Acc 0.9234
 Acc 0.9238
oral,dgl,1,830,35.2976,0.9238

epoch:832/50, training loss:0.628489077091217
Train Acc 0.9270
 Acc 0.9218
oral,dgl,1,831,35.3397,0.9218

epoch:833/50, training loss:0.6278328895568848
Train Acc 0.9247
 Acc 0.9232
oral,dgl,1,832,35.3816,0.9232

epoch:834/50, training loss:0.6273472309112549
Train Acc 0.9263
 Acc 0.9230
oral,dgl,1,833,35.4237,0.9230

epoch:835/50, training loss:0.6271533370018005
Train Acc 0.9261
 Acc 0.9222
oral,dgl,1,834,35.4658,0.9222

epoch:836/50, training loss:0.6272788047790527
Train Acc 0.9251
 Acc 0.9237
oral,dgl,1,835,35.5079,0.9237

epoch:837/50, training loss:0.6276236176490784
Train Acc 0.9269
 Acc 0.9211
oral,dgl,1,836,35.5500,0.9211

epoch:838/50, training loss:0.6279365420341492
Train Acc 0.9239
 Acc 0.9239
new best val f1: 0.923944221130465
oral,dgl,1,837,35.5921,0.9239

epoch:839/50, training loss:0.627812922000885
Train Acc 0.9268
 Acc 0.9214
oral,dgl,1,838,35.6340,0.9214

epoch:840/50, training loss:0.6275860667228699
Train Acc 0.9243
 Acc 0.9235
oral,dgl,1,839,35.6761,0.9235

epoch:841/50, training loss:0.6271491050720215
Train Acc 0.9265
 Acc 0.9225
oral,dgl,1,840,35.7182,0.9225

epoch:842/50, training loss:0.6268578171730042
Train Acc 0.9255
 Acc 0.9227
oral,dgl,1,841,35.7603,0.9227

epoch:843/50, training loss:0.6268374919891357
Train Acc 0.9257
 Acc 0.9233
oral,dgl,1,842,35.8024,0.9233

epoch:844/50, training loss:0.6268787980079651
Train Acc 0.9266
 Acc 0.9221
oral,dgl,1,843,35.8445,0.9221

epoch:845/50, training loss:0.6270768046379089
Train Acc 0.9251
 Acc 0.9237
oral,dgl,1,844,35.8866,0.9237

epoch:846/50, training loss:0.6269006133079529
Train Acc 0.9270
 Acc 0.9220
oral,dgl,1,845,35.9287,0.9220

epoch:847/50, training loss:0.6266514658927917
Train Acc 0.9249
 Acc 0.9236
oral,dgl,1,846,35.9707,0.9236

epoch:848/50, training loss:0.6260979175567627
Train Acc 0.9269
 Acc 0.9228
oral,dgl,1,847,36.0126,0.9228

epoch:849/50, training loss:0.6256980895996094
Train Acc 0.9258
 Acc 0.9231
oral,dgl,1,848,36.0548,0.9231

epoch:850/50, training loss:0.6254833936691284
Train Acc 0.9262
 Acc 0.9236
oral,dgl,1,849,36.0969,0.9236

epoch:851/50, training loss:0.6254267692565918
Train Acc 0.9267
 Acc 0.9224
oral,dgl,1,850,36.1389,0.9224

epoch:852/50, training loss:0.6254229545593262
Train Acc 0.9253
 Acc 0.9238
oral,dgl,1,851,36.1809,0.9238

epoch:853/50, training loss:0.6253842115402222
Train Acc 0.9270
 Acc 0.9222
oral,dgl,1,852,36.2232,0.9222

epoch:854/50, training loss:0.6253306269645691
Train Acc 0.9250
 Acc 0.9239
oral,dgl,1,853,36.2653,0.9239

epoch:855/50, training loss:0.6251491904258728
Train Acc 0.9272
 Acc 0.9226
oral,dgl,1,854,36.3074,0.9226

epoch:856/50, training loss:0.624908447265625
Train Acc 0.9255
 Acc 0.9237
oral,dgl,1,855,36.3494,0.9237

epoch:857/50, training loss:0.6246331930160522
Train Acc 0.9268
 Acc 0.9233
oral,dgl,1,856,36.3915,0.9233

epoch:858/50, training loss:0.6244227290153503
Train Acc 0.9263
 Acc 0.9233
oral,dgl,1,857,36.4335,0.9233

epoch:859/50, training loss:0.6243025660514832
Train Acc 0.9263
 Acc 0.9238
oral,dgl,1,858,36.4756,0.9238

epoch:860/50, training loss:0.6242913603782654
Train Acc 0.9268
 Acc 0.9227
oral,dgl,1,859,36.5177,0.9227

epoch:861/50, training loss:0.6243381500244141
Train Acc 0.9255
 Acc 0.9241
new best val f1: 0.9240633219195077
oral,dgl,1,860,36.5599,0.9241

epoch:862/50, training loss:0.6243489980697632
Train Acc 0.9273
 Acc 0.9225
oral,dgl,1,861,36.6020,0.9225

epoch:863/50, training loss:0.6242861747741699
Train Acc 0.9254
 Acc 0.9241
new best val f1: 0.9240732469852613
oral,dgl,1,862,36.6440,0.9241

epoch:864/50, training loss:0.6240872144699097
Train Acc 0.9273
 Acc 0.9227
oral,dgl,1,863,36.6861,0.9227

epoch:865/50, training loss:0.6238636374473572
Train Acc 0.9256
 Acc 0.9238
oral,dgl,1,864,36.7282,0.9238

epoch:866/50, training loss:0.6236258745193481
Train Acc 0.9269
 Acc 0.9233
oral,dgl,1,865,36.7703,0.9233

epoch:867/50, training loss:0.6234802007675171
Train Acc 0.9265
 Acc 0.9236
oral,dgl,1,866,36.8124,0.9236

epoch:868/50, training loss:0.6234143376350403
Train Acc 0.9265
 Acc 0.9235
oral,dgl,1,867,36.8543,0.9235

epoch:869/50, training loss:0.6233869791030884
Train Acc 0.9266
 Acc 0.9234
oral,dgl,1,868,36.8965,0.9234

epoch:870/50, training loss:0.6233996748924255
Train Acc 0.9263
 Acc 0.9237
oral,dgl,1,869,36.9386,0.9237

epoch:871/50, training loss:0.6233565211296082
Train Acc 0.9268
 Acc 0.9232
oral,dgl,1,870,36.9806,0.9232

epoch:872/50, training loss:0.6233646273612976
Train Acc 0.9263
 Acc 0.9238
oral,dgl,1,871,37.0227,0.9238

epoch:873/50, training loss:0.6232680678367615
Train Acc 0.9270
 Acc 0.9230
oral,dgl,1,872,37.0648,0.9230

epoch:874/50, training loss:0.6232344508171082
Train Acc 0.9260
 Acc 0.9240
oral,dgl,1,873,37.1069,0.9240

epoch:875/50, training loss:0.6230032444000244
Train Acc 0.9271
 Acc 0.9231
oral,dgl,1,874,37.1488,0.9231

epoch:876/50, training loss:0.6227763891220093
Train Acc 0.9260
 Acc 0.9241
new best val f1: 0.9241195639587779
oral,dgl,1,875,37.1910,0.9241

epoch:877/50, training loss:0.6224949359893799
Train Acc 0.9272
 Acc 0.9232
oral,dgl,1,876,37.2331,0.9232

epoch:878/50, training loss:0.622326135635376
Train Acc 0.9261
 Acc 0.9243
new best val f1: 0.9243048318528444
oral,dgl,1,877,37.2752,0.9243

epoch:879/50, training loss:0.6222363114356995
Train Acc 0.9274
 Acc 0.9229
oral,dgl,1,878,37.3172,0.9229

epoch:880/50, training loss:0.6222480535507202
Train Acc 0.9258
 Acc 0.9247
new best val f1: 0.9247216846144939
oral,dgl,1,879,37.3594,0.9247

epoch:881/50, training loss:0.6223205924034119
Train Acc 0.9277
 Acc 0.9224
oral,dgl,1,880,37.4015,0.9224

epoch:882/50, training loss:0.6224874258041382
Train Acc 0.9253
 Acc 0.9247
oral,dgl,1,881,37.4436,0.9247

epoch:883/50, training loss:0.6225999593734741
Train Acc 0.9278
 Acc 0.9220
oral,dgl,1,882,37.4857,0.9220

epoch:884/50, training loss:0.6228897571563721
Train Acc 0.9249
 Acc 0.9244
oral,dgl,1,883,37.5277,0.9244

epoch:885/50, training loss:0.6230616569519043
Train Acc 0.9275
 Acc 0.9219
oral,dgl,1,884,37.5698,0.9219

epoch:886/50, training loss:0.6234399676322937
Train Acc 0.9248
 Acc 0.9241
oral,dgl,1,885,37.6119,0.9241

epoch:887/50, training loss:0.6235240697860718
Train Acc 0.9271
 Acc 0.9227
oral,dgl,1,886,37.6540,0.9227

epoch:888/50, training loss:0.6231517791748047
Train Acc 0.9258
 Acc 0.9235
oral,dgl,1,887,37.6961,0.9235

epoch:889/50, training loss:0.6227506399154663
Train Acc 0.9264
 Acc 0.9239
oral,dgl,1,888,37.7381,0.9239

epoch:890/50, training loss:0.6217131018638611
Train Acc 0.9272
 Acc 0.9234
oral,dgl,1,889,37.7802,0.9234

epoch:891/50, training loss:0.621033787727356
Train Acc 0.9262
 Acc 0.9247
oral,dgl,1,890,37.8223,0.9247

epoch:892/50, training loss:0.620850145816803
Train Acc 0.9276
 Acc 0.9232
oral,dgl,1,891,37.8643,0.9232

epoch:893/50, training loss:0.6213130354881287
Train Acc 0.9261
 Acc 0.9242
oral,dgl,1,892,37.9064,0.9242

epoch:894/50, training loss:0.6219080090522766
Train Acc 0.9272
 Acc 0.9232
oral,dgl,1,893,37.9485,0.9232

epoch:895/50, training loss:0.6219820976257324
Train Acc 0.9263
 Acc 0.9239
oral,dgl,1,894,37.9906,0.9239

epoch:896/50, training loss:0.6217635869979858
Train Acc 0.9268
 Acc 0.9240
oral,dgl,1,895,38.0327,0.9240

epoch:897/50, training loss:0.6208125948905945
Train Acc 0.9271
 Acc 0.9238
oral,dgl,1,896,38.0748,0.9238

epoch:898/50, training loss:0.6201322078704834
Train Acc 0.9267
 Acc 0.9245
oral,dgl,1,897,38.1169,0.9245

epoch:899/50, training loss:0.6199225187301636
Train Acc 0.9275
 Acc 0.9236
oral,dgl,1,898,38.1589,0.9236

epoch:900/50, training loss:0.6201817393302917
Train Acc 0.9266
 Acc 0.9245
oral,dgl,1,899,38.2010,0.9245

epoch:901/50, training loss:0.6204780340194702
Train Acc 0.9275
 Acc 0.9233
oral,dgl,1,900,38.2432,0.9233

epoch:902/50, training loss:0.6203951835632324
Train Acc 0.9262
 Acc 0.9248
new best val f1: 0.9248407854035366
oral,dgl,1,901,38.2852,0.9248

epoch:903/50, training loss:0.6199097037315369
Train Acc 0.9278
 Acc 0.9237
oral,dgl,1,902,38.3272,0.9237

epoch:904/50, training loss:0.6194698214530945
Train Acc 0.9266
 Acc 0.9246
oral,dgl,1,903,38.3694,0.9246

epoch:905/50, training loss:0.6194103956222534
Train Acc 0.9276
 Acc 0.9239
oral,dgl,1,904,38.4115,0.9239

epoch:906/50, training loss:0.6196956038475037
Train Acc 0.9268
 Acc 0.9243
oral,dgl,1,905,38.4536,0.9243

epoch:907/50, training loss:0.6197526454925537
Train Acc 0.9273
 Acc 0.9242
oral,dgl,1,906,38.4956,0.9242

epoch:908/50, training loss:0.6195929050445557
Train Acc 0.9270
 Acc 0.9242
oral,dgl,1,907,38.5376,0.9242

epoch:909/50, training loss:0.6190827488899231
Train Acc 0.9272
 Acc 0.9245
oral,dgl,1,908,38.5798,0.9245

epoch:910/50, training loss:0.6187048554420471
Train Acc 0.9273
 Acc 0.9240
oral,dgl,1,909,38.6218,0.9240

epoch:911/50, training loss:0.6186057925224304
Train Acc 0.9270
 Acc 0.9247
oral,dgl,1,910,38.6639,0.9247

epoch:912/50, training loss:0.6187657117843628
Train Acc 0.9277
 Acc 0.9238
oral,dgl,1,911,38.7061,0.9238

epoch:913/50, training loss:0.6190072298049927
Train Acc 0.9268
 Acc 0.9247
oral,dgl,1,912,38.7481,0.9247

epoch:914/50, training loss:0.6189736127853394
Train Acc 0.9279
 Acc 0.9240
oral,dgl,1,913,38.7901,0.9240

epoch:915/50, training loss:0.618852436542511
Train Acc 0.9269
 Acc 0.9248
oral,dgl,1,914,38.8321,0.9248

epoch:916/50, training loss:0.618399441242218
Train Acc 0.9278
 Acc 0.9244
oral,dgl,1,915,38.8743,0.9244

epoch:917/50, training loss:0.6180044412612915
Train Acc 0.9273
 Acc 0.9245
oral,dgl,1,916,38.9164,0.9245

epoch:918/50, training loss:0.6178120970726013
Train Acc 0.9276
 Acc 0.9247
oral,dgl,1,917,38.9585,0.9247

epoch:919/50, training loss:0.6178538799285889
Train Acc 0.9278
 Acc 0.9242
oral,dgl,1,918,39.0006,0.9242

epoch:920/50, training loss:0.6179641485214233
Train Acc 0.9272
 Acc 0.9249
new best val f1: 0.9249466527715746
oral,dgl,1,919,39.0426,0.9249

epoch:921/50, training loss:0.6178996562957764
Train Acc 0.9279
 Acc 0.9241
oral,dgl,1,920,39.0848,0.9241

epoch:922/50, training loss:0.617707371711731
Train Acc 0.9271
 Acc 0.9249
oral,dgl,1,921,39.1269,0.9249

epoch:923/50, training loss:0.6174244284629822
Train Acc 0.9280
 Acc 0.9243
oral,dgl,1,922,39.1692,0.9243

epoch:924/50, training loss:0.6172478199005127
Train Acc 0.9273
 Acc 0.9249
oral,dgl,1,923,39.2112,0.9249

epoch:925/50, training loss:0.6171534657478333
Train Acc 0.9279
 Acc 0.9244
oral,dgl,1,924,39.2533,0.9244

epoch:926/50, training loss:0.6170721054077148
Train Acc 0.9274
 Acc 0.9249
oral,dgl,1,925,39.2953,0.9249

epoch:927/50, training loss:0.6169840097427368
Train Acc 0.9279
 Acc 0.9245
oral,dgl,1,926,39.3376,0.9245

epoch:928/50, training loss:0.616915762424469
Train Acc 0.9275
 Acc 0.9250
new best val f1: 0.9249731196135841
oral,dgl,1,927,39.3797,0.9250

epoch:929/50, training loss:0.6168520450592041
Train Acc 0.9279
 Acc 0.9244
oral,dgl,1,928,39.4218,0.9244

epoch:930/50, training loss:0.616777241230011
Train Acc 0.9274
 Acc 0.9253
new best val f1: 0.9252675632309397
oral,dgl,1,929,39.4639,0.9253

epoch:931/50, training loss:0.6166713833808899
Train Acc 0.9282
 Acc 0.9241
oral,dgl,1,930,39.5059,0.9241

epoch:932/50, training loss:0.6165584325790405
Train Acc 0.9272
 Acc 0.9253
oral,dgl,1,931,39.5481,0.9253

epoch:933/50, training loss:0.6164641976356506
Train Acc 0.9283
 Acc 0.9243
oral,dgl,1,932,39.5902,0.9243

epoch:934/50, training loss:0.6164173483848572
Train Acc 0.9273
 Acc 0.9252
oral,dgl,1,933,39.6323,0.9252

epoch:935/50, training loss:0.616344690322876
Train Acc 0.9281
 Acc 0.9244
oral,dgl,1,934,39.6742,0.9244

epoch:936/50, training loss:0.616271436214447
Train Acc 0.9275
 Acc 0.9252
oral,dgl,1,935,39.7164,0.9252

epoch:937/50, training loss:0.6161422729492188
Train Acc 0.9281
 Acc 0.9244
oral,dgl,1,936,39.7586,0.9244

epoch:938/50, training loss:0.6160277128219604
Train Acc 0.9275
 Acc 0.9254
new best val f1: 0.9253767389542289
oral,dgl,1,937,39.8006,0.9254

epoch:939/50, training loss:0.6158909797668457
Train Acc 0.9284
 Acc 0.9242
oral,dgl,1,938,39.8427,0.9242

epoch:940/50, training loss:0.6158286333084106
Train Acc 0.9273
 Acc 0.9257
new best val f1: 0.9257241162556036
oral,dgl,1,939,39.8846,0.9257

epoch:941/50, training loss:0.6159214973449707
Train Acc 0.9287
 Acc 0.9235
oral,dgl,1,940,39.9268,0.9235

epoch:942/50, training loss:0.6162430644035339
Train Acc 0.9265
 Acc 0.9256
oral,dgl,1,941,39.9689,0.9256

epoch:943/50, training loss:0.6166352033615112
Train Acc 0.9287
 Acc 0.9224
oral,dgl,1,942,40.0110,0.9224

epoch:944/50, training loss:0.6172746419906616
Train Acc 0.9254
 Acc 0.9258
new best val f1: 0.9257571998081154
oral,dgl,1,943,40.0530,0.9258

epoch:945/50, training loss:0.6175200343132019
Train Acc 0.9288
 Acc 0.9218
oral,dgl,1,944,40.0951,0.9218

epoch:946/50, training loss:0.6178788542747498
Train Acc 0.9248
 Acc 0.9259
new best val f1: 0.9258630671761534
oral,dgl,1,945,40.1372,0.9259

epoch:947/50, training loss:0.6175069212913513
Train Acc 0.9288
 Acc 0.9224
oral,dgl,1,946,40.1794,0.9224

epoch:948/50, training loss:0.6170598864555359
Train Acc 0.9254
 Acc 0.9258
oral,dgl,1,947,40.2214,0.9258

epoch:949/50, training loss:0.6161618828773499
Train Acc 0.9291
 Acc 0.9239
oral,dgl,1,948,40.2634,0.9239

epoch:950/50, training loss:0.6153554916381836
Train Acc 0.9269
 Acc 0.9255
oral,dgl,1,949,40.3055,0.9255

epoch:951/50, training loss:0.6146891713142395
Train Acc 0.9286
 Acc 0.9251
oral,dgl,1,950,40.3477,0.9251

epoch:952/50, training loss:0.6143869161605835
Train Acc 0.9281
 Acc 0.9248
oral,dgl,1,951,40.3897,0.9248

epoch:953/50, training loss:0.6143829822540283
Train Acc 0.9278
 Acc 0.9259
new best val f1: 0.9258729922419069
oral,dgl,1,952,40.4317,0.9259

epoch:954/50, training loss:0.6146232485771179
Train Acc 0.9289
 Acc 0.9239
oral,dgl,1,953,40.4738,0.9239

epoch:955/50, training loss:0.6149855256080627
Train Acc 0.9269
 Acc 0.9260
new best val f1: 0.9260351016492151
oral,dgl,1,954,40.5159,0.9260

epoch:956/50, training loss:0.6150949001312256
Train Acc 0.9292
 Acc 0.9237
oral,dgl,1,955,40.5580,0.9237

epoch:957/50, training loss:0.6149429082870483
Train Acc 0.9268
 Acc 0.9260
oral,dgl,1,956,40.5999,0.9260

epoch:958/50, training loss:0.6144821643829346
Train Acc 0.9291
 Acc 0.9245
oral,dgl,1,957,40.6421,0.9245

epoch:959/50, training loss:0.6140204071998596
Train Acc 0.9275
 Acc 0.9255
oral,dgl,1,958,40.6842,0.9255

epoch:960/50, training loss:0.613673746585846
Train Acc 0.9286
 Acc 0.9253
oral,dgl,1,959,40.7263,0.9253

epoch:961/50, training loss:0.6135038137435913
Train Acc 0.9283
 Acc 0.9250
oral,dgl,1,960,40.7685,0.9250

epoch:962/50, training loss:0.6135031580924988
Train Acc 0.9280
 Acc 0.9258
oral,dgl,1,961,40.8104,0.9258

epoch:963/50, training loss:0.6134867668151855
Train Acc 0.9288
 Acc 0.9249
oral,dgl,1,962,40.8526,0.9249

epoch:964/50, training loss:0.6133730411529541
Train Acc 0.9280
 Acc 0.9258
oral,dgl,1,963,40.8947,0.9258

epoch:965/50, training loss:0.6132228970527649
Train Acc 0.9287
 Acc 0.9252
oral,dgl,1,964,40.9368,0.9252

epoch:966/50, training loss:0.613093912601471
Train Acc 0.9282
 Acc 0.9256
oral,dgl,1,965,40.9788,0.9256

epoch:967/50, training loss:0.6129932403564453
Train Acc 0.9288
 Acc 0.9252
oral,dgl,1,966,41.0209,0.9252

epoch:968/50, training loss:0.6128844022750854
Train Acc 0.9283
 Acc 0.9256
oral,dgl,1,967,41.0630,0.9256

epoch:969/50, training loss:0.6127684116363525
Train Acc 0.9288
 Acc 0.9252
oral,dgl,1,968,41.1051,0.9252

epoch:970/50, training loss:0.6126652359962463
Train Acc 0.9283
 Acc 0.9256
oral,dgl,1,969,41.1470,0.9256

epoch:971/50, training loss:0.6125759482383728
Train Acc 0.9287
 Acc 0.9251
oral,dgl,1,970,41.1892,0.9251

epoch:972/50, training loss:0.6125032305717468
Train Acc 0.9283
 Acc 0.9257
oral,dgl,1,971,41.2314,0.9257

epoch:973/50, training loss:0.6124149560928345
Train Acc 0.9288
 Acc 0.9252
oral,dgl,1,972,41.2734,0.9252

epoch:974/50, training loss:0.6123104691505432
Train Acc 0.9283
 Acc 0.9257
oral,dgl,1,973,41.3154,0.9257

epoch:975/50, training loss:0.6122015714645386
Train Acc 0.9288
 Acc 0.9254
oral,dgl,1,974,41.3576,0.9254

epoch:976/50, training loss:0.6120911240577698
Train Acc 0.9284
 Acc 0.9256
oral,dgl,1,975,41.3997,0.9256

epoch:977/50, training loss:0.6119826436042786
Train Acc 0.9288
 Acc 0.9255
oral,dgl,1,976,41.4418,0.9255

epoch:978/50, training loss:0.6118772625923157
Train Acc 0.9286
 Acc 0.9255
oral,dgl,1,977,41.4839,0.9255

epoch:979/50, training loss:0.6117798089981079
Train Acc 0.9286
 Acc 0.9257
oral,dgl,1,978,41.5260,0.9257

epoch:980/50, training loss:0.6116818785667419
Train Acc 0.9288
 Acc 0.9255
oral,dgl,1,979,41.5682,0.9255

epoch:981/50, training loss:0.6116024255752563
Train Acc 0.9286
 Acc 0.9256
oral,dgl,1,980,41.6102,0.9256

epoch:982/50, training loss:0.6115480065345764
Train Acc 0.9288
 Acc 0.9256
oral,dgl,1,981,41.6525,0.9256

epoch:983/50, training loss:0.6114813089370728
Train Acc 0.9287
 Acc 0.9255
oral,dgl,1,982,41.6946,0.9255

epoch:984/50, training loss:0.6113858222961426
Train Acc 0.9286
 Acc 0.9260
oral,dgl,1,983,41.7367,0.9260

epoch:985/50, training loss:0.611289918422699
Train Acc 0.9292
 Acc 0.9251
oral,dgl,1,984,41.7789,0.9251

epoch:986/50, training loss:0.611315131187439
Train Acc 0.9283
 Acc 0.9262
new best val f1: 0.9262005194117744
oral,dgl,1,985,41.8209,0.9262

epoch:987/50, training loss:0.6114517450332642
Train Acc 0.9293
 Acc 0.9246
oral,dgl,1,986,41.8629,0.9246

epoch:988/50, training loss:0.6116786003112793
Train Acc 0.9279
 Acc 0.9261
oral,dgl,1,987,41.9049,0.9261

epoch:989/50, training loss:0.6117650270462036
Train Acc 0.9292
 Acc 0.9240
oral,dgl,1,988,41.9471,0.9240

epoch:990/50, training loss:0.6118555068969727
Train Acc 0.9274
 Acc 0.9264
new best val f1: 0.9264486460556135
oral,dgl,1,989,41.9892,0.9264

epoch:991/50, training loss:0.6118035316467285
Train Acc 0.9296
 Acc 0.9238
oral,dgl,1,990,42.0313,0.9238

epoch:992/50, training loss:0.6118963360786438
Train Acc 0.9271
 Acc 0.9265
new best val f1: 0.9264519544108646
oral,dgl,1,991,42.0734,0.9265

epoch:993/50, training loss:0.6118053197860718
Train Acc 0.9297
 Acc 0.9240
oral,dgl,1,992,42.1154,0.9240

epoch:994/50, training loss:0.6117523908615112
Train Acc 0.9273
 Acc 0.9267
new best val f1: 0.9267166228309596
oral,dgl,1,993,42.1575,0.9267

epoch:995/50, training loss:0.6115879416465759
Train Acc 0.9297
 Acc 0.9245
oral,dgl,1,994,42.1997,0.9245

epoch:996/50, training loss:0.611416757106781
Train Acc 0.9277
 Acc 0.9265
oral,dgl,1,995,42.2418,0.9265

epoch:997/50, training loss:0.6110484600067139
Train Acc 0.9296
 Acc 0.9251
oral,dgl,1,996,42.2839,0.9251

epoch:998/50, training loss:0.6108039617538452
Train Acc 0.9282
 Acc 0.9264
oral,dgl,1,997,42.3259,0.9264

epoch:999/50, training loss:0.6104826331138611
Train Acc 0.9295
 Acc 0.9255
oral,dgl,1,998,42.3679,0.9255

epoch:1000/50, training loss:0.6102379560470581
Train Acc 0.9285
 Acc 0.9262
oral,dgl,1,999,42.4100,0.9262

epoch:1001/50, training loss:0.6099317073822021
Train Acc 0.9293
 Acc 0.9255
oral,dgl,1,1000,42.4522,0.9255

epoch:1002/50, training loss:0.609694242477417
Train Acc 0.9288
 Acc 0.9262
oral,dgl,1,1001,42.4943,0.9262

epoch:1003/50, training loss:0.6096078157424927
Train Acc 0.9293
 Acc 0.9256
oral,dgl,1,1002,42.5365,0.9256

epoch:1004/50, training loss:0.6096423268318176
Train Acc 0.9288
 Acc 0.9261
oral,dgl,1,1003,42.5786,0.9261

epoch:1005/50, training loss:0.6097257733345032
Train Acc 0.9293
 Acc 0.9255
oral,dgl,1,1004,42.6207,0.9255

epoch:1006/50, training loss:0.6097477674484253
Train Acc 0.9285
 Acc 0.9264
oral,dgl,1,1005,42.6627,0.9264

epoch:1007/50, training loss:0.6096594333648682
Train Acc 0.9293
 Acc 0.9253
oral,dgl,1,1006,42.7046,0.9253

epoch:1008/50, training loss:0.6094804406166077
Train Acc 0.9284
 Acc 0.9264
oral,dgl,1,1007,42.7468,0.9264

epoch:1009/50, training loss:0.6092773675918579
Train Acc 0.9295
 Acc 0.9254
oral,dgl,1,1008,42.7889,0.9254

epoch:1010/50, training loss:0.6091009974479675
Train Acc 0.9287
 Acc 0.9265
oral,dgl,1,1009,42.8310,0.9265

epoch:1011/50, training loss:0.6089668869972229
Train Acc 0.9296
 Acc 0.9257
oral,dgl,1,1010,42.8730,0.9257

epoch:1012/50, training loss:0.6088711619377136
Train Acc 0.9289
 Acc 0.9263
oral,dgl,1,1011,42.9152,0.9263

epoch:1013/50, training loss:0.6087344884872437
Train Acc 0.9295
 Acc 0.9260
oral,dgl,1,1012,42.9573,0.9260

epoch:1014/50, training loss:0.6086101531982422
Train Acc 0.9292
 Acc 0.9260
oral,dgl,1,1013,42.9994,0.9260

epoch:1015/50, training loss:0.6085185408592224
Train Acc 0.9291
 Acc 0.9264
oral,dgl,1,1014,43.0414,0.9264

epoch:1016/50, training loss:0.6084801554679871
Train Acc 0.9296
 Acc 0.9255
oral,dgl,1,1015,43.0835,0.9255

epoch:1017/50, training loss:0.6084900498390198
Train Acc 0.9287
 Acc 0.9268
new best val f1: 0.9267728648702298
oral,dgl,1,1016,43.1256,0.9268

epoch:1018/50, training loss:0.6085620522499084
Train Acc 0.9298
 Acc 0.9249
oral,dgl,1,1017,43.1677,0.9249

epoch:1019/50, training loss:0.6087636351585388
Train Acc 0.9283
 Acc 0.9270
new best val f1: 0.9269845996063057
oral,dgl,1,1018,43.2097,0.9270

epoch:1020/50, training loss:0.6090660691261292
Train Acc 0.9301
 Acc 0.9241
oral,dgl,1,1019,43.2519,0.9241

epoch:1021/50, training loss:0.6096235513687134
Train Acc 0.9274
 Acc 0.9271
new best val f1: 0.9271202421716044
oral,dgl,1,1020,43.2939,0.9271

epoch:1022/50, training loss:0.6101586818695068
Train Acc 0.9300
 Acc 0.9230
oral,dgl,1,1021,43.3359,0.9230

epoch:1023/50, training loss:0.6109359264373779
Train Acc 0.9262
 Acc 0.9267
oral,dgl,1,1022,43.3780,0.9267

epoch:1024/50, training loss:0.6110695004463196
Train Acc 0.9299
 Acc 0.9227
oral,dgl,1,1023,43.4201,0.9227

epoch:1025/50, training loss:0.6111387610435486
Train Acc 0.9259
 Acc 0.9268
oral,dgl,1,1024,43.4623,0.9268

epoch:1026/50, training loss:0.6099033951759338
Train Acc 0.9299
 Acc 0.9246
oral,dgl,1,1025,43.5042,0.9246

epoch:1027/50, training loss:0.6086674928665161
Train Acc 0.9279
 Acc 0.9263
oral,dgl,1,1026,43.5464,0.9263

epoch:1028/50, training loss:0.6076663732528687
Train Acc 0.9296
 Acc 0.9268
oral,dgl,1,1027,43.5886,0.9268

epoch:1029/50, training loss:0.6075288653373718
Train Acc 0.9300
 Acc 0.9249
oral,dgl,1,1028,43.6306,0.9249

epoch:1030/50, training loss:0.6080267429351807
Train Acc 0.9283
 Acc 0.9271
oral,dgl,1,1029,43.6725,0.9271

epoch:1031/50, training loss:0.6086646318435669
Train Acc 0.9302
 Acc 0.9237
oral,dgl,1,1030,43.7146,0.9237

epoch:1032/50, training loss:0.6093149781227112
Train Acc 0.9271
 Acc 0.9270
oral,dgl,1,1031,43.7567,0.9270

epoch:1033/50, training loss:0.6091638207435608
Train Acc 0.9301
 Acc 0.9240
oral,dgl,1,1032,43.7988,0.9240

epoch:1034/50, training loss:0.6088904738426208
Train Acc 0.9273
 Acc 0.9271
oral,dgl,1,1033,43.8408,0.9271

epoch:1035/50, training loss:0.6079012155532837
Train Acc 0.9301
 Acc 0.9254
oral,dgl,1,1034,43.8831,0.9254

epoch:1036/50, training loss:0.6070786118507385
Train Acc 0.9287
 Acc 0.9265
oral,dgl,1,1035,43.9252,0.9265

epoch:1037/50, training loss:0.6065911650657654
Train Acc 0.9298
 Acc 0.9264
oral,dgl,1,1036,43.9672,0.9264

epoch:1038/50, training loss:0.606579065322876
Train Acc 0.9297
 Acc 0.9259
oral,dgl,1,1037,44.0094,0.9259

epoch:1039/50, training loss:0.6068136692047119
Train Acc 0.9290
 Acc 0.9269
oral,dgl,1,1038,44.0515,0.9269

epoch:1040/50, training loss:0.6070389747619629
Train Acc 0.9299
 Acc 0.9256
oral,dgl,1,1039,44.0936,0.9256

epoch:1041/50, training loss:0.6070169806480408
Train Acc 0.9289
 Acc 0.9269
oral,dgl,1,1040,44.1355,0.9269

epoch:1042/50, training loss:0.6067516207695007
Train Acc 0.9300
 Acc 0.9261
oral,dgl,1,1041,44.1776,0.9261

epoch:1043/50, training loss:0.6063569188117981
Train Acc 0.9292
 Acc 0.9269
oral,dgl,1,1042,44.2197,0.9269

epoch:1044/50, training loss:0.6060095429420471
Train Acc 0.9300
 Acc 0.9266
oral,dgl,1,1043,44.2617,0.9266

epoch:1045/50, training loss:0.6058578491210938
Train Acc 0.9298
 Acc 0.9264
oral,dgl,1,1044,44.3039,0.9264

epoch:1046/50, training loss:0.6058464646339417
Train Acc 0.9297
 Acc 0.9268
oral,dgl,1,1045,44.3460,0.9268

epoch:1047/50, training loss:0.6058906316757202
Train Acc 0.9300
 Acc 0.9259
oral,dgl,1,1046,44.3881,0.9259

epoch:1048/50, training loss:0.6058796048164368
Train Acc 0.9292
 Acc 0.9270
oral,dgl,1,1047,44.4300,0.9270

epoch:1049/50, training loss:0.6057597994804382
Train Acc 0.9301
 Acc 0.9260
oral,dgl,1,1048,44.4722,0.9260

epoch:1050/50, training loss:0.6056295037269592
Train Acc 0.9292
 Acc 0.9272
new best val f1: 0.9271864092766281
oral,dgl,1,1049,44.5143,0.9272

epoch:1051/50, training loss:0.6055253744125366
Train Acc 0.9302
 Acc 0.9262
oral,dgl,1,1050,44.5564,0.9262

epoch:1052/50, training loss:0.6054584980010986
Train Acc 0.9294
 Acc 0.9272
new best val f1: 0.9272161844738888
oral,dgl,1,1051,44.5985,0.9272

epoch:1053/50, training loss:0.6054311394691467
Train Acc 0.9303
 Acc 0.9262
oral,dgl,1,1052,44.6404,0.9262

epoch:1054/50, training loss:0.6054157614707947
Train Acc 0.9295
 Acc 0.9271
oral,dgl,1,1053,44.6826,0.9271

epoch:1055/50, training loss:0.6052952408790588
Train Acc 0.9302
 Acc 0.9265
oral,dgl,1,1054,44.7248,0.9265

epoch:1056/50, training loss:0.6051235795021057
Train Acc 0.9297
 Acc 0.9270
oral,dgl,1,1055,44.7669,0.9270

epoch:1057/50, training loss:0.6049108505249023
Train Acc 0.9303
 Acc 0.9265
oral,dgl,1,1056,44.8088,0.9265

epoch:1058/50, training loss:0.6047716736793518
Train Acc 0.9297
 Acc 0.9270
oral,dgl,1,1057,44.8510,0.9270

epoch:1059/50, training loss:0.6046972274780273
Train Acc 0.9302
 Acc 0.9266
oral,dgl,1,1058,44.8930,0.9266

epoch:1060/50, training loss:0.6046260595321655
Train Acc 0.9299
 Acc 0.9268
oral,dgl,1,1059,44.9351,0.9268

epoch:1061/50, training loss:0.6045496463775635
Train Acc 0.9300
 Acc 0.9268
oral,dgl,1,1060,44.9772,0.9268

epoch:1062/50, training loss:0.6044650077819824
Train Acc 0.9301
 Acc 0.9269
oral,dgl,1,1061,45.0193,0.9269

epoch:1063/50, training loss:0.6043777465820312
Train Acc 0.9301
 Acc 0.9269
oral,dgl,1,1062,45.0615,0.9269

epoch:1064/50, training loss:0.604280412197113
Train Acc 0.9302
 Acc 0.9268
oral,dgl,1,1063,45.1035,0.9268

epoch:1065/50, training loss:0.6041737794876099
Train Acc 0.9300
 Acc 0.9271
oral,dgl,1,1064,45.1457,0.9271

epoch:1066/50, training loss:0.6040674448013306
Train Acc 0.9302
 Acc 0.9269
oral,dgl,1,1065,45.1878,0.9269

epoch:1067/50, training loss:0.6039917469024658
Train Acc 0.9302
 Acc 0.9269
oral,dgl,1,1066,45.2299,0.9269

epoch:1068/50, training loss:0.6039445996284485
Train Acc 0.9301
 Acc 0.9269
oral,dgl,1,1067,45.2719,0.9269

epoch:1069/50, training loss:0.6039090156555176
Train Acc 0.9301
 Acc 0.9269
oral,dgl,1,1068,45.3141,0.9269

epoch:1070/50, training loss:0.6038388013839722
Train Acc 0.9302
 Acc 0.9267
oral,dgl,1,1069,45.3561,0.9267

epoch:1071/50, training loss:0.6037417054176331
Train Acc 0.9302
 Acc 0.9270
oral,dgl,1,1070,45.3982,0.9270

epoch:1072/50, training loss:0.6036392450332642
Train Acc 0.9302
 Acc 0.9267
oral,dgl,1,1071,45.4403,0.9267

epoch:1073/50, training loss:0.6035163402557373
Train Acc 0.9300
 Acc 0.9271
oral,dgl,1,1072,45.4824,0.9271

epoch:1074/50, training loss:0.6034119725227356
Train Acc 0.9303
 Acc 0.9269
oral,dgl,1,1073,45.5246,0.9269

epoch:1075/50, training loss:0.603318989276886
Train Acc 0.9302
 Acc 0.9271
oral,dgl,1,1074,45.5667,0.9271

epoch:1076/50, training loss:0.6032336950302124
Train Acc 0.9304
 Acc 0.9270
oral,dgl,1,1075,45.6086,0.9270

epoch:1077/50, training loss:0.6031743884086609
Train Acc 0.9301
 Acc 0.9271
oral,dgl,1,1076,45.6507,0.9271

epoch:1078/50, training loss:0.603134036064148
Train Acc 0.9304
 Acc 0.9269
oral,dgl,1,1077,45.6928,0.9269

epoch:1079/50, training loss:0.6031095385551453
Train Acc 0.9301
 Acc 0.9273
new best val f1: 0.9272889682894149
oral,dgl,1,1078,45.7349,0.9273

epoch:1080/50, training loss:0.6031010746955872
Train Acc 0.9305
 Acc 0.9266
oral,dgl,1,1079,45.7769,0.9266

epoch:1081/50, training loss:0.6031233668327332
Train Acc 0.9298
 Acc 0.9275
new best val f1: 0.9274775445387325
oral,dgl,1,1080,45.8191,0.9275

epoch:1082/50, training loss:0.6031360626220703
Train Acc 0.9306
 Acc 0.9264
oral,dgl,1,1081,45.8613,0.9264

epoch:1083/50, training loss:0.6031943559646606
Train Acc 0.9295
 Acc 0.9277
new best val f1: 0.9276859709195573
oral,dgl,1,1082,45.9034,0.9277

epoch:1084/50, training loss:0.6031714677810669
Train Acc 0.9306
 Acc 0.9264
oral,dgl,1,1083,45.9455,0.9264

epoch:1085/50, training loss:0.6031169891357422
Train Acc 0.9294
 Acc 0.9277
new best val f1: 0.9276925876300597
oral,dgl,1,1084,45.9875,0.9277

epoch:1086/50, training loss:0.6028830409049988
Train Acc 0.9307
 Acc 0.9264
oral,dgl,1,1085,46.0295,0.9264

epoch:1087/50, training loss:0.6026327610015869
Train Acc 0.9297
 Acc 0.9275
oral,dgl,1,1086,46.0717,0.9275

epoch:1088/50, training loss:0.6024135947227478
Train Acc 0.9306
 Acc 0.9268
oral,dgl,1,1087,46.1138,0.9268

epoch:1089/50, training loss:0.6022557616233826
Train Acc 0.9300
 Acc 0.9274
oral,dgl,1,1088,46.1558,0.9274

epoch:1090/50, training loss:0.6021245121955872
Train Acc 0.9306
 Acc 0.9270
oral,dgl,1,1089,46.1981,0.9270

epoch:1091/50, training loss:0.6020489931106567
Train Acc 0.9303
 Acc 0.9274
oral,dgl,1,1090,46.2402,0.9274

epoch:1092/50, training loss:0.6019925475120544
Train Acc 0.9305
 Acc 0.9272
oral,dgl,1,1091,46.2823,0.9272

epoch:1093/50, training loss:0.6019009947776794
Train Acc 0.9305
 Acc 0.9273
oral,dgl,1,1092,46.3243,0.9273

epoch:1094/50, training loss:0.6017885804176331
Train Acc 0.9304
 Acc 0.9274
oral,dgl,1,1093,46.3665,0.9274

epoch:1095/50, training loss:0.6016944050788879
Train Acc 0.9306
 Acc 0.9269
oral,dgl,1,1094,46.4086,0.9269

epoch:1096/50, training loss:0.6016255021095276
Train Acc 0.9303
 Acc 0.9276
oral,dgl,1,1095,46.4508,0.9276

epoch:1097/50, training loss:0.6016111373901367
Train Acc 0.9307
 Acc 0.9267
oral,dgl,1,1096,46.4929,0.9267

epoch:1098/50, training loss:0.6016193628311157
Train Acc 0.9300
 Acc 0.9278
new best val f1: 0.927768679800837
oral,dgl,1,1097,46.5349,0.9278

epoch:1099/50, training loss:0.6016189455986023
Train Acc 0.9307
 Acc 0.9264
oral,dgl,1,1098,46.5771,0.9264

epoch:1100/50, training loss:0.6015989184379578
Train Acc 0.9298
 Acc 0.9279
new best val f1: 0.9279043223661356
oral,dgl,1,1099,46.6192,0.9279

epoch:1101/50, training loss:0.6015103459358215
Train Acc 0.9308
 Acc 0.9265
oral,dgl,1,1100,46.6614,0.9265

epoch:1102/50, training loss:0.601398229598999
Train Acc 0.9298
 Acc 0.9279
oral,dgl,1,1101,46.7035,0.9279

epoch:1103/50, training loss:0.6013160347938538
Train Acc 0.9309
 Acc 0.9264
oral,dgl,1,1102,46.7456,0.9264

epoch:1104/50, training loss:0.6013478636741638
Train Acc 0.9297
 Acc 0.9280
new best val f1: 0.9280267315104296
oral,dgl,1,1103,46.7876,0.9280

epoch:1105/50, training loss:0.6014635562896729
Train Acc 0.9310
 Acc 0.9260
oral,dgl,1,1104,46.8298,0.9260

epoch:1106/50, training loss:0.6016930937767029
Train Acc 0.9293
 Acc 0.9281
new best val f1: 0.9280962069707045
oral,dgl,1,1105,46.8719,0.9281

epoch:1107/50, training loss:0.6019895076751709
Train Acc 0.9313
 Acc 0.9250
oral,dgl,1,1106,46.9139,0.9250

epoch:1108/50, training loss:0.6025394797325134
Train Acc 0.9285
 Acc 0.9279
oral,dgl,1,1107,46.9561,0.9279

epoch:1109/50, training loss:0.6029747128486633
Train Acc 0.9313
 Acc 0.9242
oral,dgl,1,1108,46.9981,0.9242

epoch:1110/50, training loss:0.6036137938499451
Train Acc 0.9276
 Acc 0.9275
oral,dgl,1,1109,47.0402,0.9275

epoch:1111/50, training loss:0.6036274433135986
Train Acc 0.9309
 Acc 0.9241
oral,dgl,1,1110,47.0823,0.9241

epoch:1112/50, training loss:0.6037260293960571
Train Acc 0.9274
 Acc 0.9277
oral,dgl,1,1111,47.1244,0.9277

epoch:1113/50, training loss:0.6025699973106384
Train Acc 0.9311
 Acc 0.9258
oral,dgl,1,1112,47.1665,0.9258

epoch:1114/50, training loss:0.601460874080658
Train Acc 0.9290
 Acc 0.9277
oral,dgl,1,1113,47.2085,0.9277

epoch:1115/50, training loss:0.6004009246826172
Train Acc 0.9309
 Acc 0.9276
oral,dgl,1,1114,47.2506,0.9276

epoch:1116/50, training loss:0.6000211834907532
Train Acc 0.9309
 Acc 0.9268
oral,dgl,1,1115,47.2927,0.9268

epoch:1117/50, training loss:0.6001493334770203
Train Acc 0.9301
 Acc 0.9281
oral,dgl,1,1116,47.3347,0.9281

epoch:1118/50, training loss:0.6003224849700928
Train Acc 0.9312
 Acc 0.9265
oral,dgl,1,1117,47.3769,0.9265

epoch:1119/50, training loss:0.6004265546798706
Train Acc 0.9297
 Acc 0.9280
oral,dgl,1,1118,47.4189,0.9280

epoch:1120/50, training loss:0.6003605723381042
Train Acc 0.9311
 Acc 0.9266
oral,dgl,1,1119,47.4609,0.9266

epoch:1121/50, training loss:0.6002458930015564
Train Acc 0.9297
 Acc 0.9279
oral,dgl,1,1120,47.5031,0.9279

epoch:1122/50, training loss:0.5999757647514343
Train Acc 0.9309
 Acc 0.9270
oral,dgl,1,1121,47.5452,0.9270

epoch:1123/50, training loss:0.5996723175048828
Train Acc 0.9303
 Acc 0.9277
oral,dgl,1,1122,47.5874,0.9277

epoch:1124/50, training loss:0.5993883013725281
Train Acc 0.9308
 Acc 0.9278
oral,dgl,1,1123,47.6294,0.9278

epoch:1125/50, training loss:0.5992780923843384
Train Acc 0.9309
 Acc 0.9273
oral,dgl,1,1124,47.6713,0.9273

epoch:1126/50, training loss:0.5992450714111328
Train Acc 0.9304
 Acc 0.9279
oral,dgl,1,1125,47.7134,0.9279

epoch:1127/50, training loss:0.5992134809494019
Train Acc 0.9310
 Acc 0.9272
oral,dgl,1,1126,47.7555,0.9272

epoch:1128/50, training loss:0.5991845726966858
Train Acc 0.9304
 Acc 0.9281
new best val f1: 0.9281160571022117
oral,dgl,1,1127,47.7975,0.9281

epoch:1129/50, training loss:0.5991370677947998
Train Acc 0.9312
 Acc 0.9271
oral,dgl,1,1128,47.8398,0.9271

epoch:1130/50, training loss:0.5990892052650452
Train Acc 0.9303
 Acc 0.9281
oral,dgl,1,1129,47.8819,0.9281

epoch:1131/50, training loss:0.5990447402000427
Train Acc 0.9312
 Acc 0.9272
oral,dgl,1,1130,47.9240,0.9272

epoch:1132/50, training loss:0.5990003943443298
Train Acc 0.9303
 Acc 0.9282
new best val f1: 0.9282318495360031
oral,dgl,1,1131,47.9660,0.9282

epoch:1133/50, training loss:0.5989691615104675
Train Acc 0.9313
 Acc 0.9269
oral,dgl,1,1132,48.0081,0.9269

epoch:1134/50, training loss:0.5989485383033752
Train Acc 0.9301
 Acc 0.9281
oral,dgl,1,1133,48.0502,0.9281

epoch:1135/50, training loss:0.5988890528678894
Train Acc 0.9313
 Acc 0.9269
oral,dgl,1,1134,48.0924,0.9269

epoch:1136/50, training loss:0.5988333821296692
Train Acc 0.9300
 Acc 0.9283
new best val f1: 0.9282649330885151
oral,dgl,1,1135,48.1345,0.9283

epoch:1137/50, training loss:0.5987106561660767
Train Acc 0.9315
 Acc 0.9270
oral,dgl,1,1136,48.1765,0.9270

epoch:1138/50, training loss:0.5985707640647888
Train Acc 0.9302
 Acc 0.9282
oral,dgl,1,1137,48.2187,0.9282

epoch:1139/50, training loss:0.5984475612640381
Train Acc 0.9314
 Acc 0.9272
oral,dgl,1,1138,48.2608,0.9272

epoch:1140/50, training loss:0.5983659029006958
Train Acc 0.9304
 Acc 0.9283
new best val f1: 0.9283145584172828
oral,dgl,1,1139,48.3030,0.9283

epoch:1141/50, training loss:0.5983248949050903
Train Acc 0.9315
 Acc 0.9273
oral,dgl,1,1140,48.3450,0.9273

epoch:1142/50, training loss:0.5983334183692932
Train Acc 0.9305
 Acc 0.9284
new best val f1: 0.9283575670355483
oral,dgl,1,1141,48.3870,0.9284

epoch:1143/50, training loss:0.5983163118362427
Train Acc 0.9315
 Acc 0.9271
oral,dgl,1,1142,48.4291,0.9271

epoch:1144/50, training loss:0.5983919501304626
Train Acc 0.9303
 Acc 0.9286
new best val f1: 0.9285792268373778
oral,dgl,1,1143,48.4712,0.9286

epoch:1145/50, training loss:0.5984271168708801
Train Acc 0.9318
 Acc 0.9268
oral,dgl,1,1144,48.5133,0.9268

epoch:1146/50, training loss:0.5985569357872009
Train Acc 0.9300
 Acc 0.9287
new best val f1: 0.9286520106529039
oral,dgl,1,1145,48.5554,0.9287

epoch:1147/50, training loss:0.5985742211341858
Train Acc 0.9318
 Acc 0.9265
oral,dgl,1,1146,48.5976,0.9265

epoch:1148/50, training loss:0.59861820936203
Train Acc 0.9297
 Acc 0.9287
new best val f1: 0.9287214861131788
oral,dgl,1,1147,48.6398,0.9287

epoch:1149/50, training loss:0.59847491979599
Train Acc 0.9319
 Acc 0.9265
oral,dgl,1,1148,48.6818,0.9265

epoch:1150/50, training loss:0.598355233669281
Train Acc 0.9299
 Acc 0.9286
oral,dgl,1,1149,48.7239,0.9286

epoch:1151/50, training loss:0.5980976819992065
Train Acc 0.9318
 Acc 0.9268
oral,dgl,1,1150,48.7662,0.9268

epoch:1152/50, training loss:0.5978795289993286
Train Acc 0.9300
 Acc 0.9286
oral,dgl,1,1151,48.8083,0.9286

epoch:1153/50, training loss:0.5976672172546387
Train Acc 0.9317
 Acc 0.9271
oral,dgl,1,1152,48.8504,0.9271

epoch:1154/50, training loss:0.5974770784378052
Train Acc 0.9305
 Acc 0.9284
oral,dgl,1,1153,48.8925,0.9284

epoch:1155/50, training loss:0.5972074866294861
Train Acc 0.9317
 Acc 0.9277
oral,dgl,1,1154,48.9344,0.9277

epoch:1156/50, training loss:0.5969473719596863
Train Acc 0.9309
 Acc 0.9282
oral,dgl,1,1155,48.9766,0.9282

epoch:1157/50, training loss:0.5967655777931213
Train Acc 0.9314
 Acc 0.9282
oral,dgl,1,1156,49.0187,0.9282

epoch:1158/50, training loss:0.5966863036155701
Train Acc 0.9314
 Acc 0.9277
oral,dgl,1,1157,49.0608,0.9277

epoch:1159/50, training loss:0.5966742038726807
Train Acc 0.9309
 Acc 0.9284
oral,dgl,1,1158,49.1027,0.9284

epoch:1160/50, training loss:0.5967125296592712
Train Acc 0.9316
 Acc 0.9273
oral,dgl,1,1159,49.1448,0.9273

epoch:1161/50, training loss:0.5967787504196167
Train Acc 0.9306
 Acc 0.9286
oral,dgl,1,1160,49.1869,0.9286

epoch:1162/50, training loss:0.5968231558799744
Train Acc 0.9318
 Acc 0.9271
oral,dgl,1,1161,49.2290,0.9271

epoch:1163/50, training loss:0.5968804359436035
Train Acc 0.9303
 Acc 0.9287
oral,dgl,1,1162,49.2711,0.9287

epoch:1164/50, training loss:0.5968416929244995
Train Acc 0.9318
 Acc 0.9271
oral,dgl,1,1163,49.3132,0.9271

epoch:1165/50, training loss:0.5967513918876648
Train Acc 0.9305
 Acc 0.9286
oral,dgl,1,1164,49.3552,0.9286

epoch:1166/50, training loss:0.5966073870658875
Train Acc 0.9318
 Acc 0.9275
oral,dgl,1,1165,49.3973,0.9275

epoch:1167/50, training loss:0.596467137336731
Train Acc 0.9307
 Acc 0.9286
oral,dgl,1,1166,49.4394,0.9286

epoch:1168/50, training loss:0.5962148308753967
Train Acc 0.9317
 Acc 0.9280
oral,dgl,1,1167,49.4814,0.9280

epoch:1169/50, training loss:0.5960209369659424
Train Acc 0.9310
 Acc 0.9283
oral,dgl,1,1168,49.5236,0.9283

epoch:1170/50, training loss:0.5958316922187805
Train Acc 0.9315
 Acc 0.9284
oral,dgl,1,1169,49.5656,0.9284

epoch:1171/50, training loss:0.5956965088844299
Train Acc 0.9314
 Acc 0.9282
oral,dgl,1,1170,49.6078,0.9282

epoch:1172/50, training loss:0.5955970883369446
Train Acc 0.9314
 Acc 0.9284
oral,dgl,1,1171,49.6498,0.9284

epoch:1173/50, training loss:0.595549464225769
Train Acc 0.9316
 Acc 0.9279
oral,dgl,1,1172,49.6918,0.9279

epoch:1174/50, training loss:0.5955263376235962
Train Acc 0.9311
 Acc 0.9286
oral,dgl,1,1173,49.7339,0.9286

epoch:1175/50, training loss:0.5955249071121216
Train Acc 0.9318
 Acc 0.9275
oral,dgl,1,1174,49.7760,0.9275

epoch:1176/50, training loss:0.5956056714057922
Train Acc 0.9307
 Acc 0.9287
oral,dgl,1,1175,49.8182,0.9287

epoch:1177/50, training loss:0.5958467721939087
Train Acc 0.9319
 Acc 0.9266
oral,dgl,1,1176,49.8602,0.9266

epoch:1178/50, training loss:0.5963699221611023
Train Acc 0.9299
 Acc 0.9286
oral,dgl,1,1177,49.9024,0.9286

epoch:1179/50, training loss:0.5969712138175964
Train Acc 0.9320
 Acc 0.9251
oral,dgl,1,1178,49.9444,0.9251

epoch:1180/50, training loss:0.5980625152587891
Train Acc 0.9286
 Acc 0.9280
oral,dgl,1,1179,49.9866,0.9280

epoch:1181/50, training loss:0.5986668467521667
Train Acc 0.9314
 Acc 0.9241
oral,dgl,1,1180,50.0287,0.9241

epoch:1182/50, training loss:0.5995358228683472
Train Acc 0.9274
 Acc 0.9283
oral,dgl,1,1181,50.0706,0.9283

epoch:1183/50, training loss:0.598503053188324
Train Acc 0.9315
 Acc 0.9255
oral,dgl,1,1182,50.1128,0.9255

epoch:1184/50, training loss:0.5972831845283508
Train Acc 0.9290
 Acc 0.9287
oral,dgl,1,1183,50.1549,0.9287

epoch:1185/50, training loss:0.5954777002334595
Train Acc 0.9321
 Acc 0.9281
oral,dgl,1,1184,50.1970,0.9281

epoch:1186/50, training loss:0.5945549607276917
Train Acc 0.9314
 Acc 0.9275
oral,dgl,1,1185,50.2391,0.9275

epoch:1187/50, training loss:0.594839870929718
Train Acc 0.9308
 Acc 0.9287
oral,dgl,1,1186,50.2812,0.9287

epoch:1188/50, training loss:0.5959570407867432
Train Acc 0.9322
 Acc 0.9250
oral,dgl,1,1187,50.3233,0.9250

epoch:1189/50, training loss:0.5976653099060059
Train Acc 0.9285
 Acc 0.9278
oral,dgl,1,1188,50.3654,0.9278

epoch:1190/50, training loss:0.5985631346702576
Train Acc 0.9314
 Acc 0.9239
oral,dgl,1,1189,50.4076,0.9239

epoch:1191/50, training loss:0.5994830131530762
Train Acc 0.9272
 Acc 0.9285
oral,dgl,1,1190,50.4498,0.9285

epoch:1192/50, training loss:0.5979251861572266
Train Acc 0.9319
 Acc 0.9261
oral,dgl,1,1191,50.4921,0.9261

epoch:1193/50, training loss:0.5959789752960205
Train Acc 0.9296
 Acc 0.9286
oral,dgl,1,1192,50.5342,0.9286

epoch:1194/50, training loss:0.5943174958229065
Train Acc 0.9319
 Acc 0.9287
new best val f1: 0.9287413362446859
oral,dgl,1,1193,50.5762,0.9287

epoch:1195/50, training loss:0.5945210456848145
Train Acc 0.9318
 Acc 0.9260
oral,dgl,1,1194,50.6183,0.9260

epoch:1196/50, training loss:0.5960900783538818
Train Acc 0.9293
 Acc 0.9285
oral,dgl,1,1195,50.6604,0.9285

epoch:1197/50, training loss:0.5966997146606445
Train Acc 0.9319
 Acc 0.9256
oral,dgl,1,1196,50.7025,0.9256

epoch:1198/50, training loss:0.5962451100349426
Train Acc 0.9291
 Acc 0.9291
new best val f1: 0.9290920219013118
oral,dgl,1,1197,50.7444,0.9291

epoch:1199/50, training loss:0.5946553349494934
Train Acc 0.9324
 Acc 0.9283
oral,dgl,1,1198,50.7865,0.9283

epoch:1200/50, training loss:0.5938838720321655
Train Acc 0.9313
 Acc 0.9281
oral,dgl,1,1199,50.8287,0.9281

epoch:1201/50, training loss:0.5940841436386108
Train Acc 0.9313
 Acc 0.9287
oral,dgl,1,1200,50.8708,0.9287

epoch:1202/50, training loss:0.5946800708770752
Train Acc 0.9320
 Acc 0.9268
oral,dgl,1,1201,50.9129,0.9268

epoch:1203/50, training loss:0.5949726700782776
Train Acc 0.9301
 Acc 0.9290
oral,dgl,1,1202,50.9550,0.9290

epoch:1204/50, training loss:0.5943607091903687
Train Acc 0.9323
 Acc 0.9276
oral,dgl,1,1203,50.9971,0.9276

epoch:1205/50, training loss:0.5937029123306274
Train Acc 0.9308
 Acc 0.9288
oral,dgl,1,1204,51.0392,0.9288

epoch:1206/50, training loss:0.5934008955955505
Train Acc 0.9319
 Acc 0.9284
oral,dgl,1,1205,51.0811,0.9284

epoch:1207/50, training loss:0.59365314245224
Train Acc 0.9315
 Acc 0.9277
oral,dgl,1,1206,51.1233,0.9277

epoch:1208/50, training loss:0.5940046310424805
Train Acc 0.9309
 Acc 0.9286
oral,dgl,1,1207,51.1655,0.9286

epoch:1209/50, training loss:0.5939932465553284
Train Acc 0.9320
 Acc 0.9280
oral,dgl,1,1208,51.2076,0.9280

epoch:1210/50, training loss:0.5935004353523254
Train Acc 0.9310
 Acc 0.9289
oral,dgl,1,1209,51.2498,0.9289

epoch:1211/50, training loss:0.5928487181663513
Train Acc 0.9322
 Acc 0.9288
oral,dgl,1,1210,51.2920,0.9288

epoch:1212/50, training loss:0.5926159024238586
Train Acc 0.9320
 Acc 0.9285
oral,dgl,1,1211,51.3342,0.9285

epoch:1213/50, training loss:0.5927078723907471
Train Acc 0.9317
 Acc 0.9288
oral,dgl,1,1212,51.3763,0.9288

epoch:1214/50, training loss:0.5928323864936829
Train Acc 0.9320
 Acc 0.9283
oral,dgl,1,1213,51.4184,0.9283

epoch:1215/50, training loss:0.5928216576576233
Train Acc 0.9317
 Acc 0.9287
oral,dgl,1,1214,51.4606,0.9287

epoch:1216/50, training loss:0.5926737785339355
Train Acc 0.9320
 Acc 0.9288
oral,dgl,1,1215,51.5027,0.9288

epoch:1217/50, training loss:0.5924163460731506
Train Acc 0.9320
 Acc 0.9287
oral,dgl,1,1216,51.5448,0.9287

epoch:1218/50, training loss:0.5922216176986694
Train Acc 0.9318
 Acc 0.9289
oral,dgl,1,1217,51.5868,0.9289

epoch:1219/50, training loss:0.5921486020088196
Train Acc 0.9321
 Acc 0.9283
oral,dgl,1,1218,51.6289,0.9283

epoch:1220/50, training loss:0.5921651124954224
Train Acc 0.9317
 Acc 0.9289
oral,dgl,1,1219,51.6711,0.9289

epoch:1221/50, training loss:0.592139720916748
Train Acc 0.9322
 Acc 0.9285
oral,dgl,1,1220,51.7132,0.9285

epoch:1222/50, training loss:0.5920474529266357
Train Acc 0.9320
 Acc 0.9289
oral,dgl,1,1221,51.7553,0.9289

epoch:1223/50, training loss:0.5919351577758789
Train Acc 0.9321
 Acc 0.9291
oral,dgl,1,1222,51.7973,0.9291

epoch:1224/50, training loss:0.591831624507904
Train Acc 0.9322
 Acc 0.9285
oral,dgl,1,1223,51.8393,0.9285

epoch:1225/50, training loss:0.5917994976043701
Train Acc 0.9319
 Acc 0.9292
new best val f1: 0.929201197624601
oral,dgl,1,1224,51.8815,0.9292

epoch:1226/50, training loss:0.5918107032775879
Train Acc 0.9324
 Acc 0.9281
oral,dgl,1,1225,51.9237,0.9281

epoch:1227/50, training loss:0.5918590426445007
Train Acc 0.9314
 Acc 0.9292
oral,dgl,1,1226,51.9658,0.9292

epoch:1228/50, training loss:0.5918161273002625
Train Acc 0.9324
 Acc 0.9282
oral,dgl,1,1227,52.0077,0.9282

epoch:1229/50, training loss:0.5916998386383057
Train Acc 0.9315
 Acc 0.9292
oral,dgl,1,1228,52.0499,0.9292

epoch:1230/50, training loss:0.5915516018867493
Train Acc 0.9324
 Acc 0.9284
oral,dgl,1,1229,52.0920,0.9284

epoch:1231/50, training loss:0.5914266705513
Train Acc 0.9318
 Acc 0.9292
oral,dgl,1,1230,52.1341,0.9292

epoch:1232/50, training loss:0.5912926197052002
Train Acc 0.9325
 Acc 0.9287
oral,dgl,1,1231,52.1761,0.9287

epoch:1233/50, training loss:0.5911680459976196
Train Acc 0.9320
 Acc 0.9292
oral,dgl,1,1232,52.2183,0.9292

epoch:1234/50, training loss:0.5911136865615845
Train Acc 0.9323
 Acc 0.9289
oral,dgl,1,1233,52.2604,0.9289

epoch:1235/50, training loss:0.5911095142364502
Train Acc 0.9322
 Acc 0.9289
oral,dgl,1,1234,52.3025,0.9289

epoch:1236/50, training loss:0.5910762548446655
Train Acc 0.9322
 Acc 0.9291
oral,dgl,1,1235,52.3446,0.9291

epoch:1237/50, training loss:0.5909910798072815
Train Acc 0.9323
 Acc 0.9287
oral,dgl,1,1236,52.3867,0.9287

epoch:1238/50, training loss:0.5908367037773132
Train Acc 0.9322
 Acc 0.9291
oral,dgl,1,1237,52.4290,0.9291

epoch:1239/50, training loss:0.5906928181648254
Train Acc 0.9324
 Acc 0.9290
oral,dgl,1,1238,52.4713,0.9290

epoch:1240/50, training loss:0.5906108021736145
Train Acc 0.9322
 Acc 0.9292
new best val f1: 0.929221047756108
oral,dgl,1,1239,52.5134,0.9292

epoch:1241/50, training loss:0.5905729532241821
Train Acc 0.9324
 Acc 0.9290
oral,dgl,1,1240,52.5556,0.9290

epoch:1242/50, training loss:0.5905591249465942
Train Acc 0.9323
 Acc 0.9288
oral,dgl,1,1241,52.5975,0.9288

epoch:1243/50, training loss:0.590533971786499
Train Acc 0.9323
 Acc 0.9293
new best val f1: 0.9292706730848759
oral,dgl,1,1242,52.6397,0.9293

epoch:1244/50, training loss:0.5904996395111084
Train Acc 0.9324
 Acc 0.9285
oral,dgl,1,1243,52.6818,0.9285

epoch:1245/50, training loss:0.5904731750488281
Train Acc 0.9322
 Acc 0.9293
new best val f1: 0.9293070649926389
oral,dgl,1,1244,52.7239,0.9293

epoch:1246/50, training loss:0.590452253818512
Train Acc 0.9326
 Acc 0.9283
oral,dgl,1,1245,52.7660,0.9283

epoch:1247/50, training loss:0.5904574990272522
Train Acc 0.9317
 Acc 0.9295
new best val f1: 0.9294559409789424
oral,dgl,1,1246,52.8079,0.9295

epoch:1248/50, training loss:0.5904414653778076
Train Acc 0.9327
 Acc 0.9283
oral,dgl,1,1247,52.8499,0.9283

epoch:1249/50, training loss:0.5903668403625488
Train Acc 0.9317
 Acc 0.9294
oral,dgl,1,1248,52.8920,0.9294

epoch:1250/50, training loss:0.5901762247085571
Train Acc 0.9326
 Acc 0.9286
oral,dgl,1,1249,52.9340,0.9286

epoch:1251/50, training loss:0.5899665355682373
Train Acc 0.9321
 Acc 0.9294
oral,dgl,1,1250,52.9761,0.9294

epoch:1252/50, training loss:0.5897957682609558
Train Acc 0.9325
 Acc 0.9291
oral,dgl,1,1251,53.0180,0.9291

epoch:1253/50, training loss:0.5896800756454468
Train Acc 0.9324
 Acc 0.9292
oral,dgl,1,1252,53.0601,0.9292

epoch:1254/50, training loss:0.589599072933197
Train Acc 0.9325
 Acc 0.9292
oral,dgl,1,1253,53.1023,0.9292

epoch:1255/50, training loss:0.5895352959632874
Train Acc 0.9325
 Acc 0.9293
oral,dgl,1,1254,53.1445,0.9293

epoch:1256/50, training loss:0.5894700288772583
Train Acc 0.9326
 Acc 0.9293
oral,dgl,1,1255,53.1865,0.9293

epoch:1257/50, training loss:0.589402437210083
Train Acc 0.9325
 Acc 0.9292
oral,dgl,1,1256,53.2286,0.9292

epoch:1258/50, training loss:0.5893308520317078
Train Acc 0.9325
 Acc 0.9293
oral,dgl,1,1257,53.2707,0.9293

epoch:1259/50, training loss:0.5892630219459534
Train Acc 0.9326
 Acc 0.9293
oral,dgl,1,1258,53.3127,0.9293

epoch:1260/50, training loss:0.5891808271408081
Train Acc 0.9326
 Acc 0.9294
oral,dgl,1,1259,53.3548,0.9294

epoch:1261/50, training loss:0.5891119241714478
Train Acc 0.9326
 Acc 0.9294
oral,dgl,1,1260,53.3969,0.9294

epoch:1262/50, training loss:0.5890552997589111
Train Acc 0.9325
 Acc 0.9293
oral,dgl,1,1261,53.4390,0.9293

epoch:1263/50, training loss:0.5890289545059204
Train Acc 0.9326
 Acc 0.9293
oral,dgl,1,1262,53.4810,0.9293

epoch:1264/50, training loss:0.5890511274337769
Train Acc 0.9325
 Acc 0.9293
oral,dgl,1,1263,53.5230,0.9293

epoch:1265/50, training loss:0.5890675187110901
Train Acc 0.9326
 Acc 0.9292
oral,dgl,1,1264,53.5652,0.9292

epoch:1266/50, training loss:0.5890593528747559
Train Acc 0.9325
 Acc 0.9293
oral,dgl,1,1265,53.6073,0.9293

epoch:1267/50, training loss:0.5889488458633423
Train Acc 0.9326
 Acc 0.9292
oral,dgl,1,1266,53.6494,0.9292

epoch:1268/50, training loss:0.5888147354125977
Train Acc 0.9325
 Acc 0.9295
new best val f1: 0.9294956412419566
oral,dgl,1,1267,53.6914,0.9295

epoch:1269/50, training loss:0.5886659026145935
Train Acc 0.9329
 Acc 0.9293
oral,dgl,1,1268,53.7335,0.9293

epoch:1270/50, training loss:0.5885394811630249
Train Acc 0.9324
 Acc 0.9295
new best val f1: 0.9295088746629613
oral,dgl,1,1269,53.7756,0.9295

epoch:1271/50, training loss:0.5884297490119934
Train Acc 0.9329
 Acc 0.9294
oral,dgl,1,1270,53.8177,0.9294

epoch:1272/50, training loss:0.5883563160896301
Train Acc 0.9326
 Acc 0.9294
oral,dgl,1,1271,53.8598,0.9294

epoch:1273/50, training loss:0.5882971286773682
Train Acc 0.9326
 Acc 0.9295
oral,dgl,1,1272,53.9020,0.9295

epoch:1274/50, training loss:0.5882354974746704
Train Acc 0.9329
 Acc 0.9293
oral,dgl,1,1273,53.9441,0.9293

epoch:1275/50, training loss:0.5881845355033875
Train Acc 0.9325
 Acc 0.9297
new best val f1: 0.9296643673597671
oral,dgl,1,1274,53.9862,0.9297

epoch:1276/50, training loss:0.5881510376930237
Train Acc 0.9329
 Acc 0.9289
oral,dgl,1,1275,54.0282,0.9289

epoch:1277/50, training loss:0.5881324410438538
Train Acc 0.9323
 Acc 0.9299
new best val f1: 0.929856251964336
oral,dgl,1,1276,54.0702,0.9299

epoch:1278/50, training loss:0.5881314873695374
Train Acc 0.9331
 Acc 0.9288
oral,dgl,1,1277,54.1123,0.9288

epoch:1279/50, training loss:0.5881176590919495
Train Acc 0.9323
 Acc 0.9298
oral,dgl,1,1278,54.1544,0.9298

epoch:1280/50, training loss:0.5880777835845947
Train Acc 0.9331
 Acc 0.9289
oral,dgl,1,1279,54.1965,0.9289

epoch:1281/50, training loss:0.588032603263855
Train Acc 0.9323
 Acc 0.9298
oral,dgl,1,1280,54.2386,0.9298

epoch:1282/50, training loss:0.5879557132720947
Train Acc 0.9332
 Acc 0.9289
oral,dgl,1,1281,54.2806,0.9289

epoch:1283/50, training loss:0.5878562331199646
Train Acc 0.9324
 Acc 0.9298
oral,dgl,1,1282,54.3228,0.9298

epoch:1284/50, training loss:0.5877414345741272
Train Acc 0.9332
 Acc 0.9291
oral,dgl,1,1283,54.3650,0.9291

epoch:1285/50, training loss:0.5876672863960266
Train Acc 0.9324
 Acc 0.9298
oral,dgl,1,1284,54.4072,0.9298

epoch:1286/50, training loss:0.5876235365867615
Train Acc 0.9332
 Acc 0.9292
oral,dgl,1,1285,54.4492,0.9292

epoch:1287/50, training loss:0.587620198726654
Train Acc 0.9325
 Acc 0.9299
new best val f1: 0.9299058772931037
oral,dgl,1,1286,54.4913,0.9299

epoch:1288/50, training loss:0.5875895619392395
Train Acc 0.9332
 Acc 0.9290
oral,dgl,1,1287,54.5332,0.9290

epoch:1289/50, training loss:0.5876055359840393
Train Acc 0.9324
 Acc 0.9299
new best val f1: 0.9299290357798621
oral,dgl,1,1288,54.5754,0.9299

epoch:1290/50, training loss:0.5875685214996338
Train Acc 0.9332
 Acc 0.9289
oral,dgl,1,1289,54.6175,0.9289

epoch:1291/50, training loss:0.587541937828064
Train Acc 0.9324
 Acc 0.9298
oral,dgl,1,1290,54.6596,0.9298

epoch:1292/50, training loss:0.5873913764953613
Train Acc 0.9332
 Acc 0.9291
oral,dgl,1,1291,54.7017,0.9291

epoch:1293/50, training loss:0.5872619152069092
Train Acc 0.9325
 Acc 0.9300
new best val f1: 0.929981969463881
oral,dgl,1,1292,54.7438,0.9300

epoch:1294/50, training loss:0.5870901942253113
Train Acc 0.9332
 Acc 0.9294
oral,dgl,1,1293,54.7859,0.9294

epoch:1295/50, training loss:0.5869221687316895
Train Acc 0.9327
 Acc 0.9297
oral,dgl,1,1294,54.8281,0.9297

epoch:1296/50, training loss:0.5867481827735901
Train Acc 0.9331
 Acc 0.9297
oral,dgl,1,1295,54.8701,0.9297

epoch:1297/50, training loss:0.5866601467132568
Train Acc 0.9329
 Acc 0.9291
oral,dgl,1,1296,54.9121,0.9291

epoch:1298/50, training loss:0.5866876840591431
Train Acc 0.9327
 Acc 0.9300
new best val f1: 0.9299852778191322
oral,dgl,1,1297,54.9543,0.9300

epoch:1299/50, training loss:0.5868538022041321
Train Acc 0.9333
 Acc 0.9284
oral,dgl,1,1298,54.9965,0.9284

epoch:1300/50, training loss:0.5871453285217285
Train Acc 0.9319
 Acc 0.9300
new best val f1: 0.9300382115031512
oral,dgl,1,1299,55.0385,0.9300

epoch:1301/50, training loss:0.587445080280304
Train Acc 0.9335
 Acc 0.9278
oral,dgl,1,1300,55.0805,0.9278

epoch:1302/50, training loss:0.5877874493598938
Train Acc 0.9315
 Acc 0.9299
oral,dgl,1,1301,55.1228,0.9299

epoch:1303/50, training loss:0.5879279971122742
Train Acc 0.9334
 Acc 0.9274
oral,dgl,1,1302,55.1649,0.9274

epoch:1304/50, training loss:0.5880909562110901
Train Acc 0.9310
 Acc 0.9299
oral,dgl,1,1303,55.2070,0.9299

epoch:1305/50, training loss:0.5880019068717957
Train Acc 0.9335
 Acc 0.9276
oral,dgl,1,1304,55.2491,0.9276

epoch:1306/50, training loss:0.5878426432609558
Train Acc 0.9312
 Acc 0.9301
new best val f1: 0.9300779117661655
oral,dgl,1,1305,55.2911,0.9301

epoch:1307/50, training loss:0.5872309803962708
Train Acc 0.9336
 Acc 0.9284
oral,dgl,1,1306,55.3332,0.9284

epoch:1308/50, training loss:0.5865846872329712
Train Acc 0.9320
 Acc 0.9300
oral,dgl,1,1307,55.3753,0.9300

epoch:1309/50, training loss:0.5860103964805603
Train Acc 0.9333
 Acc 0.9297
oral,dgl,1,1308,55.4173,0.9297

epoch:1310/50, training loss:0.5857651829719543
Train Acc 0.9332
 Acc 0.9295
oral,dgl,1,1309,55.4594,0.9295

epoch:1311/50, training loss:0.5858001112937927
Train Acc 0.9328
 Acc 0.9303
new best val f1: 0.9302896465022414
oral,dgl,1,1310,55.5016,0.9303

epoch:1312/50, training loss:0.586047351360321
Train Acc 0.9336
 Acc 0.9286
oral,dgl,1,1311,55.5437,0.9286

epoch:1313/50, training loss:0.5864203572273254
Train Acc 0.9322
 Acc 0.9302
oral,dgl,1,1312,55.5858,0.9302

epoch:1314/50, training loss:0.5867356657981873
Train Acc 0.9337
 Acc 0.9280
oral,dgl,1,1313,55.6278,0.9280

epoch:1315/50, training loss:0.5870303511619568
Train Acc 0.9316
 Acc 0.9302
oral,dgl,1,1314,55.6699,0.9302

epoch:1316/50, training loss:0.5870876312255859
Train Acc 0.9338
 Acc 0.9277
oral,dgl,1,1315,55.7121,0.9277

epoch:1317/50, training loss:0.5871455669403076
Train Acc 0.9313
 Acc 0.9301
oral,dgl,1,1316,55.7543,0.9301

epoch:1318/50, training loss:0.5868066549301147
Train Acc 0.9337
 Acc 0.9284
oral,dgl,1,1317,55.7963,0.9284

epoch:1319/50, training loss:0.5862360596656799
Train Acc 0.9319
 Acc 0.9302
oral,dgl,1,1318,55.8384,0.9302

epoch:1320/50, training loss:0.5855823159217834
Train Acc 0.9335
 Acc 0.9295
oral,dgl,1,1319,55.8806,0.9295

epoch:1321/50, training loss:0.5852259397506714
Train Acc 0.9329
 Acc 0.9296
oral,dgl,1,1320,55.9227,0.9296

epoch:1322/50, training loss:0.5851951837539673
Train Acc 0.9332
 Acc 0.9300
oral,dgl,1,1321,55.9648,0.9300

epoch:1323/50, training loss:0.5853951573371887
Train Acc 0.9334
 Acc 0.9287
oral,dgl,1,1322,56.0069,0.9287

epoch:1324/50, training loss:0.5857663154602051
Train Acc 0.9324
 Acc 0.9300
oral,dgl,1,1323,56.0489,0.9300

epoch:1325/50, training loss:0.5861722230911255
Train Acc 0.9336
 Acc 0.9277
oral,dgl,1,1324,56.0910,0.9277

epoch:1326/50, training loss:0.5867188572883606
Train Acc 0.9313
 Acc 0.9299
oral,dgl,1,1325,56.1331,0.9299

epoch:1327/50, training loss:0.5867756605148315
Train Acc 0.9335
 Acc 0.9275
oral,dgl,1,1326,56.1751,0.9275

epoch:1328/50, training loss:0.5868333578109741
Train Acc 0.9311
 Acc 0.9301
oral,dgl,1,1327,56.2171,0.9301

epoch:1329/50, training loss:0.5861644744873047
Train Acc 0.9336
 Acc 0.9284
oral,dgl,1,1328,56.2593,0.9284

epoch:1330/50, training loss:0.5854545831680298
Train Acc 0.9320
 Acc 0.9302
oral,dgl,1,1329,56.3014,0.9302

epoch:1331/50, training loss:0.5846624970436096
Train Acc 0.9335
 Acc 0.9297
oral,dgl,1,1330,56.3435,0.9297

epoch:1332/50, training loss:0.5843065977096558
Train Acc 0.9331
 Acc 0.9296
oral,dgl,1,1331,56.3855,0.9296

epoch:1333/50, training loss:0.5843656659126282
Train Acc 0.9331
 Acc 0.9302
oral,dgl,1,1332,56.4276,0.9302

epoch:1334/50, training loss:0.584679365158081
Train Acc 0.9335
 Acc 0.9285
oral,dgl,1,1333,56.4697,0.9285

epoch:1335/50, training loss:0.5852333307266235
Train Acc 0.9323
 Acc 0.9299
oral,dgl,1,1334,56.5119,0.9299

epoch:1336/50, training loss:0.5858399271965027
Train Acc 0.9336
 Acc 0.9273
oral,dgl,1,1335,56.5539,0.9273

epoch:1337/50, training loss:0.5866271257400513
Train Acc 0.9311
 Acc 0.9298
oral,dgl,1,1336,56.5961,0.9298

epoch:1338/50, training loss:0.5866420269012451
Train Acc 0.9334
 Acc 0.9272
oral,dgl,1,1337,56.6382,0.9272

epoch:1339/50, training loss:0.5864503979682922
Train Acc 0.9309
 Acc 0.9301
oral,dgl,1,1338,56.6803,0.9301

epoch:1340/50, training loss:0.5852544903755188
Train Acc 0.9337
 Acc 0.9290
oral,dgl,1,1339,56.7224,0.9290

epoch:1341/50, training loss:0.5842207074165344
Train Acc 0.9325
 Acc 0.9300
oral,dgl,1,1340,56.7643,0.9300

epoch:1342/50, training loss:0.5836832523345947
Train Acc 0.9333
 Acc 0.9301
oral,dgl,1,1341,56.8065,0.9301

epoch:1343/50, training loss:0.5838634371757507
Train Acc 0.9335
 Acc 0.9288
oral,dgl,1,1342,56.8486,0.9288

epoch:1344/50, training loss:0.5843642354011536
Train Acc 0.9325
 Acc 0.9303
new best val f1: 0.9303425801862604
oral,dgl,1,1343,56.8907,0.9303

epoch:1345/50, training loss:0.5845772624015808
Train Acc 0.9338
 Acc 0.9287
oral,dgl,1,1344,56.9327,0.9287

epoch:1346/50, training loss:0.5845235586166382
Train Acc 0.9325
 Acc 0.9302
oral,dgl,1,1345,56.9749,0.9302

epoch:1347/50, training loss:0.5840498805046082
Train Acc 0.9336
 Acc 0.9297
oral,dgl,1,1346,57.0170,0.9297

epoch:1348/50, training loss:0.5835385918617249
Train Acc 0.9332
 Acc 0.9303
oral,dgl,1,1347,57.0591,0.9303

epoch:1349/50, training loss:0.5831784009933472
Train Acc 0.9336
 Acc 0.9304
new best val f1: 0.9303657386730187
oral,dgl,1,1348,57.1011,0.9304

epoch:1350/50, training loss:0.5832250118255615
Train Acc 0.9337
 Acc 0.9291
oral,dgl,1,1349,57.1432,0.9291

epoch:1351/50, training loss:0.5836787819862366
Train Acc 0.9327
 Acc 0.9302
oral,dgl,1,1350,57.1853,0.9302

epoch:1352/50, training loss:0.5840651988983154
Train Acc 0.9337
 Acc 0.9288
oral,dgl,1,1351,57.2274,0.9288

epoch:1353/50, training loss:0.5842676162719727
Train Acc 0.9324
 Acc 0.9301
oral,dgl,1,1352,57.2694,0.9301

epoch:1354/50, training loss:0.5839356184005737
Train Acc 0.9336
 Acc 0.9293
oral,dgl,1,1353,57.3115,0.9293

epoch:1355/50, training loss:0.5834842920303345
Train Acc 0.9328
 Acc 0.9302
oral,dgl,1,1354,57.3537,0.9302

epoch:1356/50, training loss:0.5829852223396301
Train Acc 0.9337
 Acc 0.9304
new best val f1: 0.9304451391990471
oral,dgl,1,1355,57.3958,0.9304

epoch:1357/50, training loss:0.5826894640922546
Train Acc 0.9337
 Acc 0.9298
oral,dgl,1,1356,57.4378,0.9298

epoch:1358/50, training loss:0.5826870203018188
Train Acc 0.9333
 Acc 0.9305
new best val f1: 0.9304782227515591
oral,dgl,1,1357,57.4799,0.9305

epoch:1359/50, training loss:0.5829102396965027
Train Acc 0.9338
 Acc 0.9291
oral,dgl,1,1358,57.5220,0.9291

epoch:1360/50, training loss:0.5832181572914124
Train Acc 0.9329
 Acc 0.9305
new best val f1: 0.9305212313698245
oral,dgl,1,1359,57.5642,0.9305

epoch:1361/50, training loss:0.583313524723053
Train Acc 0.9340
 Acc 0.9290
oral,dgl,1,1360,57.6062,0.9290

epoch:1362/50, training loss:0.5832186341285706
Train Acc 0.9328
 Acc 0.9304
oral,dgl,1,1361,57.6484,0.9304

epoch:1363/50, training loss:0.5828104019165039
Train Acc 0.9339
 Acc 0.9296
oral,dgl,1,1362,57.6906,0.9296

epoch:1364/50, training loss:0.5824456810951233
Train Acc 0.9331
 Acc 0.9304
oral,dgl,1,1363,57.7327,0.9304

epoch:1365/50, training loss:0.5821768045425415
Train Acc 0.9337
 Acc 0.9303
oral,dgl,1,1364,57.7746,0.9303

epoch:1366/50, training loss:0.5820547938346863
Train Acc 0.9336
 Acc 0.9302
oral,dgl,1,1365,57.8168,0.9302

epoch:1367/50, training loss:0.5820435285568237
Train Acc 0.9335
 Acc 0.9305
oral,dgl,1,1366,57.8589,0.9305

epoch:1368/50, training loss:0.5821323394775391
Train Acc 0.9339
 Acc 0.9295
oral,dgl,1,1367,57.9009,0.9295

epoch:1369/50, training loss:0.582244336605072
Train Acc 0.9331
 Acc 0.9306
new best val f1: 0.9306304070931136
oral,dgl,1,1368,57.9431,0.9306

epoch:1370/50, training loss:0.5822820663452148
Train Acc 0.9341
 Acc 0.9293
oral,dgl,1,1369,57.9853,0.9293

epoch:1371/50, training loss:0.5822807550430298
Train Acc 0.9329
 Acc 0.9308
new best val f1: 0.930795824855673
oral,dgl,1,1370,58.0275,0.9308

epoch:1372/50, training loss:0.5822268128395081
Train Acc 0.9342
 Acc 0.9296
oral,dgl,1,1371,58.0695,0.9296

epoch:1373/50, training loss:0.5821933150291443
Train Acc 0.9331
 Acc 0.9307
oral,dgl,1,1372,58.1116,0.9307

epoch:1374/50, training loss:0.5820598006248474
Train Acc 0.9343
 Acc 0.9301
oral,dgl,1,1373,58.1537,0.9301

epoch:1375/50, training loss:0.5819516181945801
Train Acc 0.9335
 Acc 0.9305
oral,dgl,1,1374,58.1959,0.9305

epoch:1376/50, training loss:0.5817752480506897
Train Acc 0.9338
 Acc 0.9304
oral,dgl,1,1375,58.2380,0.9304

epoch:1377/50, training loss:0.5816895961761475
Train Acc 0.9337
 Acc 0.9300
oral,dgl,1,1376,58.2800,0.9300

epoch:1378/50, training loss:0.5816190838813782
Train Acc 0.9335
 Acc 0.9306
oral,dgl,1,1377,58.3220,0.9306

epoch:1379/50, training loss:0.5815832614898682
Train Acc 0.9340
 Acc 0.9295
oral,dgl,1,1378,58.3642,0.9295

epoch:1380/50, training loss:0.5816182494163513
Train Acc 0.9331
 Acc 0.9308
new best val f1: 0.9308123666319289
oral,dgl,1,1379,58.4064,0.9308

epoch:1381/50, training loss:0.5817542672157288
Train Acc 0.9343
 Acc 0.9290
oral,dgl,1,1380,58.4485,0.9290

epoch:1382/50, training loss:0.5820732712745667
Train Acc 0.9327
 Acc 0.9309
new best val f1: 0.9309248507104693
oral,dgl,1,1381,58.4904,0.9309

epoch:1383/50, training loss:0.5824164152145386
Train Acc 0.9344
 Acc 0.9289
oral,dgl,1,1382,58.5326,0.9289

epoch:1384/50, training loss:0.582772433757782
Train Acc 0.9324
 Acc 0.9310
new best val f1: 0.9309678593287347
oral,dgl,1,1383,58.5747,0.9310

epoch:1385/50, training loss:0.5825212001800537
Train Acc 0.9344
 Acc 0.9294
oral,dgl,1,1384,58.6168,0.9294

epoch:1386/50, training loss:0.5822017192840576
Train Acc 0.9329
 Acc 0.9308
oral,dgl,1,1385,58.6587,0.9308

epoch:1387/50, training loss:0.5814959406852722
Train Acc 0.9343
 Acc 0.9303
oral,dgl,1,1386,58.7008,0.9303

epoch:1388/50, training loss:0.580967903137207
Train Acc 0.9337
 Acc 0.9306
oral,dgl,1,1387,58.7430,0.9306

epoch:1389/50, training loss:0.5806350111961365
Train Acc 0.9340
 Acc 0.9308
oral,dgl,1,1388,58.7851,0.9308

epoch:1390/50, training loss:0.580646812915802
Train Acc 0.9342
 Acc 0.9299
oral,dgl,1,1389,58.8272,0.9299

epoch:1391/50, training loss:0.5809500217437744
Train Acc 0.9333
 Acc 0.9310
new best val f1: 0.9309976345259954
oral,dgl,1,1390,58.8692,0.9310

epoch:1392/50, training loss:0.5812951922416687
Train Acc 0.9345
 Acc 0.9295
oral,dgl,1,1391,58.9113,0.9295

epoch:1393/50, training loss:0.5815987586975098
Train Acc 0.9330
 Acc 0.9310
new best val f1: 0.9310373347890096
oral,dgl,1,1392,58.9534,0.9310

epoch:1394/50, training loss:0.5814523100852966
Train Acc 0.9346
 Acc 0.9297
oral,dgl,1,1393,58.9955,0.9297

epoch:1395/50, training loss:0.581150472164154
Train Acc 0.9332
 Acc 0.9309
oral,dgl,1,1394,59.0377,0.9309

epoch:1396/50, training loss:0.5805518627166748
Train Acc 0.9345
 Acc 0.9304
oral,dgl,1,1395,59.0796,0.9304

epoch:1397/50, training loss:0.5801370143890381
Train Acc 0.9338
 Acc 0.9304
oral,dgl,1,1396,59.1219,0.9304

epoch:1398/50, training loss:0.5800410509109497
Train Acc 0.9339
 Acc 0.9310
oral,dgl,1,1397,59.1640,0.9310

epoch:1399/50, training loss:0.5802677869796753
Train Acc 0.9345
 Acc 0.9296
oral,dgl,1,1398,59.2061,0.9296

epoch:1400/50, training loss:0.580666184425354
Train Acc 0.9332
 Acc 0.9310
oral,dgl,1,1399,59.2482,0.9310

epoch:1401/50, training loss:0.5808876752853394
Train Acc 0.9346
 Acc 0.9294
oral,dgl,1,1400,59.2902,0.9294

epoch:1402/50, training loss:0.5810319781303406
Train Acc 0.9329
 Acc 0.9311
new best val f1: 0.9310538765652656
oral,dgl,1,1401,59.3323,0.9311

epoch:1403/50, training loss:0.580889105796814
Train Acc 0.9346
 Acc 0.9298
oral,dgl,1,1402,59.3745,0.9298

epoch:1404/50, training loss:0.5806393027305603
Train Acc 0.9332
 Acc 0.9310
oral,dgl,1,1403,59.4166,0.9310

epoch:1405/50, training loss:0.5800492763519287
Train Acc 0.9346
 Acc 0.9304
oral,dgl,1,1404,59.4588,0.9304

epoch:1406/50, training loss:0.5795978903770447
Train Acc 0.9339
 Acc 0.9307
oral,dgl,1,1405,59.5010,0.9307

epoch:1407/50, training loss:0.5794198513031006
Train Acc 0.9343
 Acc 0.9309
oral,dgl,1,1406,59.5432,0.9309

epoch:1408/50, training loss:0.5794400572776794
Train Acc 0.9346
 Acc 0.9303
oral,dgl,1,1407,59.5853,0.9303

epoch:1409/50, training loss:0.5794692635536194
Train Acc 0.9339
 Acc 0.9309
oral,dgl,1,1408,59.6272,0.9309

epoch:1410/50, training loss:0.5793889164924622
Train Acc 0.9345
 Acc 0.9306
oral,dgl,1,1409,59.6694,0.9306

epoch:1411/50, training loss:0.5793213248252869
Train Acc 0.9342
 Acc 0.9306
oral,dgl,1,1410,59.7116,0.9306

epoch:1412/50, training loss:0.5792875289916992
Train Acc 0.9341
 Acc 0.9309
oral,dgl,1,1411,59.7537,0.9309

epoch:1413/50, training loss:0.5792736411094666
Train Acc 0.9344
 Acc 0.9302
oral,dgl,1,1412,59.7957,0.9302

epoch:1414/50, training loss:0.5792361497879028
Train Acc 0.9337
 Acc 0.9309
oral,dgl,1,1413,59.8378,0.9309

epoch:1415/50, training loss:0.5791362524032593
Train Acc 0.9346
 Acc 0.9303
oral,dgl,1,1414,59.8800,0.9303

epoch:1416/50, training loss:0.5790126323699951
Train Acc 0.9338
 Acc 0.9309
oral,dgl,1,1415,59.9221,0.9309

epoch:1417/50, training loss:0.5788922309875488
Train Acc 0.9345
 Acc 0.9307
oral,dgl,1,1416,59.9642,0.9307

epoch:1418/50, training loss:0.5788329839706421
Train Acc 0.9342
 Acc 0.9306
oral,dgl,1,1417,60.0061,0.9306

epoch:1419/50, training loss:0.5788573026657104
Train Acc 0.9341
 Acc 0.9309
oral,dgl,1,1418,60.0481,0.9309

epoch:1420/50, training loss:0.5788777470588684
Train Acc 0.9345
 Acc 0.9303
oral,dgl,1,1419,60.0902,0.9303

epoch:1421/50, training loss:0.57884281873703
Train Acc 0.9338
 Acc 0.9310
oral,dgl,1,1420,60.1324,0.9310

epoch:1422/50, training loss:0.5787287354469299
Train Acc 0.9345
 Acc 0.9304
oral,dgl,1,1421,60.1746,0.9304

epoch:1423/50, training loss:0.5785847306251526
Train Acc 0.9340
 Acc 0.9310
oral,dgl,1,1422,60.2168,0.9310

epoch:1424/50, training loss:0.5784598588943481
Train Acc 0.9345
 Acc 0.9304
oral,dgl,1,1423,60.2588,0.9304

epoch:1425/50, training loss:0.5783872604370117
Train Acc 0.9340
 Acc 0.9310
oral,dgl,1,1424,60.3009,0.9310

epoch:1426/50, training loss:0.5783637166023254
Train Acc 0.9346
 Acc 0.9306
oral,dgl,1,1425,60.3430,0.9306

epoch:1427/50, training loss:0.5783394575119019
Train Acc 0.9342
 Acc 0.9311
new best val f1: 0.9310704183415215
oral,dgl,1,1426,60.3851,0.9311

epoch:1428/50, training loss:0.5782484412193298
Train Acc 0.9346
 Acc 0.9308
oral,dgl,1,1427,60.4272,0.9308

epoch:1429/50, training loss:0.5781350135803223
Train Acc 0.9343
 Acc 0.9309
oral,dgl,1,1428,60.4691,0.9309

epoch:1430/50, training loss:0.5780107378959656
Train Acc 0.9345
 Acc 0.9310
oral,dgl,1,1429,60.5112,0.9310

epoch:1431/50, training loss:0.577894389629364
Train Acc 0.9345
 Acc 0.9309
oral,dgl,1,1430,60.5534,0.9309

epoch:1432/50, training loss:0.5778191685676575
Train Acc 0.9344
 Acc 0.9309
oral,dgl,1,1431,60.5955,0.9309

epoch:1433/50, training loss:0.5777673721313477
Train Acc 0.9344
 Acc 0.9310
oral,dgl,1,1432,60.6376,0.9310

epoch:1434/50, training loss:0.5777254104614258
Train Acc 0.9344
 Acc 0.9308
oral,dgl,1,1433,60.6798,0.9308

epoch:1435/50, training loss:0.5776836276054382
Train Acc 0.9344
 Acc 0.9310
oral,dgl,1,1434,60.7218,0.9310

epoch:1436/50, training loss:0.5776618719100952
Train Acc 0.9345
 Acc 0.9308
oral,dgl,1,1435,60.7639,0.9308

epoch:1437/50, training loss:0.5776506662368774
Train Acc 0.9342
 Acc 0.9311
new best val f1: 0.9311035018940333
oral,dgl,1,1436,60.8061,0.9311

epoch:1438/50, training loss:0.5776859521865845
Train Acc 0.9346
 Acc 0.9302
oral,dgl,1,1437,60.8481,0.9302

epoch:1439/50, training loss:0.5777930021286011
Train Acc 0.9337
 Acc 0.9311
oral,dgl,1,1438,60.8901,0.9311

epoch:1440/50, training loss:0.5778748989105225
Train Acc 0.9347
 Acc 0.9299
oral,dgl,1,1439,60.9322,0.9299

epoch:1441/50, training loss:0.5779591798782349
Train Acc 0.9335
 Acc 0.9311
new best val f1: 0.9311068102492845
oral,dgl,1,1440,60.9743,0.9311

epoch:1442/50, training loss:0.5779282450675964
Train Acc 0.9348
 Acc 0.9298
oral,dgl,1,1441,61.0164,0.9298

epoch:1443/50, training loss:0.5778368711471558
Train Acc 0.9335
 Acc 0.9311
new best val f1: 0.9311398938017964
oral,dgl,1,1442,61.0584,0.9311

epoch:1444/50, training loss:0.5776183605194092
Train Acc 0.9347
 Acc 0.9302
oral,dgl,1,1443,61.1004,0.9302

epoch:1445/50, training loss:0.5774549245834351
Train Acc 0.9338
 Acc 0.9311
oral,dgl,1,1444,61.1425,0.9311

epoch:1446/50, training loss:0.5772517919540405
Train Acc 0.9346
 Acc 0.9306
oral,dgl,1,1445,61.1846,0.9306

epoch:1447/50, training loss:0.5770829916000366
Train Acc 0.9341
 Acc 0.9310
oral,dgl,1,1446,61.2267,0.9310

epoch:1448/50, training loss:0.5769626498222351
Train Acc 0.9346
 Acc 0.9309
oral,dgl,1,1447,61.2689,0.9309

epoch:1449/50, training loss:0.5768918395042419
Train Acc 0.9344
 Acc 0.9310
oral,dgl,1,1448,61.3109,0.9310

epoch:1450/50, training loss:0.5768510103225708
Train Acc 0.9347
 Acc 0.9307
oral,dgl,1,1449,61.3531,0.9307

epoch:1451/50, training loss:0.5768571496009827
Train Acc 0.9342
 Acc 0.9312
new best val f1: 0.931202752551569
oral,dgl,1,1450,61.3953,0.9312

epoch:1452/50, training loss:0.5768945813179016
Train Acc 0.9347
 Acc 0.9302
oral,dgl,1,1451,61.4373,0.9302

epoch:1453/50, training loss:0.5769773125648499
Train Acc 0.9338
 Acc 0.9311
oral,dgl,1,1452,61.4795,0.9311

epoch:1454/50, training loss:0.5770184993743896
Train Acc 0.9349
 Acc 0.9300
oral,dgl,1,1453,61.5215,0.9300

epoch:1455/50, training loss:0.5771069526672363
Train Acc 0.9337
 Acc 0.9311
oral,dgl,1,1454,61.5637,0.9311

epoch:1456/50, training loss:0.5772286653518677
Train Acc 0.9348
 Acc 0.9294
oral,dgl,1,1455,61.6058,0.9294

epoch:1457/50, training loss:0.5775614380836487
Train Acc 0.9332
 Acc 0.9309
oral,dgl,1,1456,61.6479,0.9309

epoch:1458/50, training loss:0.577820897102356
Train Acc 0.9347
 Acc 0.9288
oral,dgl,1,1457,61.6901,0.9288

epoch:1459/50, training loss:0.5783277153968811
Train Acc 0.9325
 Acc 0.9308
oral,dgl,1,1458,61.7321,0.9308

epoch:1460/50, training loss:0.5784198641777039
Train Acc 0.9346
 Acc 0.9285
oral,dgl,1,1459,61.7741,0.9285

epoch:1461/50, training loss:0.5785402655601501
Train Acc 0.9323
 Acc 0.9309
oral,dgl,1,1460,61.8162,0.9309

epoch:1462/50, training loss:0.5778852105140686
Train Acc 0.9346
 Acc 0.9296
oral,dgl,1,1461,61.8584,0.9296

epoch:1463/50, training loss:0.5772494673728943
Train Acc 0.9332
 Acc 0.9311
oral,dgl,1,1462,61.9005,0.9311

epoch:1464/50, training loss:0.576453447341919
Train Acc 0.9348
 Acc 0.9308
oral,dgl,1,1463,61.9425,0.9308

epoch:1465/50, training loss:0.5759479999542236
Train Acc 0.9342
 Acc 0.9311
oral,dgl,1,1464,61.9846,0.9311

epoch:1466/50, training loss:0.5756621360778809
Train Acc 0.9347
 Acc 0.9314
new best val f1: 0.931414487287645
oral,dgl,1,1465,62.0267,0.9314

epoch:1467/50, training loss:0.5755701065063477
Train Acc 0.9348
 Acc 0.9309
oral,dgl,1,1466,62.0688,0.9309

epoch:1468/50, training loss:0.5755516290664673
Train Acc 0.9345
 Acc 0.9313
oral,dgl,1,1467,62.1107,0.9313

epoch:1469/50, training loss:0.5755943059921265
Train Acc 0.9350
 Acc 0.9306
oral,dgl,1,1468,62.1529,0.9306

epoch:1470/50, training loss:0.5757218599319458
Train Acc 0.9342
 Acc 0.9314
oral,dgl,1,1469,62.1950,0.9314

epoch:1471/50, training loss:0.575847327709198
Train Acc 0.9351
 Acc 0.9301
oral,dgl,1,1470,62.2371,0.9301

epoch:1472/50, training loss:0.5758983492851257
Train Acc 0.9337
 Acc 0.9312
oral,dgl,1,1471,62.2791,0.9312

epoch:1473/50, training loss:0.575741708278656
Train Acc 0.9350
 Acc 0.9303
oral,dgl,1,1472,62.3213,0.9303

epoch:1474/50, training loss:0.5755647420883179
Train Acc 0.9339
 Acc 0.9314
oral,dgl,1,1473,62.3633,0.9314

epoch:1475/50, training loss:0.5753543376922607
Train Acc 0.9350
 Acc 0.9307
oral,dgl,1,1474,62.4055,0.9307

epoch:1476/50, training loss:0.5751655697822571
Train Acc 0.9343
 Acc 0.9312
oral,dgl,1,1475,62.4475,0.9312

epoch:1477/50, training loss:0.5749489665031433
Train Acc 0.9348
 Acc 0.9312
oral,dgl,1,1476,62.4897,0.9312

epoch:1478/50, training loss:0.5747953057289124
Train Acc 0.9348
 Acc 0.9312
oral,dgl,1,1477,62.5319,0.9312

epoch:1479/50, training loss:0.5747154951095581
Train Acc 0.9348
 Acc 0.9315
new best val f1: 0.9314740376821663
oral,dgl,1,1478,62.5740,0.9315

epoch:1480/50, training loss:0.5747238397598267
Train Acc 0.9350
 Acc 0.9308
oral,dgl,1,1479,62.6160,0.9308

epoch:1481/50, training loss:0.5748288035392761
Train Acc 0.9344
 Acc 0.9314
oral,dgl,1,1480,62.6582,0.9314

epoch:1482/50, training loss:0.5749857425689697
Train Acc 0.9352
 Acc 0.9303
oral,dgl,1,1481,62.7003,0.9303

epoch:1483/50, training loss:0.5752792954444885
Train Acc 0.9338
 Acc 0.9312
oral,dgl,1,1482,62.7425,0.9312

epoch:1484/50, training loss:0.5756945610046387
Train Acc 0.9350
 Acc 0.9295
oral,dgl,1,1483,62.7846,0.9295

epoch:1485/50, training loss:0.5764058232307434
Train Acc 0.9331
 Acc 0.9308
oral,dgl,1,1484,62.8266,0.9308

epoch:1486/50, training loss:0.5767637491226196
Train Acc 0.9345
 Acc 0.9290
oral,dgl,1,1485,62.8687,0.9290

epoch:1487/50, training loss:0.5770915746688843
Train Acc 0.9327
 Acc 0.9308
oral,dgl,1,1486,62.9108,0.9308

epoch:1488/50, training loss:0.5765076875686646
Train Acc 0.9344
 Acc 0.9299
oral,dgl,1,1487,62.9527,0.9299

epoch:1489/50, training loss:0.5757852792739868
Train Acc 0.9335
 Acc 0.9312
oral,dgl,1,1488,62.9949,0.9312

epoch:1490/50, training loss:0.5747475624084473
Train Acc 0.9348
 Acc 0.9313
oral,dgl,1,1489,63.0370,0.9313

epoch:1491/50, training loss:0.5740659832954407
Train Acc 0.9349
 Acc 0.9312
oral,dgl,1,1490,63.0791,0.9312

epoch:1492/50, training loss:0.5738991498947144
Train Acc 0.9347
 Acc 0.9315
oral,dgl,1,1491,63.1213,0.9315

epoch:1493/50, training loss:0.5741528272628784
Train Acc 0.9353
 Acc 0.9303
oral,dgl,1,1492,63.1633,0.9303

epoch:1494/50, training loss:0.5746147632598877
Train Acc 0.9338
 Acc 0.9314
oral,dgl,1,1493,63.2054,0.9314

epoch:1495/50, training loss:0.5750030875205994
Train Acc 0.9350
 Acc 0.9297
oral,dgl,1,1494,63.2475,0.9297

epoch:1496/50, training loss:0.5754399299621582
Train Acc 0.9334
 Acc 0.9313
oral,dgl,1,1495,63.2896,0.9313

epoch:1497/50, training loss:0.5754207968711853
Train Acc 0.9351
 Acc 0.9295
oral,dgl,1,1496,63.3317,0.9295

epoch:1498/50, training loss:0.5752971768379211
Train Acc 0.9333
 Acc 0.9316
new best val f1: 0.9315799050502043
oral,dgl,1,1497,63.3736,0.9316

epoch:1499/50, training loss:0.5747808218002319
Train Acc 0.9352
 Acc 0.9301
oral,dgl,1,1498,63.4158,0.9301

epoch:1500/50, training loss:0.5743527412414551
Train Acc 0.9339
 Acc 0.9318
new best val f1: 0.9318181066282898
oral,dgl,1,1499,63.4578,0.9318

epoch:1501/50, training loss:0.5739105343818665
Train Acc 0.9353
 Acc 0.9308
oral,dgl,1,1500,63.5000,0.9308

epoch:1502/50, training loss:0.5736510753631592
Train Acc 0.9345
 Acc 0.9315
oral,dgl,1,1501,63.5421,0.9315

epoch:1503/50, training loss:0.5734386444091797
Train Acc 0.9351
 Acc 0.9314
oral,dgl,1,1502,63.5842,0.9314

epoch:1504/50, training loss:0.5733409523963928
Train Acc 0.9351
 Acc 0.9315
oral,dgl,1,1503,63.6261,0.9315

epoch:1505/50, training loss:0.5731576681137085
Train Acc 0.9349
 Acc 0.9316
oral,dgl,1,1504,63.6683,0.9316

epoch:1506/50, training loss:0.5730844140052795
Train Acc 0.9355
 Acc 0.9310
oral,dgl,1,1505,63.7104,0.9310

epoch:1507/50, training loss:0.5731877684593201
Train Acc 0.9346
 Acc 0.9319
new best val f1: 0.9319173572858254
oral,dgl,1,1506,63.7526,0.9319

epoch:1508/50, training loss:0.5734548568725586
Train Acc 0.9355
 Acc 0.9303
oral,dgl,1,1507,63.7944,0.9303

epoch:1509/50, training loss:0.5739135146141052
Train Acc 0.9341
 Acc 0.9320
new best val f1: 0.931963674259342
oral,dgl,1,1508,63.8367,0.9320

epoch:1510/50, training loss:0.574168860912323
Train Acc 0.9354
 Acc 0.9300
oral,dgl,1,1509,63.8788,0.9300

epoch:1511/50, training loss:0.5743110775947571
Train Acc 0.9340
 Acc 0.9319
oral,dgl,1,1510,63.9209,0.9319

epoch:1512/50, training loss:0.5738171339035034
Train Acc 0.9355
 Acc 0.9307
oral,dgl,1,1511,63.9629,0.9307

epoch:1513/50, training loss:0.5732665657997131
Train Acc 0.9345
 Acc 0.9317
oral,dgl,1,1512,64.0050,0.9317

epoch:1514/50, training loss:0.5725910663604736
Train Acc 0.9353
 Acc 0.9316
oral,dgl,1,1513,64.0471,0.9316

epoch:1515/50, training loss:0.5722489356994629
Train Acc 0.9352
 Acc 0.9313
oral,dgl,1,1514,64.0892,0.9313

epoch:1516/50, training loss:0.5722547769546509
Train Acc 0.9349
 Acc 0.9318
oral,dgl,1,1515,64.1313,0.9318

epoch:1517/50, training loss:0.5725222826004028
Train Acc 0.9356
 Acc 0.9307
oral,dgl,1,1516,64.1733,0.9307

epoch:1518/50, training loss:0.572952926158905
Train Acc 0.9345
 Acc 0.9321
new best val f1: 0.9321290920219013
oral,dgl,1,1517,64.2154,0.9321

epoch:1519/50, training loss:0.5731844305992126
Train Acc 0.9357
 Acc 0.9304
oral,dgl,1,1518,64.2575,0.9304

epoch:1520/50, training loss:0.573266863822937
Train Acc 0.9343
 Acc 0.9320
oral,dgl,1,1519,64.2995,0.9320

epoch:1521/50, training loss:0.57285475730896
Train Acc 0.9356
 Acc 0.9307
oral,dgl,1,1520,64.3414,0.9307

epoch:1522/50, training loss:0.5723802447319031
Train Acc 0.9346
 Acc 0.9317
oral,dgl,1,1521,64.3834,0.9317

epoch:1523/50, training loss:0.5719022750854492
Train Acc 0.9355
 Acc 0.9314
oral,dgl,1,1522,64.4256,0.9314

epoch:1524/50, training loss:0.5717032551765442
Train Acc 0.9350
 Acc 0.9316
oral,dgl,1,1523,64.4676,0.9316

epoch:1525/50, training loss:0.5717706084251404
Train Acc 0.9353
 Acc 0.9317
oral,dgl,1,1524,64.5096,0.9317

epoch:1526/50, training loss:0.5718550682067871
Train Acc 0.9352
 Acc 0.9315
oral,dgl,1,1525,64.5516,0.9315

epoch:1527/50, training loss:0.5718954801559448
Train Acc 0.9351
 Acc 0.9318
oral,dgl,1,1526,64.5937,0.9318

epoch:1528/50, training loss:0.5717743039131165
Train Acc 0.9355
 Acc 0.9314
oral,dgl,1,1527,64.6358,0.9314

epoch:1529/50, training loss:0.571709394454956
Train Acc 0.9351
 Acc 0.9320
oral,dgl,1,1528,64.6779,0.9320

epoch:1530/50, training loss:0.5715510249137878
Train Acc 0.9356
 Acc 0.9312
oral,dgl,1,1529,64.7200,0.9312

epoch:1531/50, training loss:0.5714694261550903
Train Acc 0.9349
 Acc 0.9320
oral,dgl,1,1530,64.7622,0.9320

epoch:1532/50, training loss:0.5715266466140747
Train Acc 0.9358
 Acc 0.9306
oral,dgl,1,1531,64.8044,0.9306

epoch:1533/50, training loss:0.5718064904212952
Train Acc 0.9343
 Acc 0.9316
oral,dgl,1,1532,64.8465,0.9316

epoch:1534/50, training loss:0.5721439123153687
Train Acc 0.9353
 Acc 0.9303
oral,dgl,1,1533,64.8885,0.9303

epoch:1535/50, training loss:0.5724472403526306
Train Acc 0.9339
 Acc 0.9316
oral,dgl,1,1534,64.9305,0.9316

epoch:1536/50, training loss:0.5722667574882507
Train Acc 0.9353
 Acc 0.9306
oral,dgl,1,1535,64.9726,0.9306

epoch:1537/50, training loss:0.5719321966171265
Train Acc 0.9341
 Acc 0.9316
oral,dgl,1,1536,65.0148,0.9316

epoch:1538/50, training loss:0.5713196396827698
Train Acc 0.9354
 Acc 0.9313
oral,dgl,1,1537,65.0568,0.9313

epoch:1539/50, training loss:0.5708091259002686
Train Acc 0.9350
 Acc 0.9318
oral,dgl,1,1538,65.0990,0.9318

epoch:1540/50, training loss:0.5705045461654663
Train Acc 0.9356
 Acc 0.9319
oral,dgl,1,1539,65.1411,0.9319

epoch:1541/50, training loss:0.5704557299613953
Train Acc 0.9357
 Acc 0.9314
oral,dgl,1,1540,65.1832,0.9314

epoch:1542/50, training loss:0.5706192255020142
Train Acc 0.9350
 Acc 0.9318
oral,dgl,1,1541,65.2253,0.9318

epoch:1543/50, training loss:0.5708516240119934
Train Acc 0.9356
 Acc 0.9310
oral,dgl,1,1542,65.2673,0.9310

epoch:1544/50, training loss:0.5710798501968384
Train Acc 0.9345
 Acc 0.9318
oral,dgl,1,1543,65.3094,0.9318

epoch:1545/50, training loss:0.5712099671363831
Train Acc 0.9354
 Acc 0.9306
oral,dgl,1,1544,65.3516,0.9306

epoch:1546/50, training loss:0.5714426636695862
Train Acc 0.9341
 Acc 0.9314
oral,dgl,1,1545,65.3936,0.9314

epoch:1547/50, training loss:0.5714427828788757
Train Acc 0.9353
 Acc 0.9305
oral,dgl,1,1546,65.4356,0.9305

epoch:1548/50, training loss:0.5714430809020996
Train Acc 0.9340
 Acc 0.9317
oral,dgl,1,1547,65.4777,0.9317

epoch:1549/50, training loss:0.571103572845459
Train Acc 0.9355
 Acc 0.9306
oral,dgl,1,1548,65.5198,0.9306

epoch:1550/50, training loss:0.5707647800445557
Train Acc 0.9341
 Acc 0.9320
oral,dgl,1,1549,65.5620,0.9320

epoch:1551/50, training loss:0.5703350901603699
Train Acc 0.9357
 Acc 0.9312
oral,dgl,1,1550,65.6040,0.9312

epoch:1552/50, training loss:0.5700355172157288
Train Acc 0.9348
 Acc 0.9321
oral,dgl,1,1551,65.6461,0.9321

epoch:1553/50, training loss:0.5698052048683167
Train Acc 0.9357
 Acc 0.9317
oral,dgl,1,1552,65.6883,0.9317

epoch:1554/50, training loss:0.569636344909668
Train Acc 0.9353
 Acc 0.9320
oral,dgl,1,1553,65.7302,0.9320

epoch:1555/50, training loss:0.5695071220397949
Train Acc 0.9356
 Acc 0.9321
oral,dgl,1,1554,65.7723,0.9321

epoch:1556/50, training loss:0.5694275498390198
Train Acc 0.9357
 Acc 0.9318
oral,dgl,1,1555,65.8144,0.9318

epoch:1557/50, training loss:0.5693756937980652
Train Acc 0.9354
 Acc 0.9321
oral,dgl,1,1556,65.8565,0.9321

epoch:1558/50, training loss:0.5693780779838562
Train Acc 0.9358
 Acc 0.9313
oral,dgl,1,1557,65.8984,0.9313

epoch:1559/50, training loss:0.5695077180862427
Train Acc 0.9349
 Acc 0.9322
new best val f1: 0.9321721006401668
oral,dgl,1,1558,65.9406,0.9322

epoch:1560/50, training loss:0.5697405338287354
Train Acc 0.9358
 Acc 0.9307
oral,dgl,1,1559,65.9828,0.9307

epoch:1561/50, training loss:0.5700238943099976
Train Acc 0.9345
 Acc 0.9319
oral,dgl,1,1560,66.0249,0.9319

epoch:1562/50, training loss:0.5701746344566345
Train Acc 0.9358
 Acc 0.9306
oral,dgl,1,1561,66.0669,0.9306

epoch:1563/50, training loss:0.5701974034309387
Train Acc 0.9343
 Acc 0.9319
oral,dgl,1,1562,66.1091,0.9319

epoch:1564/50, training loss:0.5699114799499512
Train Acc 0.9356
 Acc 0.9309
oral,dgl,1,1563,66.1512,0.9309

epoch:1565/50, training loss:0.5694839954376221
Train Acc 0.9346
 Acc 0.9321
oral,dgl,1,1564,66.1934,0.9321

epoch:1566/50, training loss:0.5690020322799683
Train Acc 0.9359
 Acc 0.9319
oral,dgl,1,1565,66.2353,0.9319

epoch:1567/50, training loss:0.5686824917793274
Train Acc 0.9356
 Acc 0.9318
oral,dgl,1,1566,66.2775,0.9318

epoch:1568/50, training loss:0.5686483383178711
Train Acc 0.9354
 Acc 0.9322
new best val f1: 0.9321853340611714
oral,dgl,1,1567,66.3196,0.9322

epoch:1569/50, training loss:0.5687901377677917
Train Acc 0.9359
 Acc 0.9312
oral,dgl,1,1568,66.3618,0.9312

epoch:1570/50, training loss:0.5690608620643616
Train Acc 0.9348
 Acc 0.9320
oral,dgl,1,1569,66.4039,0.9320

epoch:1571/50, training loss:0.5693634748458862
Train Acc 0.9359
 Acc 0.9307
oral,dgl,1,1570,66.4460,0.9307

epoch:1572/50, training loss:0.5697537064552307
Train Acc 0.9341
 Acc 0.9318
oral,dgl,1,1571,66.4880,0.9318

epoch:1573/50, training loss:0.5698281526565552
Train Acc 0.9358
 Acc 0.9305
oral,dgl,1,1572,66.5302,0.9305

epoch:1574/50, training loss:0.5697559118270874
Train Acc 0.9341
 Acc 0.9318
oral,dgl,1,1573,66.5723,0.9318

epoch:1575/50, training loss:0.5693619251251221
Train Acc 0.9358
 Acc 0.9309
oral,dgl,1,1574,66.6145,0.9309

epoch:1576/50, training loss:0.5689836740493774
Train Acc 0.9343
 Acc 0.9320
oral,dgl,1,1575,66.6564,0.9320

epoch:1577/50, training loss:0.5685070157051086
Train Acc 0.9359
 Acc 0.9315
oral,dgl,1,1576,66.6985,0.9315

epoch:1578/50, training loss:0.5681918263435364
Train Acc 0.9352
 Acc 0.9319
oral,dgl,1,1577,66.7406,0.9319

epoch:1579/50, training loss:0.5680286884307861
Train Acc 0.9357
 Acc 0.9318
oral,dgl,1,1578,66.7827,0.9318

epoch:1580/50, training loss:0.5679445266723633
Train Acc 0.9356
 Acc 0.9321
oral,dgl,1,1579,66.8248,0.9321

epoch:1581/50, training loss:0.5678520202636719
Train Acc 0.9356
 Acc 0.9320
oral,dgl,1,1580,66.8670,0.9320

epoch:1582/50, training loss:0.5677409768104553
Train Acc 0.9357
 Acc 0.9321
oral,dgl,1,1581,66.9090,0.9321

epoch:1583/50, training loss:0.5676407814025879
Train Acc 0.9356
 Acc 0.9321
oral,dgl,1,1582,66.9512,0.9321

epoch:1584/50, training loss:0.5675509572029114
Train Acc 0.9358
 Acc 0.9320
oral,dgl,1,1583,66.9933,0.9320

epoch:1585/50, training loss:0.5674808025360107
Train Acc 0.9357
 Acc 0.9321
oral,dgl,1,1584,67.0354,0.9321

epoch:1586/50, training loss:0.5674154758453369
Train Acc 0.9357
 Acc 0.9319
oral,dgl,1,1585,67.0774,0.9319

epoch:1587/50, training loss:0.5673955082893372
Train Acc 0.9356
 Acc 0.9320
oral,dgl,1,1586,67.1196,0.9320

epoch:1588/50, training loss:0.5673813223838806
Train Acc 0.9356
 Acc 0.9318
oral,dgl,1,1587,67.1618,0.9318

epoch:1589/50, training loss:0.5673586130142212
Train Acc 0.9355
 Acc 0.9321
oral,dgl,1,1588,67.2039,0.9321

epoch:1590/50, training loss:0.5673118829727173
Train Acc 0.9357
 Acc 0.9318
oral,dgl,1,1589,67.2459,0.9318

epoch:1591/50, training loss:0.5673010945320129
Train Acc 0.9354
 Acc 0.9323
new best val f1: 0.9322515011661953
oral,dgl,1,1590,67.2880,0.9323

epoch:1592/50, training loss:0.5672732591629028
Train Acc 0.9360
 Acc 0.9316
oral,dgl,1,1591,67.3301,0.9316

epoch:1593/50, training loss:0.5672604441642761
Train Acc 0.9352
 Acc 0.9322
oral,dgl,1,1592,67.3721,0.9322

epoch:1594/50, training loss:0.5672142505645752
Train Acc 0.9360
 Acc 0.9315
oral,dgl,1,1593,67.4142,0.9315

epoch:1595/50, training loss:0.5672587752342224
Train Acc 0.9352
 Acc 0.9320
oral,dgl,1,1594,67.4561,0.9320

epoch:1596/50, training loss:0.5674083828926086
Train Acc 0.9360
 Acc 0.9312
oral,dgl,1,1595,67.4984,0.9312

epoch:1597/50, training loss:0.5676905512809753
Train Acc 0.9347
 Acc 0.9315
oral,dgl,1,1596,67.5404,0.9315

epoch:1598/50, training loss:0.5679227113723755
Train Acc 0.9357
 Acc 0.9308
oral,dgl,1,1597,67.5825,0.9308

epoch:1599/50, training loss:0.5682384967803955
Train Acc 0.9342
 Acc 0.9316
oral,dgl,1,1598,67.6246,0.9316

epoch:1600/50, training loss:0.5683218836784363
Train Acc 0.9358
 Acc 0.9304
oral,dgl,1,1599,67.6667,0.9304

epoch:1601/50, training loss:0.568662703037262
Train Acc 0.9339
 Acc 0.9315
oral,dgl,1,1600,67.7088,0.9315

epoch:1602/50, training loss:0.568864643573761
Train Acc 0.9357
 Acc 0.9299
oral,dgl,1,1601,67.7509,0.9299

epoch:1603/50, training loss:0.5691080689430237
Train Acc 0.9335
 Acc 0.9318
oral,dgl,1,1602,67.7929,0.9318

epoch:1604/50, training loss:0.5687482357025146
Train Acc 0.9358
 Acc 0.9304
oral,dgl,1,1603,67.8351,0.9304

epoch:1605/50, training loss:0.5682668089866638
Train Acc 0.9340
 Acc 0.9320
oral,dgl,1,1604,67.8773,0.9320

epoch:1606/50, training loss:0.5674037337303162
Train Acc 0.9359
 Acc 0.9315
oral,dgl,1,1605,67.9193,0.9315

epoch:1607/50, training loss:0.5666167736053467
Train Acc 0.9351
 Acc 0.9324
new best val f1: 0.9324367690602617
oral,dgl,1,1606,67.9613,0.9324

epoch:1608/50, training loss:0.5661077499389648
Train Acc 0.9360
 Acc 0.9322
oral,dgl,1,1607,68.0033,0.9322

epoch:1609/50, training loss:0.5659749507904053
Train Acc 0.9358
 Acc 0.9318
oral,dgl,1,1608,68.0454,0.9318

epoch:1610/50, training loss:0.5660945773124695
Train Acc 0.9355
 Acc 0.9325
new best val f1: 0.9325095528757879
oral,dgl,1,1609,68.0875,0.9325

epoch:1611/50, training loss:0.5663534998893738
Train Acc 0.9362
 Acc 0.9313
oral,dgl,1,1610,68.1297,0.9313

epoch:1612/50, training loss:0.5665068626403809
Train Acc 0.9348
 Acc 0.9321
oral,dgl,1,1611,68.1717,0.9321

epoch:1613/50, training loss:0.5665102005004883
Train Acc 0.9360
 Acc 0.9314
oral,dgl,1,1612,68.2139,0.9314

epoch:1614/50, training loss:0.566372811794281
Train Acc 0.9349
 Acc 0.9323
oral,dgl,1,1613,68.2560,0.9323

epoch:1615/50, training loss:0.5660883188247681
Train Acc 0.9362
 Acc 0.9316
oral,dgl,1,1614,68.2982,0.9316

epoch:1616/50, training loss:0.5658552646636963
Train Acc 0.9351
 Acc 0.9322
oral,dgl,1,1615,68.3402,0.9322

epoch:1617/50, training loss:0.5656154751777649
Train Acc 0.9361
 Acc 0.9321
oral,dgl,1,1616,68.3822,0.9321

epoch:1618/50, training loss:0.5654445290565491
Train Acc 0.9358
 Acc 0.9323
oral,dgl,1,1617,68.4243,0.9323

epoch:1619/50, training loss:0.565328061580658
Train Acc 0.9358
 Acc 0.9324
oral,dgl,1,1618,68.4664,0.9324

epoch:1620/50, training loss:0.5652826428413391
Train Acc 0.9360
 Acc 0.9321
oral,dgl,1,1619,68.5085,0.9321

epoch:1621/50, training loss:0.565261721611023
Train Acc 0.9357
 Acc 0.9325
new best val f1: 0.9325161695862901
oral,dgl,1,1620,68.5505,0.9325

epoch:1622/50, training loss:0.5652850270271301
Train Acc 0.9362
 Acc 0.9318
oral,dgl,1,1621,68.5925,0.9318

epoch:1623/50, training loss:0.5653380155563354
Train Acc 0.9355
 Acc 0.9325
oral,dgl,1,1622,68.6344,0.9325

epoch:1624/50, training loss:0.5654493570327759
Train Acc 0.9363
 Acc 0.9314
oral,dgl,1,1623,68.6766,0.9314

epoch:1625/50, training loss:0.5656796097755432
Train Acc 0.9350
 Acc 0.9320
oral,dgl,1,1624,68.7187,0.9320

epoch:1626/50, training loss:0.5659568905830383
Train Acc 0.9361
 Acc 0.9310
oral,dgl,1,1625,68.7608,0.9310

epoch:1627/50, training loss:0.5663213729858398
Train Acc 0.9345
 Acc 0.9317
oral,dgl,1,1626,68.8029,0.9317

epoch:1628/50, training loss:0.5665134191513062
Train Acc 0.9359
 Acc 0.9306
oral,dgl,1,1627,68.8449,0.9306

epoch:1629/50, training loss:0.5670051574707031
Train Acc 0.9341
 Acc 0.9313
oral,dgl,1,1628,68.8870,0.9313

epoch:1630/50, training loss:0.5672194957733154
Train Acc 0.9356
 Acc 0.9302
oral,dgl,1,1629,68.9292,0.9302

epoch:1631/50, training loss:0.567723274230957
Train Acc 0.9338
 Acc 0.9312
oral,dgl,1,1630,68.9712,0.9312

epoch:1632/50, training loss:0.5674546957015991
Train Acc 0.9353
 Acc 0.9304
oral,dgl,1,1631,69.0131,0.9304

epoch:1633/50, training loss:0.5672104358673096
Train Acc 0.9340
 Acc 0.9315
oral,dgl,1,1632,69.0553,0.9315

epoch:1634/50, training loss:0.5661852955818176
Train Acc 0.9356
 Acc 0.9314
oral,dgl,1,1633,69.0974,0.9314

epoch:1635/50, training loss:0.5652163624763489
Train Acc 0.9350
 Acc 0.9325
new best val f1: 0.9325393280730485
oral,dgl,1,1634,69.1394,0.9325

epoch:1636/50, training loss:0.5644955039024353
Train Acc 0.9363
 Acc 0.9323
oral,dgl,1,1635,69.1816,0.9323

epoch:1637/50, training loss:0.5643841028213501
Train Acc 0.9359
 Acc 0.9320
oral,dgl,1,1636,69.2237,0.9320

epoch:1638/50, training loss:0.5646488666534424
Train Acc 0.9358
 Acc 0.9320
oral,dgl,1,1637,69.2656,0.9320

epoch:1639/50, training loss:0.5650516152381897
Train Acc 0.9358
 Acc 0.9313
oral,dgl,1,1638,69.3078,0.9313

epoch:1640/50, training loss:0.5656057000160217
Train Acc 0.9349
 Acc 0.9315
oral,dgl,1,1639,69.3499,0.9315

epoch:1641/50, training loss:0.565951406955719
Train Acc 0.9358
 Acc 0.9306
oral,dgl,1,1640,69.3919,0.9306

epoch:1642/50, training loss:0.5664635896682739
Train Acc 0.9340
 Acc 0.9318
oral,dgl,1,1641,69.4339,0.9318

epoch:1643/50, training loss:0.5664552450180054
Train Acc 0.9360
 Acc 0.9305
oral,dgl,1,1642,69.4762,0.9305

epoch:1644/50, training loss:0.5662546157836914
Train Acc 0.9340
 Acc 0.9322
oral,dgl,1,1643,69.5183,0.9322

epoch:1645/50, training loss:0.5653768181800842
Train Acc 0.9361
 Acc 0.9315
oral,dgl,1,1644,69.5604,0.9315

epoch:1646/50, training loss:0.5644991397857666
Train Acc 0.9351
 Acc 0.9325
oral,dgl,1,1645,69.6026,0.9325

epoch:1647/50, training loss:0.5638635754585266
Train Acc 0.9362
 Acc 0.9324
oral,dgl,1,1646,69.6447,0.9324

epoch:1648/50, training loss:0.5636698007583618
Train Acc 0.9361
 Acc 0.9318
oral,dgl,1,1647,69.6867,0.9318

epoch:1649/50, training loss:0.5638768672943115
Train Acc 0.9356
 Acc 0.9325
oral,dgl,1,1648,69.7290,0.9325

epoch:1650/50, training loss:0.5642995238304138
Train Acc 0.9364
 Acc 0.9314
oral,dgl,1,1649,69.7711,0.9314

epoch:1651/50, training loss:0.5647602081298828
Train Acc 0.9348
 Acc 0.9323
oral,dgl,1,1650,69.8132,0.9323

epoch:1652/50, training loss:0.5649545192718506
Train Acc 0.9365
 Acc 0.9313
oral,dgl,1,1651,69.8553,0.9313

epoch:1653/50, training loss:0.5648208260536194
Train Acc 0.9348
 Acc 0.9325
oral,dgl,1,1652,69.8974,0.9325

epoch:1654/50, training loss:0.5642239451408386
Train Acc 0.9365
 Acc 0.9318
oral,dgl,1,1653,69.9396,0.9318

epoch:1655/50, training loss:0.5635985136032104
Train Acc 0.9355
 Acc 0.9326
new best val f1: 0.9325724116255604
oral,dgl,1,1654,69.9817,0.9326

epoch:1656/50, training loss:0.5632271766662598
Train Acc 0.9362
 Acc 0.9326
new best val f1: 0.932585645046565
oral,dgl,1,1655,70.0238,0.9326

epoch:1657/50, training loss:0.5631870031356812
Train Acc 0.9362
 Acc 0.9318
oral,dgl,1,1656,70.0658,0.9318

epoch:1658/50, training loss:0.5635014176368713
Train Acc 0.9355
 Acc 0.9327
new best val f1: 0.9326782789935983
oral,dgl,1,1657,70.1080,0.9327

epoch:1659/50, training loss:0.5639704465866089
Train Acc 0.9365
 Acc 0.9313
oral,dgl,1,1658,70.1501,0.9313

epoch:1660/50, training loss:0.564379870891571
Train Acc 0.9348
 Acc 0.9323
oral,dgl,1,1659,70.1921,0.9323

epoch:1661/50, training loss:0.5643789768218994
Train Acc 0.9366
 Acc 0.9313
oral,dgl,1,1660,70.2340,0.9313

epoch:1662/50, training loss:0.56424480676651
Train Acc 0.9349
 Acc 0.9321
oral,dgl,1,1661,70.2761,0.9321

epoch:1663/50, training loss:0.5639663338661194
Train Acc 0.9363
 Acc 0.9318
oral,dgl,1,1662,70.3182,0.9318

epoch:1664/50, training loss:0.5636038184165955
Train Acc 0.9353
 Acc 0.9327
oral,dgl,1,1663,70.3603,0.9327

epoch:1665/50, training loss:0.5630475878715515
Train Acc 0.9364
 Acc 0.9326
oral,dgl,1,1664,70.4023,0.9326

epoch:1666/50, training loss:0.5626909136772156
Train Acc 0.9362
 Acc 0.9322
oral,dgl,1,1665,70.4445,0.9322

epoch:1667/50, training loss:0.562704861164093
Train Acc 0.9360
 Acc 0.9327
new best val f1: 0.9327477544538733
oral,dgl,1,1666,70.4866,0.9327

epoch:1668/50, training loss:0.5628854036331177
Train Acc 0.9365
 Acc 0.9317
oral,dgl,1,1667,70.5286,0.9317

epoch:1669/50, training loss:0.5630130767822266
Train Acc 0.9354
 Acc 0.9323
oral,dgl,1,1668,70.5707,0.9323

epoch:1670/50, training loss:0.5630833506584167
Train Acc 0.9365
 Acc 0.9317
oral,dgl,1,1669,70.6128,0.9317

epoch:1671/50, training loss:0.5632355809211731
Train Acc 0.9354
 Acc 0.9322
oral,dgl,1,1670,70.6549,0.9322

epoch:1672/50, training loss:0.5632524490356445
Train Acc 0.9362
 Acc 0.9318
oral,dgl,1,1671,70.6971,0.9318

epoch:1673/50, training loss:0.5632277131080627
Train Acc 0.9354
 Acc 0.9322
oral,dgl,1,1672,70.7392,0.9322

epoch:1674/50, training loss:0.5629876852035522
Train Acc 0.9363
 Acc 0.9321
oral,dgl,1,1673,70.7812,0.9321

epoch:1675/50, training loss:0.5626657605171204
Train Acc 0.9358
 Acc 0.9325
oral,dgl,1,1674,70.8232,0.9325

epoch:1676/50, training loss:0.5622689723968506
Train Acc 0.9362
 Acc 0.9327
oral,dgl,1,1675,70.8654,0.9327

epoch:1677/50, training loss:0.5620049238204956
Train Acc 0.9363
 Acc 0.9322
oral,dgl,1,1676,70.9075,0.9322

epoch:1678/50, training loss:0.5620407462120056
Train Acc 0.9360
 Acc 0.9326
oral,dgl,1,1677,70.9496,0.9326

epoch:1679/50, training loss:0.5623263716697693
Train Acc 0.9364
 Acc 0.9318
oral,dgl,1,1678,70.9918,0.9318

epoch:1680/50, training loss:0.5626985430717468
Train Acc 0.9355
 Acc 0.9321
oral,dgl,1,1679,71.0337,0.9321

epoch:1681/50, training loss:0.5627536773681641
Train Acc 0.9363
 Acc 0.9318
oral,dgl,1,1680,71.0759,0.9318

epoch:1682/50, training loss:0.5625906586647034
Train Acc 0.9355
 Acc 0.9325
oral,dgl,1,1681,71.1181,0.9325

epoch:1683/50, training loss:0.5621829032897949
Train Acc 0.9363
 Acc 0.9322
oral,dgl,1,1682,71.1602,0.9322

epoch:1684/50, training loss:0.5617895126342773
Train Acc 0.9360
 Acc 0.9327
oral,dgl,1,1683,71.2023,0.9327

epoch:1685/50, training loss:0.5615533590316772
Train Acc 0.9365
 Acc 0.9326
oral,dgl,1,1684,71.2442,0.9326

epoch:1686/50, training loss:0.5615614056587219
Train Acc 0.9363
 Acc 0.9325
oral,dgl,1,1685,71.2863,0.9325

epoch:1687/50, training loss:0.5617237091064453
Train Acc 0.9361
 Acc 0.9325
oral,dgl,1,1686,71.3285,0.9325

epoch:1688/50, training loss:0.5619316697120667
Train Acc 0.9364
 Acc 0.9319
oral,dgl,1,1687,71.3704,0.9319

epoch:1689/50, training loss:0.5621733069419861
Train Acc 0.9357
 Acc 0.9322
oral,dgl,1,1688,71.4125,0.9322

epoch:1690/50, training loss:0.5622069239616394
Train Acc 0.9364
 Acc 0.9317
oral,dgl,1,1689,71.4547,0.9317

epoch:1691/50, training loss:0.5621943473815918
Train Acc 0.9355
 Acc 0.9326
oral,dgl,1,1690,71.4967,0.9326

epoch:1692/50, training loss:0.5619499087333679
Train Acc 0.9367
 Acc 0.9318
oral,dgl,1,1691,71.5388,0.9318

epoch:1693/50, training loss:0.5617210268974304
Train Acc 0.9356
 Acc 0.9330
new best val f1: 0.9330289646502241
oral,dgl,1,1692,71.5809,0.9330

epoch:1694/50, training loss:0.5614696145057678
Train Acc 0.9366
 Acc 0.9325
oral,dgl,1,1693,71.6230,0.9325

epoch:1695/50, training loss:0.5613049864768982
Train Acc 0.9361
 Acc 0.9325
oral,dgl,1,1694,71.6652,0.9325

epoch:1696/50, training loss:0.5612801909446716
Train Acc 0.9363
 Acc 0.9326
oral,dgl,1,1695,71.7071,0.9326

epoch:1697/50, training loss:0.5614029765129089
Train Acc 0.9365
 Acc 0.9320
oral,dgl,1,1696,71.7493,0.9320

epoch:1698/50, training loss:0.5615079402923584
Train Acc 0.9358
 Acc 0.9325
oral,dgl,1,1697,71.7914,0.9325

epoch:1699/50, training loss:0.5613799691200256
Train Acc 0.9365
 Acc 0.9322
oral,dgl,1,1698,71.8335,0.9322

epoch:1700/50, training loss:0.5611883997917175
Train Acc 0.9360
 Acc 0.9328
oral,dgl,1,1699,71.8755,0.9328

epoch:1701/50, training loss:0.5609437227249146
Train Acc 0.9366
 Acc 0.9326
oral,dgl,1,1700,71.9177,0.9326

epoch:1702/50, training loss:0.5607178211212158
Train Acc 0.9364
 Acc 0.9329
oral,dgl,1,1701,71.9598,0.9329

epoch:1703/50, training loss:0.5605310797691345
Train Acc 0.9365
 Acc 0.9329
oral,dgl,1,1702,72.0020,0.9329

epoch:1704/50, training loss:0.5604804158210754
Train Acc 0.9366
 Acc 0.9324
oral,dgl,1,1703,72.0440,0.9324

epoch:1705/50, training loss:0.5605849027633667
Train Acc 0.9363
 Acc 0.9327
oral,dgl,1,1704,72.0863,0.9327

epoch:1706/50, training loss:0.5607333183288574
Train Acc 0.9366
 Acc 0.9322
oral,dgl,1,1705,72.1284,0.9322

epoch:1707/50, training loss:0.5607889890670776
Train Acc 0.9361
 Acc 0.9327
oral,dgl,1,1706,72.1705,0.9327

epoch:1708/50, training loss:0.5607119202613831
Train Acc 0.9366
 Acc 0.9323
oral,dgl,1,1707,72.2126,0.9323

epoch:1709/50, training loss:0.5605945587158203
Train Acc 0.9362
 Acc 0.9328
oral,dgl,1,1708,72.2547,0.9328

epoch:1710/50, training loss:0.5603893399238586
Train Acc 0.9366
 Acc 0.9328
oral,dgl,1,1709,72.2967,0.9328

epoch:1711/50, training loss:0.5602030754089355
Train Acc 0.9366
 Acc 0.9328
oral,dgl,1,1710,72.3388,0.9328

epoch:1712/50, training loss:0.5600699782371521
Train Acc 0.9366
 Acc 0.9330
oral,dgl,1,1711,72.3810,0.9330

epoch:1713/50, training loss:0.5599715709686279
Train Acc 0.9366
 Acc 0.9330
oral,dgl,1,1712,72.4229,0.9330

epoch:1714/50, training loss:0.559941828250885
Train Acc 0.9366
 Acc 0.9329
oral,dgl,1,1713,72.4650,0.9329

epoch:1715/50, training loss:0.5599481463432312
Train Acc 0.9366
 Acc 0.9328
oral,dgl,1,1714,72.5073,0.9328

epoch:1716/50, training loss:0.5599645376205444
Train Acc 0.9365
 Acc 0.9329
oral,dgl,1,1715,72.5493,0.9329

epoch:1717/50, training loss:0.5600401163101196
Train Acc 0.9367
 Acc 0.9324
oral,dgl,1,1716,72.5913,0.9324

epoch:1718/50, training loss:0.5602141618728638
Train Acc 0.9362
 Acc 0.9324
oral,dgl,1,1717,72.6334,0.9324

epoch:1719/50, training loss:0.560417890548706
Train Acc 0.9367
 Acc 0.9321
oral,dgl,1,1718,72.6756,0.9321

epoch:1720/50, training loss:0.5606322884559631
Train Acc 0.9358
 Acc 0.9323
oral,dgl,1,1719,72.7178,0.9323

epoch:1721/50, training loss:0.5605759620666504
Train Acc 0.9367
 Acc 0.9320
oral,dgl,1,1720,72.7600,0.9320

epoch:1722/50, training loss:0.5604074597358704
Train Acc 0.9359
 Acc 0.9327
oral,dgl,1,1721,72.8021,0.9327

epoch:1723/50, training loss:0.5600680708885193
Train Acc 0.9367
 Acc 0.9322
oral,dgl,1,1722,72.8441,0.9322

epoch:1724/50, training loss:0.5598414540290833
Train Acc 0.9362
 Acc 0.9330
oral,dgl,1,1723,72.8862,0.9330

epoch:1725/50, training loss:0.559704601764679
Train Acc 0.9369
 Acc 0.9324
oral,dgl,1,1724,72.9282,0.9324

epoch:1726/50, training loss:0.5596528053283691
Train Acc 0.9362
 Acc 0.9332
new best val f1: 0.9332076158337882
oral,dgl,1,1725,72.9703,0.9332

epoch:1727/50, training loss:0.5595987439155579
Train Acc 0.9369
 Acc 0.9324
oral,dgl,1,1726,73.0124,0.9324

epoch:1728/50, training loss:0.5595189332962036
Train Acc 0.9363
 Acc 0.9332
oral,dgl,1,1727,73.0546,0.9332

epoch:1729/50, training loss:0.5593630075454712
Train Acc 0.9369
 Acc 0.9327
oral,dgl,1,1728,73.0966,0.9327

epoch:1730/50, training loss:0.5592414140701294
Train Acc 0.9365
 Acc 0.9331
oral,dgl,1,1729,73.1388,0.9331

epoch:1731/50, training loss:0.5591841340065002
Train Acc 0.9370
 Acc 0.9327
oral,dgl,1,1730,73.1809,0.9327

epoch:1732/50, training loss:0.5591597557067871
Train Acc 0.9364
 Acc 0.9330
oral,dgl,1,1731,73.2231,0.9330

epoch:1733/50, training loss:0.5591136813163757
Train Acc 0.9368
 Acc 0.9328
oral,dgl,1,1732,73.2652,0.9328

epoch:1734/50, training loss:0.5590605139732361
Train Acc 0.9365
 Acc 0.9330
oral,dgl,1,1733,73.3072,0.9330

epoch:1735/50, training loss:0.5589427351951599
Train Acc 0.9368
 Acc 0.9329
oral,dgl,1,1734,73.3493,0.9329

epoch:1736/50, training loss:0.5588311553001404
Train Acc 0.9367
 Acc 0.9331
oral,dgl,1,1735,73.3914,0.9331

epoch:1737/50, training loss:0.5587169528007507
Train Acc 0.9368
 Acc 0.9331
oral,dgl,1,1736,73.4336,0.9331

epoch:1738/50, training loss:0.5586409568786621
Train Acc 0.9368
 Acc 0.9331
oral,dgl,1,1737,73.4756,0.9331

epoch:1739/50, training loss:0.5586004257202148
Train Acc 0.9368
 Acc 0.9330
oral,dgl,1,1738,73.5177,0.9330

epoch:1740/50, training loss:0.5585931539535522
Train Acc 0.9367
 Acc 0.9328
oral,dgl,1,1739,73.5598,0.9328

epoch:1741/50, training loss:0.5586349368095398
Train Acc 0.9368
 Acc 0.9329
oral,dgl,1,1740,73.6020,0.9329

epoch:1742/50, training loss:0.5587369203567505
Train Acc 0.9368
 Acc 0.9326
oral,dgl,1,1741,73.6443,0.9326

epoch:1743/50, training loss:0.5589321255683899
Train Acc 0.9364
 Acc 0.9325
oral,dgl,1,1742,73.6863,0.9325

epoch:1744/50, training loss:0.559212863445282
Train Acc 0.9369
 Acc 0.9319
oral,dgl,1,1743,73.7284,0.9319

epoch:1745/50, training loss:0.5595733523368835
Train Acc 0.9359
 Acc 0.9324
oral,dgl,1,1744,73.7705,0.9324

epoch:1746/50, training loss:0.5598072409629822
Train Acc 0.9368
 Acc 0.9313
oral,dgl,1,1745,73.8126,0.9313

epoch:1747/50, training loss:0.5603019595146179
Train Acc 0.9352
 Acc 0.9325
oral,dgl,1,1746,73.8546,0.9325

epoch:1748/50, training loss:0.5606111884117126
Train Acc 0.9367
 Acc 0.9309
oral,dgl,1,1747,73.8968,0.9309

epoch:1749/50, training loss:0.5609473586082458
Train Acc 0.9347
 Acc 0.9327
oral,dgl,1,1748,73.9389,0.9327

epoch:1750/50, training loss:0.5606827735900879
Train Acc 0.9370
 Acc 0.9315
oral,dgl,1,1749,73.9810,0.9315

epoch:1751/50, training loss:0.5601909160614014
Train Acc 0.9352
 Acc 0.9330
oral,dgl,1,1750,74.0231,0.9330

epoch:1752/50, training loss:0.5592891573905945
Train Acc 0.9372
 Acc 0.9324
oral,dgl,1,1751,74.0652,0.9324

epoch:1753/50, training loss:0.5585123896598816
Train Acc 0.9361
 Acc 0.9332
oral,dgl,1,1752,74.1073,0.9332

epoch:1754/50, training loss:0.5579589009284973
Train Acc 0.9370
 Acc 0.9331
oral,dgl,1,1753,74.1494,0.9331

epoch:1755/50, training loss:0.5578085780143738
Train Acc 0.9370
 Acc 0.9328
oral,dgl,1,1754,74.1914,0.9328

epoch:1756/50, training loss:0.5579304099082947
Train Acc 0.9367
 Acc 0.9334
new best val f1: 0.9334458174118737
oral,dgl,1,1755,74.2334,0.9334

epoch:1757/50, training loss:0.5582001209259033
Train Acc 0.9371
 Acc 0.9324
oral,dgl,1,1756,74.2755,0.9324

epoch:1758/50, training loss:0.5585665702819824
Train Acc 0.9362
 Acc 0.9333
oral,dgl,1,1757,74.3177,0.9333

epoch:1759/50, training loss:0.5586294531822205
Train Acc 0.9372
 Acc 0.9322
oral,dgl,1,1758,74.3597,0.9322

epoch:1760/50, training loss:0.5586330890655518
Train Acc 0.9358
 Acc 0.9329
oral,dgl,1,1759,74.4018,0.9329

epoch:1761/50, training loss:0.5585206151008606
Train Acc 0.9372
 Acc 0.9319
oral,dgl,1,1760,74.4439,0.9319

epoch:1762/50, training loss:0.5586538910865784
Train Acc 0.9359
 Acc 0.9324
oral,dgl,1,1761,74.4860,0.9324

epoch:1763/50, training loss:0.5587902069091797
Train Acc 0.9368
 Acc 0.9319
oral,dgl,1,1762,74.5280,0.9319

epoch:1764/50, training loss:0.5592017769813538
Train Acc 0.9359
 Acc 0.9321
oral,dgl,1,1763,74.5701,0.9321

epoch:1765/50, training loss:0.5592931509017944
Train Acc 0.9365
 Acc 0.9321
oral,dgl,1,1764,74.6122,0.9321

epoch:1766/50, training loss:0.5591508746147156
Train Acc 0.9360
 Acc 0.9323
oral,dgl,1,1765,74.6543,0.9323

epoch:1767/50, training loss:0.5585964322090149
Train Acc 0.9369
 Acc 0.9323
oral,dgl,1,1766,74.6964,0.9323

epoch:1768/50, training loss:0.5579911470413208
Train Acc 0.9363
 Acc 0.9331
oral,dgl,1,1767,74.7385,0.9331

epoch:1769/50, training loss:0.5574761629104614
Train Acc 0.9370
 Acc 0.9327
oral,dgl,1,1768,74.7806,0.9327

epoch:1770/50, training loss:0.5573469996452332
Train Acc 0.9367
 Acc 0.9334
oral,dgl,1,1769,74.8227,0.9334

epoch:1771/50, training loss:0.5574573874473572
Train Acc 0.9371
 Acc 0.9329
oral,dgl,1,1770,74.8648,0.9329

epoch:1772/50, training loss:0.5576099753379822
Train Acc 0.9367
 Acc 0.9330
oral,dgl,1,1771,74.9069,0.9330

epoch:1773/50, training loss:0.5575432181358337
Train Acc 0.9366
 Acc 0.9327
oral,dgl,1,1772,74.9491,0.9327

epoch:1774/50, training loss:0.5574145317077637
Train Acc 0.9370
 Acc 0.9325
oral,dgl,1,1773,74.9912,0.9325

epoch:1775/50, training loss:0.5573757886886597
Train Acc 0.9363
 Acc 0.9331
oral,dgl,1,1774,75.0334,0.9331

epoch:1776/50, training loss:0.5575832724571228
Train Acc 0.9372
 Acc 0.9320
oral,dgl,1,1775,75.0754,0.9320

epoch:1777/50, training loss:0.5579372644424438
Train Acc 0.9358
 Acc 0.9332
oral,dgl,1,1776,75.1174,0.9332

epoch:1778/50, training loss:0.5579004287719727
Train Acc 0.9372
 Acc 0.9323
oral,dgl,1,1777,75.1595,0.9323

epoch:1779/50, training loss:0.5575612187385559
Train Acc 0.9361
 Acc 0.9332
oral,dgl,1,1778,75.2017,0.9332

epoch:1780/50, training loss:0.5569489598274231
Train Acc 0.9372
 Acc 0.9331
oral,dgl,1,1779,75.2439,0.9331

epoch:1781/50, training loss:0.5565335750579834
Train Acc 0.9369
 Acc 0.9332
oral,dgl,1,1780,75.2860,0.9332

epoch:1782/50, training loss:0.5564192533493042
Train Acc 0.9370
 Acc 0.9334
oral,dgl,1,1781,75.3281,0.9334

epoch:1783/50, training loss:0.5564484596252441
Train Acc 0.9371
 Acc 0.9330
oral,dgl,1,1782,75.3702,0.9330

epoch:1784/50, training loss:0.5565165877342224
Train Acc 0.9369
 Acc 0.9333
oral,dgl,1,1783,75.4123,0.9333

epoch:1785/50, training loss:0.5565996766090393
Train Acc 0.9372
 Acc 0.9327
oral,dgl,1,1784,75.4546,0.9327

epoch:1786/50, training loss:0.5566912293434143
Train Acc 0.9366
 Acc 0.9331
oral,dgl,1,1785,75.4966,0.9331

epoch:1787/50, training loss:0.5567658543586731
Train Acc 0.9373
 Acc 0.9325
oral,dgl,1,1786,75.5386,0.9325

epoch:1788/50, training loss:0.5568256974220276
Train Acc 0.9363
 Acc 0.9328
oral,dgl,1,1787,75.5808,0.9328

epoch:1789/50, training loss:0.5568141937255859
Train Acc 0.9372
 Acc 0.9324
oral,dgl,1,1788,75.6230,0.9324

epoch:1790/50, training loss:0.5568440556526184
Train Acc 0.9363
 Acc 0.9327
oral,dgl,1,1789,75.6650,0.9327

epoch:1791/50, training loss:0.5567454099655151
Train Acc 0.9371
 Acc 0.9325
oral,dgl,1,1790,75.7092,0.9325

epoch:1792/50, training loss:0.5567059516906738
Train Acc 0.9364
 Acc 0.9327
oral,dgl,1,1791,75.7512,0.9327

epoch:1793/50, training loss:0.5566300749778748
Train Acc 0.9370
 Acc 0.9325
oral,dgl,1,1792,75.7931,0.9325

epoch:1794/50, training loss:0.5566502213478088
Train Acc 0.9363
 Acc 0.9328
oral,dgl,1,1793,75.8352,0.9328

epoch:1795/50, training loss:0.5567330718040466
Train Acc 0.9373
 Acc 0.9321
oral,dgl,1,1794,75.8773,0.9321

epoch:1796/50, training loss:0.5570085644721985
Train Acc 0.9360
 Acc 0.9329
oral,dgl,1,1795,75.9195,0.9329

epoch:1797/50, training loss:0.557177722454071
Train Acc 0.9372
 Acc 0.9318
oral,dgl,1,1796,75.9615,0.9318

epoch:1798/50, training loss:0.5574541687965393
Train Acc 0.9357
 Acc 0.9327
oral,dgl,1,1797,76.0036,0.9327

epoch:1799/50, training loss:0.5574111342430115
Train Acc 0.9371
 Acc 0.9319
oral,dgl,1,1798,76.0457,0.9319

epoch:1800/50, training loss:0.5573157668113708
Train Acc 0.9359
 Acc 0.9327
oral,dgl,1,1799,76.0877,0.9327

epoch:1801/50, training loss:0.5568525791168213
Train Acc 0.9371
 Acc 0.9324
oral,dgl,1,1800,76.1297,0.9324

epoch:1802/50, training loss:0.5564543604850769
Train Acc 0.9363
 Acc 0.9329
oral,dgl,1,1801,76.1719,0.9329

epoch:1803/50, training loss:0.5559232831001282
Train Acc 0.9371
 Acc 0.9331
oral,dgl,1,1802,76.2141,0.9331

epoch:1804/50, training loss:0.5555206537246704
Train Acc 0.9369
 Acc 0.9334
oral,dgl,1,1803,76.2562,0.9334

epoch:1805/50, training loss:0.5552994012832642
Train Acc 0.9370
 Acc 0.9335
new best val f1: 0.9334954427406414
oral,dgl,1,1804,76.2983,0.9335

epoch:1806/50, training loss:0.5552260279655457
Train Acc 0.9372
 Acc 0.9333
oral,dgl,1,1805,76.3406,0.9333

epoch:1807/50, training loss:0.5552124977111816
Train Acc 0.9370
 Acc 0.9335
oral,dgl,1,1806,76.3827,0.9335

epoch:1808/50, training loss:0.5552259087562561
Train Acc 0.9373
 Acc 0.9331
oral,dgl,1,1807,76.4247,0.9331

epoch:1809/50, training loss:0.555218517780304
Train Acc 0.9369
 Acc 0.9333
oral,dgl,1,1808,76.4668,0.9333

epoch:1810/50, training loss:0.5552307367324829
Train Acc 0.9374
 Acc 0.9332
oral,dgl,1,1809,76.5088,0.9332

epoch:1811/50, training loss:0.5552114844322205
Train Acc 0.9369
 Acc 0.9333
oral,dgl,1,1810,76.5510,0.9333

epoch:1812/50, training loss:0.5551616549491882
Train Acc 0.9372
 Acc 0.9332
oral,dgl,1,1811,76.5933,0.9332

epoch:1813/50, training loss:0.5551363825798035
Train Acc 0.9369
 Acc 0.9332
oral,dgl,1,1812,76.6355,0.9332

epoch:1814/50, training loss:0.5551460385322571
Train Acc 0.9370
 Acc 0.9332
oral,dgl,1,1813,76.6774,0.9332

epoch:1815/50, training loss:0.5551389455795288
Train Acc 0.9370
 Acc 0.9332
oral,dgl,1,1814,76.7197,0.9332

epoch:1816/50, training loss:0.5550155639648438
Train Acc 0.9371
 Acc 0.9333
oral,dgl,1,1815,76.7618,0.9333

epoch:1817/50, training loss:0.5548028945922852
Train Acc 0.9370
 Acc 0.9335
new best val f1: 0.9335285262931533
oral,dgl,1,1816,76.8039,0.9335

epoch:1818/50, training loss:0.5546412467956543
Train Acc 0.9372
 Acc 0.9335
oral,dgl,1,1817,76.8458,0.9335

epoch:1819/50, training loss:0.5546165704727173
Train Acc 0.9372
 Acc 0.9334
oral,dgl,1,1818,76.8880,0.9334

epoch:1820/50, training loss:0.5546767711639404
Train Acc 0.9372
 Acc 0.9332
oral,dgl,1,1819,76.9301,0.9332

epoch:1821/50, training loss:0.5547447800636292
Train Acc 0.9372
 Acc 0.9333
oral,dgl,1,1820,76.9722,0.9333

epoch:1822/50, training loss:0.5547254681587219
Train Acc 0.9370
 Acc 0.9334
oral,dgl,1,1821,77.0143,0.9334

epoch:1823/50, training loss:0.5546126961708069
Train Acc 0.9374
 Acc 0.9331
oral,dgl,1,1822,77.0562,0.9331

epoch:1824/50, training loss:0.5545129179954529
Train Acc 0.9369
 Acc 0.9335
oral,dgl,1,1823,77.0984,0.9335

epoch:1825/50, training loss:0.5544829368591309
Train Acc 0.9375
 Acc 0.9331
oral,dgl,1,1824,77.1405,0.9331

epoch:1826/50, training loss:0.5545356869697571
Train Acc 0.9369
 Acc 0.9336
new best val f1: 0.93357484326667
oral,dgl,1,1825,77.1825,0.9336

epoch:1827/50, training loss:0.5545951128005981
Train Acc 0.9374
 Acc 0.9331
oral,dgl,1,1826,77.2246,0.9331

epoch:1828/50, training loss:0.5545799732208252
Train Acc 0.9368
 Acc 0.9337
new best val f1: 0.9336674772137032
oral,dgl,1,1827,77.2668,0.9337

epoch:1829/50, training loss:0.5544069409370422
Train Acc 0.9376
 Acc 0.9332
oral,dgl,1,1828,77.3089,0.9332

epoch:1830/50, training loss:0.5542685985565186
Train Acc 0.9370
 Acc 0.9333
oral,dgl,1,1829,77.3510,0.9333

epoch:1831/50, training loss:0.5542755126953125
Train Acc 0.9375
 Acc 0.9329
oral,dgl,1,1830,77.3929,0.9329

epoch:1832/50, training loss:0.5544435381889343
Train Acc 0.9368
 Acc 0.9330
oral,dgl,1,1831,77.4350,0.9330

epoch:1833/50, training loss:0.554756760597229
Train Acc 0.9374
 Acc 0.9325
oral,dgl,1,1832,77.4771,0.9325

epoch:1834/50, training loss:0.5552221536636353
Train Acc 0.9364
 Acc 0.9327
oral,dgl,1,1833,77.5193,0.9327

epoch:1835/50, training loss:0.5554444789886475
Train Acc 0.9372
 Acc 0.9322
oral,dgl,1,1834,77.5614,0.9322

epoch:1836/50, training loss:0.5555146336555481
Train Acc 0.9362
 Acc 0.9328
oral,dgl,1,1835,77.6037,0.9328

epoch:1837/50, training loss:0.555038571357727
Train Acc 0.9374
 Acc 0.9326
oral,dgl,1,1836,77.6458,0.9326

epoch:1838/50, training loss:0.5546010136604309
Train Acc 0.9365
 Acc 0.9332
oral,dgl,1,1837,77.6878,0.9332

epoch:1839/50, training loss:0.5540605187416077
Train Acc 0.9374
 Acc 0.9333
oral,dgl,1,1838,77.7299,0.9333

epoch:1840/50, training loss:0.553693413734436
Train Acc 0.9370
 Acc 0.9336
oral,dgl,1,1839,77.7719,0.9336

epoch:1841/50, training loss:0.5535354018211365
Train Acc 0.9374
 Acc 0.9337
oral,dgl,1,1840,77.8140,0.9337

epoch:1842/50, training loss:0.5534958839416504
Train Acc 0.9374
 Acc 0.9335
oral,dgl,1,1841,77.8561,0.9335

epoch:1843/50, training loss:0.5534995198249817
Train Acc 0.9373
 Acc 0.9335
oral,dgl,1,1842,77.8981,0.9335

epoch:1844/50, training loss:0.5535271167755127
Train Acc 0.9376
 Acc 0.9331
oral,dgl,1,1843,77.9402,0.9331

epoch:1845/50, training loss:0.5535672903060913
Train Acc 0.9371
 Acc 0.9335
oral,dgl,1,1844,77.9824,0.9335

epoch:1846/50, training loss:0.5535993576049805
Train Acc 0.9376
 Acc 0.9331
oral,dgl,1,1845,78.0245,0.9331

epoch:1847/50, training loss:0.553642988204956
Train Acc 0.9370
 Acc 0.9334
oral,dgl,1,1846,78.0666,0.9334

epoch:1848/50, training loss:0.5536869764328003
Train Acc 0.9376
 Acc 0.9329
oral,dgl,1,1847,78.1087,0.9329

epoch:1849/50, training loss:0.5538567304611206
Train Acc 0.9366
 Acc 0.9331
oral,dgl,1,1848,78.1506,0.9331

epoch:1850/50, training loss:0.5542569160461426
Train Acc 0.9376
 Acc 0.9320
oral,dgl,1,1849,78.1927,0.9320

epoch:1851/50, training loss:0.5550945401191711
Train Acc 0.9360
 Acc 0.9325
oral,dgl,1,1850,78.2348,0.9325

epoch:1852/50, training loss:0.5560300946235657
Train Acc 0.9369
 Acc 0.9305
oral,dgl,1,1851,78.2769,0.9305

epoch:1853/50, training loss:0.5575007796287537
Train Acc 0.9345
 Acc 0.9319
oral,dgl,1,1852,78.3190,0.9319

epoch:1854/50, training loss:0.557857871055603
Train Acc 0.9361
 Acc 0.9303
oral,dgl,1,1853,78.3609,0.9303

epoch:1855/50, training loss:0.5580524206161499
Train Acc 0.9343
 Acc 0.9325
oral,dgl,1,1854,78.4031,0.9325

epoch:1856/50, training loss:0.5562844276428223
Train Acc 0.9371
 Acc 0.9322
oral,dgl,1,1855,78.4452,0.9322

epoch:1857/50, training loss:0.5544346570968628
Train Acc 0.9361
 Acc 0.9335
oral,dgl,1,1856,78.4873,0.9335

epoch:1858/50, training loss:0.5529952049255371
Train Acc 0.9375
 Acc 0.9334
oral,dgl,1,1857,78.5295,0.9334

epoch:1859/50, training loss:0.55284583568573
Train Acc 0.9374
 Acc 0.9325
oral,dgl,1,1858,78.5714,0.9325

epoch:1860/50, training loss:0.5537512898445129
Train Acc 0.9366
 Acc 0.9329
oral,dgl,1,1859,78.6135,0.9329

epoch:1861/50, training loss:0.5549079775810242
Train Acc 0.9372
 Acc 0.9312
oral,dgl,1,1860,78.6557,0.9312

epoch:1862/50, training loss:0.5562326908111572
Train Acc 0.9352
 Acc 0.9323
oral,dgl,1,1861,78.6978,0.9323

epoch:1863/50, training loss:0.5564945340156555
Train Acc 0.9366
 Acc 0.9312
oral,dgl,1,1862,78.7399,0.9312

epoch:1864/50, training loss:0.5564214587211609
Train Acc 0.9353
 Acc 0.9329
oral,dgl,1,1863,78.7819,0.9329

epoch:1865/50, training loss:0.5546998977661133
Train Acc 0.9374
 Acc 0.9330
oral,dgl,1,1864,78.8241,0.9330

epoch:1866/50, training loss:0.5531061291694641
Train Acc 0.9370
 Acc 0.9335
oral,dgl,1,1865,78.8662,0.9335

epoch:1867/50, training loss:0.5523958206176758
Train Acc 0.9374
 Acc 0.9334
oral,dgl,1,1866,78.9083,0.9334

epoch:1868/50, training loss:0.5532786250114441
Train Acc 0.9377
 Acc 0.9315
oral,dgl,1,1867,78.9505,0.9315

epoch:1869/50, training loss:0.5551351308822632
Train Acc 0.9355
 Acc 0.9323
oral,dgl,1,1868,78.9924,0.9323

epoch:1870/50, training loss:0.5563988089561462
Train Acc 0.9366
 Acc 0.9305
oral,dgl,1,1869,79.0344,0.9305

epoch:1871/50, training loss:0.55721515417099
Train Acc 0.9345
 Acc 0.9326
oral,dgl,1,1870,79.0766,0.9326

epoch:1872/50, training loss:0.5554541349411011
Train Acc 0.9369
 Acc 0.9327
oral,dgl,1,1871,79.1187,0.9327

epoch:1873/50, training loss:0.553306519985199
Train Acc 0.9367
 Acc 0.9337
new best val f1: 0.9336740939242055
oral,dgl,1,1872,79.1609,0.9337

epoch:1874/50, training loss:0.5521606802940369
Train Acc 0.9377
 Acc 0.9334
oral,dgl,1,1873,79.2029,0.9334

epoch:1875/50, training loss:0.5531001687049866
Train Acc 0.9376
 Acc 0.9315
oral,dgl,1,1874,79.2450,0.9315

epoch:1876/50, training loss:0.5550307631492615
Train Acc 0.9356
 Acc 0.9325
oral,dgl,1,1875,79.2871,0.9325

epoch:1877/50, training loss:0.5557235479354858
Train Acc 0.9368
 Acc 0.9312
oral,dgl,1,1876,79.3292,0.9312

epoch:1878/50, training loss:0.55550616979599
Train Acc 0.9352
 Acc 0.9330
oral,dgl,1,1877,79.3713,0.9330

epoch:1879/50, training loss:0.5536760091781616
Train Acc 0.9373
 Acc 0.9332
oral,dgl,1,1878,79.4134,0.9332

epoch:1880/50, training loss:0.5521438717842102
Train Acc 0.9371
 Acc 0.9337
oral,dgl,1,1879,79.4555,0.9337

epoch:1881/50, training loss:0.5517995953559875
Train Acc 0.9375
 Acc 0.9333
oral,dgl,1,1880,79.4976,0.9333

epoch:1882/50, training loss:0.5526941418647766
Train Acc 0.9378
 Acc 0.9322
oral,dgl,1,1881,79.5397,0.9322

epoch:1883/50, training loss:0.5537912845611572
Train Acc 0.9363
 Acc 0.9332
oral,dgl,1,1882,79.5818,0.9332

epoch:1884/50, training loss:0.5537186861038208
Train Acc 0.9376
 Acc 0.9327
oral,dgl,1,1883,79.6239,0.9327

epoch:1885/50, training loss:0.5529599785804749
Train Acc 0.9365
 Acc 0.9337
oral,dgl,1,1884,79.6660,0.9337

epoch:1886/50, training loss:0.5518903732299805
Train Acc 0.9380
 Acc 0.9337
new best val f1: 0.9337071774767175
oral,dgl,1,1885,79.7082,0.9337

epoch:1887/50, training loss:0.5515660047531128
Train Acc 0.9375
 Acc 0.9332
oral,dgl,1,1886,79.7503,0.9332

epoch:1888/50, training loss:0.5522170662879944
Train Acc 0.9369
 Acc 0.9332
oral,dgl,1,1887,79.7924,0.9332

epoch:1889/50, training loss:0.5529904365539551
Train Acc 0.9375
 Acc 0.9323
oral,dgl,1,1888,79.8344,0.9323

epoch:1890/50, training loss:0.5533874034881592
Train Acc 0.9364
 Acc 0.9334
oral,dgl,1,1889,79.8765,0.9334

epoch:1891/50, training loss:0.5527682900428772
Train Acc 0.9378
 Acc 0.9332
oral,dgl,1,1890,79.9187,0.9332

epoch:1892/50, training loss:0.5518848896026611
Train Acc 0.9369
 Acc 0.9338
new best val f1: 0.9338395116867649
oral,dgl,1,1891,79.9608,0.9338

epoch:1893/50, training loss:0.5513325929641724
Train Acc 0.9378
 Acc 0.9335
oral,dgl,1,1892,80.0029,0.9335

epoch:1894/50, training loss:0.5514296889305115
Train Acc 0.9376
 Acc 0.9332
oral,dgl,1,1893,80.0448,0.9332

epoch:1895/50, training loss:0.5519354939460754
Train Acc 0.9371
 Acc 0.9332
oral,dgl,1,1894,80.0869,0.9332

epoch:1896/50, training loss:0.5521188378334045
Train Acc 0.9378
 Acc 0.9330
oral,dgl,1,1895,80.1292,0.9330

epoch:1897/50, training loss:0.5519243478775024
Train Acc 0.9370
 Acc 0.9336
oral,dgl,1,1896,80.1712,0.9336

epoch:1898/50, training loss:0.5513519048690796
Train Acc 0.9378
 Acc 0.9337
oral,dgl,1,1897,80.2133,0.9337

epoch:1899/50, training loss:0.550930917263031
Train Acc 0.9377
 Acc 0.9337
oral,dgl,1,1898,80.2555,0.9337

epoch:1900/50, training loss:0.5508813858032227
Train Acc 0.9376
 Acc 0.9337
oral,dgl,1,1899,80.2976,0.9337

epoch:1901/50, training loss:0.5510420203208923
Train Acc 0.9380
 Acc 0.9334
oral,dgl,1,1900,80.3398,0.9334

epoch:1902/50, training loss:0.5511789917945862
Train Acc 0.9373
 Acc 0.9336
oral,dgl,1,1901,80.3818,0.9336

epoch:1903/50, training loss:0.5511167049407959
Train Acc 0.9379
 Acc 0.9335
oral,dgl,1,1902,80.4238,0.9335

epoch:1904/50, training loss:0.550972580909729
Train Acc 0.9375
 Acc 0.9337
oral,dgl,1,1903,80.4660,0.9337

epoch:1905/50, training loss:0.5507914423942566
Train Acc 0.9377
 Acc 0.9337
oral,dgl,1,1904,80.5081,0.9337

epoch:1906/50, training loss:0.5506398677825928
Train Acc 0.9378
 Acc 0.9337
oral,dgl,1,1905,80.5501,0.9337

epoch:1907/50, training loss:0.5505691766738892
Train Acc 0.9376
 Acc 0.9338
oral,dgl,1,1906,80.5922,0.9338

epoch:1908/50, training loss:0.5505587458610535
Train Acc 0.9378
 Acc 0.9336
oral,dgl,1,1907,80.6344,0.9336

epoch:1909/50, training loss:0.5505847334861755
Train Acc 0.9376
 Acc 0.9337
oral,dgl,1,1908,80.6765,0.9337

epoch:1910/50, training loss:0.5505839586257935
Train Acc 0.9378
 Acc 0.9336
oral,dgl,1,1909,80.7186,0.9336

epoch:1911/50, training loss:0.5505487322807312
Train Acc 0.9375
 Acc 0.9338
oral,dgl,1,1910,80.7607,0.9338

epoch:1912/50, training loss:0.550416886806488
Train Acc 0.9378
 Acc 0.9338
oral,dgl,1,1911,80.8026,0.9338

epoch:1913/50, training loss:0.5503254532814026
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1912,80.8448,0.9338

epoch:1914/50, training loss:0.5502816438674927
Train Acc 0.9377
 Acc 0.9339
new best val f1: 0.9339023704365375
oral,dgl,1,1913,80.8869,0.9339

epoch:1915/50, training loss:0.5502393841743469
Train Acc 0.9379
 Acc 0.9336
oral,dgl,1,1914,80.9290,0.9336

epoch:1916/50, training loss:0.5501782894134521
Train Acc 0.9377
 Acc 0.9339
oral,dgl,1,1915,80.9710,0.9339

epoch:1917/50, training loss:0.550092339515686
Train Acc 0.9379
 Acc 0.9338
oral,dgl,1,1916,81.0130,0.9338

epoch:1918/50, training loss:0.550031304359436
Train Acc 0.9378
 Acc 0.9338
oral,dgl,1,1917,81.0551,0.9338

epoch:1919/50, training loss:0.5500348210334778
Train Acc 0.9378
 Acc 0.9340
new best val f1: 0.9339883876730684
oral,dgl,1,1918,81.0973,0.9340

epoch:1920/50, training loss:0.5500440001487732
Train Acc 0.9380
 Acc 0.9337
oral,dgl,1,1919,81.1391,0.9337

epoch:1921/50, training loss:0.5500194430351257
Train Acc 0.9376
 Acc 0.9339
oral,dgl,1,1920,81.1813,0.9339

epoch:1922/50, training loss:0.5499242544174194
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1921,81.2234,0.9338

epoch:1923/50, training loss:0.5498652458190918
Train Acc 0.9377
 Acc 0.9338
oral,dgl,1,1922,81.2654,0.9338

epoch:1924/50, training loss:0.549842119216919
Train Acc 0.9379
 Acc 0.9338
oral,dgl,1,1923,81.3074,0.9338

epoch:1925/50, training loss:0.5498232245445251
Train Acc 0.9378
 Acc 0.9339
oral,dgl,1,1924,81.3496,0.9339

epoch:1926/50, training loss:0.5497525334358215
Train Acc 0.9378
 Acc 0.9338
oral,dgl,1,1925,81.3916,0.9338

epoch:1927/50, training loss:0.5496686100959778
Train Acc 0.9379
 Acc 0.9339
oral,dgl,1,1926,81.4336,0.9339

epoch:1928/50, training loss:0.5496048331260681
Train Acc 0.9379
 Acc 0.9339
oral,dgl,1,1927,81.4759,0.9339

epoch:1929/50, training loss:0.5495677590370178
Train Acc 0.9380
 Acc 0.9337
oral,dgl,1,1928,81.5180,0.9337

epoch:1930/50, training loss:0.5495395660400391
Train Acc 0.9378
 Acc 0.9339
oral,dgl,1,1929,81.5602,0.9339

epoch:1931/50, training loss:0.549476683139801
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1930,81.6022,0.9338

epoch:1932/50, training loss:0.5494075417518616
Train Acc 0.9378
 Acc 0.9340
new best val f1: 0.9340214712255802
oral,dgl,1,1931,81.6442,0.9340

epoch:1933/50, training loss:0.5493512153625488
Train Acc 0.9380
 Acc 0.9340
oral,dgl,1,1932,81.6864,0.9340

epoch:1934/50, training loss:0.5493283271789551
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1933,81.7285,0.9338

epoch:1935/50, training loss:0.5492956042289734
Train Acc 0.9378
 Acc 0.9340
oral,dgl,1,1934,81.7706,0.9340

epoch:1936/50, training loss:0.5492492914199829
Train Acc 0.9380
 Acc 0.9339
oral,dgl,1,1935,81.8125,0.9339

epoch:1937/50, training loss:0.5491868853569031
Train Acc 0.9379
 Acc 0.9339
oral,dgl,1,1936,81.8547,0.9339

epoch:1938/50, training loss:0.549140453338623
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1937,81.8969,0.9338

epoch:1939/50, training loss:0.5491105318069458
Train Acc 0.9378
 Acc 0.9340
new best val f1: 0.9340413213570873
oral,dgl,1,1938,81.9390,0.9340

epoch:1940/50, training loss:0.5490989685058594
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1939,81.9811,0.9338

epoch:1941/50, training loss:0.549109935760498
Train Acc 0.9377
 Acc 0.9341
new best val f1: 0.9340843299753527
oral,dgl,1,1940,82.0231,0.9341

epoch:1942/50, training loss:0.5491214394569397
Train Acc 0.9381
 Acc 0.9338
oral,dgl,1,1941,82.0651,0.9338

epoch:1943/50, training loss:0.5491272807121277
Train Acc 0.9376
 Acc 0.9340
oral,dgl,1,1942,82.1073,0.9340

epoch:1944/50, training loss:0.5490624904632568
Train Acc 0.9380
 Acc 0.9337
oral,dgl,1,1943,82.1494,0.9337

epoch:1945/50, training loss:0.5489776134490967
Train Acc 0.9376
 Acc 0.9341
oral,dgl,1,1944,82.1916,0.9341

epoch:1946/50, training loss:0.5488792657852173
Train Acc 0.9381
 Acc 0.9339
oral,dgl,1,1945,82.2336,0.9339

epoch:1947/50, training loss:0.5487948060035706
Train Acc 0.9379
 Acc 0.9339
oral,dgl,1,1946,82.2757,0.9339

epoch:1948/50, training loss:0.5487427115440369
Train Acc 0.9379
 Acc 0.9340
oral,dgl,1,1947,82.3178,0.9340

epoch:1949/50, training loss:0.5487163066864014
Train Acc 0.9381
 Acc 0.9338
oral,dgl,1,1948,82.3600,0.9338

epoch:1950/50, training loss:0.5487381219863892
Train Acc 0.9377
 Acc 0.9340
oral,dgl,1,1949,82.4021,0.9340

epoch:1951/50, training loss:0.5487884879112244
Train Acc 0.9381
 Acc 0.9336
oral,dgl,1,1950,82.4442,0.9336

epoch:1952/50, training loss:0.5488287806510925
Train Acc 0.9376
 Acc 0.9340
oral,dgl,1,1951,82.4862,0.9340

epoch:1953/50, training loss:0.5488351583480835
Train Acc 0.9382
 Acc 0.9334
oral,dgl,1,1952,82.5283,0.9334

epoch:1954/50, training loss:0.5488895773887634
Train Acc 0.9375
 Acc 0.9339
oral,dgl,1,1953,82.5704,0.9339

epoch:1955/50, training loss:0.5490091443061829
Train Acc 0.9381
 Acc 0.9332
oral,dgl,1,1954,82.6125,0.9332

epoch:1956/50, training loss:0.5491840243339539
Train Acc 0.9373
 Acc 0.9337
oral,dgl,1,1955,82.6546,0.9337

epoch:1957/50, training loss:0.5491412878036499
Train Acc 0.9381
 Acc 0.9334
oral,dgl,1,1956,82.6967,0.9334

epoch:1958/50, training loss:0.548987865447998
Train Acc 0.9375
 Acc 0.9338
oral,dgl,1,1957,82.7388,0.9338

epoch:1959/50, training loss:0.5486708283424377
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1958,82.7809,0.9338

epoch:1960/50, training loss:0.548346996307373
Train Acc 0.9380
 Acc 0.9341
oral,dgl,1,1959,82.8231,0.9341

epoch:1961/50, training loss:0.548112154006958
Train Acc 0.9380
 Acc 0.9340
oral,dgl,1,1960,82.8651,0.9340

epoch:1962/50, training loss:0.5481171607971191
Train Acc 0.9380
 Acc 0.9336
oral,dgl,1,1961,82.9072,0.9336

epoch:1963/50, training loss:0.5483376383781433
Train Acc 0.9377
 Acc 0.9339
oral,dgl,1,1962,82.9493,0.9339

epoch:1964/50, training loss:0.548599123954773
Train Acc 0.9381
 Acc 0.9332
oral,dgl,1,1963,82.9914,0.9332

epoch:1965/50, training loss:0.5488548874855042
Train Acc 0.9373
 Acc 0.9339
oral,dgl,1,1964,83.0336,0.9339

epoch:1966/50, training loss:0.548834502696991
Train Acc 0.9383
 Acc 0.9333
oral,dgl,1,1965,83.0756,0.9333

epoch:1967/50, training loss:0.5487003326416016
Train Acc 0.9374
 Acc 0.9340
oral,dgl,1,1966,83.1177,0.9340

epoch:1968/50, training loss:0.548433244228363
Train Acc 0.9382
 Acc 0.9336
oral,dgl,1,1967,83.1598,0.9336

epoch:1969/50, training loss:0.5481137037277222
Train Acc 0.9376
 Acc 0.9340
oral,dgl,1,1968,83.2019,0.9340

epoch:1970/50, training loss:0.5478275418281555
Train Acc 0.9382
 Acc 0.9340
oral,dgl,1,1969,83.2439,0.9340

epoch:1971/50, training loss:0.5476942658424377
Train Acc 0.9380
 Acc 0.9340
oral,dgl,1,1970,83.2861,0.9340

epoch:1972/50, training loss:0.5476992726325989
Train Acc 0.9379
 Acc 0.9340
oral,dgl,1,1971,83.3282,0.9340

epoch:1973/50, training loss:0.5478370189666748
Train Acc 0.9381
 Acc 0.9335
oral,dgl,1,1972,83.3702,0.9335

epoch:1974/50, training loss:0.5481007695198059
Train Acc 0.9377
 Acc 0.9338
oral,dgl,1,1973,83.4122,0.9338

epoch:1975/50, training loss:0.548487663269043
Train Acc 0.9381
 Acc 0.9330
oral,dgl,1,1974,83.4545,0.9330

epoch:1976/50, training loss:0.5490218997001648
Train Acc 0.9371
 Acc 0.9336
oral,dgl,1,1975,83.4965,0.9336

epoch:1977/50, training loss:0.5490816235542297
Train Acc 0.9379
 Acc 0.9330
oral,dgl,1,1976,83.5386,0.9330

epoch:1978/50, training loss:0.5489193797111511
Train Acc 0.9372
 Acc 0.9336
oral,dgl,1,1977,83.5807,0.9336

epoch:1979/50, training loss:0.5482657551765442
Train Acc 0.9380
 Acc 0.9338
oral,dgl,1,1978,83.6227,0.9338

epoch:1980/50, training loss:0.5476953387260437
Train Acc 0.9379
 Acc 0.9340
oral,dgl,1,1979,83.6648,0.9340

epoch:1981/50, training loss:0.5473239421844482
Train Acc 0.9381
 Acc 0.9342
new best val f1: 0.9341637305013812
oral,dgl,1,1980,83.7069,0.9342

epoch:1982/50, training loss:0.5472824573516846
Train Acc 0.9382
 Acc 0.9337
oral,dgl,1,1981,83.7488,0.9337

epoch:1983/50, training loss:0.5474853515625
Train Acc 0.9377
 Acc 0.9340
oral,dgl,1,1982,83.7909,0.9340

epoch:1984/50, training loss:0.5478079915046692
Train Acc 0.9383
 Acc 0.9332
oral,dgl,1,1983,83.8330,0.9332

epoch:1985/50, training loss:0.5481924414634705
Train Acc 0.9374
 Acc 0.9337
oral,dgl,1,1984,83.8751,0.9337

epoch:1986/50, training loss:0.5483835339546204
Train Acc 0.9380
 Acc 0.9333
oral,dgl,1,1985,83.9173,0.9333

epoch:1987/50, training loss:0.5484963059425354
Train Acc 0.9374
 Acc 0.9336
oral,dgl,1,1986,83.9594,0.9336

epoch:1988/50, training loss:0.5481742024421692
Train Acc 0.9380
 Acc 0.9337
oral,dgl,1,1987,84.0015,0.9337

epoch:1989/50, training loss:0.5477039813995361
Train Acc 0.9377
 Acc 0.9339
oral,dgl,1,1988,84.0436,0.9339

epoch:1990/50, training loss:0.547107994556427
Train Acc 0.9381
 Acc 0.9342
new best val f1: 0.9341835806328883
oral,dgl,1,1989,84.0857,0.9342

epoch:1991/50, training loss:0.5468348860740662
Train Acc 0.9382
 Acc 0.9338
oral,dgl,1,1990,84.1277,0.9338

epoch:1992/50, training loss:0.5469616055488586
Train Acc 0.9379
 Acc 0.9340
oral,dgl,1,1991,84.1698,0.9340

epoch:1993/50, training loss:0.5473321080207825
Train Acc 0.9382
 Acc 0.9333
oral,dgl,1,1992,84.2119,0.9333

epoch:1994/50, training loss:0.5478224158287048
Train Acc 0.9374
 Acc 0.9339
oral,dgl,1,1993,84.2539,0.9339

epoch:1995/50, training loss:0.5480074286460876
Train Acc 0.9380
 Acc 0.9331
oral,dgl,1,1994,84.2961,0.9331

epoch:1996/50, training loss:0.5479389429092407
Train Acc 0.9373
 Acc 0.9341
oral,dgl,1,1995,84.3381,0.9341

epoch:1997/50, training loss:0.5474123358726501
Train Acc 0.9384
 Acc 0.9338
oral,dgl,1,1996,84.3802,0.9338

epoch:1998/50, training loss:0.5468985438346863
Train Acc 0.9378
 Acc 0.9342
new best val f1: 0.9342464393826609
oral,dgl,1,1997,84.4224,0.9342

epoch:1999/50, training loss:0.546501874923706
Train Acc 0.9383
 Acc 0.9341
oral,dgl,1,1998,84.4645,0.9341

epoch:2000/50, training loss:0.5465396642684937
Train Acc 0.9382
 Acc 0.9337
oral,dgl,1,1999,84.5065,0.9337

training using time 305.2253189086914
Traceback (most recent call last):
  File "dgl/train_full_load.py", line 340, in <module>
    main(args)
  File "dgl/train_full_load.py", line 317, in main
    model, g, g.ndata['label'], test_mask, False)
TypeError: cannot unpack non-iterable float object
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2122196) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2122196 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     dgl/train_full_load.py FAILED     
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:24:53
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2122196)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

[master efb54c0] Jul12-02:02:30-MDT2022
 18 files changed, 72540 insertions(+), 66 deletions(-)
 create mode 100644 full_new.csv
 create mode 100644 log/reddit/test/loggings
 create mode 100644 slurm-4924843.out
 create mode 100644 slurm-4924845.out
 create mode 100644 test_acc_dgl.csv
remote: error: Trace: f04c392616d40618b0df5b37cb2653af1b7eb1652a7508785cf27c6a94c05361        
remote: error: See http://git.io/iEPt8g for more information.        
remote: error: File oral_adj_part_rk0 is 414.56 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: File arctic25_adj_part_rk0 is 880.77 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: File meta_adj_part_rk0 is 994.57 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.        
To github.com:JoseYan/GNNs.git
 ! [remote rejected] master -> master (pre-receive hook declined)
error: failed to push some refs to 'github.com:JoseYan/GNNs.git'
run.sh: line 28: pushd: ../GNN_logs: No such file or directory
[master f2147b4] Jul12-02:02:30-MDT2022
 1 file changed, 17 insertions(+)
remote: error: Trace: f2c0d4324837ab9a4d9f1b69176d35a4239f0dffc998dd78d186e49c66e35856        
remote: error: See http://git.io/iEPt8g for more information.        
remote: error: File oral_adj_part_rk0 is 414.56 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: File arctic25_adj_part_rk0 is 880.77 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: File meta_adj_part_rk0 is 994.57 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.        
To github.com:JoseYan/GNNs.git
 ! [remote rejected] master -> master (pre-receive hook declined)
error: failed to push some refs to 'github.com:JoseYan/GNNs.git'
run.sh: line 32: popd: directory stack empty
+ sh run.sh sh GNN-RDM/genacc_cag.sh
Date:  Jul12-02:27:13-MDT2022
/uufs/chpc.utah.edu/common/home/u1320844/GNNs_log/Jul12-02:27:13-MDT2022_env_info.txt
/uufs/chpc.utah.edu/common/home/u1320844/GNNs_log/Jul12-02:27:13-MDT2022_running_log.txt
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='ogbn-arxiv', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: ogbn-arxiv timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 1
device: cuda:0
tensor([[104447,  15858, 107156,  ...,  45118,  45118,  45118],
        [ 13091,  47283,  69161,  ..., 162473, 162537,  72717]],
       device='cuda:0')
tensor([[ 13091,  47283,  69161,  ..., 162473, 162537,  72717],
        [104447,  15858, 107156,  ...,  45118,  45118,  45118]])
rank: 0 adj_matrix_loc.size: torch.Size([169343, 169343])
rank: 0 inputs.size: torch.Size([169343, 128])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 0 run 0
Epoch: 000
ogbn-arxiv,CAGNET-1D,1,0,0.0353,0.0327

Epoch: 001
ogbn-arxiv,CAGNET-1D,1,1,0.0698,0.0213

Epoch: 002
ogbn-arxiv,CAGNET-1D,1,2,0.1045,0.0150

Epoch: 003
ogbn-arxiv,CAGNET-1D,1,3,0.1399,0.0215

Epoch: 004
ogbn-arxiv,CAGNET-1D,1,4,0.1749,0.2069

Epoch: 005
ogbn-arxiv,CAGNET-1D,1,5,0.2099,0.1028

Epoch: 006
ogbn-arxiv,CAGNET-1D,1,6,0.2448,0.0280

Epoch: 007
ogbn-arxiv,CAGNET-1D,1,7,0.2799,0.0633

Epoch: 008
ogbn-arxiv,CAGNET-1D,1,8,0.3150,0.1846

Epoch: 009
ogbn-arxiv,CAGNET-1D,1,9,0.3507,0.1738

Epoch: 010
ogbn-arxiv,CAGNET-1D,1,10,0.3863,0.0584

Epoch: 011
ogbn-arxiv,CAGNET-1D,1,11,0.4221,0.0334

Epoch: 012
ogbn-arxiv,CAGNET-1D,1,12,0.4567,0.0664

Epoch: 013
ogbn-arxiv,CAGNET-1D,1,13,0.4918,0.2008

Epoch: 014
ogbn-arxiv,CAGNET-1D,1,14,0.5267,0.1695

Epoch: 015
ogbn-arxiv,CAGNET-1D,1,15,0.5622,0.0456

Epoch: 016
ogbn-arxiv,CAGNET-1D,1,16,0.5980,0.0461

Epoch: 017
ogbn-arxiv,CAGNET-1D,1,17,0.6339,0.0344

Epoch: 018
ogbn-arxiv,CAGNET-1D,1,18,0.6702,0.0271

Epoch: 019
ogbn-arxiv,CAGNET-1D,1,19,0.7075,0.0313

Epoch: 020
ogbn-arxiv,CAGNET-1D,1,20,0.7454,0.0378

Epoch: 021
ogbn-arxiv,CAGNET-1D,1,21,0.7859,0.0593

Epoch: 022
ogbn-arxiv,CAGNET-1D,1,22,0.8243,0.1325

Epoch: 023
ogbn-arxiv,CAGNET-1D,1,23,0.8611,0.1681

Epoch: 024
ogbn-arxiv,CAGNET-1D,1,24,0.8990,0.1374

Epoch: 025
ogbn-arxiv,CAGNET-1D,1,25,0.9360,0.0814

Epoch: 026
ogbn-arxiv,CAGNET-1D,1,26,0.9730,0.0534

Epoch: 027
ogbn-arxiv,CAGNET-1D,1,27,1.0101,0.0507

Epoch: 028
ogbn-arxiv,CAGNET-1D,1,28,1.0473,0.0527

Epoch: 029
ogbn-arxiv,CAGNET-1D,1,29,1.0848,0.0612

Epoch: 030
ogbn-arxiv,CAGNET-1D,1,30,1.1229,0.0808

Epoch: 031
ogbn-arxiv,CAGNET-1D,1,31,1.1608,0.0691

Epoch: 032
ogbn-arxiv,CAGNET-1D,1,32,1.1985,0.0585

Epoch: 033
ogbn-arxiv,CAGNET-1D,1,33,1.2368,0.0617

Epoch: 034
ogbn-arxiv,CAGNET-1D,1,34,1.2745,0.0613

Epoch: 035
ogbn-arxiv,CAGNET-1D,1,35,1.3125,0.0606

Epoch: 036
ogbn-arxiv,CAGNET-1D,1,36,1.3511,0.0690

Epoch: 037
ogbn-arxiv,CAGNET-1D,1,37,1.3884,0.0911

Epoch: 038
ogbn-arxiv,CAGNET-1D,1,38,1.4253,0.1057

Epoch: 039
ogbn-arxiv,CAGNET-1D,1,39,1.4633,0.1028

Epoch: 040
ogbn-arxiv,CAGNET-1D,1,40,1.5007,0.0868

Epoch: 041
ogbn-arxiv,CAGNET-1D,1,41,1.5380,0.0720

Epoch: 042
ogbn-arxiv,CAGNET-1D,1,42,1.5757,0.0623

Epoch: 043
ogbn-arxiv,CAGNET-1D,1,43,1.6136,0.0611

Epoch: 044
ogbn-arxiv,CAGNET-1D,1,44,1.6520,0.0683

Epoch: 045
ogbn-arxiv,CAGNET-1D,1,45,1.6922,0.0836

Epoch: 046
ogbn-arxiv,CAGNET-1D,1,46,1.7300,0.0913

Epoch: 047
ogbn-arxiv,CAGNET-1D,1,47,1.7679,0.0909

Epoch: 048
ogbn-arxiv,CAGNET-1D,1,48,1.8057,0.0855

Epoch: 049
ogbn-arxiv,CAGNET-1D,1,49,1.8437,0.0814

Epoch: 050
ogbn-arxiv,CAGNET-1D,1,50,1.8819,0.0818

Epoch: 051
ogbn-arxiv,CAGNET-1D,1,51,1.9195,0.0862

Epoch: 052
ogbn-arxiv,CAGNET-1D,1,52,1.9571,0.0897

Epoch: 053
ogbn-arxiv,CAGNET-1D,1,53,1.9941,0.0853

Epoch: 054
ogbn-arxiv,CAGNET-1D,1,54,2.0310,0.0803

Epoch: 055
ogbn-arxiv,CAGNET-1D,1,55,2.0675,0.0790

Epoch: 056
ogbn-arxiv,CAGNET-1D,1,56,2.1049,0.0795

Epoch: 057
ogbn-arxiv,CAGNET-1D,1,57,2.1420,0.0813

Epoch: 058
ogbn-arxiv,CAGNET-1D,1,58,2.1801,0.0857

Epoch: 059
ogbn-arxiv,CAGNET-1D,1,59,2.2172,0.0919

Epoch: 060
ogbn-arxiv,CAGNET-1D,1,60,2.2542,0.0976

Epoch: 061
ogbn-arxiv,CAGNET-1D,1,61,2.2910,0.0963

Epoch: 062
ogbn-arxiv,CAGNET-1D,1,62,2.3282,0.0902

Epoch: 063
ogbn-arxiv,CAGNET-1D,1,63,2.3645,0.0818

Epoch: 064
ogbn-arxiv,CAGNET-1D,1,64,2.4015,0.0754

Epoch: 065
ogbn-arxiv,CAGNET-1D,1,65,2.4382,0.0765

Epoch: 066
ogbn-arxiv,CAGNET-1D,1,66,2.4763,0.0816

Epoch: 067
ogbn-arxiv,CAGNET-1D,1,67,2.5138,0.0887

Epoch: 068
ogbn-arxiv,CAGNET-1D,1,68,2.5512,0.0944

Epoch: 069
ogbn-arxiv,CAGNET-1D,1,69,2.5885,0.0958

Epoch: 070
ogbn-arxiv,CAGNET-1D,1,70,2.6262,0.0933

Epoch: 071
ogbn-arxiv,CAGNET-1D,1,71,2.6630,0.0870

Epoch: 072
ogbn-arxiv,CAGNET-1D,1,72,2.6999,0.0828

Epoch: 073
ogbn-arxiv,CAGNET-1D,1,73,2.7366,0.0803

Epoch: 074
ogbn-arxiv,CAGNET-1D,1,74,2.7732,0.0803

Epoch: 075
ogbn-arxiv,CAGNET-1D,1,75,2.8096,0.0837

Epoch: 076
ogbn-arxiv,CAGNET-1D,1,76,2.8466,0.0876

Epoch: 077
ogbn-arxiv,CAGNET-1D,1,77,2.8836,0.0902

Epoch: 078
ogbn-arxiv,CAGNET-1D,1,78,2.9202,0.0904

Epoch: 079
ogbn-arxiv,CAGNET-1D,1,79,2.9572,0.0876

Epoch: 080
ogbn-arxiv,CAGNET-1D,1,80,2.9943,0.0856

Epoch: 081
ogbn-arxiv,CAGNET-1D,1,81,3.0317,0.0829

Epoch: 082
ogbn-arxiv,CAGNET-1D,1,82,3.0702,0.0823

Epoch: 083
ogbn-arxiv,CAGNET-1D,1,83,3.1082,0.0835

Epoch: 084
ogbn-arxiv,CAGNET-1D,1,84,3.1457,0.0843

Epoch: 085
ogbn-arxiv,CAGNET-1D,1,85,3.1834,0.0852

Epoch: 086
ogbn-arxiv,CAGNET-1D,1,86,3.2200,0.0857

Epoch: 087
ogbn-arxiv,CAGNET-1D,1,87,3.2572,0.0857

Epoch: 088
ogbn-arxiv,CAGNET-1D,1,88,3.2954,0.0845

Epoch: 089
ogbn-arxiv,CAGNET-1D,1,89,3.3332,0.0838

Epoch: 090
ogbn-arxiv,CAGNET-1D,1,90,3.3704,0.0839

Epoch: 091
ogbn-arxiv,CAGNET-1D,1,91,3.4078,0.0840

Epoch: 092
ogbn-arxiv,CAGNET-1D,1,92,3.4449,0.0843

Epoch: 093
ogbn-arxiv,CAGNET-1D,1,93,3.4821,0.0836

Epoch: 094
ogbn-arxiv,CAGNET-1D,1,94,3.5218,0.0831

Epoch: 095
ogbn-arxiv,CAGNET-1D,1,95,3.5597,0.0826

Epoch: 096
ogbn-arxiv,CAGNET-1D,1,96,3.5968,0.0827

Epoch: 097
ogbn-arxiv,CAGNET-1D,1,97,3.6337,0.0832

Epoch: 098
ogbn-arxiv,CAGNET-1D,1,98,3.6706,0.0836

Epoch: 099
ogbn-arxiv,CAGNET-1D,1,99,3.7073,0.0834

Epoch: 100
ogbn-arxiv,CAGNET-1D,1,100,3.7441,0.0826

Epoch: 101
ogbn-arxiv,CAGNET-1D,1,101,3.7810,0.0826

Epoch: 102
ogbn-arxiv,CAGNET-1D,1,102,3.8185,0.0822

Epoch: 103
ogbn-arxiv,CAGNET-1D,1,103,3.8565,0.0818

Epoch: 104
ogbn-arxiv,CAGNET-1D,1,104,3.8943,0.0819

Epoch: 105
ogbn-arxiv,CAGNET-1D,1,105,3.9320,0.0815

Epoch: 106
ogbn-arxiv,CAGNET-1D,1,106,3.9695,0.0812

Epoch: 107
ogbn-arxiv,CAGNET-1D,1,107,4.0068,0.0820

Epoch: 108
ogbn-arxiv,CAGNET-1D,1,108,4.0445,0.0813

Epoch: 109
ogbn-arxiv,CAGNET-1D,1,109,4.0817,0.0812

Epoch: 110
ogbn-arxiv,CAGNET-1D,1,110,4.1185,0.0818

Epoch: 111
ogbn-arxiv,CAGNET-1D,1,111,4.1556,0.0819

Epoch: 112
ogbn-arxiv,CAGNET-1D,1,112,4.1925,0.0808

Epoch: 113
ogbn-arxiv,CAGNET-1D,1,113,4.2296,0.0806

Epoch: 114
ogbn-arxiv,CAGNET-1D,1,114,4.2672,0.0806

Epoch: 115
ogbn-arxiv,CAGNET-1D,1,115,4.3052,0.0803

Epoch: 116
ogbn-arxiv,CAGNET-1D,1,116,4.3429,0.0809

Epoch: 117
ogbn-arxiv,CAGNET-1D,1,117,4.3805,0.0809

Epoch: 118
ogbn-arxiv,CAGNET-1D,1,118,4.4197,0.0810

Epoch: 119
ogbn-arxiv,CAGNET-1D,1,119,4.4571,0.0807

Epoch: 120
ogbn-arxiv,CAGNET-1D,1,120,4.4945,0.0811

Epoch: 121
ogbn-arxiv,CAGNET-1D,1,121,4.5324,0.0806

Epoch: 122
ogbn-arxiv,CAGNET-1D,1,122,4.5692,0.0800

Epoch: 123
ogbn-arxiv,CAGNET-1D,1,123,4.6060,0.0799

Epoch: 124
ogbn-arxiv,CAGNET-1D,1,124,4.6434,0.0806

Epoch: 125
ogbn-arxiv,CAGNET-1D,1,125,4.6811,0.0810

Epoch: 126
ogbn-arxiv,CAGNET-1D,1,126,4.7190,0.0804

Epoch: 127
ogbn-arxiv,CAGNET-1D,1,127,4.7567,0.0809

Epoch: 128
ogbn-arxiv,CAGNET-1D,1,128,4.7943,0.0801

Epoch: 129
ogbn-arxiv,CAGNET-1D,1,129,4.8316,0.0798

Epoch: 130
ogbn-arxiv,CAGNET-1D,1,130,4.8687,0.0796

Epoch: 131
ogbn-arxiv,CAGNET-1D,1,131,4.9058,0.0796

Epoch: 132
ogbn-arxiv,CAGNET-1D,1,132,4.9428,0.0799

Epoch: 133
ogbn-arxiv,CAGNET-1D,1,133,4.9797,0.0809

Epoch: 134
ogbn-arxiv,CAGNET-1D,1,134,5.0166,0.0806

Epoch: 135
ogbn-arxiv,CAGNET-1D,1,135,5.0540,0.0802

Epoch: 136
ogbn-arxiv,CAGNET-1D,1,136,5.0914,0.0798

Epoch: 137
ogbn-arxiv,CAGNET-1D,1,137,5.1291,0.0809

Epoch: 138
ogbn-arxiv,CAGNET-1D,1,138,5.1668,0.0808

Epoch: 139
ogbn-arxiv,CAGNET-1D,1,139,5.2043,0.0812

Epoch: 140
ogbn-arxiv,CAGNET-1D,1,140,5.2420,0.0814

Epoch: 141
ogbn-arxiv,CAGNET-1D,1,141,5.2797,0.0817

Epoch: 142
ogbn-arxiv,CAGNET-1D,1,142,5.3172,0.0823

Epoch: 143
ogbn-arxiv,CAGNET-1D,1,143,5.3552,0.0823

Epoch: 144
ogbn-arxiv,CAGNET-1D,1,144,5.3925,0.0824

Epoch: 145
ogbn-arxiv,CAGNET-1D,1,145,5.4295,0.0822

Epoch: 146
ogbn-arxiv,CAGNET-1D,1,146,5.4668,0.0826

Epoch: 147
ogbn-arxiv,CAGNET-1D,1,147,5.5039,0.0829

Epoch: 148
ogbn-arxiv,CAGNET-1D,1,148,5.5411,0.0834

Epoch: 149
ogbn-arxiv,CAGNET-1D,1,149,5.5789,0.0836

Epoch: 150
ogbn-arxiv,CAGNET-1D,1,150,5.6175,0.0836

Epoch: 151
ogbn-arxiv,CAGNET-1D,1,151,5.6554,0.0838

Epoch: 152
ogbn-arxiv,CAGNET-1D,1,152,5.6935,0.0838

Epoch: 153
ogbn-arxiv,CAGNET-1D,1,153,5.7313,0.0843

Epoch: 154
ogbn-arxiv,CAGNET-1D,1,154,5.7689,0.0850

Epoch: 155
ogbn-arxiv,CAGNET-1D,1,155,5.8063,0.0851

Epoch: 156
ogbn-arxiv,CAGNET-1D,1,156,5.8435,0.0848

Epoch: 157
ogbn-arxiv,CAGNET-1D,1,157,5.8822,0.0847

Epoch: 158
ogbn-arxiv,CAGNET-1D,1,158,5.9201,0.0847

Epoch: 159
ogbn-arxiv,CAGNET-1D,1,159,5.9580,0.0845

Epoch: 160
ogbn-arxiv,CAGNET-1D,1,160,5.9960,0.0841

Epoch: 161
ogbn-arxiv,CAGNET-1D,1,161,6.0342,0.0848

Epoch: 162
ogbn-arxiv,CAGNET-1D,1,162,6.0723,0.0842

Epoch: 163
ogbn-arxiv,CAGNET-1D,1,163,6.1103,0.0841

Epoch: 164
ogbn-arxiv,CAGNET-1D,1,164,6.1483,0.0832

Epoch: 165
ogbn-arxiv,CAGNET-1D,1,165,6.1859,0.0825

Epoch: 166
ogbn-arxiv,CAGNET-1D,1,166,6.2233,0.0828

Epoch: 167
ogbn-arxiv,CAGNET-1D,1,167,6.2621,0.0825

Epoch: 168
ogbn-arxiv,CAGNET-1D,1,168,6.3001,0.0823

Epoch: 169
ogbn-arxiv,CAGNET-1D,1,169,6.3371,0.0818

Epoch: 170
ogbn-arxiv,CAGNET-1D,1,170,6.3741,0.0814

Epoch: 171
ogbn-arxiv,CAGNET-1D,1,171,6.4110,0.0800

Epoch: 172
ogbn-arxiv,CAGNET-1D,1,172,6.4477,0.0803

Epoch: 173
ogbn-arxiv,CAGNET-1D,1,173,6.4843,0.0799

Epoch: 174
ogbn-arxiv,CAGNET-1D,1,174,6.5211,0.0795

Epoch: 175
ogbn-arxiv,CAGNET-1D,1,175,6.5581,0.0790

Epoch: 176
ogbn-arxiv,CAGNET-1D,1,176,6.5954,0.0782

Epoch: 177
ogbn-arxiv,CAGNET-1D,1,177,6.6335,0.0786

Epoch: 178
ogbn-arxiv,CAGNET-1D,1,178,6.6716,0.0776

Epoch: 179
ogbn-arxiv,CAGNET-1D,1,179,6.7097,0.0759

Epoch: 180
ogbn-arxiv,CAGNET-1D,1,180,6.7482,0.0758

Epoch: 181
ogbn-arxiv,CAGNET-1D,1,181,6.7856,0.0753

Epoch: 182
ogbn-arxiv,CAGNET-1D,1,182,6.8231,0.0756

Epoch: 183
ogbn-arxiv,CAGNET-1D,1,183,6.8602,0.0740

Epoch: 184
ogbn-arxiv,CAGNET-1D,1,184,6.8975,0.0729

Epoch: 185
ogbn-arxiv,CAGNET-1D,1,185,6.9345,0.0725

Epoch: 186
ogbn-arxiv,CAGNET-1D,1,186,6.9715,0.0722

Epoch: 187
ogbn-arxiv,CAGNET-1D,1,187,7.0090,0.0715

Epoch: 188
ogbn-arxiv,CAGNET-1D,1,188,7.0466,0.0712

Epoch: 189
ogbn-arxiv,CAGNET-1D,1,189,7.0842,0.0711

Epoch: 190
ogbn-arxiv,CAGNET-1D,1,190,7.1225,0.0701

Epoch: 191
ogbn-arxiv,CAGNET-1D,1,191,7.1612,0.0680

Epoch: 192
ogbn-arxiv,CAGNET-1D,1,192,7.1994,0.0689

Epoch: 193
ogbn-arxiv,CAGNET-1D,1,193,7.2372,0.0694

Epoch: 194
ogbn-arxiv,CAGNET-1D,1,194,7.2749,0.0685

Epoch: 195
ogbn-arxiv,CAGNET-1D,1,195,7.3125,0.0671

Epoch: 196
ogbn-arxiv,CAGNET-1D,1,196,7.3500,0.0659

Epoch: 197
ogbn-arxiv,CAGNET-1D,1,197,7.3878,0.0668

Epoch: 198
ogbn-arxiv,CAGNET-1D,1,198,7.4238,0.0659

Epoch: 199
ogbn-arxiv,CAGNET-1D,1,199,7.4609,0.0644

Epoch: 200
ogbn-arxiv,CAGNET-1D,1,200,7.4969,0.0652

Epoch: 201
ogbn-arxiv,CAGNET-1D,1,201,7.5331,0.0659

Epoch: 202
ogbn-arxiv,CAGNET-1D,1,202,7.5700,0.0636

Epoch: 203
ogbn-arxiv,CAGNET-1D,1,203,7.6062,0.0622

Epoch: 204
ogbn-arxiv,CAGNET-1D,1,204,7.6435,0.0633

Epoch: 205
ogbn-arxiv,CAGNET-1D,1,205,7.6804,0.0639

Epoch: 206
ogbn-arxiv,CAGNET-1D,1,206,7.7180,0.0627

Epoch: 207
ogbn-arxiv,CAGNET-1D,1,207,7.7549,0.0613

Epoch: 208
ogbn-arxiv,CAGNET-1D,1,208,7.7929,0.0623

Epoch: 209
ogbn-arxiv,CAGNET-1D,1,209,7.8303,0.0621

Epoch: 210
ogbn-arxiv,CAGNET-1D,1,210,7.8669,0.0610

Epoch: 211
ogbn-arxiv,CAGNET-1D,1,211,7.9036,0.0602

Epoch: 212
ogbn-arxiv,CAGNET-1D,1,212,7.9410,0.0620

Epoch: 213
ogbn-arxiv,CAGNET-1D,1,213,7.9776,0.0619

Epoch: 214
ogbn-arxiv,CAGNET-1D,1,214,8.0145,0.0597

Epoch: 215
ogbn-arxiv,CAGNET-1D,1,215,8.0534,0.0602

Epoch: 216
ogbn-arxiv,CAGNET-1D,1,216,8.0909,0.0617

Epoch: 217
ogbn-arxiv,CAGNET-1D,1,217,8.1275,0.0600

Epoch: 218
ogbn-arxiv,CAGNET-1D,1,218,8.1647,0.0590

Epoch: 219
ogbn-arxiv,CAGNET-1D,1,219,8.2013,0.0601

Epoch: 220
ogbn-arxiv,CAGNET-1D,1,220,8.2383,0.0611

Epoch: 221
ogbn-arxiv,CAGNET-1D,1,221,8.2750,0.0599

Epoch: 222
ogbn-arxiv,CAGNET-1D,1,222,8.3122,0.0590

Epoch: 223
ogbn-arxiv,CAGNET-1D,1,223,8.3493,0.0595

Epoch: 224
ogbn-arxiv,CAGNET-1D,1,224,8.3872,0.0602

Epoch: 225
ogbn-arxiv,CAGNET-1D,1,225,8.4247,0.0594

Epoch: 226
ogbn-arxiv,CAGNET-1D,1,226,8.4628,0.0590

Epoch: 227
ogbn-arxiv,CAGNET-1D,1,227,8.4995,0.0592

Epoch: 228
ogbn-arxiv,CAGNET-1D,1,228,8.5364,0.0602

Epoch: 229
ogbn-arxiv,CAGNET-1D,1,229,8.5731,0.0590

Epoch: 230
ogbn-arxiv,CAGNET-1D,1,230,8.6101,0.0587

Epoch: 231
ogbn-arxiv,CAGNET-1D,1,231,8.6475,0.0592

Epoch: 232
ogbn-arxiv,CAGNET-1D,1,232,8.6849,0.0600

Epoch: 233
ogbn-arxiv,CAGNET-1D,1,233,8.7224,0.0588

Epoch: 234
ogbn-arxiv,CAGNET-1D,1,234,8.7596,0.0585

Epoch: 235
ogbn-arxiv,CAGNET-1D,1,235,8.7966,0.0592

Epoch: 236
ogbn-arxiv,CAGNET-1D,1,236,8.8348,0.0597

Epoch: 237
ogbn-arxiv,CAGNET-1D,1,237,8.8724,0.0582

Epoch: 238
ogbn-arxiv,CAGNET-1D,1,238,8.9094,0.0587

Epoch: 239
ogbn-arxiv,CAGNET-1D,1,239,8.9472,0.0600

Epoch: 240
ogbn-arxiv,CAGNET-1D,1,240,8.9845,0.0582

Epoch: 241
ogbn-arxiv,CAGNET-1D,1,241,9.0209,0.0580

Epoch: 242
ogbn-arxiv,CAGNET-1D,1,242,9.0584,0.0599

Epoch: 243
ogbn-arxiv,CAGNET-1D,1,243,9.0952,0.0593

Epoch: 244
ogbn-arxiv,CAGNET-1D,1,244,9.1328,0.0589

Epoch: 245
ogbn-arxiv,CAGNET-1D,1,245,9.1696,0.0589

Epoch: 246
ogbn-arxiv,CAGNET-1D,1,246,9.2075,0.0592

Epoch: 247
ogbn-arxiv,CAGNET-1D,1,247,9.2444,0.0590

Epoch: 248
ogbn-arxiv,CAGNET-1D,1,248,9.2818,0.0585

Epoch: 249
ogbn-arxiv,CAGNET-1D,1,249,9.3190,0.0600

Epoch: 250
ogbn-arxiv,CAGNET-1D,1,250,9.3566,0.0599

Epoch: 251
ogbn-arxiv,CAGNET-1D,1,251,9.3947,0.0583

Epoch: 252
ogbn-arxiv,CAGNET-1D,1,252,9.4326,0.0590

Epoch: 253
ogbn-arxiv,CAGNET-1D,1,253,9.4700,0.0597

Epoch: 254
ogbn-arxiv,CAGNET-1D,1,254,9.5074,0.0593

Epoch: 255
ogbn-arxiv,CAGNET-1D,1,255,9.5446,0.0589

Epoch: 256
ogbn-arxiv,CAGNET-1D,1,256,9.5819,0.0599

Epoch: 257
ogbn-arxiv,CAGNET-1D,1,257,9.6192,0.0595

Epoch: 258
ogbn-arxiv,CAGNET-1D,1,258,9.6561,0.0584

Epoch: 259
ogbn-arxiv,CAGNET-1D,1,259,9.6930,0.0589

Epoch: 260
ogbn-arxiv,CAGNET-1D,1,260,9.7297,0.0608

Epoch: 261
ogbn-arxiv,CAGNET-1D,1,261,9.7668,0.0588

Epoch: 262
ogbn-arxiv,CAGNET-1D,1,262,9.8043,0.0592

Epoch: 263
ogbn-arxiv,CAGNET-1D,1,263,9.8422,0.0599

Epoch: 264
ogbn-arxiv,CAGNET-1D,1,264,9.8815,0.0590

Epoch: 265
ogbn-arxiv,CAGNET-1D,1,265,9.9194,0.0587

Epoch: 266
ogbn-arxiv,CAGNET-1D,1,266,9.9570,0.0597

Epoch: 267
ogbn-arxiv,CAGNET-1D,1,267,9.9944,0.0602

Epoch: 268
ogbn-arxiv,CAGNET-1D,1,268,10.0311,0.0587

Epoch: 269
ogbn-arxiv,CAGNET-1D,1,269,10.0680,0.0582

Epoch: 270
ogbn-arxiv,CAGNET-1D,1,270,10.1044,0.0609

Epoch: 271
ogbn-arxiv,CAGNET-1D,1,271,10.1413,0.0598

Epoch: 272
ogbn-arxiv,CAGNET-1D,1,272,10.1785,0.0583

Epoch: 273
ogbn-arxiv,CAGNET-1D,1,273,10.2160,0.0595

Epoch: 274
ogbn-arxiv,CAGNET-1D,1,274,10.2535,0.0597

Epoch: 275
ogbn-arxiv,CAGNET-1D,1,275,10.2920,0.0587

Epoch: 276
ogbn-arxiv,CAGNET-1D,1,276,10.3299,0.0595

Epoch: 277
ogbn-arxiv,CAGNET-1D,1,277,10.3677,0.0598

Epoch: 278
ogbn-arxiv,CAGNET-1D,1,278,10.4051,0.0592

Epoch: 279
ogbn-arxiv,CAGNET-1D,1,279,10.4426,0.0588

Epoch: 280
ogbn-arxiv,CAGNET-1D,1,280,10.4799,0.0590

Epoch: 281
ogbn-arxiv,CAGNET-1D,1,281,10.5173,0.0596

Epoch: 282
ogbn-arxiv,CAGNET-1D,1,282,10.5545,0.0593

Epoch: 283
ogbn-arxiv,CAGNET-1D,1,283,10.5914,0.0593

Epoch: 284
ogbn-arxiv,CAGNET-1D,1,284,10.6285,0.0593

Epoch: 285
ogbn-arxiv,CAGNET-1D,1,285,10.6659,0.0590

Epoch: 286
ogbn-arxiv,CAGNET-1D,1,286,10.7042,0.0594

Epoch: 287
ogbn-arxiv,CAGNET-1D,1,287,10.7416,0.0596

Epoch: 288
ogbn-arxiv,CAGNET-1D,1,288,10.7795,0.0597

Epoch: 289
ogbn-arxiv,CAGNET-1D,1,289,10.8171,0.0593

Epoch: 290
ogbn-arxiv,CAGNET-1D,1,290,10.8541,0.0594

Epoch: 291
ogbn-arxiv,CAGNET-1D,1,291,10.8911,0.0595

Epoch: 292
ogbn-arxiv,CAGNET-1D,1,292,10.9281,0.0596

Epoch: 293
ogbn-arxiv,CAGNET-1D,1,293,10.9654,0.0590

Epoch: 294
ogbn-arxiv,CAGNET-1D,1,294,11.0021,0.0598

Epoch: 295
ogbn-arxiv,CAGNET-1D,1,295,11.0387,0.0597

Epoch: 296
ogbn-arxiv,CAGNET-1D,1,296,11.0755,0.0594

Epoch: 297
ogbn-arxiv,CAGNET-1D,1,297,11.1128,0.0595

Epoch: 298
ogbn-arxiv,CAGNET-1D,1,298,11.1504,0.0593

Epoch: 299
ogbn-arxiv,CAGNET-1D,1,299,11.1882,0.0594

Epoch: 300
ogbn-arxiv,CAGNET-1D,1,300,11.2257,0.0596

Epoch: 301
ogbn-arxiv,CAGNET-1D,1,301,11.2636,0.0597

Epoch: 302
ogbn-arxiv,CAGNET-1D,1,302,11.3016,0.0596

Epoch: 303
ogbn-arxiv,CAGNET-1D,1,303,11.3388,0.0590

Epoch: 304
ogbn-arxiv,CAGNET-1D,1,304,11.3767,0.0591

Epoch: 305
ogbn-arxiv,CAGNET-1D,1,305,11.4143,0.0595

Epoch: 306
ogbn-arxiv,CAGNET-1D,1,306,11.4529,0.0590

Epoch: 307
ogbn-arxiv,CAGNET-1D,1,307,11.4902,0.0593

Epoch: 308
ogbn-arxiv,CAGNET-1D,1,308,11.5274,0.0594

Epoch: 309
ogbn-arxiv,CAGNET-1D,1,309,11.5644,0.0590

Epoch: 310
ogbn-arxiv,CAGNET-1D,1,310,11.6013,0.0594

Epoch: 311
ogbn-arxiv,CAGNET-1D,1,311,11.6383,0.0593

Epoch: 312
ogbn-arxiv,CAGNET-1D,1,312,11.6776,0.0594

Epoch: 313
ogbn-arxiv,CAGNET-1D,1,313,11.7151,0.0588

Epoch: 314
ogbn-arxiv,CAGNET-1D,1,314,11.7538,0.0600

Epoch: 315
ogbn-arxiv,CAGNET-1D,1,315,11.7911,0.0597

Epoch: 316
ogbn-arxiv,CAGNET-1D,1,316,11.8283,0.0593

Epoch: 317
ogbn-arxiv,CAGNET-1D,1,317,11.8654,0.0600

Epoch: 318
ogbn-arxiv,CAGNET-1D,1,318,11.9022,0.0597

Epoch: 319
ogbn-arxiv,CAGNET-1D,1,319,11.9393,0.0585

Epoch: 320
ogbn-arxiv,CAGNET-1D,1,320,11.9765,0.0595

Epoch: 321
ogbn-arxiv,CAGNET-1D,1,321,12.0140,0.0603

Epoch: 322
ogbn-arxiv,CAGNET-1D,1,322,12.0514,0.0593

Epoch: 323
ogbn-arxiv,CAGNET-1D,1,323,12.0891,0.0591

Epoch: 324
ogbn-arxiv,CAGNET-1D,1,324,12.1266,0.0593

Epoch: 325
ogbn-arxiv,CAGNET-1D,1,325,12.1641,0.0588

Epoch: 326
ogbn-arxiv,CAGNET-1D,1,326,12.2019,0.0592

Epoch: 327
ogbn-arxiv,CAGNET-1D,1,327,12.2394,0.0596

Epoch: 328
ogbn-arxiv,CAGNET-1D,1,328,12.2768,0.0592

Epoch: 329
ogbn-arxiv,CAGNET-1D,1,329,12.3145,0.0584

Epoch: 330
ogbn-arxiv,CAGNET-1D,1,330,12.3516,0.0604

Epoch: 331
ogbn-arxiv,CAGNET-1D,1,331,12.3887,0.0583

Epoch: 332
ogbn-arxiv,CAGNET-1D,1,332,12.4255,0.0582

Epoch: 333
ogbn-arxiv,CAGNET-1D,1,333,12.4624,0.0610

Epoch: 334
ogbn-arxiv,CAGNET-1D,1,334,12.4996,0.0586

Epoch: 335
ogbn-arxiv,CAGNET-1D,1,335,12.5371,0.0584

Epoch: 336
ogbn-arxiv,CAGNET-1D,1,336,12.5757,0.0590

Epoch: 337
ogbn-arxiv,CAGNET-1D,1,337,12.6139,0.0590

Epoch: 338
ogbn-arxiv,CAGNET-1D,1,338,12.6518,0.0573

Epoch: 339
ogbn-arxiv,CAGNET-1D,1,339,12.6897,0.0604

Epoch: 340
ogbn-arxiv,CAGNET-1D,1,340,12.7266,0.0588

Epoch: 341
ogbn-arxiv,CAGNET-1D,1,341,12.7638,0.0581

Epoch: 342
ogbn-arxiv,CAGNET-1D,1,342,12.8014,0.0590

Epoch: 343
ogbn-arxiv,CAGNET-1D,1,343,12.8388,0.0583

Epoch: 344
ogbn-arxiv,CAGNET-1D,1,344,12.8767,0.0576

Epoch: 345
ogbn-arxiv,CAGNET-1D,1,345,12.9141,0.0599

Epoch: 346
ogbn-arxiv,CAGNET-1D,1,346,12.9518,0.0582

Epoch: 347
ogbn-arxiv,CAGNET-1D,1,347,12.9893,0.0586

Epoch: 348
ogbn-arxiv,CAGNET-1D,1,348,13.0267,0.0585

Epoch: 349
ogbn-arxiv,CAGNET-1D,1,349,13.0639,0.0579

Epoch: 350
ogbn-arxiv,CAGNET-1D,1,350,13.1016,0.0578

Epoch: 351
ogbn-arxiv,CAGNET-1D,1,351,13.1395,0.0585

Epoch: 352
ogbn-arxiv,CAGNET-1D,1,352,13.1772,0.0582

Epoch: 353
ogbn-arxiv,CAGNET-1D,1,353,13.2148,0.0577

Epoch: 354
ogbn-arxiv,CAGNET-1D,1,354,13.2523,0.0577

Epoch: 355
ogbn-arxiv,CAGNET-1D,1,355,13.2896,0.0571

Epoch: 356
ogbn-arxiv,CAGNET-1D,1,356,13.3273,0.0579

Epoch: 357
ogbn-arxiv,CAGNET-1D,1,357,13.3648,0.0587

Epoch: 358
ogbn-arxiv,CAGNET-1D,1,358,13.4020,0.0565

Epoch: 359
ogbn-arxiv,CAGNET-1D,1,359,13.4388,0.0577

Epoch: 360
ogbn-arxiv,CAGNET-1D,1,360,13.4761,0.0574

Epoch: 361
ogbn-arxiv,CAGNET-1D,1,361,13.5143,0.0575

Epoch: 362
ogbn-arxiv,CAGNET-1D,1,362,13.5516,0.0573

Epoch: 363
ogbn-arxiv,CAGNET-1D,1,363,13.5890,0.0568

Epoch: 364
ogbn-arxiv,CAGNET-1D,1,364,13.6262,0.0573

Epoch: 365
ogbn-arxiv,CAGNET-1D,1,365,13.6634,0.0579

Epoch: 366
ogbn-arxiv,CAGNET-1D,1,366,13.7017,0.0573

Epoch: 367
ogbn-arxiv,CAGNET-1D,1,367,13.7396,0.0562

Epoch: 368
ogbn-arxiv,CAGNET-1D,1,368,13.7771,0.0581

Epoch: 369
ogbn-arxiv,CAGNET-1D,1,369,13.8147,0.0563

Epoch: 370
ogbn-arxiv,CAGNET-1D,1,370,13.8520,0.0573

Epoch: 371
ogbn-arxiv,CAGNET-1D,1,371,13.8893,0.0576

Epoch: 372
ogbn-arxiv,CAGNET-1D,1,372,13.9265,0.0557

Epoch: 373
ogbn-arxiv,CAGNET-1D,1,373,13.9635,0.0571

Epoch: 374
ogbn-arxiv,CAGNET-1D,1,374,14.0011,0.0574

Epoch: 375
ogbn-arxiv,CAGNET-1D,1,375,14.0385,0.0556

Epoch: 376
ogbn-arxiv,CAGNET-1D,1,376,14.0773,0.0562

Epoch: 377
ogbn-arxiv,CAGNET-1D,1,377,14.1153,0.0561

Epoch: 378
ogbn-arxiv,CAGNET-1D,1,378,14.1530,0.0561

Epoch: 379
ogbn-arxiv,CAGNET-1D,1,379,14.1905,0.0563

Epoch: 380
ogbn-arxiv,CAGNET-1D,1,380,14.2280,0.0566

Epoch: 381
ogbn-arxiv,CAGNET-1D,1,381,14.2652,0.0547

Epoch: 382
ogbn-arxiv,CAGNET-1D,1,382,14.3023,0.0566

Epoch: 383
ogbn-arxiv,CAGNET-1D,1,383,14.3402,0.0557

Epoch: 384
ogbn-arxiv,CAGNET-1D,1,384,14.3775,0.0551

Epoch: 385
ogbn-arxiv,CAGNET-1D,1,385,14.4161,0.0570

Epoch: 386
ogbn-arxiv,CAGNET-1D,1,386,14.4539,0.0549

Epoch: 387
ogbn-arxiv,CAGNET-1D,1,387,14.4929,0.0549

Epoch: 388
ogbn-arxiv,CAGNET-1D,1,388,14.5302,0.0563

Epoch: 389
ogbn-arxiv,CAGNET-1D,1,389,14.5676,0.0545

Epoch: 390
ogbn-arxiv,CAGNET-1D,1,390,14.6063,0.0555

Epoch: 391
ogbn-arxiv,CAGNET-1D,1,391,14.6435,0.0550

Epoch: 392
ogbn-arxiv,CAGNET-1D,1,392,14.6806,0.0543

Epoch: 393
ogbn-arxiv,CAGNET-1D,1,393,14.7173,0.0554

Epoch: 394
ogbn-arxiv,CAGNET-1D,1,394,14.7541,0.0539

Epoch: 395
ogbn-arxiv,CAGNET-1D,1,395,14.7907,0.0538

Epoch: 396
ogbn-arxiv,CAGNET-1D,1,396,14.8274,0.0556

Epoch: 397
ogbn-arxiv,CAGNET-1D,1,397,14.8650,0.0530

Epoch: 398
ogbn-arxiv,CAGNET-1D,1,398,14.9023,0.0546

Epoch: 399
ogbn-arxiv,CAGNET-1D,1,399,14.9399,0.0547

Epoch: 400
ogbn-arxiv,CAGNET-1D,1,400,14.9777,0.0525

Epoch: 401
ogbn-arxiv,CAGNET-1D,1,401,15.0161,0.0557

Epoch: 402
ogbn-arxiv,CAGNET-1D,1,402,15.0541,0.0530

Epoch: 403
ogbn-arxiv,CAGNET-1D,1,403,15.0921,0.0527

Epoch: 404
ogbn-arxiv,CAGNET-1D,1,404,15.1296,0.0567

Epoch: 405
ogbn-arxiv,CAGNET-1D,1,405,15.1669,0.0520

Epoch: 406
ogbn-arxiv,CAGNET-1D,1,406,15.2041,0.0531

Epoch: 407
ogbn-arxiv,CAGNET-1D,1,407,15.2418,0.0542

Epoch: 408
ogbn-arxiv,CAGNET-1D,1,408,15.2790,0.0522

Epoch: 409
ogbn-arxiv,CAGNET-1D,1,409,15.3170,0.0544

Epoch: 410
ogbn-arxiv,CAGNET-1D,1,410,15.3540,0.0525

Epoch: 411
ogbn-arxiv,CAGNET-1D,1,411,15.3911,0.0522

Epoch: 412
ogbn-arxiv,CAGNET-1D,1,412,15.4289,0.0524

Epoch: 413
ogbn-arxiv,CAGNET-1D,1,413,15.4667,0.0524

Epoch: 414
ogbn-arxiv,CAGNET-1D,1,414,15.5050,0.0523

Epoch: 415
ogbn-arxiv,CAGNET-1D,1,415,15.5430,0.0529

Epoch: 416
ogbn-arxiv,CAGNET-1D,1,416,15.5810,0.0522

Epoch: 417
ogbn-arxiv,CAGNET-1D,1,417,15.6189,0.0517

Epoch: 418
ogbn-arxiv,CAGNET-1D,1,418,15.6567,0.0521

Epoch: 419
ogbn-arxiv,CAGNET-1D,1,419,15.6945,0.0523

Epoch: 420
ogbn-arxiv,CAGNET-1D,1,420,15.7326,0.0515

Epoch: 421
ogbn-arxiv,CAGNET-1D,1,421,15.7698,0.0528

Epoch: 422
ogbn-arxiv,CAGNET-1D,1,422,15.8076,0.0510

Epoch: 423
ogbn-arxiv,CAGNET-1D,1,423,15.8448,0.0518

Epoch: 424
ogbn-arxiv,CAGNET-1D,1,424,15.8827,0.0517

Epoch: 425
ogbn-arxiv,CAGNET-1D,1,425,15.9209,0.0508

Epoch: 426
ogbn-arxiv,CAGNET-1D,1,426,15.9593,0.0522

Epoch: 427
ogbn-arxiv,CAGNET-1D,1,427,15.9973,0.0508

Epoch: 428
ogbn-arxiv,CAGNET-1D,1,428,16.0351,0.0508

Epoch: 429
ogbn-arxiv,CAGNET-1D,1,429,16.0728,0.0521

Epoch: 430
ogbn-arxiv,CAGNET-1D,1,430,16.1105,0.0501

Epoch: 431
ogbn-arxiv,CAGNET-1D,1,431,16.1479,0.0507

Epoch: 432
ogbn-arxiv,CAGNET-1D,1,432,16.1851,0.0516

Epoch: 433
ogbn-arxiv,CAGNET-1D,1,433,16.2254,0.0505

Epoch: 434
ogbn-arxiv,CAGNET-1D,1,434,16.2627,0.0521

Epoch: 435
ogbn-arxiv,CAGNET-1D,1,435,16.3000,0.0502

Epoch: 436
ogbn-arxiv,CAGNET-1D,1,436,16.3385,0.0501

Epoch: 437
ogbn-arxiv,CAGNET-1D,1,437,16.3756,0.0544

Epoch: 438
ogbn-arxiv,CAGNET-1D,1,438,16.4127,0.0498

Epoch: 439
ogbn-arxiv,CAGNET-1D,1,439,16.4499,0.0551

Epoch: 440
ogbn-arxiv,CAGNET-1D,1,440,16.4874,0.0496

Epoch: 441
ogbn-arxiv,CAGNET-1D,1,441,16.5254,0.0506

Epoch: 442
ogbn-arxiv,CAGNET-1D,1,442,16.5637,0.0512

Epoch: 443
ogbn-arxiv,CAGNET-1D,1,443,16.6018,0.0496

Epoch: 444
ogbn-arxiv,CAGNET-1D,1,444,16.6400,0.0526

Epoch: 445
ogbn-arxiv,CAGNET-1D,1,445,16.6777,0.0494

Epoch: 446
ogbn-arxiv,CAGNET-1D,1,446,16.7150,0.0502

Epoch: 447
ogbn-arxiv,CAGNET-1D,1,447,16.7522,0.0499

Epoch: 448
ogbn-arxiv,CAGNET-1D,1,448,16.7895,0.0497

Epoch: 449
ogbn-arxiv,CAGNET-1D,1,449,16.8266,0.0498

Epoch: 450
ogbn-arxiv,CAGNET-1D,1,450,16.8636,0.0501

Epoch: 451
ogbn-arxiv,CAGNET-1D,1,451,16.9017,0.0489

Epoch: 452
ogbn-arxiv,CAGNET-1D,1,452,16.9383,0.0498

Epoch: 453
ogbn-arxiv,CAGNET-1D,1,453,16.9756,0.0493

Epoch: 454
ogbn-arxiv,CAGNET-1D,1,454,17.0131,0.0503

Epoch: 455
ogbn-arxiv,CAGNET-1D,1,455,17.0513,0.0491

Epoch: 456
ogbn-arxiv,CAGNET-1D,1,456,17.0896,0.0495

Epoch: 457
ogbn-arxiv,CAGNET-1D,1,457,17.1293,0.0491

Epoch: 458
ogbn-arxiv,CAGNET-1D,1,458,17.1670,0.0490

Epoch: 459
ogbn-arxiv,CAGNET-1D,1,459,17.2046,0.0494

Epoch: 460
ogbn-arxiv,CAGNET-1D,1,460,17.2422,0.0490

Epoch: 461
ogbn-arxiv,CAGNET-1D,1,461,17.2794,0.0494

Epoch: 462
ogbn-arxiv,CAGNET-1D,1,462,17.3165,0.0492

Epoch: 463
ogbn-arxiv,CAGNET-1D,1,463,17.3535,0.0487

Epoch: 464
ogbn-arxiv,CAGNET-1D,1,464,17.3906,0.0498

Epoch: 465
ogbn-arxiv,CAGNET-1D,1,465,17.4276,0.0485

Epoch: 466
ogbn-arxiv,CAGNET-1D,1,466,17.4647,0.0506

Epoch: 467
ogbn-arxiv,CAGNET-1D,1,467,17.5028,0.0486

Epoch: 468
ogbn-arxiv,CAGNET-1D,1,468,17.5405,0.0491

Epoch: 469
ogbn-arxiv,CAGNET-1D,1,469,17.5785,0.0492

Epoch: 470
ogbn-arxiv,CAGNET-1D,1,470,17.6164,0.0486

Epoch: 471
ogbn-arxiv,CAGNET-1D,1,471,17.6540,0.0503

Epoch: 472
ogbn-arxiv,CAGNET-1D,1,472,17.6913,0.0485

Epoch: 473
ogbn-arxiv,CAGNET-1D,1,473,17.7285,0.0492

Epoch: 474
ogbn-arxiv,CAGNET-1D,1,474,17.7655,0.0488

Epoch: 475
ogbn-arxiv,CAGNET-1D,1,475,17.8030,0.0485

Epoch: 476
ogbn-arxiv,CAGNET-1D,1,476,17.8409,0.0510

Epoch: 477
ogbn-arxiv,CAGNET-1D,1,477,17.8788,0.0483

Epoch: 478
ogbn-arxiv,CAGNET-1D,1,478,17.9175,0.0491

Epoch: 479
ogbn-arxiv,CAGNET-1D,1,479,17.9549,0.0483

Epoch: 480
ogbn-arxiv,CAGNET-1D,1,480,17.9923,0.0482

Epoch: 481
ogbn-arxiv,CAGNET-1D,1,481,18.0301,0.0496

Epoch: 482
ogbn-arxiv,CAGNET-1D,1,482,18.0680,0.0482

Epoch: 483
ogbn-arxiv,CAGNET-1D,1,483,18.1052,0.0490

Epoch: 484
ogbn-arxiv,CAGNET-1D,1,484,18.1422,0.0481

Epoch: 485
ogbn-arxiv,CAGNET-1D,1,485,18.1794,0.0487

Epoch: 486
ogbn-arxiv,CAGNET-1D,1,486,18.2164,0.0485

Epoch: 487
ogbn-arxiv,CAGNET-1D,1,487,18.2536,0.0484

Epoch: 488
ogbn-arxiv,CAGNET-1D,1,488,18.2907,0.0480

Epoch: 489
ogbn-arxiv,CAGNET-1D,1,489,18.3281,0.0480

Epoch: 490
ogbn-arxiv,CAGNET-1D,1,490,18.3658,0.0484

Epoch: 491
ogbn-arxiv,CAGNET-1D,1,491,18.4035,0.0485

Epoch: 492
ogbn-arxiv,CAGNET-1D,1,492,18.4410,0.0484

Epoch: 493
ogbn-arxiv,CAGNET-1D,1,493,18.4786,0.0479

Epoch: 494
ogbn-arxiv,CAGNET-1D,1,494,18.5159,0.0486

Epoch: 495
ogbn-arxiv,CAGNET-1D,1,495,18.5532,0.0481

Epoch: 496
ogbn-arxiv,CAGNET-1D,1,496,18.5917,0.0485

Epoch: 497
ogbn-arxiv,CAGNET-1D,1,497,18.6295,0.0480

Epoch: 498
ogbn-arxiv,CAGNET-1D,1,498,18.6674,0.0487

Epoch: 499
ogbn-arxiv,CAGNET-1D,1,499,18.7052,0.0477

Epoch: 500
ogbn-arxiv,CAGNET-1D,1,500,18.7424,0.0484

Epoch: 501
ogbn-arxiv,CAGNET-1D,1,501,18.7798,0.0481

Epoch: 502
ogbn-arxiv,CAGNET-1D,1,502,18.8169,0.0479

Epoch: 503
ogbn-arxiv,CAGNET-1D,1,503,18.8544,0.0476

Epoch: 504
ogbn-arxiv,CAGNET-1D,1,504,18.8912,0.0479

Epoch: 505
ogbn-arxiv,CAGNET-1D,1,505,18.9287,0.0480

Epoch: 506
ogbn-arxiv,CAGNET-1D,1,506,18.9663,0.0479

Epoch: 507
ogbn-arxiv,CAGNET-1D,1,507,19.0035,0.0477

Epoch: 508
ogbn-arxiv,CAGNET-1D,1,508,19.0406,0.0479

Epoch: 509
ogbn-arxiv,CAGNET-1D,1,509,19.0773,0.0482

Epoch: 510
ogbn-arxiv,CAGNET-1D,1,510,19.1143,0.0478

Epoch: 511
ogbn-arxiv,CAGNET-1D,1,511,19.1512,0.0489

Epoch: 512
ogbn-arxiv,CAGNET-1D,1,512,19.1884,0.0481

Epoch: 513
ogbn-arxiv,CAGNET-1D,1,513,19.2262,0.0491

Epoch: 514
ogbn-arxiv,CAGNET-1D,1,514,19.2640,0.0479

Epoch: 515
ogbn-arxiv,CAGNET-1D,1,515,19.3016,0.0481

Epoch: 516
ogbn-arxiv,CAGNET-1D,1,516,19.3395,0.0478

Epoch: 517
ogbn-arxiv,CAGNET-1D,1,517,19.3774,0.0479

Epoch: 518
ogbn-arxiv,CAGNET-1D,1,518,19.4153,0.0481

Epoch: 519
ogbn-arxiv,CAGNET-1D,1,519,19.4527,0.0477

Epoch: 520
ogbn-arxiv,CAGNET-1D,1,520,19.4906,0.0481

Epoch: 521
ogbn-arxiv,CAGNET-1D,1,521,19.5281,0.0480

Epoch: 522
ogbn-arxiv,CAGNET-1D,1,522,19.5653,0.0495

Epoch: 523
ogbn-arxiv,CAGNET-1D,1,523,19.6023,0.0483

Epoch: 524
ogbn-arxiv,CAGNET-1D,1,524,19.6394,0.0491

Epoch: 525
ogbn-arxiv,CAGNET-1D,1,525,19.6767,0.0479

Epoch: 526
ogbn-arxiv,CAGNET-1D,1,526,19.7141,0.0483

Epoch: 527
ogbn-arxiv,CAGNET-1D,1,527,19.7516,0.0479

Epoch: 528
ogbn-arxiv,CAGNET-1D,1,528,19.7892,0.0481

Epoch: 529
ogbn-arxiv,CAGNET-1D,1,529,19.8267,0.0487

Epoch: 530
ogbn-arxiv,CAGNET-1D,1,530,19.8655,0.0480

Epoch: 531
ogbn-arxiv,CAGNET-1D,1,531,19.9029,0.0495

Epoch: 532
ogbn-arxiv,CAGNET-1D,1,532,19.9399,0.0484

Epoch: 533
ogbn-arxiv,CAGNET-1D,1,533,19.9770,0.0502

Epoch: 534
ogbn-arxiv,CAGNET-1D,1,534,20.0142,0.0485

Epoch: 535
ogbn-arxiv,CAGNET-1D,1,535,20.0508,0.0516

Epoch: 536
ogbn-arxiv,CAGNET-1D,1,536,20.0879,0.0487

Epoch: 537
ogbn-arxiv,CAGNET-1D,1,537,20.1250,0.0507

Epoch: 538
ogbn-arxiv,CAGNET-1D,1,538,20.1628,0.0484

Epoch: 539
ogbn-arxiv,CAGNET-1D,1,539,20.2009,0.0491

Epoch: 540
ogbn-arxiv,CAGNET-1D,1,540,20.2386,0.0481

Epoch: 541
ogbn-arxiv,CAGNET-1D,1,541,20.2761,0.0480

Epoch: 542
ogbn-arxiv,CAGNET-1D,1,542,20.3134,0.0481

Epoch: 543
ogbn-arxiv,CAGNET-1D,1,543,20.3510,0.0482

Epoch: 544
ogbn-arxiv,CAGNET-1D,1,544,20.3885,0.0487

Epoch: 545
ogbn-arxiv,CAGNET-1D,1,545,20.4257,0.0480

Epoch: 546
ogbn-arxiv,CAGNET-1D,1,546,20.4629,0.0485

Epoch: 547
ogbn-arxiv,CAGNET-1D,1,547,20.5003,0.0483

Epoch: 548
ogbn-arxiv,CAGNET-1D,1,548,20.5371,0.0483

Epoch: 549
ogbn-arxiv,CAGNET-1D,1,549,20.5741,0.0493

Epoch: 550
ogbn-arxiv,CAGNET-1D,1,550,20.6115,0.0486

Epoch: 551
ogbn-arxiv,CAGNET-1D,1,551,20.6490,0.0503

Epoch: 552
ogbn-arxiv,CAGNET-1D,1,552,20.6868,0.0488

Epoch: 553
ogbn-arxiv,CAGNET-1D,1,553,20.7246,0.0496

Epoch: 554
ogbn-arxiv,CAGNET-1D,1,554,20.7628,0.0483

Epoch: 555
ogbn-arxiv,CAGNET-1D,1,555,20.8004,0.0487

Epoch: 556
ogbn-arxiv,CAGNET-1D,1,556,20.8376,0.0488

Epoch: 557
ogbn-arxiv,CAGNET-1D,1,557,20.8746,0.0486

Epoch: 558
ogbn-arxiv,CAGNET-1D,1,558,20.9116,0.0491

Epoch: 559
ogbn-arxiv,CAGNET-1D,1,559,20.9485,0.0490

Epoch: 560
ogbn-arxiv,CAGNET-1D,1,560,20.9853,0.0562

Epoch: 561
ogbn-arxiv,CAGNET-1D,1,561,21.0231,0.0491

Epoch: 562
ogbn-arxiv,CAGNET-1D,1,562,21.0607,0.0736

Epoch: 563
ogbn-arxiv,CAGNET-1D,1,563,21.0987,0.0495

Epoch: 564
ogbn-arxiv,CAGNET-1D,1,564,21.1361,0.0674

Epoch: 565
ogbn-arxiv,CAGNET-1D,1,565,21.1742,0.0490

Epoch: 566
ogbn-arxiv,CAGNET-1D,1,566,21.2114,0.0496

Epoch: 567
ogbn-arxiv,CAGNET-1D,1,567,21.2489,0.0494

Epoch: 568
ogbn-arxiv,CAGNET-1D,1,568,21.2862,0.0491

Epoch: 569
ogbn-arxiv,CAGNET-1D,1,569,21.3240,0.0529

Epoch: 570
ogbn-arxiv,CAGNET-1D,1,570,21.3621,0.0492

Epoch: 571
ogbn-arxiv,CAGNET-1D,1,571,21.3994,0.0651

Epoch: 572
ogbn-arxiv,CAGNET-1D,1,572,21.4368,0.0491

Epoch: 573
ogbn-arxiv,CAGNET-1D,1,573,21.4741,0.0510

Epoch: 574
ogbn-arxiv,CAGNET-1D,1,574,21.5112,0.0491

Epoch: 575
ogbn-arxiv,CAGNET-1D,1,575,21.5491,0.0491

Epoch: 576
ogbn-arxiv,CAGNET-1D,1,576,21.5867,0.0494

Epoch: 577
ogbn-arxiv,CAGNET-1D,1,577,21.6240,0.0494

Epoch: 578
ogbn-arxiv,CAGNET-1D,1,578,21.6616,0.0541

Epoch: 579
ogbn-arxiv,CAGNET-1D,1,579,21.6994,0.0494

Epoch: 580
ogbn-arxiv,CAGNET-1D,1,580,21.7366,0.0509

Epoch: 581
ogbn-arxiv,CAGNET-1D,1,581,21.7740,0.0496

Epoch: 582
ogbn-arxiv,CAGNET-1D,1,582,21.8117,0.0501

Epoch: 583
ogbn-arxiv,CAGNET-1D,1,583,21.8504,0.0490

Epoch: 584
ogbn-arxiv,CAGNET-1D,1,584,21.8884,0.0492

Epoch: 585
ogbn-arxiv,CAGNET-1D,1,585,21.9259,0.0520

Epoch: 586
ogbn-arxiv,CAGNET-1D,1,586,21.9634,0.0495

Epoch: 587
ogbn-arxiv,CAGNET-1D,1,587,22.0009,0.0529

Epoch: 588
ogbn-arxiv,CAGNET-1D,1,588,22.0382,0.0497

Epoch: 589
ogbn-arxiv,CAGNET-1D,1,589,22.0757,0.0524

Epoch: 590
ogbn-arxiv,CAGNET-1D,1,590,22.1128,0.0493

Epoch: 591
ogbn-arxiv,CAGNET-1D,1,591,22.1501,0.0498

Epoch: 592
ogbn-arxiv,CAGNET-1D,1,592,22.1871,0.0502

Epoch: 593
ogbn-arxiv,CAGNET-1D,1,593,22.2242,0.0494

Epoch: 594
ogbn-arxiv,CAGNET-1D,1,594,22.2621,0.0507

Epoch: 595
ogbn-arxiv,CAGNET-1D,1,595,22.3002,0.0498

Epoch: 596
ogbn-arxiv,CAGNET-1D,1,596,22.3379,0.0520

Epoch: 597
ogbn-arxiv,CAGNET-1D,1,597,22.3755,0.0494

Epoch: 598
ogbn-arxiv,CAGNET-1D,1,598,22.4126,0.0506

Epoch: 599
ogbn-arxiv,CAGNET-1D,1,599,22.4498,0.0496

Epoch: 600
ogbn-arxiv,CAGNET-1D,1,600,22.4870,0.0499

Epoch: 601
ogbn-arxiv,CAGNET-1D,1,601,22.5243,0.0498

Epoch: 602
ogbn-arxiv,CAGNET-1D,1,602,22.5634,0.0498

Epoch: 603
ogbn-arxiv,CAGNET-1D,1,603,22.6012,0.0518

Epoch: 604
ogbn-arxiv,CAGNET-1D,1,604,22.6386,0.0499

Epoch: 605
ogbn-arxiv,CAGNET-1D,1,605,22.6759,0.0600

Epoch: 606
ogbn-arxiv,CAGNET-1D,1,606,22.7132,0.0504

Epoch: 607
ogbn-arxiv,CAGNET-1D,1,607,22.7502,0.0603

Epoch: 608
ogbn-arxiv,CAGNET-1D,1,608,22.7873,0.0496

Epoch: 609
ogbn-arxiv,CAGNET-1D,1,609,22.8248,0.0508

Epoch: 610
ogbn-arxiv,CAGNET-1D,1,610,22.8617,0.0509

Epoch: 611
ogbn-arxiv,CAGNET-1D,1,611,22.8985,0.0497

Epoch: 612
ogbn-arxiv,CAGNET-1D,1,612,22.9357,0.0521

Epoch: 613
ogbn-arxiv,CAGNET-1D,1,613,22.9730,0.0501

Epoch: 614
ogbn-arxiv,CAGNET-1D,1,614,23.0106,0.0655

Epoch: 615
ogbn-arxiv,CAGNET-1D,1,615,23.0480,0.0499

Epoch: 616
ogbn-arxiv,CAGNET-1D,1,616,23.0854,0.0520

Epoch: 617
ogbn-arxiv,CAGNET-1D,1,617,23.1226,0.0500

Epoch: 618
ogbn-arxiv,CAGNET-1D,1,618,23.1597,0.0499

Epoch: 619
ogbn-arxiv,CAGNET-1D,1,619,23.1968,0.0506

Epoch: 620
ogbn-arxiv,CAGNET-1D,1,620,23.2339,0.0498

Epoch: 621
ogbn-arxiv,CAGNET-1D,1,621,23.2714,0.0570

Epoch: 622
ogbn-arxiv,CAGNET-1D,1,622,23.3092,0.0501

Epoch: 623
ogbn-arxiv,CAGNET-1D,1,623,23.3467,0.0540

Epoch: 624
ogbn-arxiv,CAGNET-1D,1,624,23.3844,0.0501

Epoch: 625
ogbn-arxiv,CAGNET-1D,1,625,23.4217,0.0536

Epoch: 626
ogbn-arxiv,CAGNET-1D,1,626,23.4586,0.0494

Epoch: 627
ogbn-arxiv,CAGNET-1D,1,627,23.4976,0.0499

Epoch: 628
ogbn-arxiv,CAGNET-1D,1,628,23.5341,0.0523

Epoch: 629
ogbn-arxiv,CAGNET-1D,1,629,23.5706,0.0498

Epoch: 630
ogbn-arxiv,CAGNET-1D,1,630,23.6075,0.0515

Epoch: 631
ogbn-arxiv,CAGNET-1D,1,631,23.6451,0.0502

Epoch: 632
ogbn-arxiv,CAGNET-1D,1,632,23.6830,0.0529

Epoch: 633
ogbn-arxiv,CAGNET-1D,1,633,23.7212,0.0499

Epoch: 634
ogbn-arxiv,CAGNET-1D,1,634,23.7591,0.0501

Epoch: 635
ogbn-arxiv,CAGNET-1D,1,635,23.7973,0.0524

Epoch: 636
ogbn-arxiv,CAGNET-1D,1,636,23.8350,0.0501

Epoch: 637
ogbn-arxiv,CAGNET-1D,1,637,23.8724,0.0528

Epoch: 638
ogbn-arxiv,CAGNET-1D,1,638,23.9097,0.0510

Epoch: 639
ogbn-arxiv,CAGNET-1D,1,639,23.9470,0.0718

Epoch: 640
ogbn-arxiv,CAGNET-1D,1,640,23.9842,0.0502

Epoch: 641
ogbn-arxiv,CAGNET-1D,1,641,24.0215,0.0515

Epoch: 642
ogbn-arxiv,CAGNET-1D,1,642,24.0591,0.0525

Epoch: 643
ogbn-arxiv,CAGNET-1D,1,643,24.0965,0.0507

Epoch: 644
ogbn-arxiv,CAGNET-1D,1,644,24.1340,0.0563

Epoch: 645
ogbn-arxiv,CAGNET-1D,1,645,24.1713,0.0512

Epoch: 646
ogbn-arxiv,CAGNET-1D,1,646,24.2088,0.0968

Epoch: 647
ogbn-arxiv,CAGNET-1D,1,647,24.2465,0.0507

Epoch: 648
ogbn-arxiv,CAGNET-1D,1,648,24.2842,0.0507

Epoch: 649
ogbn-arxiv,CAGNET-1D,1,649,24.3228,0.0722

Epoch: 650
ogbn-arxiv,CAGNET-1D,1,650,24.3607,0.0514

Epoch: 651
ogbn-arxiv,CAGNET-1D,1,651,24.4017,0.0667

Epoch: 652
ogbn-arxiv,CAGNET-1D,1,652,24.4386,0.0519

Epoch: 653
ogbn-arxiv,CAGNET-1D,1,653,24.4756,0.0549

Epoch: 654
ogbn-arxiv,CAGNET-1D,1,654,24.5127,0.0510

Epoch: 655
ogbn-arxiv,CAGNET-1D,1,655,24.5498,0.0515

Epoch: 656
ogbn-arxiv,CAGNET-1D,1,656,24.5871,0.1396

Epoch: 657
ogbn-arxiv,CAGNET-1D,1,657,24.6248,0.0515

Epoch: 658
ogbn-arxiv,CAGNET-1D,1,658,24.6626,0.0517

Epoch: 659
ogbn-arxiv,CAGNET-1D,1,659,24.7005,0.0564

Epoch: 660
ogbn-arxiv,CAGNET-1D,1,660,24.7380,0.0517

Epoch: 661
ogbn-arxiv,CAGNET-1D,1,661,24.7761,0.0631

Epoch: 662
ogbn-arxiv,CAGNET-1D,1,662,24.8138,0.0518

Epoch: 663
ogbn-arxiv,CAGNET-1D,1,663,24.8512,0.0580

Epoch: 664
ogbn-arxiv,CAGNET-1D,1,664,24.8885,0.0509

Epoch: 665
ogbn-arxiv,CAGNET-1D,1,665,24.9257,0.0513

Epoch: 666
ogbn-arxiv,CAGNET-1D,1,666,24.9633,0.0667

Epoch: 667
ogbn-arxiv,CAGNET-1D,1,667,25.0005,0.0515

Epoch: 668
ogbn-arxiv,CAGNET-1D,1,668,25.0372,0.0510

Epoch: 669
ogbn-arxiv,CAGNET-1D,1,669,25.0742,0.0515

Epoch: 670
ogbn-arxiv,CAGNET-1D,1,670,25.1111,0.0517

Epoch: 671
ogbn-arxiv,CAGNET-1D,1,671,25.1488,0.0519

Epoch: 672
ogbn-arxiv,CAGNET-1D,1,672,25.1862,0.0514

Epoch: 673
ogbn-arxiv,CAGNET-1D,1,673,25.2238,0.0517

Epoch: 674
ogbn-arxiv,CAGNET-1D,1,674,25.2617,0.0538

Epoch: 675
ogbn-arxiv,CAGNET-1D,1,675,25.3023,0.0516

Epoch: 676
ogbn-arxiv,CAGNET-1D,1,676,25.3394,0.0515

Epoch: 677
ogbn-arxiv,CAGNET-1D,1,677,25.3761,0.0535

Epoch: 678
ogbn-arxiv,CAGNET-1D,1,678,25.4128,0.0518

Epoch: 679
ogbn-arxiv,CAGNET-1D,1,679,25.4498,0.0516

Epoch: 680
ogbn-arxiv,CAGNET-1D,1,680,25.4863,0.0521

Epoch: 681
ogbn-arxiv,CAGNET-1D,1,681,25.5227,0.0525

Epoch: 682
ogbn-arxiv,CAGNET-1D,1,682,25.5594,0.0517

Epoch: 683
ogbn-arxiv,CAGNET-1D,1,683,25.5965,0.0521

Epoch: 684
ogbn-arxiv,CAGNET-1D,1,684,25.6339,0.0533

Epoch: 685
ogbn-arxiv,CAGNET-1D,1,685,25.6715,0.0523

Epoch: 686
ogbn-arxiv,CAGNET-1D,1,686,25.7093,0.0519

Epoch: 687
ogbn-arxiv,CAGNET-1D,1,687,25.7476,0.0525

Epoch: 688
ogbn-arxiv,CAGNET-1D,1,688,25.7856,0.0525

Epoch: 689
ogbn-arxiv,CAGNET-1D,1,689,25.8230,0.0519

Epoch: 690
ogbn-arxiv,CAGNET-1D,1,690,25.8607,0.0523

Epoch: 691
ogbn-arxiv,CAGNET-1D,1,691,25.8980,0.0529

Epoch: 692
ogbn-arxiv,CAGNET-1D,1,692,25.9356,0.0524

Epoch: 693
ogbn-arxiv,CAGNET-1D,1,693,25.9733,0.0523

Epoch: 694
ogbn-arxiv,CAGNET-1D,1,694,26.0104,0.0525

Epoch: 695
ogbn-arxiv,CAGNET-1D,1,695,26.0475,0.0527

Epoch: 696
ogbn-arxiv,CAGNET-1D,1,696,26.0843,0.0522

Epoch: 697
ogbn-arxiv,CAGNET-1D,1,697,26.1213,0.0526

Epoch: 698
ogbn-arxiv,CAGNET-1D,1,698,26.1584,0.0529

Epoch: 699
ogbn-arxiv,CAGNET-1D,1,699,26.1954,0.0528

Epoch: 700
ogbn-arxiv,CAGNET-1D,1,700,26.2336,0.0526

Epoch: 701
ogbn-arxiv,CAGNET-1D,1,701,26.2709,0.0524

Epoch: 702
ogbn-arxiv,CAGNET-1D,1,702,26.3082,0.0536

Epoch: 703
ogbn-arxiv,CAGNET-1D,1,703,26.3461,0.0526

Epoch: 704
ogbn-arxiv,CAGNET-1D,1,704,26.3825,0.0533

Epoch: 705
ogbn-arxiv,CAGNET-1D,1,705,26.4193,0.0526

Epoch: 706
ogbn-arxiv,CAGNET-1D,1,706,26.4557,0.0529

Epoch: 707
ogbn-arxiv,CAGNET-1D,1,707,26.4930,0.0545

Epoch: 708
ogbn-arxiv,CAGNET-1D,1,708,26.5300,0.0529

Epoch: 709
ogbn-arxiv,CAGNET-1D,1,709,26.5673,0.0573

Epoch: 710
ogbn-arxiv,CAGNET-1D,1,710,26.6047,0.0529

Epoch: 711
ogbn-arxiv,CAGNET-1D,1,711,26.6423,0.0534

Epoch: 712
ogbn-arxiv,CAGNET-1D,1,712,26.6793,0.0529

Epoch: 713
ogbn-arxiv,CAGNET-1D,1,713,26.7165,0.0531

Epoch: 714
ogbn-arxiv,CAGNET-1D,1,714,26.7539,0.0543

Epoch: 715
ogbn-arxiv,CAGNET-1D,1,715,26.7910,0.0531

Epoch: 716
ogbn-arxiv,CAGNET-1D,1,716,26.8276,0.0540

Epoch: 717
ogbn-arxiv,CAGNET-1D,1,717,26.8641,0.0532

Epoch: 718
ogbn-arxiv,CAGNET-1D,1,718,26.9007,0.0534

Epoch: 719
ogbn-arxiv,CAGNET-1D,1,719,26.9381,0.0538

Epoch: 720
ogbn-arxiv,CAGNET-1D,1,720,26.9750,0.0530

Epoch: 721
ogbn-arxiv,CAGNET-1D,1,721,27.0123,0.0536

Epoch: 722
ogbn-arxiv,CAGNET-1D,1,722,27.0494,0.0536

Epoch: 723
ogbn-arxiv,CAGNET-1D,1,723,27.0866,0.0537

Epoch: 724
ogbn-arxiv,CAGNET-1D,1,724,27.1270,0.0529

Epoch: 725
ogbn-arxiv,CAGNET-1D,1,725,27.1654,0.0534

Epoch: 726
ogbn-arxiv,CAGNET-1D,1,726,27.2016,0.0533

Epoch: 727
ogbn-arxiv,CAGNET-1D,1,727,27.2380,0.0533

Epoch: 728
ogbn-arxiv,CAGNET-1D,1,728,27.2752,0.0534

Epoch: 729
ogbn-arxiv,CAGNET-1D,1,729,27.3124,0.0532

Epoch: 730
ogbn-arxiv,CAGNET-1D,1,730,27.3497,0.0535

Epoch: 731
ogbn-arxiv,CAGNET-1D,1,731,27.3871,0.0533

Epoch: 732
ogbn-arxiv,CAGNET-1D,1,732,27.4237,0.0558

Epoch: 733
ogbn-arxiv,CAGNET-1D,1,733,27.4601,0.0534

Epoch: 734
ogbn-arxiv,CAGNET-1D,1,734,27.4972,0.0627

Epoch: 735
ogbn-arxiv,CAGNET-1D,1,735,27.5345,0.0535

Epoch: 736
ogbn-arxiv,CAGNET-1D,1,736,27.5722,0.0574

Epoch: 737
ogbn-arxiv,CAGNET-1D,1,737,27.6093,0.0534

Epoch: 738
ogbn-arxiv,CAGNET-1D,1,738,27.6474,0.0539

Epoch: 739
ogbn-arxiv,CAGNET-1D,1,739,27.6858,0.0537

Epoch: 740
ogbn-arxiv,CAGNET-1D,1,740,27.7236,0.0533

Epoch: 741
ogbn-arxiv,CAGNET-1D,1,741,27.7615,0.0544

Epoch: 742
ogbn-arxiv,CAGNET-1D,1,742,27.7988,0.0536

Epoch: 743
ogbn-arxiv,CAGNET-1D,1,743,27.8356,0.0539

Epoch: 744
ogbn-arxiv,CAGNET-1D,1,744,27.8732,0.0536

Epoch: 745
ogbn-arxiv,CAGNET-1D,1,745,27.9112,0.0547

Epoch: 746
ogbn-arxiv,CAGNET-1D,1,746,27.9473,0.0536

Epoch: 747
ogbn-arxiv,CAGNET-1D,1,747,27.9836,0.0550

Epoch: 748
ogbn-arxiv,CAGNET-1D,1,748,28.0225,0.0538

Epoch: 749
ogbn-arxiv,CAGNET-1D,1,749,28.0609,0.0549

Epoch: 750
ogbn-arxiv,CAGNET-1D,1,750,28.0981,0.0538

Epoch: 751
ogbn-arxiv,CAGNET-1D,1,751,28.1349,0.0538

Epoch: 752
ogbn-arxiv,CAGNET-1D,1,752,28.1715,0.0539

Epoch: 753
ogbn-arxiv,CAGNET-1D,1,753,28.2084,0.0539

Epoch: 754
ogbn-arxiv,CAGNET-1D,1,754,28.2453,0.0539

Epoch: 755
ogbn-arxiv,CAGNET-1D,1,755,28.2822,0.0536

Epoch: 756
ogbn-arxiv,CAGNET-1D,1,756,28.3194,0.0537

Epoch: 757
ogbn-arxiv,CAGNET-1D,1,757,28.3564,0.0542

Epoch: 758
ogbn-arxiv,CAGNET-1D,1,758,28.3935,0.0536

Epoch: 759
ogbn-arxiv,CAGNET-1D,1,759,28.4308,0.0544

Epoch: 760
ogbn-arxiv,CAGNET-1D,1,760,28.4680,0.0537

Epoch: 761
ogbn-arxiv,CAGNET-1D,1,761,28.5051,0.0545

Epoch: 762
ogbn-arxiv,CAGNET-1D,1,762,28.5413,0.0535

Epoch: 763
ogbn-arxiv,CAGNET-1D,1,763,28.5782,0.0546

Epoch: 764
ogbn-arxiv,CAGNET-1D,1,764,28.6151,0.0535

Epoch: 765
ogbn-arxiv,CAGNET-1D,1,765,28.6520,0.0543

Epoch: 766
ogbn-arxiv,CAGNET-1D,1,766,28.6894,0.0537

Epoch: 767
ogbn-arxiv,CAGNET-1D,1,767,28.7265,0.0536

Epoch: 768
ogbn-arxiv,CAGNET-1D,1,768,28.7639,0.0556

Epoch: 769
ogbn-arxiv,CAGNET-1D,1,769,28.8010,0.0536

Epoch: 770
ogbn-arxiv,CAGNET-1D,1,770,28.8383,0.0566

Epoch: 771
ogbn-arxiv,CAGNET-1D,1,771,28.8751,0.0536

Epoch: 772
ogbn-arxiv,CAGNET-1D,1,772,28.9120,0.0557

Epoch: 773
ogbn-arxiv,CAGNET-1D,1,773,28.9512,0.0535

Epoch: 774
ogbn-arxiv,CAGNET-1D,1,774,28.9881,0.0549

Epoch: 775
ogbn-arxiv,CAGNET-1D,1,775,29.0253,0.0534

Epoch: 776
ogbn-arxiv,CAGNET-1D,1,776,29.0621,0.0540

Epoch: 777
ogbn-arxiv,CAGNET-1D,1,777,29.0992,0.0536

Epoch: 778
ogbn-arxiv,CAGNET-1D,1,778,29.1359,0.0544

Epoch: 779
ogbn-arxiv,CAGNET-1D,1,779,29.1735,0.0533

Epoch: 780
ogbn-arxiv,CAGNET-1D,1,780,29.2109,0.0537

Epoch: 781
ogbn-arxiv,CAGNET-1D,1,781,29.2483,0.0533

Epoch: 782
ogbn-arxiv,CAGNET-1D,1,782,29.2857,0.0542

Epoch: 783
ogbn-arxiv,CAGNET-1D,1,783,29.3234,0.0532

Epoch: 784
ogbn-arxiv,CAGNET-1D,1,784,29.3604,0.0538

Epoch: 785
ogbn-arxiv,CAGNET-1D,1,785,29.3973,0.0533

Epoch: 786
ogbn-arxiv,CAGNET-1D,1,786,29.4348,0.0550

Epoch: 787
ogbn-arxiv,CAGNET-1D,1,787,29.4721,0.0531

Epoch: 788
ogbn-arxiv,CAGNET-1D,1,788,29.5093,0.0549

Epoch: 789
ogbn-arxiv,CAGNET-1D,1,789,29.5467,0.0534

Epoch: 790
ogbn-arxiv,CAGNET-1D,1,790,29.5839,0.0537

Epoch: 791
ogbn-arxiv,CAGNET-1D,1,791,29.6210,0.0532

Epoch: 792
ogbn-arxiv,CAGNET-1D,1,792,29.6582,0.0533

Epoch: 793
ogbn-arxiv,CAGNET-1D,1,793,29.6963,0.0542

Epoch: 794
ogbn-arxiv,CAGNET-1D,1,794,29.7335,0.0533

Epoch: 795
ogbn-arxiv,CAGNET-1D,1,795,29.7706,0.0544

Epoch: 796
ogbn-arxiv,CAGNET-1D,1,796,29.8073,0.0533

Epoch: 797
ogbn-arxiv,CAGNET-1D,1,797,29.8467,0.0545

Epoch: 798
ogbn-arxiv,CAGNET-1D,1,798,29.8840,0.0532

Epoch: 799
ogbn-arxiv,CAGNET-1D,1,799,29.9208,0.0541

Epoch: 800
ogbn-arxiv,CAGNET-1D,1,800,29.9575,0.0530

Epoch: 801
ogbn-arxiv,CAGNET-1D,1,801,29.9946,0.0535

Epoch: 802
ogbn-arxiv,CAGNET-1D,1,802,30.0315,0.0533

Epoch: 803
ogbn-arxiv,CAGNET-1D,1,803,30.0692,0.0533

Epoch: 804
ogbn-arxiv,CAGNET-1D,1,804,30.1063,0.0535

Epoch: 805
ogbn-arxiv,CAGNET-1D,1,805,30.1436,0.0532

Epoch: 806
ogbn-arxiv,CAGNET-1D,1,806,30.1809,0.0534

Epoch: 807
ogbn-arxiv,CAGNET-1D,1,807,30.2184,0.0533

Epoch: 808
ogbn-arxiv,CAGNET-1D,1,808,30.2559,0.0537

Epoch: 809
ogbn-arxiv,CAGNET-1D,1,809,30.2925,0.0531

Epoch: 810
ogbn-arxiv,CAGNET-1D,1,810,30.3298,0.0538

Epoch: 811
ogbn-arxiv,CAGNET-1D,1,811,30.3669,0.0530

Epoch: 812
ogbn-arxiv,CAGNET-1D,1,812,30.4043,0.0543

Epoch: 813
ogbn-arxiv,CAGNET-1D,1,813,30.4415,0.0530

Epoch: 814
ogbn-arxiv,CAGNET-1D,1,814,30.4783,0.0541

Epoch: 815
ogbn-arxiv,CAGNET-1D,1,815,30.5152,0.0532

Epoch: 816
ogbn-arxiv,CAGNET-1D,1,816,30.5528,0.0536

Epoch: 817
ogbn-arxiv,CAGNET-1D,1,817,30.5899,0.0532

Epoch: 818
ogbn-arxiv,CAGNET-1D,1,818,30.6273,0.0537

Epoch: 819
ogbn-arxiv,CAGNET-1D,1,819,30.6646,0.0533

Epoch: 820
ogbn-arxiv,CAGNET-1D,1,820,30.7016,0.0531

Epoch: 821
ogbn-arxiv,CAGNET-1D,1,821,30.7391,0.0534

Epoch: 822
ogbn-arxiv,CAGNET-1D,1,822,30.7769,0.0529

Epoch: 823
ogbn-arxiv,CAGNET-1D,1,823,30.8138,0.0549

Epoch: 824
ogbn-arxiv,CAGNET-1D,1,824,30.8508,0.0528

Epoch: 825
ogbn-arxiv,CAGNET-1D,1,825,30.8878,0.0612

Epoch: 826
ogbn-arxiv,CAGNET-1D,1,826,30.9247,0.0529

Epoch: 827
ogbn-arxiv,CAGNET-1D,1,827,30.9623,0.0718

Epoch: 828
ogbn-arxiv,CAGNET-1D,1,828,30.9995,0.0526

Epoch: 829
ogbn-arxiv,CAGNET-1D,1,829,31.0361,0.0780

Epoch: 830
ogbn-arxiv,CAGNET-1D,1,830,31.0733,0.0527

Epoch: 831
ogbn-arxiv,CAGNET-1D,1,831,31.1108,0.0628

Epoch: 832
ogbn-arxiv,CAGNET-1D,1,832,31.1478,0.0527

Epoch: 833
ogbn-arxiv,CAGNET-1D,1,833,31.1848,0.0540

Epoch: 834
ogbn-arxiv,CAGNET-1D,1,834,31.2213,0.0539

Epoch: 835
ogbn-arxiv,CAGNET-1D,1,835,31.2582,0.0525

Epoch: 836
ogbn-arxiv,CAGNET-1D,1,836,31.2950,0.0691

Epoch: 837
ogbn-arxiv,CAGNET-1D,1,837,31.3325,0.0526

Epoch: 838
ogbn-arxiv,CAGNET-1D,1,838,31.3697,0.0722

Epoch: 839
ogbn-arxiv,CAGNET-1D,1,839,31.4071,0.0524

Epoch: 840
ogbn-arxiv,CAGNET-1D,1,840,31.4441,0.0574

Epoch: 841
ogbn-arxiv,CAGNET-1D,1,841,31.4834,0.0525

Epoch: 842
ogbn-arxiv,CAGNET-1D,1,842,31.5204,0.0526

Epoch: 843
ogbn-arxiv,CAGNET-1D,1,843,31.5579,0.0579

Epoch: 844
ogbn-arxiv,CAGNET-1D,1,844,31.5946,0.0523

Epoch: 845
ogbn-arxiv,CAGNET-1D,1,845,31.6317,0.0780

Epoch: 846
ogbn-arxiv,CAGNET-1D,1,846,31.6723,0.0525

Epoch: 847
ogbn-arxiv,CAGNET-1D,1,847,31.7095,0.0861

Epoch: 848
ogbn-arxiv,CAGNET-1D,1,848,31.7464,0.0523

Epoch: 849
ogbn-arxiv,CAGNET-1D,1,849,31.7833,0.0546

Epoch: 850
ogbn-arxiv,CAGNET-1D,1,850,31.8198,0.0563

Epoch: 851
ogbn-arxiv,CAGNET-1D,1,851,31.8568,0.0524

Epoch: 852
ogbn-arxiv,CAGNET-1D,1,852,31.8935,0.0883

Epoch: 853
ogbn-arxiv,CAGNET-1D,1,853,31.9308,0.0524

Epoch: 854
ogbn-arxiv,CAGNET-1D,1,854,31.9681,0.0705

Epoch: 855
ogbn-arxiv,CAGNET-1D,1,855,32.0055,0.0525

Epoch: 856
ogbn-arxiv,CAGNET-1D,1,856,32.0431,0.0533

Epoch: 857
ogbn-arxiv,CAGNET-1D,1,857,32.0801,0.0588

Epoch: 858
ogbn-arxiv,CAGNET-1D,1,858,32.1176,0.0525

Epoch: 859
ogbn-arxiv,CAGNET-1D,1,859,32.1545,0.0713

Epoch: 860
ogbn-arxiv,CAGNET-1D,1,860,32.1915,0.0525

Epoch: 861
ogbn-arxiv,CAGNET-1D,1,861,32.2285,0.0581

Epoch: 862
ogbn-arxiv,CAGNET-1D,1,862,32.2654,0.0529

Epoch: 863
ogbn-arxiv,CAGNET-1D,1,863,32.3025,0.0529

Epoch: 864
ogbn-arxiv,CAGNET-1D,1,864,32.3401,0.0617

Epoch: 865
ogbn-arxiv,CAGNET-1D,1,865,32.3775,0.0527

Epoch: 866
ogbn-arxiv,CAGNET-1D,1,866,32.4151,0.0620

Epoch: 867
ogbn-arxiv,CAGNET-1D,1,867,32.4527,0.0530

Epoch: 868
ogbn-arxiv,CAGNET-1D,1,868,32.4899,0.0552

Epoch: 869
ogbn-arxiv,CAGNET-1D,1,869,32.5271,0.0534

Epoch: 870
ogbn-arxiv,CAGNET-1D,1,870,32.5640,0.0535

Epoch: 871
ogbn-arxiv,CAGNET-1D,1,871,32.6013,0.0574

Epoch: 872
ogbn-arxiv,CAGNET-1D,1,872,32.6381,0.0529

Epoch: 873
ogbn-arxiv,CAGNET-1D,1,873,32.6755,0.0638

Epoch: 874
ogbn-arxiv,CAGNET-1D,1,874,32.7129,0.0530

Epoch: 875
ogbn-arxiv,CAGNET-1D,1,875,32.7505,0.0563

Epoch: 876
ogbn-arxiv,CAGNET-1D,1,876,32.7878,0.0535

Epoch: 877
ogbn-arxiv,CAGNET-1D,1,877,32.8258,0.0536

Epoch: 878
ogbn-arxiv,CAGNET-1D,1,878,32.8625,0.0563

Epoch: 879
ogbn-arxiv,CAGNET-1D,1,879,32.9011,0.0532

Epoch: 880
ogbn-arxiv,CAGNET-1D,1,880,32.9376,0.0567

Epoch: 881
ogbn-arxiv,CAGNET-1D,1,881,32.9750,0.0534

Epoch: 882
ogbn-arxiv,CAGNET-1D,1,882,33.0127,0.0546

Epoch: 883
ogbn-arxiv,CAGNET-1D,1,883,33.0496,0.0543

Epoch: 884
ogbn-arxiv,CAGNET-1D,1,884,33.0870,0.0537

Epoch: 885
ogbn-arxiv,CAGNET-1D,1,885,33.1237,0.0568

Epoch: 886
ogbn-arxiv,CAGNET-1D,1,886,33.1608,0.0535

Epoch: 887
ogbn-arxiv,CAGNET-1D,1,887,33.1979,0.0567

Epoch: 888
ogbn-arxiv,CAGNET-1D,1,888,33.2361,0.0544

Epoch: 889
ogbn-arxiv,CAGNET-1D,1,889,33.2735,0.0541

Epoch: 890
ogbn-arxiv,CAGNET-1D,1,890,33.3112,0.0563

Epoch: 891
ogbn-arxiv,CAGNET-1D,1,891,33.3481,0.0542

Epoch: 892
ogbn-arxiv,CAGNET-1D,1,892,33.3850,0.0627

Epoch: 893
ogbn-arxiv,CAGNET-1D,1,893,33.4228,0.0542

Epoch: 894
ogbn-arxiv,CAGNET-1D,1,894,33.4599,0.0601

Epoch: 895
ogbn-arxiv,CAGNET-1D,1,895,33.4981,0.0544

Epoch: 896
ogbn-arxiv,CAGNET-1D,1,896,33.5351,0.0570

Epoch: 897
ogbn-arxiv,CAGNET-1D,1,897,33.5724,0.0546

Epoch: 898
ogbn-arxiv,CAGNET-1D,1,898,33.6089,0.0557

Epoch: 899
ogbn-arxiv,CAGNET-1D,1,899,33.6462,0.0551

Epoch: 900
ogbn-arxiv,CAGNET-1D,1,900,33.6833,0.0553

Epoch: 901
ogbn-arxiv,CAGNET-1D,1,901,33.7210,0.0562

Epoch: 902
ogbn-arxiv,CAGNET-1D,1,902,33.7584,0.0548

Epoch: 903
ogbn-arxiv,CAGNET-1D,1,903,33.7961,0.0562

Epoch: 904
ogbn-arxiv,CAGNET-1D,1,904,33.8333,0.0555

Epoch: 905
ogbn-arxiv,CAGNET-1D,1,905,33.8704,0.0557

Epoch: 906
ogbn-arxiv,CAGNET-1D,1,906,33.9070,0.0555

Epoch: 907
ogbn-arxiv,CAGNET-1D,1,907,33.9441,0.0553

Epoch: 908
ogbn-arxiv,CAGNET-1D,1,908,33.9821,0.0569

Epoch: 909
ogbn-arxiv,CAGNET-1D,1,909,34.0191,0.0550

Epoch: 910
ogbn-arxiv,CAGNET-1D,1,910,34.0561,0.0571

Epoch: 911
ogbn-arxiv,CAGNET-1D,1,911,34.0928,0.0551

Epoch: 912
ogbn-arxiv,CAGNET-1D,1,912,34.1298,0.0580

Epoch: 913
ogbn-arxiv,CAGNET-1D,1,913,34.1665,0.0546

Epoch: 914
ogbn-arxiv,CAGNET-1D,1,914,34.2035,0.0583

Epoch: 915
ogbn-arxiv,CAGNET-1D,1,915,34.2411,0.0551

Epoch: 916
ogbn-arxiv,CAGNET-1D,1,916,34.2785,0.0572

Epoch: 917
ogbn-arxiv,CAGNET-1D,1,917,34.3162,0.0549

Epoch: 918
ogbn-arxiv,CAGNET-1D,1,918,34.3532,0.0571

Epoch: 919
ogbn-arxiv,CAGNET-1D,1,919,34.3925,0.0553

Epoch: 920
ogbn-arxiv,CAGNET-1D,1,920,34.4297,0.0551

Epoch: 921
ogbn-arxiv,CAGNET-1D,1,921,34.4665,0.0578

Epoch: 922
ogbn-arxiv,CAGNET-1D,1,922,34.5027,0.0546

Epoch: 923
ogbn-arxiv,CAGNET-1D,1,923,34.5397,0.0588

Epoch: 924
ogbn-arxiv,CAGNET-1D,1,924,34.5768,0.0546

Epoch: 925
ogbn-arxiv,CAGNET-1D,1,925,34.6139,0.0585

Epoch: 926
ogbn-arxiv,CAGNET-1D,1,926,34.6510,0.0547

Epoch: 927
ogbn-arxiv,CAGNET-1D,1,927,34.6881,0.0556

Epoch: 928
ogbn-arxiv,CAGNET-1D,1,928,34.7250,0.0563

Epoch: 929
ogbn-arxiv,CAGNET-1D,1,929,34.7620,0.0551

Epoch: 930
ogbn-arxiv,CAGNET-1D,1,930,34.7990,0.0578

Epoch: 931
ogbn-arxiv,CAGNET-1D,1,931,34.8360,0.0542

Epoch: 932
ogbn-arxiv,CAGNET-1D,1,932,34.8735,0.0582

Epoch: 933
ogbn-arxiv,CAGNET-1D,1,933,34.9104,0.0548

Epoch: 934
ogbn-arxiv,CAGNET-1D,1,934,34.9469,0.0561

Epoch: 935
ogbn-arxiv,CAGNET-1D,1,935,34.9835,0.0550

Epoch: 936
ogbn-arxiv,CAGNET-1D,1,936,35.0203,0.0555

Epoch: 937
ogbn-arxiv,CAGNET-1D,1,937,35.0575,0.0561

Epoch: 938
ogbn-arxiv,CAGNET-1D,1,938,35.0944,0.0542

Epoch: 939
ogbn-arxiv,CAGNET-1D,1,939,35.1324,0.0597

Epoch: 940
ogbn-arxiv,CAGNET-1D,1,940,35.1697,0.0542

Epoch: 941
ogbn-arxiv,CAGNET-1D,1,941,35.2070,0.0582

Epoch: 942
ogbn-arxiv,CAGNET-1D,1,942,35.2442,0.0546

Epoch: 943
ogbn-arxiv,CAGNET-1D,1,943,35.2814,0.0584

Epoch: 944
ogbn-arxiv,CAGNET-1D,1,944,35.3189,0.0542

Epoch: 945
ogbn-arxiv,CAGNET-1D,1,945,35.3558,0.0575

Epoch: 946
ogbn-arxiv,CAGNET-1D,1,946,35.3935,0.0553

Epoch: 947
ogbn-arxiv,CAGNET-1D,1,947,35.4304,0.0553

Epoch: 948
ogbn-arxiv,CAGNET-1D,1,948,35.4682,0.0560

Epoch: 949
ogbn-arxiv,CAGNET-1D,1,949,35.5051,0.0551

Epoch: 950
ogbn-arxiv,CAGNET-1D,1,950,35.5421,0.0585

Epoch: 951
ogbn-arxiv,CAGNET-1D,1,951,35.5791,0.0542

Epoch: 952
ogbn-arxiv,CAGNET-1D,1,952,35.6159,0.0605

Epoch: 953
ogbn-arxiv,CAGNET-1D,1,953,35.6523,0.0546

Epoch: 954
ogbn-arxiv,CAGNET-1D,1,954,35.6890,0.0579

Epoch: 955
ogbn-arxiv,CAGNET-1D,1,955,35.7256,0.0549

Epoch: 956
ogbn-arxiv,CAGNET-1D,1,956,35.7624,0.0569

Epoch: 957
ogbn-arxiv,CAGNET-1D,1,957,35.7992,0.0549

Epoch: 958
ogbn-arxiv,CAGNET-1D,1,958,35.8362,0.0581

Epoch: 959
ogbn-arxiv,CAGNET-1D,1,959,35.8735,0.0544

Epoch: 960
ogbn-arxiv,CAGNET-1D,1,960,35.9109,0.0637

Epoch: 961
ogbn-arxiv,CAGNET-1D,1,961,35.9480,0.0544

Epoch: 962
ogbn-arxiv,CAGNET-1D,1,962,35.9852,0.0733

Epoch: 963
ogbn-arxiv,CAGNET-1D,1,963,36.0236,0.0543

Epoch: 964
ogbn-arxiv,CAGNET-1D,1,964,36.0597,0.0625

Epoch: 965
ogbn-arxiv,CAGNET-1D,1,965,36.0961,0.0549

Epoch: 966
ogbn-arxiv,CAGNET-1D,1,966,36.1327,0.0565

Epoch: 967
ogbn-arxiv,CAGNET-1D,1,967,36.1690,0.0555

Epoch: 968
ogbn-arxiv,CAGNET-1D,1,968,36.2067,0.0550

Epoch: 969
ogbn-arxiv,CAGNET-1D,1,969,36.2432,0.0583

Epoch: 970
ogbn-arxiv,CAGNET-1D,1,970,36.2802,0.0547

Epoch: 971
ogbn-arxiv,CAGNET-1D,1,971,36.3166,0.0579

Epoch: 972
ogbn-arxiv,CAGNET-1D,1,972,36.3534,0.0549

Epoch: 973
ogbn-arxiv,CAGNET-1D,1,973,36.3903,0.0584

Epoch: 974
ogbn-arxiv,CAGNET-1D,1,974,36.4276,0.0544

Epoch: 975
ogbn-arxiv,CAGNET-1D,1,975,36.4652,0.0586

Epoch: 976
ogbn-arxiv,CAGNET-1D,1,976,36.5027,0.0546

Epoch: 977
ogbn-arxiv,CAGNET-1D,1,977,36.5402,0.0584

Epoch: 978
ogbn-arxiv,CAGNET-1D,1,978,36.5774,0.0543

Epoch: 979
ogbn-arxiv,CAGNET-1D,1,979,36.6146,0.0596

Epoch: 980
ogbn-arxiv,CAGNET-1D,1,980,36.6522,0.0543

Epoch: 981
ogbn-arxiv,CAGNET-1D,1,981,36.6905,0.0588

Epoch: 982
ogbn-arxiv,CAGNET-1D,1,982,36.7292,0.0548

Epoch: 983
ogbn-arxiv,CAGNET-1D,1,983,36.7665,0.0554

Epoch: 984
ogbn-arxiv,CAGNET-1D,1,984,36.8042,0.0557

Epoch: 985
ogbn-arxiv,CAGNET-1D,1,985,36.8413,0.0552

Epoch: 986
ogbn-arxiv,CAGNET-1D,1,986,36.8784,0.0572

Epoch: 987
ogbn-arxiv,CAGNET-1D,1,987,36.9157,0.0541

Epoch: 988
ogbn-arxiv,CAGNET-1D,1,988,36.9527,0.0582

Epoch: 989
ogbn-arxiv,CAGNET-1D,1,989,36.9896,0.0551

Epoch: 990
ogbn-arxiv,CAGNET-1D,1,990,37.0268,0.0556

Epoch: 991
ogbn-arxiv,CAGNET-1D,1,991,37.0642,0.0548

Epoch: 992
ogbn-arxiv,CAGNET-1D,1,992,37.1066,0.0561

Epoch: 993
ogbn-arxiv,CAGNET-1D,1,993,37.1440,0.0549

Epoch: 994
ogbn-arxiv,CAGNET-1D,1,994,37.1811,0.0550

Epoch: 995
ogbn-arxiv,CAGNET-1D,1,995,37.2182,0.0556

Epoch: 996
ogbn-arxiv,CAGNET-1D,1,996,37.2554,0.0551

Epoch: 997
ogbn-arxiv,CAGNET-1D,1,997,37.2923,0.0563

Epoch: 998
ogbn-arxiv,CAGNET-1D,1,998,37.3293,0.0539

Epoch: 999
ogbn-arxiv,CAGNET-1D,1,999,37.3662,0.0582

Epoch: 1000
ogbn-arxiv,CAGNET-1D,1,1000,37.4032,0.0541

Epoch: 1001
ogbn-arxiv,CAGNET-1D,1,1001,37.4403,0.0643

Epoch: 1002
ogbn-arxiv,CAGNET-1D,1,1002,37.4772,0.0537

Epoch: 1003
ogbn-arxiv,CAGNET-1D,1,1003,37.5142,0.0657

Epoch: 1004
ogbn-arxiv,CAGNET-1D,1,1004,37.5509,0.0536

Epoch: 1005
ogbn-arxiv,CAGNET-1D,1,1005,37.5883,0.0610

Epoch: 1006
ogbn-arxiv,CAGNET-1D,1,1006,37.6252,0.0538

Epoch: 1007
ogbn-arxiv,CAGNET-1D,1,1007,37.6619,0.0622

Epoch: 1008
ogbn-arxiv,CAGNET-1D,1,1008,37.6989,0.0536

Epoch: 1009
ogbn-arxiv,CAGNET-1D,1,1009,37.7360,0.0650

Epoch: 1010
ogbn-arxiv,CAGNET-1D,1,1010,37.7731,0.0540

Epoch: 1011
ogbn-arxiv,CAGNET-1D,1,1011,37.8101,0.0670

Epoch: 1012
ogbn-arxiv,CAGNET-1D,1,1012,37.8470,0.0538

Epoch: 1013
ogbn-arxiv,CAGNET-1D,1,1013,37.8839,0.0655

Epoch: 1014
ogbn-arxiv,CAGNET-1D,1,1014,37.9209,0.0536

Epoch: 1015
ogbn-arxiv,CAGNET-1D,1,1015,37.9576,0.0706

Epoch: 1016
ogbn-arxiv,CAGNET-1D,1,1016,37.9944,0.0535

Epoch: 1017
ogbn-arxiv,CAGNET-1D,1,1017,38.0326,0.0669

Epoch: 1018
ogbn-arxiv,CAGNET-1D,1,1018,38.0699,0.0536

Epoch: 1019
ogbn-arxiv,CAGNET-1D,1,1019,38.1078,0.0568

Epoch: 1020
ogbn-arxiv,CAGNET-1D,1,1020,38.1459,0.0547

Epoch: 1021
ogbn-arxiv,CAGNET-1D,1,1021,38.1842,0.0534

Epoch: 1022
ogbn-arxiv,CAGNET-1D,1,1022,38.2220,0.0658

Epoch: 1023
ogbn-arxiv,CAGNET-1D,1,1023,38.2599,0.0536

Epoch: 1024
ogbn-arxiv,CAGNET-1D,1,1024,38.2975,0.1070

Epoch: 1025
ogbn-arxiv,CAGNET-1D,1,1025,38.3351,0.0535

Epoch: 1026
ogbn-arxiv,CAGNET-1D,1,1026,38.3726,0.0829

Epoch: 1027
ogbn-arxiv,CAGNET-1D,1,1027,38.4098,0.0533

Epoch: 1028
ogbn-arxiv,CAGNET-1D,1,1028,38.4470,0.0559

Epoch: 1029
ogbn-arxiv,CAGNET-1D,1,1029,38.4843,0.0556

Epoch: 1030
ogbn-arxiv,CAGNET-1D,1,1030,38.5218,0.0535

Epoch: 1031
ogbn-arxiv,CAGNET-1D,1,1031,38.5590,0.0765

Epoch: 1032
ogbn-arxiv,CAGNET-1D,1,1032,38.5963,0.0535

Epoch: 1033
ogbn-arxiv,CAGNET-1D,1,1033,38.6339,0.0873

Epoch: 1034
ogbn-arxiv,CAGNET-1D,1,1034,38.6716,0.0532

Epoch: 1035
ogbn-arxiv,CAGNET-1D,1,1035,38.7096,0.0604

Epoch: 1036
ogbn-arxiv,CAGNET-1D,1,1036,38.7473,0.0540

Epoch: 1037
ogbn-arxiv,CAGNET-1D,1,1037,38.7846,0.0532

Epoch: 1038
ogbn-arxiv,CAGNET-1D,1,1038,38.8220,0.0747

Epoch: 1039
ogbn-arxiv,CAGNET-1D,1,1039,38.8593,0.0535

Epoch: 1040
ogbn-arxiv,CAGNET-1D,1,1040,38.8962,0.0835

Epoch: 1041
ogbn-arxiv,CAGNET-1D,1,1041,38.9365,0.0532

Epoch: 1042
ogbn-arxiv,CAGNET-1D,1,1042,38.9744,0.0627

Epoch: 1043
ogbn-arxiv,CAGNET-1D,1,1043,39.0115,0.0530

Epoch: 1044
ogbn-arxiv,CAGNET-1D,1,1044,39.0487,0.0565

Epoch: 1045
ogbn-arxiv,CAGNET-1D,1,1045,39.0856,0.0535

Epoch: 1046
ogbn-arxiv,CAGNET-1D,1,1046,39.1232,0.0538

Epoch: 1047
ogbn-arxiv,CAGNET-1D,1,1047,39.1604,0.0613

Epoch: 1048
ogbn-arxiv,CAGNET-1D,1,1048,39.1975,0.0528

Epoch: 1049
ogbn-arxiv,CAGNET-1D,1,1049,39.2350,0.0692

Epoch: 1050
ogbn-arxiv,CAGNET-1D,1,1050,39.2722,0.0531

Epoch: 1051
ogbn-arxiv,CAGNET-1D,1,1051,39.3095,0.0613

Epoch: 1052
ogbn-arxiv,CAGNET-1D,1,1052,39.3472,0.0530

Epoch: 1053
ogbn-arxiv,CAGNET-1D,1,1053,39.3852,0.0564

Epoch: 1054
ogbn-arxiv,CAGNET-1D,1,1054,39.4230,0.0534

Epoch: 1055
ogbn-arxiv,CAGNET-1D,1,1055,39.4609,0.0535

Epoch: 1056
ogbn-arxiv,CAGNET-1D,1,1056,39.4985,0.0593

Epoch: 1057
ogbn-arxiv,CAGNET-1D,1,1057,39.5357,0.0532

Epoch: 1058
ogbn-arxiv,CAGNET-1D,1,1058,39.5730,0.0633

Epoch: 1059
ogbn-arxiv,CAGNET-1D,1,1059,39.6104,0.0531

Epoch: 1060
ogbn-arxiv,CAGNET-1D,1,1060,39.6479,0.0602

Epoch: 1061
ogbn-arxiv,CAGNET-1D,1,1061,39.6856,0.0533

Epoch: 1062
ogbn-arxiv,CAGNET-1D,1,1062,39.7231,0.0577

Epoch: 1063
ogbn-arxiv,CAGNET-1D,1,1063,39.7613,0.0532

Epoch: 1064
ogbn-arxiv,CAGNET-1D,1,1064,39.7979,0.0543

Epoch: 1065
ogbn-arxiv,CAGNET-1D,1,1065,39.8352,0.0543

Epoch: 1066
ogbn-arxiv,CAGNET-1D,1,1066,39.8731,0.0530

Epoch: 1067
ogbn-arxiv,CAGNET-1D,1,1067,39.9103,0.0560

Epoch: 1068
ogbn-arxiv,CAGNET-1D,1,1068,39.9472,0.0531

Epoch: 1069
ogbn-arxiv,CAGNET-1D,1,1069,39.9845,0.0560

Epoch: 1070
ogbn-arxiv,CAGNET-1D,1,1070,40.0218,0.0533

Epoch: 1071
ogbn-arxiv,CAGNET-1D,1,1071,40.0588,0.0573

Epoch: 1072
ogbn-arxiv,CAGNET-1D,1,1072,40.0958,0.0528

Epoch: 1073
ogbn-arxiv,CAGNET-1D,1,1073,40.1333,0.0552

Epoch: 1074
ogbn-arxiv,CAGNET-1D,1,1074,40.1704,0.0537

Epoch: 1075
ogbn-arxiv,CAGNET-1D,1,1075,40.2075,0.0535

Epoch: 1076
ogbn-arxiv,CAGNET-1D,1,1076,40.2446,0.0539

Epoch: 1077
ogbn-arxiv,CAGNET-1D,1,1077,40.2817,0.0535

Epoch: 1078
ogbn-arxiv,CAGNET-1D,1,1078,40.3187,0.0546

Epoch: 1079
ogbn-arxiv,CAGNET-1D,1,1079,40.3560,0.0535

Epoch: 1080
ogbn-arxiv,CAGNET-1D,1,1080,40.3933,0.0567

Epoch: 1081
ogbn-arxiv,CAGNET-1D,1,1081,40.4306,0.0529

Epoch: 1082
ogbn-arxiv,CAGNET-1D,1,1082,40.4684,0.0554

Epoch: 1083
ogbn-arxiv,CAGNET-1D,1,1083,40.5056,0.0537

Epoch: 1084
ogbn-arxiv,CAGNET-1D,1,1084,40.5425,0.0537

Epoch: 1085
ogbn-arxiv,CAGNET-1D,1,1085,40.5801,0.0541

Epoch: 1086
ogbn-arxiv,CAGNET-1D,1,1086,40.6169,0.0566

Epoch: 1087
ogbn-arxiv,CAGNET-1D,1,1087,40.6543,0.0531

Epoch: 1088
ogbn-arxiv,CAGNET-1D,1,1088,40.6905,0.0619

Epoch: 1089
ogbn-arxiv,CAGNET-1D,1,1089,40.7273,0.0539

Epoch: 1090
ogbn-arxiv,CAGNET-1D,1,1090,40.7660,0.0613

Epoch: 1091
ogbn-arxiv,CAGNET-1D,1,1091,40.8041,0.0536

Epoch: 1092
ogbn-arxiv,CAGNET-1D,1,1092,40.8412,0.0678

Epoch: 1093
ogbn-arxiv,CAGNET-1D,1,1093,40.8790,0.0535

Epoch: 1094
ogbn-arxiv,CAGNET-1D,1,1094,40.9158,0.0782

Epoch: 1095
ogbn-arxiv,CAGNET-1D,1,1095,40.9530,0.0534

Epoch: 1096
ogbn-arxiv,CAGNET-1D,1,1096,40.9897,0.0660

Epoch: 1097
ogbn-arxiv,CAGNET-1D,1,1097,41.0269,0.0538

Epoch: 1098
ogbn-arxiv,CAGNET-1D,1,1098,41.0643,0.0587

Epoch: 1099
ogbn-arxiv,CAGNET-1D,1,1099,41.1019,0.0535

Epoch: 1100
ogbn-arxiv,CAGNET-1D,1,1100,41.1389,0.0559

Epoch: 1101
ogbn-arxiv,CAGNET-1D,1,1101,41.1758,0.0539

Epoch: 1102
ogbn-arxiv,CAGNET-1D,1,1102,41.2130,0.0539

Epoch: 1103
ogbn-arxiv,CAGNET-1D,1,1103,41.2502,0.0552

Epoch: 1104
ogbn-arxiv,CAGNET-1D,1,1104,41.2873,0.0537

Epoch: 1105
ogbn-arxiv,CAGNET-1D,1,1105,41.3244,0.0563

Epoch: 1106
ogbn-arxiv,CAGNET-1D,1,1106,41.3614,0.0539

Epoch: 1107
ogbn-arxiv,CAGNET-1D,1,1107,41.3982,0.0559

Epoch: 1108
ogbn-arxiv,CAGNET-1D,1,1108,41.4351,0.0539

Epoch: 1109
ogbn-arxiv,CAGNET-1D,1,1109,41.4722,0.0619

Epoch: 1110
ogbn-arxiv,CAGNET-1D,1,1110,41.5092,0.0537

Epoch: 1111
ogbn-arxiv,CAGNET-1D,1,1111,41.5462,0.0660

Epoch: 1112
ogbn-arxiv,CAGNET-1D,1,1112,41.5832,0.0538

Epoch: 1113
ogbn-arxiv,CAGNET-1D,1,1113,41.6201,0.0570

Epoch: 1114
ogbn-arxiv,CAGNET-1D,1,1114,41.6571,0.0544

Epoch: 1115
ogbn-arxiv,CAGNET-1D,1,1115,41.6947,0.0546

Epoch: 1116
ogbn-arxiv,CAGNET-1D,1,1116,41.7311,0.0551

Epoch: 1117
ogbn-arxiv,CAGNET-1D,1,1117,41.7681,0.0544

Epoch: 1118
ogbn-arxiv,CAGNET-1D,1,1118,41.8048,0.0588

Epoch: 1119
ogbn-arxiv,CAGNET-1D,1,1119,41.8414,0.0540

Epoch: 1120
ogbn-arxiv,CAGNET-1D,1,1120,41.8782,0.0584

Epoch: 1121
ogbn-arxiv,CAGNET-1D,1,1121,41.9152,0.0543

Epoch: 1122
ogbn-arxiv,CAGNET-1D,1,1122,41.9521,0.0558

Epoch: 1123
ogbn-arxiv,CAGNET-1D,1,1123,41.9888,0.0547

Epoch: 1124
ogbn-arxiv,CAGNET-1D,1,1124,42.0251,0.0551

Epoch: 1125
ogbn-arxiv,CAGNET-1D,1,1125,42.0616,0.0547

Epoch: 1126
ogbn-arxiv,CAGNET-1D,1,1126,42.0985,0.0545

Epoch: 1127
ogbn-arxiv,CAGNET-1D,1,1127,42.1353,0.0553

Epoch: 1128
ogbn-arxiv,CAGNET-1D,1,1128,42.1721,0.0540

Epoch: 1129
ogbn-arxiv,CAGNET-1D,1,1129,42.2091,0.0579

Epoch: 1130
ogbn-arxiv,CAGNET-1D,1,1130,42.2456,0.0539

Epoch: 1131
ogbn-arxiv,CAGNET-1D,1,1131,42.2820,0.0562

Epoch: 1132
ogbn-arxiv,CAGNET-1D,1,1132,42.3186,0.0543

Epoch: 1133
ogbn-arxiv,CAGNET-1D,1,1133,42.3552,0.0559

Epoch: 1134
ogbn-arxiv,CAGNET-1D,1,1134,42.3918,0.0535

Epoch: 1135
ogbn-arxiv,CAGNET-1D,1,1135,42.4290,0.0564

Epoch: 1136
ogbn-arxiv,CAGNET-1D,1,1136,42.4657,0.0543

Epoch: 1137
ogbn-arxiv,CAGNET-1D,1,1137,42.5021,0.0546

Epoch: 1138
ogbn-arxiv,CAGNET-1D,1,1138,42.5383,0.0545

Epoch: 1139
ogbn-arxiv,CAGNET-1D,1,1139,42.5753,0.0551

Epoch: 1140
ogbn-arxiv,CAGNET-1D,1,1140,42.6117,0.0538

Epoch: 1141
ogbn-arxiv,CAGNET-1D,1,1141,42.6482,0.0566

Epoch: 1142
ogbn-arxiv,CAGNET-1D,1,1142,42.6844,0.0542

Epoch: 1143
ogbn-arxiv,CAGNET-1D,1,1143,42.7214,0.0560

Epoch: 1144
ogbn-arxiv,CAGNET-1D,1,1144,42.7580,0.0542

Epoch: 1145
ogbn-arxiv,CAGNET-1D,1,1145,42.7949,0.0560

Epoch: 1146
ogbn-arxiv,CAGNET-1D,1,1146,42.8318,0.0542

Epoch: 1147
ogbn-arxiv,CAGNET-1D,1,1147,42.8682,0.0612

Epoch: 1148
ogbn-arxiv,CAGNET-1D,1,1148,42.9047,0.0537

Epoch: 1149
ogbn-arxiv,CAGNET-1D,1,1149,42.9414,0.0673

Epoch: 1150
ogbn-arxiv,CAGNET-1D,1,1150,42.9784,0.0536

Epoch: 1151
ogbn-arxiv,CAGNET-1D,1,1151,43.0150,0.0701

Epoch: 1152
ogbn-arxiv,CAGNET-1D,1,1152,43.0512,0.0537

Epoch: 1153
ogbn-arxiv,CAGNET-1D,1,1153,43.0881,0.0773

Epoch: 1154
ogbn-arxiv,CAGNET-1D,1,1154,43.1252,0.0537

Epoch: 1155
ogbn-arxiv,CAGNET-1D,1,1155,43.1622,0.0791

Epoch: 1156
ogbn-arxiv,CAGNET-1D,1,1156,43.1991,0.0537

Epoch: 1157
ogbn-arxiv,CAGNET-1D,1,1157,43.2358,0.0689

Epoch: 1158
ogbn-arxiv,CAGNET-1D,1,1158,43.2723,0.0542

Epoch: 1159
ogbn-arxiv,CAGNET-1D,1,1159,43.3094,0.0612

Epoch: 1160
ogbn-arxiv,CAGNET-1D,1,1160,43.3463,0.0545

Epoch: 1161
ogbn-arxiv,CAGNET-1D,1,1161,43.3836,0.0583

Epoch: 1162
ogbn-arxiv,CAGNET-1D,1,1162,43.4211,0.0546

Epoch: 1163
ogbn-arxiv,CAGNET-1D,1,1163,43.4593,0.0564

Epoch: 1164
ogbn-arxiv,CAGNET-1D,1,1164,43.4963,0.0563

Epoch: 1165
ogbn-arxiv,CAGNET-1D,1,1165,43.5336,0.0549

Epoch: 1166
ogbn-arxiv,CAGNET-1D,1,1166,43.5709,0.0580

Epoch: 1167
ogbn-arxiv,CAGNET-1D,1,1167,43.6080,0.0547

Epoch: 1168
ogbn-arxiv,CAGNET-1D,1,1168,43.6454,0.0589

Epoch: 1169
ogbn-arxiv,CAGNET-1D,1,1169,43.6825,0.0542

Epoch: 1170
ogbn-arxiv,CAGNET-1D,1,1170,43.7195,0.0629

Epoch: 1171
ogbn-arxiv,CAGNET-1D,1,1171,43.7566,0.0539

Epoch: 1172
ogbn-arxiv,CAGNET-1D,1,1172,43.7945,0.0730

Epoch: 1173
ogbn-arxiv,CAGNET-1D,1,1173,43.8311,0.0539

Epoch: 1174
ogbn-arxiv,CAGNET-1D,1,1174,43.8684,0.0837

Epoch: 1175
ogbn-arxiv,CAGNET-1D,1,1175,43.9058,0.0536

Epoch: 1176
ogbn-arxiv,CAGNET-1D,1,1176,43.9428,0.0791

Epoch: 1177
ogbn-arxiv,CAGNET-1D,1,1177,43.9796,0.0543

Epoch: 1178
ogbn-arxiv,CAGNET-1D,1,1178,44.0166,0.0681

Epoch: 1179
ogbn-arxiv,CAGNET-1D,1,1179,44.0535,0.0540

Epoch: 1180
ogbn-arxiv,CAGNET-1D,1,1180,44.0904,0.0615

Epoch: 1181
ogbn-arxiv,CAGNET-1D,1,1181,44.1278,0.0551

Epoch: 1182
ogbn-arxiv,CAGNET-1D,1,1182,44.1649,0.0570

Epoch: 1183
ogbn-arxiv,CAGNET-1D,1,1183,44.2020,0.0596

Epoch: 1184
ogbn-arxiv,CAGNET-1D,1,1184,44.2386,0.0543

Epoch: 1185
ogbn-arxiv,CAGNET-1D,1,1185,44.2751,0.0739

Epoch: 1186
ogbn-arxiv,CAGNET-1D,1,1186,44.3115,0.0541

Epoch: 1187
ogbn-arxiv,CAGNET-1D,1,1187,44.3489,0.1063

Epoch: 1188
ogbn-arxiv,CAGNET-1D,1,1188,44.3859,0.0537

Epoch: 1189
ogbn-arxiv,CAGNET-1D,1,1189,44.4229,0.1263

Epoch: 1190
ogbn-arxiv,CAGNET-1D,1,1190,44.4600,0.0538

Epoch: 1191
ogbn-arxiv,CAGNET-1D,1,1191,44.4970,0.0817

Epoch: 1192
ogbn-arxiv,CAGNET-1D,1,1192,44.5341,0.0545

Epoch: 1193
ogbn-arxiv,CAGNET-1D,1,1193,44.5712,0.0601

Epoch: 1194
ogbn-arxiv,CAGNET-1D,1,1194,44.6085,0.0572

Epoch: 1195
ogbn-arxiv,CAGNET-1D,1,1195,44.6454,0.0558

Epoch: 1196
ogbn-arxiv,CAGNET-1D,1,1196,44.6825,0.0622

Epoch: 1197
ogbn-arxiv,CAGNET-1D,1,1197,44.7196,0.0550

Epoch: 1198
ogbn-arxiv,CAGNET-1D,1,1198,44.7568,0.0648

Epoch: 1199
ogbn-arxiv,CAGNET-1D,1,1199,44.7937,0.0551

Epoch: 1200
ogbn-arxiv,CAGNET-1D,1,1200,44.8309,0.0653

Epoch: 1201
ogbn-arxiv,CAGNET-1D,1,1201,44.8682,0.0550

Epoch: 1202
ogbn-arxiv,CAGNET-1D,1,1202,44.9047,0.0738

Epoch: 1203
ogbn-arxiv,CAGNET-1D,1,1203,44.9417,0.0546

Epoch: 1204
ogbn-arxiv,CAGNET-1D,1,1204,44.9779,0.0845

Epoch: 1205
ogbn-arxiv,CAGNET-1D,1,1205,45.0145,0.0543

Epoch: 1206
ogbn-arxiv,CAGNET-1D,1,1206,45.0517,0.0964

Epoch: 1207
ogbn-arxiv,CAGNET-1D,1,1207,45.0884,0.0546

Epoch: 1208
ogbn-arxiv,CAGNET-1D,1,1208,45.1252,0.0778

Epoch: 1209
ogbn-arxiv,CAGNET-1D,1,1209,45.1619,0.0552

Epoch: 1210
ogbn-arxiv,CAGNET-1D,1,1210,45.1987,0.0610

Epoch: 1211
ogbn-arxiv,CAGNET-1D,1,1211,45.2362,0.0594

Epoch: 1212
ogbn-arxiv,CAGNET-1D,1,1212,45.2731,0.0557

Epoch: 1213
ogbn-arxiv,CAGNET-1D,1,1213,45.3101,0.0684

Epoch: 1214
ogbn-arxiv,CAGNET-1D,1,1214,45.3469,0.0550

Epoch: 1215
ogbn-arxiv,CAGNET-1D,1,1215,45.3841,0.0805

Epoch: 1216
ogbn-arxiv,CAGNET-1D,1,1216,45.4215,0.0551

Epoch: 1217
ogbn-arxiv,CAGNET-1D,1,1217,45.4587,0.0779

Epoch: 1218
ogbn-arxiv,CAGNET-1D,1,1218,45.4961,0.0551

Epoch: 1219
ogbn-arxiv,CAGNET-1D,1,1219,45.5333,0.0825

Epoch: 1220
ogbn-arxiv,CAGNET-1D,1,1220,45.5705,0.0549

Epoch: 1221
ogbn-arxiv,CAGNET-1D,1,1221,45.6074,0.0693

Epoch: 1222
ogbn-arxiv,CAGNET-1D,1,1222,45.6447,0.0572

Epoch: 1223
ogbn-arxiv,CAGNET-1D,1,1223,45.6814,0.0572

Epoch: 1224
ogbn-arxiv,CAGNET-1D,1,1224,45.7179,0.0618

Epoch: 1225
ogbn-arxiv,CAGNET-1D,1,1225,45.7546,0.0562

Epoch: 1226
ogbn-arxiv,CAGNET-1D,1,1226,45.7912,0.0654

Epoch: 1227
ogbn-arxiv,CAGNET-1D,1,1227,45.8276,0.0550

Epoch: 1228
ogbn-arxiv,CAGNET-1D,1,1228,45.8645,0.0881

Epoch: 1229
ogbn-arxiv,CAGNET-1D,1,1229,45.9013,0.0542

Epoch: 1230
ogbn-arxiv,CAGNET-1D,1,1230,45.9377,0.0840

Epoch: 1231
ogbn-arxiv,CAGNET-1D,1,1231,45.9741,0.0555

Epoch: 1232
ogbn-arxiv,CAGNET-1D,1,1232,46.0109,0.0650

Epoch: 1233
ogbn-arxiv,CAGNET-1D,1,1233,46.0479,0.0561

Epoch: 1234
ogbn-arxiv,CAGNET-1D,1,1234,46.0844,0.0656

Epoch: 1235
ogbn-arxiv,CAGNET-1D,1,1235,46.1229,0.0561

Epoch: 1236
ogbn-arxiv,CAGNET-1D,1,1236,46.1596,0.0602

Epoch: 1237
ogbn-arxiv,CAGNET-1D,1,1237,46.1974,0.0604

Epoch: 1238
ogbn-arxiv,CAGNET-1D,1,1238,46.2347,0.0562

Epoch: 1239
ogbn-arxiv,CAGNET-1D,1,1239,46.2716,0.0624

Epoch: 1240
ogbn-arxiv,CAGNET-1D,1,1240,46.3082,0.0582

Epoch: 1241
ogbn-arxiv,CAGNET-1D,1,1241,46.3451,0.0575

Epoch: 1242
ogbn-arxiv,CAGNET-1D,1,1242,46.3819,0.0606

Epoch: 1243
ogbn-arxiv,CAGNET-1D,1,1243,46.4187,0.0583

Epoch: 1244
ogbn-arxiv,CAGNET-1D,1,1244,46.4554,0.0575

Epoch: 1245
ogbn-arxiv,CAGNET-1D,1,1245,46.4922,0.0627

Epoch: 1246
ogbn-arxiv,CAGNET-1D,1,1246,46.5289,0.0560

Epoch: 1247
ogbn-arxiv,CAGNET-1D,1,1247,46.5654,0.0626

Epoch: 1248
ogbn-arxiv,CAGNET-1D,1,1248,46.6022,0.0575

Epoch: 1249
ogbn-arxiv,CAGNET-1D,1,1249,46.6394,0.0588

Epoch: 1250
ogbn-arxiv,CAGNET-1D,1,1250,46.6762,0.0594

Epoch: 1251
ogbn-arxiv,CAGNET-1D,1,1251,46.7138,0.0595

Epoch: 1252
ogbn-arxiv,CAGNET-1D,1,1252,46.7505,0.0573

Epoch: 1253
ogbn-arxiv,CAGNET-1D,1,1253,46.7872,0.0662

Epoch: 1254
ogbn-arxiv,CAGNET-1D,1,1254,46.8236,0.0555

Epoch: 1255
ogbn-arxiv,CAGNET-1D,1,1255,46.8603,0.0692

Epoch: 1256
ogbn-arxiv,CAGNET-1D,1,1256,46.8968,0.0556

Epoch: 1257
ogbn-arxiv,CAGNET-1D,1,1257,46.9336,0.0779

Epoch: 1258
ogbn-arxiv,CAGNET-1D,1,1258,46.9709,0.0548

Epoch: 1259
ogbn-arxiv,CAGNET-1D,1,1259,47.0102,0.1119

Epoch: 1260
ogbn-arxiv,CAGNET-1D,1,1260,47.0470,0.0543

Epoch: 1261
ogbn-arxiv,CAGNET-1D,1,1261,47.0845,0.0956

Epoch: 1262
ogbn-arxiv,CAGNET-1D,1,1262,47.1211,0.0559

Epoch: 1263
ogbn-arxiv,CAGNET-1D,1,1263,47.1571,0.0628

Epoch: 1264
ogbn-arxiv,CAGNET-1D,1,1264,47.1939,0.0600

Epoch: 1265
ogbn-arxiv,CAGNET-1D,1,1265,47.2304,0.0574

Epoch: 1266
ogbn-arxiv,CAGNET-1D,1,1266,47.2673,0.0692

Epoch: 1267
ogbn-arxiv,CAGNET-1D,1,1267,47.3041,0.0553

Epoch: 1268
ogbn-arxiv,CAGNET-1D,1,1268,47.3417,0.0896

Epoch: 1269
ogbn-arxiv,CAGNET-1D,1,1269,47.3796,0.0554

Epoch: 1270
ogbn-arxiv,CAGNET-1D,1,1270,47.4169,0.0835

Epoch: 1271
ogbn-arxiv,CAGNET-1D,1,1271,47.4544,0.0556

Epoch: 1272
ogbn-arxiv,CAGNET-1D,1,1272,47.4915,0.0677

Epoch: 1273
ogbn-arxiv,CAGNET-1D,1,1273,47.5282,0.0572

Epoch: 1274
ogbn-arxiv,CAGNET-1D,1,1274,47.5650,0.0606

Epoch: 1275
ogbn-arxiv,CAGNET-1D,1,1275,47.6019,0.0610

Epoch: 1276
ogbn-arxiv,CAGNET-1D,1,1276,47.6393,0.0575

Epoch: 1277
ogbn-arxiv,CAGNET-1D,1,1277,47.6771,0.0675

Epoch: 1278
ogbn-arxiv,CAGNET-1D,1,1278,47.7145,0.0558

Epoch: 1279
ogbn-arxiv,CAGNET-1D,1,1279,47.7517,0.0744

Epoch: 1280
ogbn-arxiv,CAGNET-1D,1,1280,47.7890,0.0557

Epoch: 1281
ogbn-arxiv,CAGNET-1D,1,1281,47.8258,0.0751

Epoch: 1282
ogbn-arxiv,CAGNET-1D,1,1282,47.8624,0.0558

Epoch: 1283
ogbn-arxiv,CAGNET-1D,1,1283,47.8999,0.0716

Epoch: 1284
ogbn-arxiv,CAGNET-1D,1,1284,47.9397,0.0565

Epoch: 1285
ogbn-arxiv,CAGNET-1D,1,1285,47.9764,0.0630

Epoch: 1286
ogbn-arxiv,CAGNET-1D,1,1286,48.0135,0.0594

Epoch: 1287
ogbn-arxiv,CAGNET-1D,1,1287,48.0502,0.0578

Epoch: 1288
ogbn-arxiv,CAGNET-1D,1,1288,48.0870,0.0710

Epoch: 1289
ogbn-arxiv,CAGNET-1D,1,1289,48.1234,0.0550

Epoch: 1290
ogbn-arxiv,CAGNET-1D,1,1290,48.1604,0.1126

Epoch: 1291
ogbn-arxiv,CAGNET-1D,1,1291,48.1974,0.0546

Epoch: 1292
ogbn-arxiv,CAGNET-1D,1,1292,48.2349,0.1136

Epoch: 1293
ogbn-arxiv,CAGNET-1D,1,1293,48.2733,0.0556

Epoch: 1294
ogbn-arxiv,CAGNET-1D,1,1294,48.3106,0.0709

Epoch: 1295
ogbn-arxiv,CAGNET-1D,1,1295,48.3486,0.0570

Epoch: 1296
ogbn-arxiv,CAGNET-1D,1,1296,48.3853,0.0593

Epoch: 1297
ogbn-arxiv,CAGNET-1D,1,1297,48.4224,0.0649

Epoch: 1298
ogbn-arxiv,CAGNET-1D,1,1298,48.4596,0.0555

Epoch: 1299
ogbn-arxiv,CAGNET-1D,1,1299,48.4967,0.1010

Epoch: 1300
ogbn-arxiv,CAGNET-1D,1,1300,48.5339,0.0554

Epoch: 1301
ogbn-arxiv,CAGNET-1D,1,1301,48.5713,0.0966

Epoch: 1302
ogbn-arxiv,CAGNET-1D,1,1302,48.6087,0.0558

Epoch: 1303
ogbn-arxiv,CAGNET-1D,1,1303,48.6456,0.0677

Epoch: 1304
ogbn-arxiv,CAGNET-1D,1,1304,48.6828,0.0571

Epoch: 1305
ogbn-arxiv,CAGNET-1D,1,1305,48.7201,0.0598

Epoch: 1306
ogbn-arxiv,CAGNET-1D,1,1306,48.7568,0.0618

Epoch: 1307
ogbn-arxiv,CAGNET-1D,1,1307,48.7936,0.0571

Epoch: 1308
ogbn-arxiv,CAGNET-1D,1,1308,48.8335,0.0649

Epoch: 1309
ogbn-arxiv,CAGNET-1D,1,1309,48.8711,0.0579

Epoch: 1310
ogbn-arxiv,CAGNET-1D,1,1310,48.9092,0.0682

Epoch: 1311
ogbn-arxiv,CAGNET-1D,1,1311,48.9461,0.0567

Epoch: 1312
ogbn-arxiv,CAGNET-1D,1,1312,48.9830,0.0642

Epoch: 1313
ogbn-arxiv,CAGNET-1D,1,1313,49.0208,0.0585

Epoch: 1314
ogbn-arxiv,CAGNET-1D,1,1314,49.0569,0.0586

Epoch: 1315
ogbn-arxiv,CAGNET-1D,1,1315,49.0940,0.0627

Epoch: 1316
ogbn-arxiv,CAGNET-1D,1,1316,49.1312,0.0568

Epoch: 1317
ogbn-arxiv,CAGNET-1D,1,1317,49.1684,0.0690

Epoch: 1318
ogbn-arxiv,CAGNET-1D,1,1318,49.2067,0.0566

Epoch: 1319
ogbn-arxiv,CAGNET-1D,1,1319,49.2439,0.0775

Epoch: 1320
ogbn-arxiv,CAGNET-1D,1,1320,49.2812,0.0561

Epoch: 1321
ogbn-arxiv,CAGNET-1D,1,1321,49.3178,0.0675

Epoch: 1322
ogbn-arxiv,CAGNET-1D,1,1322,49.3553,0.0575

Epoch: 1323
ogbn-arxiv,CAGNET-1D,1,1323,49.3923,0.0589

Epoch: 1324
ogbn-arxiv,CAGNET-1D,1,1324,49.4292,0.0606

Epoch: 1325
ogbn-arxiv,CAGNET-1D,1,1325,49.4657,0.0574

Epoch: 1326
ogbn-arxiv,CAGNET-1D,1,1326,49.5020,0.0630

Epoch: 1327
ogbn-arxiv,CAGNET-1D,1,1327,49.5386,0.0565

Epoch: 1328
ogbn-arxiv,CAGNET-1D,1,1328,49.5755,0.0695

Epoch: 1329
ogbn-arxiv,CAGNET-1D,1,1329,49.6128,0.0563

Epoch: 1330
ogbn-arxiv,CAGNET-1D,1,1330,49.6504,0.0674

Epoch: 1331
ogbn-arxiv,CAGNET-1D,1,1331,49.6879,0.0566

Epoch: 1332
ogbn-arxiv,CAGNET-1D,1,1332,49.7270,0.0623

Epoch: 1333
ogbn-arxiv,CAGNET-1D,1,1333,49.7635,0.0571

Epoch: 1334
ogbn-arxiv,CAGNET-1D,1,1334,49.8001,0.0612

Epoch: 1335
ogbn-arxiv,CAGNET-1D,1,1335,49.8371,0.0575

Epoch: 1336
ogbn-arxiv,CAGNET-1D,1,1336,49.8737,0.0601

Epoch: 1337
ogbn-arxiv,CAGNET-1D,1,1337,49.9102,0.0577

Epoch: 1338
ogbn-arxiv,CAGNET-1D,1,1338,49.9463,0.0591

Epoch: 1339
ogbn-arxiv,CAGNET-1D,1,1339,49.9831,0.0591

Epoch: 1340
ogbn-arxiv,CAGNET-1D,1,1340,50.0198,0.0583

Epoch: 1341
ogbn-arxiv,CAGNET-1D,1,1341,50.0566,0.0592

Epoch: 1342
ogbn-arxiv,CAGNET-1D,1,1342,50.0934,0.0582

Epoch: 1343
ogbn-arxiv,CAGNET-1D,1,1343,50.1299,0.0590

Epoch: 1344
ogbn-arxiv,CAGNET-1D,1,1344,50.1665,0.0580

Epoch: 1345
ogbn-arxiv,CAGNET-1D,1,1345,50.2031,0.0598

Epoch: 1346
ogbn-arxiv,CAGNET-1D,1,1346,50.2399,0.0574

Epoch: 1347
ogbn-arxiv,CAGNET-1D,1,1347,50.2765,0.0599

Epoch: 1348
ogbn-arxiv,CAGNET-1D,1,1348,50.3130,0.0575

Epoch: 1349
ogbn-arxiv,CAGNET-1D,1,1349,50.3494,0.0612

Epoch: 1350
ogbn-arxiv,CAGNET-1D,1,1350,50.3865,0.0566

Epoch: 1351
ogbn-arxiv,CAGNET-1D,1,1351,50.4239,0.0660

Epoch: 1352
ogbn-arxiv,CAGNET-1D,1,1352,50.4610,0.0556

Epoch: 1353
ogbn-arxiv,CAGNET-1D,1,1353,50.4980,0.0722

Epoch: 1354
ogbn-arxiv,CAGNET-1D,1,1354,50.5350,0.0557

Epoch: 1355
ogbn-arxiv,CAGNET-1D,1,1355,50.5718,0.0700

Epoch: 1356
ogbn-arxiv,CAGNET-1D,1,1356,50.6090,0.0559

Epoch: 1357
ogbn-arxiv,CAGNET-1D,1,1357,50.6468,0.0658

Epoch: 1358
ogbn-arxiv,CAGNET-1D,1,1358,50.6846,0.0562

Epoch: 1359
ogbn-arxiv,CAGNET-1D,1,1359,50.7222,0.0613

Epoch: 1360
ogbn-arxiv,CAGNET-1D,1,1360,50.7597,0.0571

Epoch: 1361
ogbn-arxiv,CAGNET-1D,1,1361,50.7971,0.0581

Epoch: 1362
ogbn-arxiv,CAGNET-1D,1,1362,50.8342,0.0605

Epoch: 1363
ogbn-arxiv,CAGNET-1D,1,1363,50.8721,0.0560

Epoch: 1364
ogbn-arxiv,CAGNET-1D,1,1364,50.9090,0.0673

Epoch: 1365
ogbn-arxiv,CAGNET-1D,1,1365,50.9461,0.0555

Epoch: 1366
ogbn-arxiv,CAGNET-1D,1,1366,50.9832,0.0745

Epoch: 1367
ogbn-arxiv,CAGNET-1D,1,1367,51.0202,0.0552

Epoch: 1368
ogbn-arxiv,CAGNET-1D,1,1368,51.0574,0.0894

Epoch: 1369
ogbn-arxiv,CAGNET-1D,1,1369,51.0946,0.0550

Epoch: 1370
ogbn-arxiv,CAGNET-1D,1,1370,51.1318,0.0967

Epoch: 1371
ogbn-arxiv,CAGNET-1D,1,1371,51.1690,0.0555

Epoch: 1372
ogbn-arxiv,CAGNET-1D,1,1372,51.2070,0.0828

Epoch: 1373
ogbn-arxiv,CAGNET-1D,1,1373,51.2448,0.0554

Epoch: 1374
ogbn-arxiv,CAGNET-1D,1,1374,51.2823,0.0676

Epoch: 1375
ogbn-arxiv,CAGNET-1D,1,1375,51.3199,0.0563

Epoch: 1376
ogbn-arxiv,CAGNET-1D,1,1376,51.3578,0.0598

Epoch: 1377
ogbn-arxiv,CAGNET-1D,1,1377,51.3953,0.0573

Epoch: 1378
ogbn-arxiv,CAGNET-1D,1,1378,51.4323,0.0573

Epoch: 1379
ogbn-arxiv,CAGNET-1D,1,1379,51.4691,0.0623

Epoch: 1380
ogbn-arxiv,CAGNET-1D,1,1380,51.5063,0.0558

Epoch: 1381
ogbn-arxiv,CAGNET-1D,1,1381,51.5442,0.0739

Epoch: 1382
ogbn-arxiv,CAGNET-1D,1,1382,51.5825,0.0555

Epoch: 1383
ogbn-arxiv,CAGNET-1D,1,1383,51.6206,0.0673

Epoch: 1384
ogbn-arxiv,CAGNET-1D,1,1384,51.6584,0.0564

Epoch: 1385
ogbn-arxiv,CAGNET-1D,1,1385,51.6957,0.0593

Epoch: 1386
ogbn-arxiv,CAGNET-1D,1,1386,51.7329,0.0580

Epoch: 1387
ogbn-arxiv,CAGNET-1D,1,1387,51.7700,0.0579

Epoch: 1388
ogbn-arxiv,CAGNET-1D,1,1388,51.8079,0.0586

Epoch: 1389
ogbn-arxiv,CAGNET-1D,1,1389,51.8452,0.0581

Epoch: 1390
ogbn-arxiv,CAGNET-1D,1,1390,51.8824,0.0583

Epoch: 1391
ogbn-arxiv,CAGNET-1D,1,1391,51.9199,0.0584

Epoch: 1392
ogbn-arxiv,CAGNET-1D,1,1392,51.9579,0.0581

Epoch: 1393
ogbn-arxiv,CAGNET-1D,1,1393,51.9959,0.0588

Epoch: 1394
ogbn-arxiv,CAGNET-1D,1,1394,52.0335,0.0577

Epoch: 1395
ogbn-arxiv,CAGNET-1D,1,1395,52.0711,0.0593

Epoch: 1396
ogbn-arxiv,CAGNET-1D,1,1396,52.1085,0.0568

Epoch: 1397
ogbn-arxiv,CAGNET-1D,1,1397,52.1455,0.0619

Epoch: 1398
ogbn-arxiv,CAGNET-1D,1,1398,52.1827,0.0560

Epoch: 1399
ogbn-arxiv,CAGNET-1D,1,1399,52.2201,0.0682

Epoch: 1400
ogbn-arxiv,CAGNET-1D,1,1400,52.2579,0.0561

Epoch: 1401
ogbn-arxiv,CAGNET-1D,1,1401,52.2958,0.0721

Epoch: 1402
ogbn-arxiv,CAGNET-1D,1,1402,52.3339,0.0557

Epoch: 1403
ogbn-arxiv,CAGNET-1D,1,1403,52.3717,0.0899

Epoch: 1404
ogbn-arxiv,CAGNET-1D,1,1404,52.4093,0.0554

Epoch: 1405
ogbn-arxiv,CAGNET-1D,1,1405,52.4467,0.1008

Epoch: 1406
ogbn-arxiv,CAGNET-1D,1,1406,52.4843,0.0556

Epoch: 1407
ogbn-arxiv,CAGNET-1D,1,1407,52.5214,0.0852

Epoch: 1408
ogbn-arxiv,CAGNET-1D,1,1408,52.5586,0.0556

Epoch: 1409
ogbn-arxiv,CAGNET-1D,1,1409,52.5960,0.0761

Epoch: 1410
ogbn-arxiv,CAGNET-1D,1,1410,52.6333,0.0554

Epoch: 1411
ogbn-arxiv,CAGNET-1D,1,1411,52.6706,0.0737

Epoch: 1412
ogbn-arxiv,CAGNET-1D,1,1412,52.7082,0.0556

Epoch: 1413
ogbn-arxiv,CAGNET-1D,1,1413,52.7461,0.0701

Epoch: 1414
ogbn-arxiv,CAGNET-1D,1,1414,52.7843,0.0559

Epoch: 1415
ogbn-arxiv,CAGNET-1D,1,1415,52.8223,0.0658

Epoch: 1416
ogbn-arxiv,CAGNET-1D,1,1416,52.8604,0.0558

Epoch: 1417
ogbn-arxiv,CAGNET-1D,1,1417,52.8984,0.0668

Epoch: 1418
ogbn-arxiv,CAGNET-1D,1,1418,52.9366,0.0560

Epoch: 1419
ogbn-arxiv,CAGNET-1D,1,1419,52.9745,0.0737

Epoch: 1420
ogbn-arxiv,CAGNET-1D,1,1420,53.0126,0.0557

Epoch: 1421
ogbn-arxiv,CAGNET-1D,1,1421,53.0501,0.0762

Epoch: 1422
ogbn-arxiv,CAGNET-1D,1,1422,53.0877,0.0556

Epoch: 1423
ogbn-arxiv,CAGNET-1D,1,1423,53.1255,0.0760

Epoch: 1424
ogbn-arxiv,CAGNET-1D,1,1424,53.1638,0.0556

Epoch: 1425
ogbn-arxiv,CAGNET-1D,1,1425,53.2019,0.0737

Epoch: 1426
ogbn-arxiv,CAGNET-1D,1,1426,53.2398,0.0557

Epoch: 1427
ogbn-arxiv,CAGNET-1D,1,1427,53.2776,0.0667

Epoch: 1428
ogbn-arxiv,CAGNET-1D,1,1428,53.3151,0.0559

Epoch: 1429
ogbn-arxiv,CAGNET-1D,1,1429,53.3537,0.0590

Epoch: 1430
ogbn-arxiv,CAGNET-1D,1,1430,53.3917,0.0576

Epoch: 1431
ogbn-arxiv,CAGNET-1D,1,1431,53.4301,0.0564

Epoch: 1432
ogbn-arxiv,CAGNET-1D,1,1432,53.4682,0.0630

Epoch: 1433
ogbn-arxiv,CAGNET-1D,1,1433,53.5059,0.0554

Epoch: 1434
ogbn-arxiv,CAGNET-1D,1,1434,53.5436,0.0740

Epoch: 1435
ogbn-arxiv,CAGNET-1D,1,1435,53.5810,0.0556

Epoch: 1436
ogbn-arxiv,CAGNET-1D,1,1436,53.6182,0.0894

Epoch: 1437
ogbn-arxiv,CAGNET-1D,1,1437,53.6558,0.0555

Epoch: 1438
ogbn-arxiv,CAGNET-1D,1,1438,53.6936,0.0939

Epoch: 1439
ogbn-arxiv,CAGNET-1D,1,1439,53.7310,0.0554

Epoch: 1440
ogbn-arxiv,CAGNET-1D,1,1440,53.7681,0.0698

Epoch: 1441
ogbn-arxiv,CAGNET-1D,1,1441,53.8055,0.0561

Epoch: 1442
ogbn-arxiv,CAGNET-1D,1,1442,53.8428,0.0614

Epoch: 1443
ogbn-arxiv,CAGNET-1D,1,1443,53.8801,0.0562

Epoch: 1444
ogbn-arxiv,CAGNET-1D,1,1444,53.9174,0.0609

Epoch: 1445
ogbn-arxiv,CAGNET-1D,1,1445,53.9547,0.0559

Epoch: 1446
ogbn-arxiv,CAGNET-1D,1,1446,53.9922,0.0588

Epoch: 1447
ogbn-arxiv,CAGNET-1D,1,1447,54.0293,0.0567

Epoch: 1448
ogbn-arxiv,CAGNET-1D,1,1448,54.0661,0.0583

Epoch: 1449
ogbn-arxiv,CAGNET-1D,1,1449,54.1030,0.0562

Epoch: 1450
ogbn-arxiv,CAGNET-1D,1,1450,54.1403,0.0610

Epoch: 1451
ogbn-arxiv,CAGNET-1D,1,1451,54.1775,0.0556

Epoch: 1452
ogbn-arxiv,CAGNET-1D,1,1452,54.2145,0.0681

Epoch: 1453
ogbn-arxiv,CAGNET-1D,1,1453,54.2515,0.0556

Epoch: 1454
ogbn-arxiv,CAGNET-1D,1,1454,54.2919,0.0722

Epoch: 1455
ogbn-arxiv,CAGNET-1D,1,1455,54.3294,0.0556

Epoch: 1456
ogbn-arxiv,CAGNET-1D,1,1456,54.3667,0.0697

Epoch: 1457
ogbn-arxiv,CAGNET-1D,1,1457,54.4042,0.0554

Epoch: 1458
ogbn-arxiv,CAGNET-1D,1,1458,54.4421,0.0767

Epoch: 1459
ogbn-arxiv,CAGNET-1D,1,1459,54.4796,0.0555

Epoch: 1460
ogbn-arxiv,CAGNET-1D,1,1460,54.5169,0.0744

Epoch: 1461
ogbn-arxiv,CAGNET-1D,1,1461,54.5544,0.0556

Epoch: 1462
ogbn-arxiv,CAGNET-1D,1,1462,54.5920,0.0620

Epoch: 1463
ogbn-arxiv,CAGNET-1D,1,1463,54.6295,0.0559

Epoch: 1464
ogbn-arxiv,CAGNET-1D,1,1464,54.6672,0.0593

Epoch: 1465
ogbn-arxiv,CAGNET-1D,1,1465,54.7047,0.0557

Epoch: 1466
ogbn-arxiv,CAGNET-1D,1,1466,54.7419,0.0591

Epoch: 1467
ogbn-arxiv,CAGNET-1D,1,1467,54.7790,0.0557

Epoch: 1468
ogbn-arxiv,CAGNET-1D,1,1468,54.8161,0.0588

Epoch: 1469
ogbn-arxiv,CAGNET-1D,1,1469,54.8537,0.0559

Epoch: 1470
ogbn-arxiv,CAGNET-1D,1,1470,54.8915,0.0574

Epoch: 1471
ogbn-arxiv,CAGNET-1D,1,1471,54.9297,0.0566

Epoch: 1472
ogbn-arxiv,CAGNET-1D,1,1472,54.9677,0.0566

Epoch: 1473
ogbn-arxiv,CAGNET-1D,1,1473,55.0053,0.0571

Epoch: 1474
ogbn-arxiv,CAGNET-1D,1,1474,55.0430,0.0570

Epoch: 1475
ogbn-arxiv,CAGNET-1D,1,1475,55.0811,0.0570

Epoch: 1476
ogbn-arxiv,CAGNET-1D,1,1476,55.1190,0.0575

Epoch: 1477
ogbn-arxiv,CAGNET-1D,1,1477,55.1564,0.0560

Epoch: 1478
ogbn-arxiv,CAGNET-1D,1,1478,55.1943,0.0586

Epoch: 1479
ogbn-arxiv,CAGNET-1D,1,1479,55.2315,0.0557

Epoch: 1480
ogbn-arxiv,CAGNET-1D,1,1480,55.2686,0.0582

Epoch: 1481
ogbn-arxiv,CAGNET-1D,1,1481,55.3058,0.0562

Epoch: 1482
ogbn-arxiv,CAGNET-1D,1,1482,55.3429,0.0564

Epoch: 1483
ogbn-arxiv,CAGNET-1D,1,1483,55.3802,0.0580

Epoch: 1484
ogbn-arxiv,CAGNET-1D,1,1484,55.4179,0.0560

Epoch: 1485
ogbn-arxiv,CAGNET-1D,1,1485,55.4558,0.0588

Epoch: 1486
ogbn-arxiv,CAGNET-1D,1,1486,55.4950,0.0554

Epoch: 1487
ogbn-arxiv,CAGNET-1D,1,1487,55.5330,0.0619

Epoch: 1488
ogbn-arxiv,CAGNET-1D,1,1488,55.5711,0.0545

Epoch: 1489
ogbn-arxiv,CAGNET-1D,1,1489,55.6087,0.0695

Epoch: 1490
ogbn-arxiv,CAGNET-1D,1,1490,55.6459,0.0544

Epoch: 1491
ogbn-arxiv,CAGNET-1D,1,1491,55.6831,0.0994

Epoch: 1492
ogbn-arxiv,CAGNET-1D,1,1492,55.7202,0.0546

Epoch: 1493
ogbn-arxiv,CAGNET-1D,1,1493,55.7571,0.0943

Epoch: 1494
ogbn-arxiv,CAGNET-1D,1,1494,55.7945,0.0543

Epoch: 1495
ogbn-arxiv,CAGNET-1D,1,1495,55.8322,0.0631

Epoch: 1496
ogbn-arxiv,CAGNET-1D,1,1496,55.8700,0.0549

Epoch: 1497
ogbn-arxiv,CAGNET-1D,1,1497,55.9081,0.0571

Epoch: 1498
ogbn-arxiv,CAGNET-1D,1,1498,55.9459,0.0568

Epoch: 1499
ogbn-arxiv,CAGNET-1D,1,1499,55.9835,0.0550

Epoch: 1500
ogbn-arxiv,CAGNET-1D,1,1500,56.0209,0.0640

Epoch: 1501
ogbn-arxiv,CAGNET-1D,1,1501,56.0584,0.0539

Epoch: 1502
ogbn-arxiv,CAGNET-1D,1,1502,56.0974,0.1225

Epoch: 1503
ogbn-arxiv,CAGNET-1D,1,1503,56.1350,0.0539

Epoch: 1504
ogbn-arxiv,CAGNET-1D,1,1504,56.1719,0.1051

Epoch: 1505
ogbn-arxiv,CAGNET-1D,1,1505,56.2090,0.0543

Epoch: 1506
ogbn-arxiv,CAGNET-1D,1,1506,56.2463,0.0592

Epoch: 1507
ogbn-arxiv,CAGNET-1D,1,1507,56.2839,0.0553

Epoch: 1508
ogbn-arxiv,CAGNET-1D,1,1508,56.3218,0.0563

Epoch: 1509
ogbn-arxiv,CAGNET-1D,1,1509,56.3596,0.0564

Epoch: 1510
ogbn-arxiv,CAGNET-1D,1,1510,56.3973,0.0551

Epoch: 1511
ogbn-arxiv,CAGNET-1D,1,1511,56.4348,0.0620

Epoch: 1512
ogbn-arxiv,CAGNET-1D,1,1512,56.4720,0.0546

Epoch: 1513
ogbn-arxiv,CAGNET-1D,1,1513,56.5098,0.0715

Epoch: 1514
ogbn-arxiv,CAGNET-1D,1,1514,56.5476,0.0542

Epoch: 1515
ogbn-arxiv,CAGNET-1D,1,1515,56.5849,0.0667

Epoch: 1516
ogbn-arxiv,CAGNET-1D,1,1516,56.6222,0.0539

Epoch: 1517
ogbn-arxiv,CAGNET-1D,1,1517,56.6595,0.0617

Epoch: 1518
ogbn-arxiv,CAGNET-1D,1,1518,56.6973,0.0543

Epoch: 1519
ogbn-arxiv,CAGNET-1D,1,1519,56.7352,0.0594

Epoch: 1520
ogbn-arxiv,CAGNET-1D,1,1520,56.7734,0.0548

Epoch: 1521
ogbn-arxiv,CAGNET-1D,1,1521,56.8116,0.0577

Epoch: 1522
ogbn-arxiv,CAGNET-1D,1,1522,56.8498,0.0555

Epoch: 1523
ogbn-arxiv,CAGNET-1D,1,1523,56.8860,0.0560

Epoch: 1524
ogbn-arxiv,CAGNET-1D,1,1524,56.9220,0.0557

Epoch: 1525
ogbn-arxiv,CAGNET-1D,1,1525,56.9585,0.0556

Epoch: 1526
ogbn-arxiv,CAGNET-1D,1,1526,56.9955,0.0564

Epoch: 1527
ogbn-arxiv,CAGNET-1D,1,1527,57.0349,0.0552

Epoch: 1528
ogbn-arxiv,CAGNET-1D,1,1528,57.0718,0.0557

Epoch: 1529
ogbn-arxiv,CAGNET-1D,1,1529,57.1084,0.0561

Epoch: 1530
ogbn-arxiv,CAGNET-1D,1,1530,57.1452,0.0549

Epoch: 1531
ogbn-arxiv,CAGNET-1D,1,1531,57.1825,0.0590

Epoch: 1532
ogbn-arxiv,CAGNET-1D,1,1532,57.2197,0.0539

Epoch: 1533
ogbn-arxiv,CAGNET-1D,1,1533,57.2570,0.0645

Epoch: 1534
ogbn-arxiv,CAGNET-1D,1,1534,57.2943,0.0538

Epoch: 1535
ogbn-arxiv,CAGNET-1D,1,1535,57.3316,0.0667

Epoch: 1536
ogbn-arxiv,CAGNET-1D,1,1536,57.3693,0.0539

Epoch: 1537
ogbn-arxiv,CAGNET-1D,1,1537,57.4063,0.0717

Epoch: 1538
ogbn-arxiv,CAGNET-1D,1,1538,57.4434,0.0539

Epoch: 1539
ogbn-arxiv,CAGNET-1D,1,1539,57.4808,0.0839

Epoch: 1540
ogbn-arxiv,CAGNET-1D,1,1540,57.5183,0.0533

Epoch: 1541
ogbn-arxiv,CAGNET-1D,1,1541,57.5553,0.0849

Epoch: 1542
ogbn-arxiv,CAGNET-1D,1,1542,57.5921,0.0535

Epoch: 1543
ogbn-arxiv,CAGNET-1D,1,1543,57.6295,0.0704

Epoch: 1544
ogbn-arxiv,CAGNET-1D,1,1544,57.6671,0.0537

Epoch: 1545
ogbn-arxiv,CAGNET-1D,1,1545,57.7045,0.0597

Epoch: 1546
ogbn-arxiv,CAGNET-1D,1,1546,57.7415,0.0546

Epoch: 1547
ogbn-arxiv,CAGNET-1D,1,1547,57.7786,0.0553

Epoch: 1548
ogbn-arxiv,CAGNET-1D,1,1548,57.8157,0.0590

Epoch: 1549
ogbn-arxiv,CAGNET-1D,1,1549,57.8530,0.0537

Epoch: 1550
ogbn-arxiv,CAGNET-1D,1,1550,57.8904,0.0668

Epoch: 1551
ogbn-arxiv,CAGNET-1D,1,1551,57.9319,0.0533

Epoch: 1552
ogbn-arxiv,CAGNET-1D,1,1552,57.9687,0.0781

Epoch: 1553
ogbn-arxiv,CAGNET-1D,1,1553,58.0056,0.0541

Epoch: 1554
ogbn-arxiv,CAGNET-1D,1,1554,58.0428,0.0822

Epoch: 1555
ogbn-arxiv,CAGNET-1D,1,1555,58.0801,0.0541

Epoch: 1556
ogbn-arxiv,CAGNET-1D,1,1556,58.1181,0.0746

Epoch: 1557
ogbn-arxiv,CAGNET-1D,1,1557,58.1551,0.0540

Epoch: 1558
ogbn-arxiv,CAGNET-1D,1,1558,58.1918,0.0678

Epoch: 1559
ogbn-arxiv,CAGNET-1D,1,1559,58.2288,0.0543

Epoch: 1560
ogbn-arxiv,CAGNET-1D,1,1560,58.2655,0.0616

Epoch: 1561
ogbn-arxiv,CAGNET-1D,1,1561,58.3030,0.0543

Epoch: 1562
ogbn-arxiv,CAGNET-1D,1,1562,58.3401,0.0615

Epoch: 1563
ogbn-arxiv,CAGNET-1D,1,1563,58.3773,0.0545

Epoch: 1564
ogbn-arxiv,CAGNET-1D,1,1564,58.4151,0.0649

Epoch: 1565
ogbn-arxiv,CAGNET-1D,1,1565,58.4525,0.0541

Epoch: 1566
ogbn-arxiv,CAGNET-1D,1,1566,58.4904,0.0759

Epoch: 1567
ogbn-arxiv,CAGNET-1D,1,1567,58.5274,0.0539

Epoch: 1568
ogbn-arxiv,CAGNET-1D,1,1568,58.5646,0.0814

Epoch: 1569
ogbn-arxiv,CAGNET-1D,1,1569,58.6012,0.0542

Epoch: 1570
ogbn-arxiv,CAGNET-1D,1,1570,58.6385,0.0702

Epoch: 1571
ogbn-arxiv,CAGNET-1D,1,1571,58.6766,0.0541

Epoch: 1572
ogbn-arxiv,CAGNET-1D,1,1572,58.7135,0.0623

Epoch: 1573
ogbn-arxiv,CAGNET-1D,1,1573,58.7500,0.0544

Epoch: 1574
ogbn-arxiv,CAGNET-1D,1,1574,58.7868,0.0565

Epoch: 1575
ogbn-arxiv,CAGNET-1D,1,1575,58.8247,0.0579

Epoch: 1576
ogbn-arxiv,CAGNET-1D,1,1576,58.8630,0.0550

Epoch: 1577
ogbn-arxiv,CAGNET-1D,1,1577,58.8998,0.0623

Epoch: 1578
ogbn-arxiv,CAGNET-1D,1,1578,58.9368,0.0546

Epoch: 1579
ogbn-arxiv,CAGNET-1D,1,1579,58.9735,0.0666

Epoch: 1580
ogbn-arxiv,CAGNET-1D,1,1580,59.0102,0.0544

Epoch: 1581
ogbn-arxiv,CAGNET-1D,1,1581,59.0474,0.0703

Epoch: 1582
ogbn-arxiv,CAGNET-1D,1,1582,59.0846,0.0538

Epoch: 1583
ogbn-arxiv,CAGNET-1D,1,1583,59.1226,0.0730

Epoch: 1584
ogbn-arxiv,CAGNET-1D,1,1584,59.1599,0.0537

Epoch: 1585
ogbn-arxiv,CAGNET-1D,1,1585,59.1973,0.0765

Epoch: 1586
ogbn-arxiv,CAGNET-1D,1,1586,59.2342,0.0538

Epoch: 1587
ogbn-arxiv,CAGNET-1D,1,1587,59.2713,0.0765

Epoch: 1588
ogbn-arxiv,CAGNET-1D,1,1588,59.3085,0.0539

Epoch: 1589
ogbn-arxiv,CAGNET-1D,1,1589,59.3454,0.0813

Epoch: 1590
ogbn-arxiv,CAGNET-1D,1,1590,59.3820,0.0539

Epoch: 1591
ogbn-arxiv,CAGNET-1D,1,1591,59.4189,0.0851

Epoch: 1592
ogbn-arxiv,CAGNET-1D,1,1592,59.4562,0.0541

Epoch: 1593
ogbn-arxiv,CAGNET-1D,1,1593,59.4940,0.0766

Epoch: 1594
ogbn-arxiv,CAGNET-1D,1,1594,59.5312,0.0543

Epoch: 1595
ogbn-arxiv,CAGNET-1D,1,1595,59.5687,0.0687

Epoch: 1596
ogbn-arxiv,CAGNET-1D,1,1596,59.6063,0.0549

Epoch: 1597
ogbn-arxiv,CAGNET-1D,1,1597,59.6436,0.0626

Epoch: 1598
ogbn-arxiv,CAGNET-1D,1,1598,59.6808,0.0554

Epoch: 1599
ogbn-arxiv,CAGNET-1D,1,1599,59.7179,0.0575

Epoch: 1600
ogbn-arxiv,CAGNET-1D,1,1600,59.7557,0.0600

Epoch: 1601
ogbn-arxiv,CAGNET-1D,1,1601,59.7929,0.0549

Epoch: 1602
ogbn-arxiv,CAGNET-1D,1,1602,59.8301,0.0628

Epoch: 1603
ogbn-arxiv,CAGNET-1D,1,1603,59.8673,0.0554

Epoch: 1604
ogbn-arxiv,CAGNET-1D,1,1604,59.9042,0.0610

Epoch: 1605
ogbn-arxiv,CAGNET-1D,1,1605,59.9415,0.0563

Epoch: 1606
ogbn-arxiv,CAGNET-1D,1,1606,59.9781,0.0594

Epoch: 1607
ogbn-arxiv,CAGNET-1D,1,1607,60.0158,0.0573

Epoch: 1608
ogbn-arxiv,CAGNET-1D,1,1608,60.0530,0.0581

Epoch: 1609
ogbn-arxiv,CAGNET-1D,1,1609,60.0907,0.0579

Epoch: 1610
ogbn-arxiv,CAGNET-1D,1,1610,60.1280,0.0578

Epoch: 1611
ogbn-arxiv,CAGNET-1D,1,1611,60.1656,0.0588

Epoch: 1612
ogbn-arxiv,CAGNET-1D,1,1612,60.2027,0.0567

Epoch: 1613
ogbn-arxiv,CAGNET-1D,1,1613,60.2402,0.0618

Epoch: 1614
ogbn-arxiv,CAGNET-1D,1,1614,60.2772,0.0552

Epoch: 1615
ogbn-arxiv,CAGNET-1D,1,1615,60.3147,0.0631

Epoch: 1616
ogbn-arxiv,CAGNET-1D,1,1616,60.3519,0.0557

Epoch: 1617
ogbn-arxiv,CAGNET-1D,1,1617,60.3887,0.0625

Epoch: 1618
ogbn-arxiv,CAGNET-1D,1,1618,60.4256,0.0574

Epoch: 1619
ogbn-arxiv,CAGNET-1D,1,1619,60.4627,0.0579

Epoch: 1620
ogbn-arxiv,CAGNET-1D,1,1620,60.5009,0.0613

Epoch: 1621
ogbn-arxiv,CAGNET-1D,1,1621,60.5387,0.0550

Epoch: 1622
ogbn-arxiv,CAGNET-1D,1,1622,60.5762,0.0799

Epoch: 1623
ogbn-arxiv,CAGNET-1D,1,1623,60.6136,0.0541

Epoch: 1624
ogbn-arxiv,CAGNET-1D,1,1624,60.6529,0.1255

Epoch: 1625
ogbn-arxiv,CAGNET-1D,1,1625,60.6898,0.0537

Epoch: 1626
ogbn-arxiv,CAGNET-1D,1,1626,60.7263,0.1046

Epoch: 1627
ogbn-arxiv,CAGNET-1D,1,1627,60.7631,0.0544

Epoch: 1628
ogbn-arxiv,CAGNET-1D,1,1628,60.7998,0.0651

Epoch: 1629
ogbn-arxiv,CAGNET-1D,1,1629,60.8364,0.0581

Epoch: 1630
ogbn-arxiv,CAGNET-1D,1,1630,60.8733,0.0572

Epoch: 1631
ogbn-arxiv,CAGNET-1D,1,1631,60.9118,0.0649

Epoch: 1632
ogbn-arxiv,CAGNET-1D,1,1632,60.9490,0.0552

Epoch: 1633
ogbn-arxiv,CAGNET-1D,1,1633,60.9860,0.0742

Epoch: 1634
ogbn-arxiv,CAGNET-1D,1,1634,61.0240,0.0543

Epoch: 1635
ogbn-arxiv,CAGNET-1D,1,1635,61.0609,0.0790

Epoch: 1636
ogbn-arxiv,CAGNET-1D,1,1636,61.0981,0.0542

Epoch: 1637
ogbn-arxiv,CAGNET-1D,1,1637,61.1353,0.0914

Epoch: 1638
ogbn-arxiv,CAGNET-1D,1,1638,61.1727,0.0539

Epoch: 1639
ogbn-arxiv,CAGNET-1D,1,1639,61.2102,0.0937

Epoch: 1640
ogbn-arxiv,CAGNET-1D,1,1640,61.2471,0.0540

Epoch: 1641
ogbn-arxiv,CAGNET-1D,1,1641,61.2845,0.0802

Epoch: 1642
ogbn-arxiv,CAGNET-1D,1,1642,61.3210,0.0550

Epoch: 1643
ogbn-arxiv,CAGNET-1D,1,1643,61.3577,0.0652

Epoch: 1644
ogbn-arxiv,CAGNET-1D,1,1644,61.3943,0.0592

Epoch: 1645
ogbn-arxiv,CAGNET-1D,1,1645,61.4314,0.0573

Epoch: 1646
ogbn-arxiv,CAGNET-1D,1,1646,61.4679,0.0675

Epoch: 1647
ogbn-arxiv,CAGNET-1D,1,1647,61.5051,0.0556

Epoch: 1648
ogbn-arxiv,CAGNET-1D,1,1648,61.5418,0.0685

Epoch: 1649
ogbn-arxiv,CAGNET-1D,1,1649,61.5795,0.0561

Epoch: 1650
ogbn-arxiv,CAGNET-1D,1,1650,61.6165,0.0686

Epoch: 1651
ogbn-arxiv,CAGNET-1D,1,1651,61.6540,0.0554

Epoch: 1652
ogbn-arxiv,CAGNET-1D,1,1652,61.6912,0.0766

Epoch: 1653
ogbn-arxiv,CAGNET-1D,1,1653,61.7289,0.0541

Epoch: 1654
ogbn-arxiv,CAGNET-1D,1,1654,61.7662,0.0808

Epoch: 1655
ogbn-arxiv,CAGNET-1D,1,1655,61.8036,0.0540

Epoch: 1656
ogbn-arxiv,CAGNET-1D,1,1656,61.8406,0.0747

Epoch: 1657
ogbn-arxiv,CAGNET-1D,1,1657,61.8783,0.0553

Epoch: 1658
ogbn-arxiv,CAGNET-1D,1,1658,61.9155,0.0660

Epoch: 1659
ogbn-arxiv,CAGNET-1D,1,1659,61.9531,0.0563

Epoch: 1660
ogbn-arxiv,CAGNET-1D,1,1660,61.9899,0.0683

Epoch: 1661
ogbn-arxiv,CAGNET-1D,1,1661,62.0269,0.0543

Epoch: 1662
ogbn-arxiv,CAGNET-1D,1,1662,62.0637,0.0880

Epoch: 1663
ogbn-arxiv,CAGNET-1D,1,1663,62.1006,0.0529

Epoch: 1664
ogbn-arxiv,CAGNET-1D,1,1664,62.1376,0.1008

Epoch: 1665
ogbn-arxiv,CAGNET-1D,1,1665,62.1756,0.0530

Epoch: 1666
ogbn-arxiv,CAGNET-1D,1,1666,62.2127,0.0977

Epoch: 1667
ogbn-arxiv,CAGNET-1D,1,1667,62.2502,0.0537

Epoch: 1668
ogbn-arxiv,CAGNET-1D,1,1668,62.2874,0.0741

Epoch: 1669
ogbn-arxiv,CAGNET-1D,1,1669,62.3247,0.0564

Epoch: 1670
ogbn-arxiv,CAGNET-1D,1,1670,62.3614,0.0594

Epoch: 1671
ogbn-arxiv,CAGNET-1D,1,1671,62.3982,0.0628

Epoch: 1672
ogbn-arxiv,CAGNET-1D,1,1672,62.4354,0.0556

Epoch: 1673
ogbn-arxiv,CAGNET-1D,1,1673,62.4756,0.0716

Epoch: 1674
ogbn-arxiv,CAGNET-1D,1,1674,62.5128,0.0550

Epoch: 1675
ogbn-arxiv,CAGNET-1D,1,1675,62.5502,0.0707

Epoch: 1676
ogbn-arxiv,CAGNET-1D,1,1676,62.5869,0.0554

Epoch: 1677
ogbn-arxiv,CAGNET-1D,1,1677,62.6239,0.0706

Epoch: 1678
ogbn-arxiv,CAGNET-1D,1,1678,62.6610,0.0548

Epoch: 1679
ogbn-arxiv,CAGNET-1D,1,1679,62.6974,0.0815

Epoch: 1680
ogbn-arxiv,CAGNET-1D,1,1680,62.7341,0.0533

Epoch: 1681
ogbn-arxiv,CAGNET-1D,1,1681,62.7704,0.0885

Epoch: 1682
ogbn-arxiv,CAGNET-1D,1,1682,62.8071,0.0538

Epoch: 1683
ogbn-arxiv,CAGNET-1D,1,1683,62.8447,0.0783

Epoch: 1684
ogbn-arxiv,CAGNET-1D,1,1684,62.8821,0.0544

Epoch: 1685
ogbn-arxiv,CAGNET-1D,1,1685,62.9200,0.0716

Epoch: 1686
ogbn-arxiv,CAGNET-1D,1,1686,62.9569,0.0560

Epoch: 1687
ogbn-arxiv,CAGNET-1D,1,1687,62.9939,0.0652

Epoch: 1688
ogbn-arxiv,CAGNET-1D,1,1688,63.0305,0.0568

Epoch: 1689
ogbn-arxiv,CAGNET-1D,1,1689,63.0671,0.0644

Epoch: 1690
ogbn-arxiv,CAGNET-1D,1,1690,63.1038,0.0565

Epoch: 1691
ogbn-arxiv,CAGNET-1D,1,1691,63.1411,0.0659

Epoch: 1692
ogbn-arxiv,CAGNET-1D,1,1692,63.1780,0.0556

Epoch: 1693
ogbn-arxiv,CAGNET-1D,1,1693,63.2147,0.0676

Epoch: 1694
ogbn-arxiv,CAGNET-1D,1,1694,63.2513,0.0560

Epoch: 1695
ogbn-arxiv,CAGNET-1D,1,1695,63.2878,0.0674

Epoch: 1696
ogbn-arxiv,CAGNET-1D,1,1696,63.3244,0.0561

Epoch: 1697
ogbn-arxiv,CAGNET-1D,1,1697,63.3609,0.0663

Epoch: 1698
ogbn-arxiv,CAGNET-1D,1,1698,63.3978,0.0565

Epoch: 1699
ogbn-arxiv,CAGNET-1D,1,1699,63.4340,0.0634

Epoch: 1700
ogbn-arxiv,CAGNET-1D,1,1700,63.4703,0.0572

Epoch: 1701
ogbn-arxiv,CAGNET-1D,1,1701,63.5068,0.0663

Epoch: 1702
ogbn-arxiv,CAGNET-1D,1,1702,63.5433,0.0554

Epoch: 1703
ogbn-arxiv,CAGNET-1D,1,1703,63.5799,0.0866

Epoch: 1704
ogbn-arxiv,CAGNET-1D,1,1704,63.6160,0.0539

Epoch: 1705
ogbn-arxiv,CAGNET-1D,1,1705,63.6524,0.1042

Epoch: 1706
ogbn-arxiv,CAGNET-1D,1,1706,63.6897,0.0542

Epoch: 1707
ogbn-arxiv,CAGNET-1D,1,1707,63.7271,0.0832

Epoch: 1708
ogbn-arxiv,CAGNET-1D,1,1708,63.7640,0.0559

Epoch: 1709
ogbn-arxiv,CAGNET-1D,1,1709,63.8014,0.0709

Epoch: 1710
ogbn-arxiv,CAGNET-1D,1,1710,63.8388,0.0557

Epoch: 1711
ogbn-arxiv,CAGNET-1D,1,1711,63.8763,0.0701

Epoch: 1712
ogbn-arxiv,CAGNET-1D,1,1712,63.9132,0.0557

Epoch: 1713
ogbn-arxiv,CAGNET-1D,1,1713,63.9504,0.0674

Epoch: 1714
ogbn-arxiv,CAGNET-1D,1,1714,63.9872,0.0561

Epoch: 1715
ogbn-arxiv,CAGNET-1D,1,1715,64.0246,0.0621

Epoch: 1716
ogbn-arxiv,CAGNET-1D,1,1716,64.0617,0.0585

Epoch: 1717
ogbn-arxiv,CAGNET-1D,1,1717,64.0993,0.0568

Epoch: 1718
ogbn-arxiv,CAGNET-1D,1,1718,64.1369,0.0686

Epoch: 1719
ogbn-arxiv,CAGNET-1D,1,1719,64.1745,0.0543

Epoch: 1720
ogbn-arxiv,CAGNET-1D,1,1720,64.2118,0.1133

Epoch: 1721
ogbn-arxiv,CAGNET-1D,1,1721,64.2489,0.0530

Epoch: 1722
ogbn-arxiv,CAGNET-1D,1,1722,64.2881,0.1605

Epoch: 1723
ogbn-arxiv,CAGNET-1D,1,1723,64.3246,0.0530

Epoch: 1724
ogbn-arxiv,CAGNET-1D,1,1724,64.3606,0.1045

Epoch: 1725
ogbn-arxiv,CAGNET-1D,1,1725,64.3975,0.0545

Epoch: 1726
ogbn-arxiv,CAGNET-1D,1,1726,64.4343,0.0600

Epoch: 1727
ogbn-arxiv,CAGNET-1D,1,1727,64.4713,0.0644

Epoch: 1728
ogbn-arxiv,CAGNET-1D,1,1728,64.5088,0.0540

Epoch: 1729
ogbn-arxiv,CAGNET-1D,1,1729,64.5459,0.1021

Epoch: 1730
ogbn-arxiv,CAGNET-1D,1,1730,64.5832,0.0532

Epoch: 1731
ogbn-arxiv,CAGNET-1D,1,1731,64.6207,0.1327

Epoch: 1732
ogbn-arxiv,CAGNET-1D,1,1732,64.6572,0.0530

Epoch: 1733
ogbn-arxiv,CAGNET-1D,1,1733,64.6943,0.1066

Epoch: 1734
ogbn-arxiv,CAGNET-1D,1,1734,64.7316,0.0537

Epoch: 1735
ogbn-arxiv,CAGNET-1D,1,1735,64.7689,0.0744

Epoch: 1736
ogbn-arxiv,CAGNET-1D,1,1736,64.8061,0.0554

Epoch: 1737
ogbn-arxiv,CAGNET-1D,1,1737,64.8440,0.0615

Epoch: 1738
ogbn-arxiv,CAGNET-1D,1,1738,64.8807,0.0574

Epoch: 1739
ogbn-arxiv,CAGNET-1D,1,1739,64.9178,0.0583

Epoch: 1740
ogbn-arxiv,CAGNET-1D,1,1740,64.9543,0.0602

Epoch: 1741
ogbn-arxiv,CAGNET-1D,1,1741,64.9909,0.0558

Epoch: 1742
ogbn-arxiv,CAGNET-1D,1,1742,65.0277,0.0677

Epoch: 1743
ogbn-arxiv,CAGNET-1D,1,1743,65.0653,0.0541

Epoch: 1744
ogbn-arxiv,CAGNET-1D,1,1744,65.1025,0.0843

Epoch: 1745
ogbn-arxiv,CAGNET-1D,1,1745,65.1401,0.0536

Epoch: 1746
ogbn-arxiv,CAGNET-1D,1,1746,65.1795,0.0879

Epoch: 1747
ogbn-arxiv,CAGNET-1D,1,1747,65.2166,0.0538

Epoch: 1748
ogbn-arxiv,CAGNET-1D,1,1748,65.2539,0.0861

Epoch: 1749
ogbn-arxiv,CAGNET-1D,1,1749,65.2913,0.0536

Epoch: 1750
ogbn-arxiv,CAGNET-1D,1,1750,65.3280,0.0766

Epoch: 1751
ogbn-arxiv,CAGNET-1D,1,1751,65.3649,0.0541

Epoch: 1752
ogbn-arxiv,CAGNET-1D,1,1752,65.4018,0.0642

Epoch: 1753
ogbn-arxiv,CAGNET-1D,1,1753,65.4385,0.0559

Epoch: 1754
ogbn-arxiv,CAGNET-1D,1,1754,65.4757,0.0591

Epoch: 1755
ogbn-arxiv,CAGNET-1D,1,1755,65.5134,0.0586

Epoch: 1756
ogbn-arxiv,CAGNET-1D,1,1756,65.5510,0.0567

Epoch: 1757
ogbn-arxiv,CAGNET-1D,1,1757,65.5887,0.0611

Epoch: 1758
ogbn-arxiv,CAGNET-1D,1,1758,65.6268,0.0554

Epoch: 1759
ogbn-arxiv,CAGNET-1D,1,1759,65.6634,0.0634

Epoch: 1760
ogbn-arxiv,CAGNET-1D,1,1760,65.7005,0.0551

Epoch: 1761
ogbn-arxiv,CAGNET-1D,1,1761,65.7372,0.0632

Epoch: 1762
ogbn-arxiv,CAGNET-1D,1,1762,65.7747,0.0548

Epoch: 1763
ogbn-arxiv,CAGNET-1D,1,1763,65.8118,0.0625

Epoch: 1764
ogbn-arxiv,CAGNET-1D,1,1764,65.8489,0.0550

Epoch: 1765
ogbn-arxiv,CAGNET-1D,1,1765,65.8860,0.0604

Epoch: 1766
ogbn-arxiv,CAGNET-1D,1,1766,65.9237,0.0562

Epoch: 1767
ogbn-arxiv,CAGNET-1D,1,1767,65.9612,0.0611

Epoch: 1768
ogbn-arxiv,CAGNET-1D,1,1768,65.9988,0.0549

Epoch: 1769
ogbn-arxiv,CAGNET-1D,1,1769,66.0361,0.0707

Epoch: 1770
ogbn-arxiv,CAGNET-1D,1,1770,66.0741,0.0533

Epoch: 1771
ogbn-arxiv,CAGNET-1D,1,1771,66.1117,0.0832

Epoch: 1772
ogbn-arxiv,CAGNET-1D,1,1772,66.1484,0.0529

Epoch: 1773
ogbn-arxiv,CAGNET-1D,1,1773,66.1862,0.0920

Epoch: 1774
ogbn-arxiv,CAGNET-1D,1,1774,66.2229,0.0529

Epoch: 1775
ogbn-arxiv,CAGNET-1D,1,1775,66.2604,0.0758

Epoch: 1776
ogbn-arxiv,CAGNET-1D,1,1776,66.2975,0.0546

Epoch: 1777
ogbn-arxiv,CAGNET-1D,1,1777,66.3350,0.0588

Epoch: 1778
ogbn-arxiv,CAGNET-1D,1,1778,66.3725,0.0580

Epoch: 1779
ogbn-arxiv,CAGNET-1D,1,1779,66.4099,0.0551

Epoch: 1780
ogbn-arxiv,CAGNET-1D,1,1780,66.4474,0.0627

Epoch: 1781
ogbn-arxiv,CAGNET-1D,1,1781,66.4847,0.0541

Epoch: 1782
ogbn-arxiv,CAGNET-1D,1,1782,66.5226,0.0699

Epoch: 1783
ogbn-arxiv,CAGNET-1D,1,1783,66.5594,0.0532

Epoch: 1784
ogbn-arxiv,CAGNET-1D,1,1784,66.5966,0.0978

Epoch: 1785
ogbn-arxiv,CAGNET-1D,1,1785,66.6336,0.0531

Epoch: 1786
ogbn-arxiv,CAGNET-1D,1,1786,66.6709,0.1397

Epoch: 1787
ogbn-arxiv,CAGNET-1D,1,1787,66.7092,0.0528

Epoch: 1788
ogbn-arxiv,CAGNET-1D,1,1788,66.7458,0.1001

Epoch: 1789
ogbn-arxiv,CAGNET-1D,1,1789,66.7837,0.0531

Epoch: 1790
ogbn-arxiv,CAGNET-1D,1,1790,66.8204,0.0635

Epoch: 1791
ogbn-arxiv,CAGNET-1D,1,1791,66.8582,0.0569

Epoch: 1792
ogbn-arxiv,CAGNET-1D,1,1792,66.8946,0.0549

Epoch: 1793
ogbn-arxiv,CAGNET-1D,1,1793,66.9331,0.0716

Epoch: 1794
ogbn-arxiv,CAGNET-1D,1,1794,66.9698,0.0533

Epoch: 1795
ogbn-arxiv,CAGNET-1D,1,1795,67.0073,0.1049

Epoch: 1796
ogbn-arxiv,CAGNET-1D,1,1796,67.0457,0.0530

Epoch: 1797
ogbn-arxiv,CAGNET-1D,1,1797,67.0826,0.0985

Epoch: 1798
ogbn-arxiv,CAGNET-1D,1,1798,67.1197,0.0530

Epoch: 1799
ogbn-arxiv,CAGNET-1D,1,1799,67.1568,0.0701

Epoch: 1800
ogbn-arxiv,CAGNET-1D,1,1800,67.1941,0.0548

Epoch: 1801
ogbn-arxiv,CAGNET-1D,1,1801,67.2317,0.0573

Epoch: 1802
ogbn-arxiv,CAGNET-1D,1,1802,67.2692,0.0618

Epoch: 1803
ogbn-arxiv,CAGNET-1D,1,1803,67.3069,0.0535

Epoch: 1804
ogbn-arxiv,CAGNET-1D,1,1804,67.3446,0.0764

Epoch: 1805
ogbn-arxiv,CAGNET-1D,1,1805,67.3813,0.0530

Epoch: 1806
ogbn-arxiv,CAGNET-1D,1,1806,67.4183,0.0831

Epoch: 1807
ogbn-arxiv,CAGNET-1D,1,1807,67.4548,0.0529

Epoch: 1808
ogbn-arxiv,CAGNET-1D,1,1808,67.4917,0.0746

Epoch: 1809
ogbn-arxiv,CAGNET-1D,1,1809,67.5285,0.0541

Epoch: 1810
ogbn-arxiv,CAGNET-1D,1,1810,67.5656,0.0681

Epoch: 1811
ogbn-arxiv,CAGNET-1D,1,1811,67.6026,0.0539

Epoch: 1812
ogbn-arxiv,CAGNET-1D,1,1812,67.6396,0.0683

Epoch: 1813
ogbn-arxiv,CAGNET-1D,1,1813,67.6771,0.0523

Epoch: 1814
ogbn-arxiv,CAGNET-1D,1,1814,67.7146,0.0659

Epoch: 1815
ogbn-arxiv,CAGNET-1D,1,1815,67.7520,0.0539

Epoch: 1816
ogbn-arxiv,CAGNET-1D,1,1816,67.7894,0.0628

Epoch: 1817
ogbn-arxiv,CAGNET-1D,1,1817,67.8266,0.0548

Epoch: 1818
ogbn-arxiv,CAGNET-1D,1,1818,67.8637,0.0595

Epoch: 1819
ogbn-arxiv,CAGNET-1D,1,1819,67.9049,0.0565

Epoch: 1820
ogbn-arxiv,CAGNET-1D,1,1820,67.9416,0.0558

Epoch: 1821
ogbn-arxiv,CAGNET-1D,1,1821,67.9782,0.0598

Epoch: 1822
ogbn-arxiv,CAGNET-1D,1,1822,68.0148,0.0551

Epoch: 1823
ogbn-arxiv,CAGNET-1D,1,1823,68.0516,0.0632

Epoch: 1824
ogbn-arxiv,CAGNET-1D,1,1824,68.0885,0.0552

Epoch: 1825
ogbn-arxiv,CAGNET-1D,1,1825,68.1254,0.0591

Epoch: 1826
ogbn-arxiv,CAGNET-1D,1,1826,68.1624,0.0568

Epoch: 1827
ogbn-arxiv,CAGNET-1D,1,1827,68.1999,0.0558

Epoch: 1828
ogbn-arxiv,CAGNET-1D,1,1828,68.2378,0.0579

Epoch: 1829
ogbn-arxiv,CAGNET-1D,1,1829,68.2747,0.0574

Epoch: 1830
ogbn-arxiv,CAGNET-1D,1,1830,68.3122,0.0553

Epoch: 1831
ogbn-arxiv,CAGNET-1D,1,1831,68.3487,0.0614

Epoch: 1832
ogbn-arxiv,CAGNET-1D,1,1832,68.3867,0.0551

Epoch: 1833
ogbn-arxiv,CAGNET-1D,1,1833,68.4232,0.0644

Epoch: 1834
ogbn-arxiv,CAGNET-1D,1,1834,68.4610,0.0542

Epoch: 1835
ogbn-arxiv,CAGNET-1D,1,1835,68.4986,0.0667

Epoch: 1836
ogbn-arxiv,CAGNET-1D,1,1836,68.5355,0.0544

Epoch: 1837
ogbn-arxiv,CAGNET-1D,1,1837,68.5727,0.0657

Epoch: 1838
ogbn-arxiv,CAGNET-1D,1,1838,68.6094,0.0544

Epoch: 1839
ogbn-arxiv,CAGNET-1D,1,1839,68.6463,0.0649

Epoch: 1840
ogbn-arxiv,CAGNET-1D,1,1840,68.6834,0.0542

Epoch: 1841
ogbn-arxiv,CAGNET-1D,1,1841,68.7207,0.0687

Epoch: 1842
ogbn-arxiv,CAGNET-1D,1,1842,68.7588,0.0544

Epoch: 1843
ogbn-arxiv,CAGNET-1D,1,1843,68.7966,0.0768

Epoch: 1844
ogbn-arxiv,CAGNET-1D,1,1844,68.8347,0.0537

Epoch: 1845
ogbn-arxiv,CAGNET-1D,1,1845,68.8717,0.0853

Epoch: 1846
ogbn-arxiv,CAGNET-1D,1,1846,68.9087,0.0542

Epoch: 1847
ogbn-arxiv,CAGNET-1D,1,1847,68.9455,0.0782

Epoch: 1848
ogbn-arxiv,CAGNET-1D,1,1848,68.9822,0.0540

Epoch: 1849
ogbn-arxiv,CAGNET-1D,1,1849,69.0193,0.0700

Epoch: 1850
ogbn-arxiv,CAGNET-1D,1,1850,69.0565,0.0543

Epoch: 1851
ogbn-arxiv,CAGNET-1D,1,1851,69.0939,0.0689

Epoch: 1852
ogbn-arxiv,CAGNET-1D,1,1852,69.1332,0.0544

Epoch: 1853
ogbn-arxiv,CAGNET-1D,1,1853,69.1705,0.0668

Epoch: 1854
ogbn-arxiv,CAGNET-1D,1,1854,69.2076,0.0550

Epoch: 1855
ogbn-arxiv,CAGNET-1D,1,1855,69.2448,0.0602

Epoch: 1856
ogbn-arxiv,CAGNET-1D,1,1856,69.2812,0.0557

Epoch: 1857
ogbn-arxiv,CAGNET-1D,1,1857,69.3188,0.0563

Epoch: 1858
ogbn-arxiv,CAGNET-1D,1,1858,69.3561,0.0597

Epoch: 1859
ogbn-arxiv,CAGNET-1D,1,1859,69.3938,0.0549

Epoch: 1860
ogbn-arxiv,CAGNET-1D,1,1860,69.4311,0.0708

Epoch: 1861
ogbn-arxiv,CAGNET-1D,1,1861,69.4690,0.0541

Epoch: 1862
ogbn-arxiv,CAGNET-1D,1,1862,69.5061,0.0868

Epoch: 1863
ogbn-arxiv,CAGNET-1D,1,1863,69.5434,0.0541

Epoch: 1864
ogbn-arxiv,CAGNET-1D,1,1864,69.5808,0.0812

Epoch: 1865
ogbn-arxiv,CAGNET-1D,1,1865,69.6177,0.0540

Epoch: 1866
ogbn-arxiv,CAGNET-1D,1,1866,69.6553,0.0672

Epoch: 1867
ogbn-arxiv,CAGNET-1D,1,1867,69.6922,0.0547

Epoch: 1868
ogbn-arxiv,CAGNET-1D,1,1868,69.7295,0.0606

Epoch: 1869
ogbn-arxiv,CAGNET-1D,1,1869,69.7665,0.0554

Epoch: 1870
ogbn-arxiv,CAGNET-1D,1,1870,69.8035,0.0600

Epoch: 1871
ogbn-arxiv,CAGNET-1D,1,1871,69.8408,0.0552

Epoch: 1872
ogbn-arxiv,CAGNET-1D,1,1872,69.8778,0.0617

Epoch: 1873
ogbn-arxiv,CAGNET-1D,1,1873,69.9157,0.0547

Epoch: 1874
ogbn-arxiv,CAGNET-1D,1,1874,69.9524,0.0606

Epoch: 1875
ogbn-arxiv,CAGNET-1D,1,1875,69.9890,0.0547

Epoch: 1876
ogbn-arxiv,CAGNET-1D,1,1876,70.0262,0.0600

Epoch: 1877
ogbn-arxiv,CAGNET-1D,1,1877,70.0626,0.0545

Epoch: 1878
ogbn-arxiv,CAGNET-1D,1,1878,70.1008,0.0647

Epoch: 1879
ogbn-arxiv,CAGNET-1D,1,1879,70.1379,0.0540

Epoch: 1880
ogbn-arxiv,CAGNET-1D,1,1880,70.1752,0.0719

Epoch: 1881
ogbn-arxiv,CAGNET-1D,1,1881,70.2117,0.0546

Epoch: 1882
ogbn-arxiv,CAGNET-1D,1,1882,70.2483,0.0670

Epoch: 1883
ogbn-arxiv,CAGNET-1D,1,1883,70.2852,0.0550

Epoch: 1884
ogbn-arxiv,CAGNET-1D,1,1884,70.3229,0.0603

Epoch: 1885
ogbn-arxiv,CAGNET-1D,1,1885,70.3599,0.0560

Epoch: 1886
ogbn-arxiv,CAGNET-1D,1,1886,70.3975,0.0557

Epoch: 1887
ogbn-arxiv,CAGNET-1D,1,1887,70.4351,0.0583

Epoch: 1888
ogbn-arxiv,CAGNET-1D,1,1888,70.4721,0.0554

Epoch: 1889
ogbn-arxiv,CAGNET-1D,1,1889,70.5089,0.0598

Epoch: 1890
ogbn-arxiv,CAGNET-1D,1,1890,70.5462,0.0551

Epoch: 1891
ogbn-arxiv,CAGNET-1D,1,1891,70.5831,0.0614

Epoch: 1892
ogbn-arxiv,CAGNET-1D,1,1892,70.6215,0.0549

Epoch: 1893
ogbn-arxiv,CAGNET-1D,1,1893,70.6583,0.0642

Epoch: 1894
ogbn-arxiv,CAGNET-1D,1,1894,70.6953,0.0544

Epoch: 1895
ogbn-arxiv,CAGNET-1D,1,1895,70.7314,0.0749

Epoch: 1896
ogbn-arxiv,CAGNET-1D,1,1896,70.7689,0.0538

Epoch: 1897
ogbn-arxiv,CAGNET-1D,1,1897,70.8054,0.0793

Epoch: 1898
ogbn-arxiv,CAGNET-1D,1,1898,70.8420,0.0545

Epoch: 1899
ogbn-arxiv,CAGNET-1D,1,1899,70.8795,0.0638

Epoch: 1900
ogbn-arxiv,CAGNET-1D,1,1900,70.9166,0.0558

Epoch: 1901
ogbn-arxiv,CAGNET-1D,1,1901,70.9546,0.0574

Epoch: 1902
ogbn-arxiv,CAGNET-1D,1,1902,70.9919,0.0593

Epoch: 1903
ogbn-arxiv,CAGNET-1D,1,1903,71.0290,0.0555

Epoch: 1904
ogbn-arxiv,CAGNET-1D,1,1904,71.0664,0.0663

Epoch: 1905
ogbn-arxiv,CAGNET-1D,1,1905,71.1034,0.0551

Epoch: 1906
ogbn-arxiv,CAGNET-1D,1,1906,71.1401,0.0812

Epoch: 1907
ogbn-arxiv,CAGNET-1D,1,1907,71.1775,0.0548

Epoch: 1908
ogbn-arxiv,CAGNET-1D,1,1908,71.2147,0.0770

Epoch: 1909
ogbn-arxiv,CAGNET-1D,1,1909,71.2521,0.0549

Epoch: 1910
ogbn-arxiv,CAGNET-1D,1,1910,71.2897,0.0625

Epoch: 1911
ogbn-arxiv,CAGNET-1D,1,1911,71.3271,0.0560

Epoch: 1912
ogbn-arxiv,CAGNET-1D,1,1912,71.3640,0.0573

Epoch: 1913
ogbn-arxiv,CAGNET-1D,1,1913,71.4009,0.0604

Epoch: 1914
ogbn-arxiv,CAGNET-1D,1,1914,71.4376,0.0559

Epoch: 1915
ogbn-arxiv,CAGNET-1D,1,1915,71.4748,0.0689

Epoch: 1916
ogbn-arxiv,CAGNET-1D,1,1916,71.5118,0.0551

Epoch: 1917
ogbn-arxiv,CAGNET-1D,1,1917,71.5490,0.0689

Epoch: 1918
ogbn-arxiv,CAGNET-1D,1,1918,71.5859,0.0548

Epoch: 1919
ogbn-arxiv,CAGNET-1D,1,1919,71.6232,0.0693

Epoch: 1920
ogbn-arxiv,CAGNET-1D,1,1920,71.6603,0.0549

Epoch: 1921
ogbn-arxiv,CAGNET-1D,1,1921,71.6980,0.0802

Epoch: 1922
ogbn-arxiv,CAGNET-1D,1,1922,71.7349,0.0557

Epoch: 1923
ogbn-arxiv,CAGNET-1D,1,1923,71.7725,0.0698

Epoch: 1924
ogbn-arxiv,CAGNET-1D,1,1924,71.8099,0.0560

Epoch: 1925
ogbn-arxiv,CAGNET-1D,1,1925,71.8477,0.0608

Epoch: 1926
ogbn-arxiv,CAGNET-1D,1,1926,71.8851,0.0560

Epoch: 1927
ogbn-arxiv,CAGNET-1D,1,1927,71.9217,0.0588

Epoch: 1928
ogbn-arxiv,CAGNET-1D,1,1928,71.9587,0.0575

Epoch: 1929
ogbn-arxiv,CAGNET-1D,1,1929,71.9958,0.0574

Epoch: 1930
ogbn-arxiv,CAGNET-1D,1,1930,72.0328,0.0628

Epoch: 1931
ogbn-arxiv,CAGNET-1D,1,1931,72.0700,0.0554

Epoch: 1932
ogbn-arxiv,CAGNET-1D,1,1932,72.1065,0.0667

Epoch: 1933
ogbn-arxiv,CAGNET-1D,1,1933,72.1434,0.0546

Epoch: 1934
ogbn-arxiv,CAGNET-1D,1,1934,72.1800,0.0652

Epoch: 1935
ogbn-arxiv,CAGNET-1D,1,1935,72.2165,0.0549

Epoch: 1936
ogbn-arxiv,CAGNET-1D,1,1936,72.2527,0.0615

Epoch: 1937
ogbn-arxiv,CAGNET-1D,1,1937,72.2888,0.0559

Epoch: 1938
ogbn-arxiv,CAGNET-1D,1,1938,72.3255,0.0595

Epoch: 1939
ogbn-arxiv,CAGNET-1D,1,1939,72.3631,0.0573

Epoch: 1940
ogbn-arxiv,CAGNET-1D,1,1940,72.3999,0.0594

Epoch: 1941
ogbn-arxiv,CAGNET-1D,1,1941,72.4390,0.0574

Epoch: 1942
ogbn-arxiv,CAGNET-1D,1,1942,72.4760,0.0608

Epoch: 1943
ogbn-arxiv,CAGNET-1D,1,1943,72.5124,0.0560

Epoch: 1944
ogbn-arxiv,CAGNET-1D,1,1944,72.5494,0.0643

Epoch: 1945
ogbn-arxiv,CAGNET-1D,1,1945,72.5868,0.0543

Epoch: 1946
ogbn-arxiv,CAGNET-1D,1,1946,72.6235,0.0653

Epoch: 1947
ogbn-arxiv,CAGNET-1D,1,1947,72.6608,0.0543

Epoch: 1948
ogbn-arxiv,CAGNET-1D,1,1948,72.6978,0.0636

Epoch: 1949
ogbn-arxiv,CAGNET-1D,1,1949,72.7350,0.0548

Epoch: 1950
ogbn-arxiv,CAGNET-1D,1,1950,72.7722,0.0799

Epoch: 1951
ogbn-arxiv,CAGNET-1D,1,1951,72.8096,0.0547

Epoch: 1952
ogbn-arxiv,CAGNET-1D,1,1952,72.8467,0.1240

Epoch: 1953
ogbn-arxiv,CAGNET-1D,1,1953,72.8833,0.0545

Epoch: 1954
ogbn-arxiv,CAGNET-1D,1,1954,72.9208,0.0889

Epoch: 1955
ogbn-arxiv,CAGNET-1D,1,1955,72.9576,0.0545

Epoch: 1956
ogbn-arxiv,CAGNET-1D,1,1956,72.9945,0.0611

Epoch: 1957
ogbn-arxiv,CAGNET-1D,1,1957,73.0319,0.0579

Epoch: 1958
ogbn-arxiv,CAGNET-1D,1,1958,73.0689,0.0566

Epoch: 1959
ogbn-arxiv,CAGNET-1D,1,1959,73.1070,0.0715

Epoch: 1960
ogbn-arxiv,CAGNET-1D,1,1960,73.1441,0.0542

Epoch: 1961
ogbn-arxiv,CAGNET-1D,1,1961,73.1815,0.0787

Epoch: 1962
ogbn-arxiv,CAGNET-1D,1,1962,73.2194,0.0547

Epoch: 1963
ogbn-arxiv,CAGNET-1D,1,1963,73.2559,0.0722

Epoch: 1964
ogbn-arxiv,CAGNET-1D,1,1964,73.2925,0.0549

Epoch: 1965
ogbn-arxiv,CAGNET-1D,1,1965,73.3313,0.0631

Epoch: 1966
ogbn-arxiv,CAGNET-1D,1,1966,73.3680,0.0564

Epoch: 1967
ogbn-arxiv,CAGNET-1D,1,1967,73.4047,0.0589

Epoch: 1968
ogbn-arxiv,CAGNET-1D,1,1968,73.4413,0.0582

Epoch: 1969
ogbn-arxiv,CAGNET-1D,1,1969,73.4792,0.0559

Epoch: 1970
ogbn-arxiv,CAGNET-1D,1,1970,73.5163,0.0647

Epoch: 1971
ogbn-arxiv,CAGNET-1D,1,1971,73.5536,0.0553

Epoch: 1972
ogbn-arxiv,CAGNET-1D,1,1972,73.5903,0.0808

Epoch: 1973
ogbn-arxiv,CAGNET-1D,1,1973,73.6277,0.0553

Epoch: 1974
ogbn-arxiv,CAGNET-1D,1,1974,73.6644,0.0786

Epoch: 1975
ogbn-arxiv,CAGNET-1D,1,1975,73.7018,0.0548

Epoch: 1976
ogbn-arxiv,CAGNET-1D,1,1976,73.7388,0.0649

Epoch: 1977
ogbn-arxiv,CAGNET-1D,1,1977,73.7768,0.0560

Epoch: 1978
ogbn-arxiv,CAGNET-1D,1,1978,73.8138,0.0603

Epoch: 1979
ogbn-arxiv,CAGNET-1D,1,1979,73.8508,0.0565

Epoch: 1980
ogbn-arxiv,CAGNET-1D,1,1980,73.8877,0.0612

Epoch: 1981
ogbn-arxiv,CAGNET-1D,1,1981,73.9249,0.0555

Epoch: 1982
ogbn-arxiv,CAGNET-1D,1,1982,73.9620,0.0633

Epoch: 1983
ogbn-arxiv,CAGNET-1D,1,1983,74.0000,0.0551

Epoch: 1984
ogbn-arxiv,CAGNET-1D,1,1984,74.0369,0.0635

Epoch: 1985
ogbn-arxiv,CAGNET-1D,1,1985,74.0739,0.0557

Epoch: 1986
ogbn-arxiv,CAGNET-1D,1,1986,74.1109,0.0638

Epoch: 1987
ogbn-arxiv,CAGNET-1D,1,1987,74.1478,0.0560

Epoch: 1988
ogbn-arxiv,CAGNET-1D,1,1988,74.1848,0.0690

Epoch: 1989
ogbn-arxiv,CAGNET-1D,1,1989,74.2243,0.0551

Epoch: 1990
ogbn-arxiv,CAGNET-1D,1,1990,74.2621,0.0891

Epoch: 1991
ogbn-arxiv,CAGNET-1D,1,1991,74.2988,0.0548

Epoch: 1992
ogbn-arxiv,CAGNET-1D,1,1992,74.3352,0.1147

Epoch: 1993
ogbn-arxiv,CAGNET-1D,1,1993,74.3721,0.0545

Epoch: 1994
ogbn-arxiv,CAGNET-1D,1,1994,74.4092,0.0923

Epoch: 1995
ogbn-arxiv,CAGNET-1D,1,1995,74.4460,0.0546

Epoch: 1996
ogbn-arxiv,CAGNET-1D,1,1996,74.4832,0.0639

Epoch: 1997
ogbn-arxiv,CAGNET-1D,1,1997,74.5215,0.0582

Epoch: 1998
ogbn-arxiv,CAGNET-1D,1,1998,74.5588,0.0567

Epoch: 1999
ogbn-arxiv,CAGNET-1D,1,1999,74.5962,0.0661

total_times_r0: [82.93714690208435]
rank: 0 median_run: 0
rank: 0 total_time: 82.93714690208435
rank: 0 comm_time: 1.829833984375
rank: 0 comp_time: 58.39988851547241
rank: 0 scomp_time: 54.98915100097656
rank: 0 dcomp_time: 3.4107375144958496
rank: 0 bcast_comm_time: 1.279557228088379
rank: 0 barrier_time: 3.316746473312378
rank: 0 barrier_subset_time: 0.0
rank: 0 op1_comm_time: 0.0
rank: 0 op2_comm_time: 0.5502767562866211
rank: 0 tensor([[-6.6919, -7.0994, -2.9581,  ..., -3.6301, -4.6124, -3.8015],
        [-4.6557, -5.3533, -3.3326,  ..., -3.9999, -3.4961, -3.1215],
        [-4.1431, -4.2282, -3.2350,  ..., -3.5035, -3.4555, -3.4850],
        ...,
        [-3.8774, -3.9896, -3.4403,  ..., -3.5035, -3.7025, -3.6366],
        [-3.9737, -3.9838, -3.4030,  ..., -3.5649, -3.7376, -3.5524],
        [-3.9531, -3.7958, -3.4856,  ..., -3.6556, -3.5454, -3.6810]],
       device='cuda:0', grad_fn=<CatBackward>)
Epoch: 900, Train: 0.1496, Val: 0.2133, Test: 0.2069
None
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='ogbn-arxiv', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: ogbn-arxiv timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 1
tensor([[104447,  15858, 107156,  ...,  45118,  45118,  45118],
        [ 13091,  47283,  69161,  ..., 162473, 162537,  72717]],
       device='cuda:0')
tensor([[ 13091,  47283,  69161,  ..., 162473, 162537,  72717],
        [104447,  15858, 107156,  ...,  45118,  45118,  45118]])
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 582, in run
    row_groups, col_groups = get_proc_groups(rank, size)
  File "GNN-RDM/src/gcn_distr_15d.py", line 414, in get_proc_groups
    row_groups.append(dist.new_group(row_procs[i]))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 2690, in new_group
    raise RuntimeError("the new group's world size should be less or "
RuntimeError: the new group's world size should be less or equal to the world size set by init_process_group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2122761) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2122761 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:31:13
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2122761)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='ogbn-arxiv', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: ogbn-arxiv timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='ogbn-arxiv', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: ogbn-arxiv timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 2
device: cuda:0
Processes: 2
device: cuda:1
tensor([[104447,  15858, 107156,  ...,  45118,  45118,  45118],
        [ 13091,  47283,  69161,  ..., 162473, 162537,  72717]],
       device='cuda:0')
tensor([[104447,  15858, 107156,  ...,  45118,  45118,  45118],
        [ 13091,  47283,  69161,  ..., 162473, 162537,  72717]],
       device='cuda:1')
tensor([[ 13091,  47283,  69161,  ..., 162473, 162537,  72717],
        [104447,  15858, 107156,  ...,  45118,  45118,  45118]])
tensor([[ 13091,  47283,  69161,  ..., 162473, 162537,  72717],
        [104447,  15858, 107156,  ...,  45118,  45118,  45118]])
rank: 0 adj_matrix_loc.size: torch.Size([169343, 84672])
rank: 0 inputs.size: torch.Size([169343, 128])
rank: 1 adj_matrix_loc.size: torch.Size([169343, 84671])
rank: 1 inputs.size: torch.Size([169343, 128])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 1 run 0
Starting training... rank 0 run 0
Epoch: 000
Epoch: 000
ogbn-arxiv,CAGNET-1D,2,0,0.0516,0.0581

Epoch: 001
Epoch: 001
ogbn-arxiv,CAGNET-1D,2,1,0.1002,0.0577

Epoch: 002
Epoch: 002
ogbn-arxiv,CAGNET-1D,2,2,0.1489,0.2108

Epoch: 003
Epoch: 003
ogbn-arxiv,CAGNET-1D,2,3,0.1977,0.2180

Epoch: 004
Epoch: 004
ogbn-arxiv,CAGNET-1D,2,4,0.2467,0.2037

Epoch: 005
Epoch: 005
ogbn-arxiv,CAGNET-1D,2,5,0.2955,0.2347

Epoch: 006
Epoch: 006
ogbn-arxiv,CAGNET-1D,2,6,0.3440,0.1829

Epoch: 007
Epoch: 007
ogbn-arxiv,CAGNET-1D,2,7,0.3925,0.0674

Epoch: 008
Epoch: 008
ogbn-arxiv,CAGNET-1D,2,8,0.4412,0.0803

Epoch: 009
Epoch: 009
ogbn-arxiv,CAGNET-1D,2,9,0.4898,0.2554

Epoch: 010
Epoch: 010
ogbn-arxiv,CAGNET-1D,2,10,0.5387,0.3395

Epoch: 011
Epoch: 011
ogbn-arxiv,CAGNET-1D,2,11,0.5878,0.3432

Epoch: 012
Epoch: 012
ogbn-arxiv,CAGNET-1D,2,12,0.6384,0.3358

Epoch: 013
Epoch: 013
ogbn-arxiv,CAGNET-1D,2,13,0.6883,0.3320

Epoch: 014
Epoch: 014
ogbn-arxiv,CAGNET-1D,2,14,0.7383,0.3392

Epoch: 015
Epoch: 015
ogbn-arxiv,CAGNET-1D,2,15,0.7908,0.3429

Epoch: 016
Epoch: 016
ogbn-arxiv,CAGNET-1D,2,16,0.8409,0.3425

Epoch: 017
Epoch: 017
ogbn-arxiv,CAGNET-1D,2,17,0.8907,0.3380

Epoch: 018
Epoch: 018
ogbn-arxiv,CAGNET-1D,2,18,0.9433,0.3313

Epoch: 019
Epoch: 019
ogbn-arxiv,CAGNET-1D,2,19,0.9945,0.3290

Epoch: 020Epoch: 020

ogbn-arxiv,CAGNET-1D,2,20,1.0453,0.3316

Epoch: 021
Epoch: 021
ogbn-arxiv,CAGNET-1D,2,21,1.0952,0.3453

Epoch: 022
Epoch: 022
ogbn-arxiv,CAGNET-1D,2,22,1.1453,0.3678

Epoch: 023
Epoch: 023
ogbn-arxiv,CAGNET-1D,2,23,1.1954,0.3810

Epoch: 024
Epoch: 024
ogbn-arxiv,CAGNET-1D,2,24,1.2452,0.3865

Epoch: 025
Epoch: 025
ogbn-arxiv,CAGNET-1D,2,25,1.2949,0.3910

Epoch: 026Epoch: 026

ogbn-arxiv,CAGNET-1D,2,26,1.3447,0.3984

Epoch: 027Epoch: 027

ogbn-arxiv,CAGNET-1D,2,27,1.3950,0.4040

Epoch: 028
Epoch: 028
ogbn-arxiv,CAGNET-1D,2,28,1.4453,0.4079

Epoch: 029
Epoch: 029
ogbn-arxiv,CAGNET-1D,2,29,1.4950,0.4131

Epoch: 030
Epoch: 030
ogbn-arxiv,CAGNET-1D,2,30,1.5449,0.4172

Epoch: 031
Epoch: 031
ogbn-arxiv,CAGNET-1D,2,31,1.5950,0.4208

Epoch: 032
Epoch: 032
ogbn-arxiv,CAGNET-1D,2,32,1.6449,0.4254

Epoch: 033
Epoch: 033
ogbn-arxiv,CAGNET-1D,2,33,1.7012,0.4303

Epoch: 034
Epoch: 034
ogbn-arxiv,CAGNET-1D,2,34,1.7528,0.4390

Epoch: 035
Epoch: 035
ogbn-arxiv,CAGNET-1D,2,35,1.8034,0.4496

Epoch: 036
Epoch: 036
ogbn-arxiv,CAGNET-1D,2,36,1.8537,0.4586

Epoch: 037
Epoch: 037
ogbn-arxiv,CAGNET-1D,2,37,1.9037,0.4660

Epoch: 038Epoch: 038

ogbn-arxiv,CAGNET-1D,2,38,1.9539,0.4744

Epoch: 039Epoch: 039

ogbn-arxiv,CAGNET-1D,2,39,2.0039,0.4824

Epoch: 040
Epoch: 040
ogbn-arxiv,CAGNET-1D,2,40,2.0537,0.4905

Epoch: 041
Epoch: 041
ogbn-arxiv,CAGNET-1D,2,41,2.1036,0.4988

Epoch: 042
Epoch: 042
ogbn-arxiv,CAGNET-1D,2,42,2.1535,0.5071

Epoch: 043
Epoch: 043
ogbn-arxiv,CAGNET-1D,2,43,2.2036,0.5119

Epoch: 044
Epoch: 044
ogbn-arxiv,CAGNET-1D,2,44,2.2537,0.5138

Epoch: 045
Epoch: 045
ogbn-arxiv,CAGNET-1D,2,45,2.3040,0.5147

Epoch: 046
Epoch: 046
ogbn-arxiv,CAGNET-1D,2,46,2.3536,0.5168

Epoch: 047Epoch: 047

ogbn-arxiv,CAGNET-1D,2,47,2.4036,0.5183

Epoch: 048
Epoch: 048
ogbn-arxiv,CAGNET-1D,2,48,2.4540,0.5208

Epoch: 049
Epoch: 049
ogbn-arxiv,CAGNET-1D,2,49,2.5040,0.5234

Epoch: 050
Epoch: 050
ogbn-arxiv,CAGNET-1D,2,50,2.5538,0.5262

Epoch: 051
Epoch: 051
ogbn-arxiv,CAGNET-1D,2,51,2.6065,0.5304

Epoch: 052
Epoch: 052
ogbn-arxiv,CAGNET-1D,2,52,2.6580,0.5363

Epoch: 053
Epoch: 053
ogbn-arxiv,CAGNET-1D,2,53,2.7079,0.5414

Epoch: 054
Epoch: 054
ogbn-arxiv,CAGNET-1D,2,54,2.7582,0.5454

Epoch: 055
Epoch: 055
ogbn-arxiv,CAGNET-1D,2,55,2.8079,0.5477

Epoch: 056Epoch: 056

ogbn-arxiv,CAGNET-1D,2,56,2.8577,0.5480

Epoch: 057
Epoch: 057
ogbn-arxiv,CAGNET-1D,2,57,2.9079,0.5473

Epoch: 058Epoch: 058

ogbn-arxiv,CAGNET-1D,2,58,2.9579,0.5467

Epoch: 059
Epoch: 059
ogbn-arxiv,CAGNET-1D,2,59,3.0078,0.5482

Epoch: 060
Epoch: 060
ogbn-arxiv,CAGNET-1D,2,60,3.0574,0.5526

Epoch: 061Epoch: 061

ogbn-arxiv,CAGNET-1D,2,61,3.1070,0.5571

Epoch: 062
Epoch: 062
ogbn-arxiv,CAGNET-1D,2,62,3.1575,0.5619

Epoch: 063
Epoch: 063
ogbn-arxiv,CAGNET-1D,2,63,3.2076,0.5643

Epoch: 064Epoch: 064

ogbn-arxiv,CAGNET-1D,2,64,3.2573,0.5642

Epoch: 065
Epoch: 065
ogbn-arxiv,CAGNET-1D,2,65,3.3067,0.5632

Epoch: 066
Epoch: 066
ogbn-arxiv,CAGNET-1D,2,66,3.3566,0.5614

Epoch: 067
Epoch: 067
ogbn-arxiv,CAGNET-1D,2,67,3.4068,0.5615

Epoch: 068
Epoch: 068
ogbn-arxiv,CAGNET-1D,2,68,3.4569,0.5647

Epoch: 069
Epoch: 069
ogbn-arxiv,CAGNET-1D,2,69,3.5067,0.5694

Epoch: 070
Epoch: 070
ogbn-arxiv,CAGNET-1D,2,70,3.5613,0.5732

Epoch: 071
Epoch: 071
ogbn-arxiv,CAGNET-1D,2,71,3.6113,0.5757

Epoch: 072Epoch: 072

ogbn-arxiv,CAGNET-1D,2,72,3.6612,0.5764

Epoch: 073
Epoch: 073
ogbn-arxiv,CAGNET-1D,2,73,3.7112,0.5758

Epoch: 074
Epoch: 074
ogbn-arxiv,CAGNET-1D,2,74,3.7613,0.5758

Epoch: 075
Epoch: 075
ogbn-arxiv,CAGNET-1D,2,75,3.8108,0.5769

Epoch: 076
Epoch: 076
ogbn-arxiv,CAGNET-1D,2,76,3.8609,0.5789

Epoch: 077
Epoch: 077
ogbn-arxiv,CAGNET-1D,2,77,3.9110,0.5808

Epoch: 078
Epoch: 078
ogbn-arxiv,CAGNET-1D,2,78,3.9607,0.5826

Epoch: 079
Epoch: 079
ogbn-arxiv,CAGNET-1D,2,79,4.0103,0.5835

Epoch: 080
Epoch: 080
ogbn-arxiv,CAGNET-1D,2,80,4.0605,0.5835

Epoch: 081
Epoch: 081
ogbn-arxiv,CAGNET-1D,2,81,4.1105,0.5843

Epoch: 082Epoch: 082

ogbn-arxiv,CAGNET-1D,2,82,4.1605,0.5855

Epoch: 083
Epoch: 083
ogbn-arxiv,CAGNET-1D,2,83,4.2101,0.5869

Epoch: 084
Epoch: 084
ogbn-arxiv,CAGNET-1D,2,84,4.2599,0.5871

Epoch: 085
Epoch: 085
ogbn-arxiv,CAGNET-1D,2,85,4.3103,0.5878

Epoch: 086
Epoch: 086
ogbn-arxiv,CAGNET-1D,2,86,4.3602,0.5886

Epoch: 087
Epoch: 087
ogbn-arxiv,CAGNET-1D,2,87,4.4102,0.5894

Epoch: 088
Epoch: 088
ogbn-arxiv,CAGNET-1D,2,88,4.4619,0.5903

Epoch: 089
Epoch: 089
ogbn-arxiv,CAGNET-1D,2,89,4.5120,0.5914

Epoch: 090
Epoch: 090
ogbn-arxiv,CAGNET-1D,2,90,4.5618,0.5924

Epoch: 091
Epoch: 091
ogbn-arxiv,CAGNET-1D,2,91,4.6113,0.5932

Epoch: 092
Epoch: 092
ogbn-arxiv,CAGNET-1D,2,92,4.6613,0.5934

Epoch: 093
Epoch: 093
ogbn-arxiv,CAGNET-1D,2,93,4.7110,0.5941

Epoch: 094
Epoch: 094
ogbn-arxiv,CAGNET-1D,2,94,4.7608,0.5963

Epoch: 095Epoch: 095

ogbn-arxiv,CAGNET-1D,2,95,4.8103,0.5966

Epoch: 096
Epoch: 096
ogbn-arxiv,CAGNET-1D,2,96,4.8604,0.5967

Epoch: 097Epoch: 097

ogbn-arxiv,CAGNET-1D,2,97,4.9105,0.5968

Epoch: 098
Epoch: 098
ogbn-arxiv,CAGNET-1D,2,98,4.9602,0.5968

Epoch: 099Epoch: 099

ogbn-arxiv,CAGNET-1D,2,99,5.0096,0.5973

Epoch: 100Epoch: 100

ogbn-arxiv,CAGNET-1D,2,100,5.0595,0.5983

Epoch: 101
Epoch: 101
ogbn-arxiv,CAGNET-1D,2,101,5.1095,0.5993

Epoch: 102
Epoch: 102
ogbn-arxiv,CAGNET-1D,2,102,5.1595,0.6008

Epoch: 103Epoch: 103

ogbn-arxiv,CAGNET-1D,2,103,5.2092,0.6006

Epoch: 104
Epoch: 104
ogbn-arxiv,CAGNET-1D,2,104,5.2591,0.6005

Epoch: 105
Epoch: 105
ogbn-arxiv,CAGNET-1D,2,105,5.3092,0.6008

Epoch: 106
Epoch: 106
ogbn-arxiv,CAGNET-1D,2,106,5.3593,0.6017

Epoch: 107
Epoch: 107
ogbn-arxiv,CAGNET-1D,2,107,5.4142,0.6025

Epoch: 108
Epoch: 108
ogbn-arxiv,CAGNET-1D,2,108,5.4642,0.6032

Epoch: 109
Epoch: 109
ogbn-arxiv,CAGNET-1D,2,109,5.5139,0.6034

Epoch: 110
Epoch: 110
ogbn-arxiv,CAGNET-1D,2,110,5.5638,0.6039

Epoch: 111
Epoch: 111
ogbn-arxiv,CAGNET-1D,2,111,5.6136,0.6043

Epoch: 112
Epoch: 112
ogbn-arxiv,CAGNET-1D,2,112,5.6634,0.6042

Epoch: 113
Epoch: 113
ogbn-arxiv,CAGNET-1D,2,113,5.7132,0.6044

Epoch: 114
Epoch: 114
ogbn-arxiv,CAGNET-1D,2,114,5.7632,0.6043

Epoch: 115
Epoch: 115
ogbn-arxiv,CAGNET-1D,2,115,5.8132,0.6045

Epoch: 116
Epoch: 116
ogbn-arxiv,CAGNET-1D,2,116,5.8631,0.6055

Epoch: 117
Epoch: 117
ogbn-arxiv,CAGNET-1D,2,117,5.9130,0.6064

Epoch: 118
Epoch: 118
ogbn-arxiv,CAGNET-1D,2,118,5.9631,0.6071

Epoch: 119
Epoch: 119
ogbn-arxiv,CAGNET-1D,2,119,6.0131,0.6072

Epoch: 120
Epoch: 120
ogbn-arxiv,CAGNET-1D,2,120,6.0632,0.6070

Epoch: 121
Epoch: 121
ogbn-arxiv,CAGNET-1D,2,121,6.1132,0.6073

Epoch: 122
Epoch: 122
ogbn-arxiv,CAGNET-1D,2,122,6.1631,0.6079

Epoch: 123
Epoch: 123
ogbn-arxiv,CAGNET-1D,2,123,6.2128,0.6087

Epoch: 124
Epoch: 124
ogbn-arxiv,CAGNET-1D,2,124,6.2628,0.6090

Epoch: 125
Epoch: 125
ogbn-arxiv,CAGNET-1D,2,125,6.3185,0.6092

Epoch: 126
Epoch: 126
ogbn-arxiv,CAGNET-1D,2,126,6.3688,0.6093

Epoch: 127
Epoch: 127
ogbn-arxiv,CAGNET-1D,2,127,6.4194,0.6096

Epoch: 128
Epoch: 128
ogbn-arxiv,CAGNET-1D,2,128,6.4694,0.6100

Epoch: 129
Epoch: 129
ogbn-arxiv,CAGNET-1D,2,129,6.5196,0.6107

Epoch: 130
Epoch: 130
ogbn-arxiv,CAGNET-1D,2,130,6.5696,0.6106

Epoch: 131
Epoch: 131
ogbn-arxiv,CAGNET-1D,2,131,6.6195,0.6109

Epoch: 132
Epoch: 132
ogbn-arxiv,CAGNET-1D,2,132,6.6697,0.6115

Epoch: 133
Epoch: 133
ogbn-arxiv,CAGNET-1D,2,133,6.7196,0.6118

Epoch: 134
Epoch: 134
ogbn-arxiv,CAGNET-1D,2,134,6.7695,0.6119

Epoch: 135
Epoch: 135
ogbn-arxiv,CAGNET-1D,2,135,6.8193,0.6118

Epoch: 136
Epoch: 136
ogbn-arxiv,CAGNET-1D,2,136,6.8697,0.6119

Epoch: 137
Epoch: 137
ogbn-arxiv,CAGNET-1D,2,137,6.9196,0.6130

Epoch: 138
Epoch: 138
ogbn-arxiv,CAGNET-1D,2,138,6.9695,0.6141

Epoch: 139
Epoch: 139
ogbn-arxiv,CAGNET-1D,2,139,7.0195,0.6137

Epoch: 140
Epoch: 140
ogbn-arxiv,CAGNET-1D,2,140,7.0695,0.6137

Epoch: 141
Epoch: 141
ogbn-arxiv,CAGNET-1D,2,141,7.1195,0.6140

Epoch: 142
Epoch: 142
ogbn-arxiv,CAGNET-1D,2,142,7.1695,0.6146

Epoch: 143
Epoch: 143
ogbn-arxiv,CAGNET-1D,2,143,7.2227,0.6148

Epoch: 144
Epoch: 144
ogbn-arxiv,CAGNET-1D,2,144,7.2733,0.6147

Epoch: 145
Epoch: 145
ogbn-arxiv,CAGNET-1D,2,145,7.3238,0.6146

Epoch: 146
Epoch: 146
ogbn-arxiv,CAGNET-1D,2,146,7.3739,0.6153

Epoch: 147
Epoch: 147
ogbn-arxiv,CAGNET-1D,2,147,7.4236,0.6156

Epoch: 148
Epoch: 148
ogbn-arxiv,CAGNET-1D,2,148,7.4732,0.6159

Epoch: 149
Epoch: 149
ogbn-arxiv,CAGNET-1D,2,149,7.5232,0.6161

Epoch: 150
Epoch: 150
ogbn-arxiv,CAGNET-1D,2,150,7.5733,0.6164

Epoch: 151
Epoch: 151
ogbn-arxiv,CAGNET-1D,2,151,7.6236,0.6163

Epoch: 152Epoch: 152

ogbn-arxiv,CAGNET-1D,2,152,7.6730,0.6168

Epoch: 153
Epoch: 153
ogbn-arxiv,CAGNET-1D,2,153,7.7227,0.6172

Epoch: 154
Epoch: 154
ogbn-arxiv,CAGNET-1D,2,154,7.7729,0.6176

Epoch: 155
Epoch: 155
ogbn-arxiv,CAGNET-1D,2,155,7.8229,0.6178

Epoch: 156Epoch: 156

ogbn-arxiv,CAGNET-1D,2,156,7.8726,0.6176

Epoch: 157
Epoch: 157
ogbn-arxiv,CAGNET-1D,2,157,7.9222,0.6183

Epoch: 158
Epoch: 158
ogbn-arxiv,CAGNET-1D,2,158,7.9722,0.6184

Epoch: 159
Epoch: 159
ogbn-arxiv,CAGNET-1D,2,159,8.0222,0.6189

Epoch: 160
Epoch: 160
ogbn-arxiv,CAGNET-1D,2,160,8.0721,0.6189

Epoch: 161Epoch: 161

ogbn-arxiv,CAGNET-1D,2,161,8.1215,0.6188

Epoch: 162
Epoch: 162
ogbn-arxiv,CAGNET-1D,2,162,8.1753,0.6191

Epoch: 163Epoch: 163

ogbn-arxiv,CAGNET-1D,2,163,8.2252,0.6197

Epoch: 164
Epoch: 164
ogbn-arxiv,CAGNET-1D,2,164,8.2748,0.6198

Epoch: 165
Epoch: 165
ogbn-arxiv,CAGNET-1D,2,165,8.3248,0.6198

Epoch: 166
Epoch: 166
ogbn-arxiv,CAGNET-1D,2,166,8.3751,0.6201

Epoch: 167
Epoch: 167
ogbn-arxiv,CAGNET-1D,2,167,8.4248,0.6207

Epoch: 168
Epoch: 168
ogbn-arxiv,CAGNET-1D,2,168,8.4745,0.6211

Epoch: 169Epoch: 169

ogbn-arxiv,CAGNET-1D,2,169,8.5244,0.6203

Epoch: 170
Epoch: 170
ogbn-arxiv,CAGNET-1D,2,170,8.5745,0.6206

Epoch: 171
Epoch: 171
ogbn-arxiv,CAGNET-1D,2,171,8.6243,0.6214

Epoch: 172
Epoch: 172
ogbn-arxiv,CAGNET-1D,2,172,8.6741,0.6221

Epoch: 173
Epoch: 173
ogbn-arxiv,CAGNET-1D,2,173,8.7241,0.6221

Epoch: 174
Epoch: 174
ogbn-arxiv,CAGNET-1D,2,174,8.7740,0.6215

Epoch: 175
Epoch: 175
ogbn-arxiv,CAGNET-1D,2,175,8.8240,0.6222

Epoch: 176
Epoch: 176
ogbn-arxiv,CAGNET-1D,2,176,8.8741,0.6228

Epoch: 177
Epoch: 177
ogbn-arxiv,CAGNET-1D,2,177,8.9247,0.6228

Epoch: 178
Epoch: 178
ogbn-arxiv,CAGNET-1D,2,178,8.9746,0.6228

Epoch: 179
Epoch: 179
ogbn-arxiv,CAGNET-1D,2,179,9.0248,0.6228

Epoch: 180
Epoch: 180
ogbn-arxiv,CAGNET-1D,2,180,9.0750,0.6233

Epoch: 181
Epoch: 181
ogbn-arxiv,CAGNET-1D,2,181,9.1274,0.6236

Epoch: 182
Epoch: 182
ogbn-arxiv,CAGNET-1D,2,182,9.1778,0.6237

Epoch: 183
Epoch: 183
ogbn-arxiv,CAGNET-1D,2,183,9.2277,0.6234

Epoch: 184
Epoch: 184
ogbn-arxiv,CAGNET-1D,2,184,9.2777,0.6237

Epoch: 185
Epoch: 185
ogbn-arxiv,CAGNET-1D,2,185,9.3278,0.6244

Epoch: 186
Epoch: 186
ogbn-arxiv,CAGNET-1D,2,186,9.3777,0.6239

Epoch: 187
Epoch: 187
ogbn-arxiv,CAGNET-1D,2,187,9.4275,0.6240

Epoch: 188
Epoch: 188
ogbn-arxiv,CAGNET-1D,2,188,9.4775,0.6246

Epoch: 189
Epoch: 189
ogbn-arxiv,CAGNET-1D,2,189,9.5275,0.6250

Epoch: 190
Epoch: 190
ogbn-arxiv,CAGNET-1D,2,190,9.5775,0.6249

Epoch: 191
Epoch: 191
ogbn-arxiv,CAGNET-1D,2,191,9.6273,0.6247

Epoch: 192
Epoch: 192
ogbn-arxiv,CAGNET-1D,2,192,9.6775,0.6249

Epoch: 193
Epoch: 193
ogbn-arxiv,CAGNET-1D,2,193,9.7277,0.6256

Epoch: 194
Epoch: 194
ogbn-arxiv,CAGNET-1D,2,194,9.7775,0.6257

Epoch: 195
Epoch: 195
ogbn-arxiv,CAGNET-1D,2,195,9.8272,0.6256

Epoch: 196
Epoch: 196
ogbn-arxiv,CAGNET-1D,2,196,9.8772,0.6252

Epoch: 197
Epoch: 197
ogbn-arxiv,CAGNET-1D,2,197,9.9270,0.6260

Epoch: 198
Epoch: 198
ogbn-arxiv,CAGNET-1D,2,198,9.9772,0.6262

Epoch: 199
Epoch: 199
ogbn-arxiv,CAGNET-1D,2,199,10.0311,0.6258

Epoch: 200
Epoch: 200
ogbn-arxiv,CAGNET-1D,2,200,10.0817,0.6260

Epoch: 201
Epoch: 201
ogbn-arxiv,CAGNET-1D,2,201,10.1316,0.6265

Epoch: 202
Epoch: 202
ogbn-arxiv,CAGNET-1D,2,202,10.1816,0.6263

Epoch: 203
Epoch: 203
ogbn-arxiv,CAGNET-1D,2,203,10.2315,0.6265

Epoch: 204
Epoch: 204
ogbn-arxiv,CAGNET-1D,2,204,10.2813,0.6266

Epoch: 205Epoch: 205

ogbn-arxiv,CAGNET-1D,2,205,10.3318,0.6270

Epoch: 206
Epoch: 206
ogbn-arxiv,CAGNET-1D,2,206,10.3819,0.6273

Epoch: 207
Epoch: 207
ogbn-arxiv,CAGNET-1D,2,207,10.4317,0.6275

Epoch: 208
Epoch: 208
ogbn-arxiv,CAGNET-1D,2,208,10.4817,0.6276

Epoch: 209Epoch: 209

ogbn-arxiv,CAGNET-1D,2,209,10.5317,0.6282

Epoch: 210
Epoch: 210
ogbn-arxiv,CAGNET-1D,2,210,10.5818,0.6285

Epoch: 211
Epoch: 211
ogbn-arxiv,CAGNET-1D,2,211,10.6318,0.6281

Epoch: 212
Epoch: 212
ogbn-arxiv,CAGNET-1D,2,212,10.6819,0.6283

Epoch: 213
Epoch: 213
ogbn-arxiv,CAGNET-1D,2,213,10.7317,0.6289

Epoch: 214
Epoch: 214
ogbn-arxiv,CAGNET-1D,2,214,10.7818,0.6289

Epoch: 215
Epoch: 215
ogbn-arxiv,CAGNET-1D,2,215,10.8317,0.6286

Epoch: 216
Epoch: 216
ogbn-arxiv,CAGNET-1D,2,216,10.8817,0.6293

Epoch: 217
Epoch: 217
ogbn-arxiv,CAGNET-1D,2,217,10.9317,0.6299

Epoch: 218
Epoch: 218
ogbn-arxiv,CAGNET-1D,2,218,10.9859,0.6292

Epoch: 219
Epoch: 219
ogbn-arxiv,CAGNET-1D,2,219,11.0358,0.6286

Epoch: 220
Epoch: 220
ogbn-arxiv,CAGNET-1D,2,220,11.0866,0.6295

Epoch: 221
Epoch: 221
ogbn-arxiv,CAGNET-1D,2,221,11.1379,0.6300

Epoch: 222
Epoch: 222
ogbn-arxiv,CAGNET-1D,2,222,11.1882,0.6296

Epoch: 223
Epoch: 223
ogbn-arxiv,CAGNET-1D,2,223,11.2382,0.6297

Epoch: 224
Epoch: 224
ogbn-arxiv,CAGNET-1D,2,224,11.2884,0.6303

Epoch: 225
Epoch: 225
ogbn-arxiv,CAGNET-1D,2,225,11.3385,0.6304

Epoch: 226
Epoch: 226
ogbn-arxiv,CAGNET-1D,2,226,11.3881,0.6300

Epoch: 227
Epoch: 227
ogbn-arxiv,CAGNET-1D,2,227,11.4387,0.6300

Epoch: 228
Epoch: 228
ogbn-arxiv,CAGNET-1D,2,228,11.4889,0.6312

Epoch: 229Epoch: 229

ogbn-arxiv,CAGNET-1D,2,229,11.5386,0.6310

Epoch: 230
Epoch: 230
ogbn-arxiv,CAGNET-1D,2,230,11.5882,0.6304

Epoch: 231Epoch: 231

ogbn-arxiv,CAGNET-1D,2,231,11.6378,0.6305

Epoch: 232Epoch: 232

ogbn-arxiv,CAGNET-1D,2,232,11.6877,0.6314

Epoch: 233Epoch: 233

ogbn-arxiv,CAGNET-1D,2,233,11.7378,0.6318

Epoch: 234
Epoch: 234
ogbn-arxiv,CAGNET-1D,2,234,11.7878,0.6308

Epoch: 235
Epoch: 235
ogbn-arxiv,CAGNET-1D,2,235,11.8374,0.6316

Epoch: 236
Epoch: 236
ogbn-arxiv,CAGNET-1D,2,236,11.8911,0.6326

Epoch: 237
Epoch: 237
ogbn-arxiv,CAGNET-1D,2,237,11.9410,0.6318

Epoch: 238
Epoch: 238
ogbn-arxiv,CAGNET-1D,2,238,11.9908,0.6312

Epoch: 239Epoch: 239

ogbn-arxiv,CAGNET-1D,2,239,12.0406,0.6325

Epoch: 240Epoch: 240

ogbn-arxiv,CAGNET-1D,2,240,12.0908,0.6329

Epoch: 241Epoch: 241

ogbn-arxiv,CAGNET-1D,2,241,12.1408,0.6315

Epoch: 242
Epoch: 242
ogbn-arxiv,CAGNET-1D,2,242,12.1904,0.6321

Epoch: 243
Epoch: 243
ogbn-arxiv,CAGNET-1D,2,243,12.2403,0.6334

Epoch: 244Epoch: 244

ogbn-arxiv,CAGNET-1D,2,244,12.2903,0.6328

Epoch: 245Epoch: 245

ogbn-arxiv,CAGNET-1D,2,245,12.3404,0.6327

Epoch: 246
Epoch: 246
ogbn-arxiv,CAGNET-1D,2,246,12.3902,0.6332

Epoch: 247
Epoch: 247
ogbn-arxiv,CAGNET-1D,2,247,12.4402,0.6331

Epoch: 248
Epoch: 248
ogbn-arxiv,CAGNET-1D,2,248,12.4900,0.6327

Epoch: 249
Epoch: 249
ogbn-arxiv,CAGNET-1D,2,249,12.5401,0.6334

Epoch: 250
Epoch: 250
ogbn-arxiv,CAGNET-1D,2,250,12.5902,0.6334

Epoch: 251
Epoch: 251
ogbn-arxiv,CAGNET-1D,2,251,12.6400,0.6337

Epoch: 252
Epoch: 252
ogbn-arxiv,CAGNET-1D,2,252,12.6897,0.6339

Epoch: 253
Epoch: 253
ogbn-arxiv,CAGNET-1D,2,253,12.7397,0.6340

Epoch: 254
Epoch: 254
ogbn-arxiv,CAGNET-1D,2,254,12.7898,0.6337

Epoch: 255
Epoch: 255
ogbn-arxiv,CAGNET-1D,2,255,12.8418,0.6341

Epoch: 256
Epoch: 256
ogbn-arxiv,CAGNET-1D,2,256,12.8921,0.6345

Epoch: 257
Epoch: 257
ogbn-arxiv,CAGNET-1D,2,257,12.9424,0.6336

Epoch: 258
Epoch: 258
ogbn-arxiv,CAGNET-1D,2,258,12.9923,0.6343

Epoch: 259
Epoch: 259
ogbn-arxiv,CAGNET-1D,2,259,13.0418,0.6352

Epoch: 260Epoch: 260

ogbn-arxiv,CAGNET-1D,2,260,13.0914,0.6344

Epoch: 261
Epoch: 261
ogbn-arxiv,CAGNET-1D,2,261,13.1412,0.6343

Epoch: 262
Epoch: 262
ogbn-arxiv,CAGNET-1D,2,262,13.1915,0.6352

Epoch: 263
Epoch: 263
ogbn-arxiv,CAGNET-1D,2,263,13.2411,0.6354

Epoch: 264
Epoch: 264
ogbn-arxiv,CAGNET-1D,2,264,13.2906,0.6340

Epoch: 265
Epoch: 265
ogbn-arxiv,CAGNET-1D,2,265,13.3403,0.6348

Epoch: 266
Epoch: 266
ogbn-arxiv,CAGNET-1D,2,266,13.3907,0.6357

Epoch: 267
Epoch: 267
ogbn-arxiv,CAGNET-1D,2,267,13.4405,0.6354

Epoch: 268
Epoch: 268
ogbn-arxiv,CAGNET-1D,2,268,13.4903,0.6350

Epoch: 269Epoch: 269

ogbn-arxiv,CAGNET-1D,2,269,13.5402,0.6356

Epoch: 270
Epoch: 270
ogbn-arxiv,CAGNET-1D,2,270,13.5903,0.6361

Epoch: 271
Epoch: 271
ogbn-arxiv,CAGNET-1D,2,271,13.6402,0.6342

Epoch: 272
Epoch: 272
ogbn-arxiv,CAGNET-1D,2,272,13.6902,0.6358

Epoch: 273
Epoch: 273
ogbn-arxiv,CAGNET-1D,2,273,13.7427,0.6362

Epoch: 274
Epoch: 274
ogbn-arxiv,CAGNET-1D,2,274,13.7932,0.6354

Epoch: 275
Epoch: 275
ogbn-arxiv,CAGNET-1D,2,275,13.8431,0.6353

Epoch: 276
Epoch: 276
ogbn-arxiv,CAGNET-1D,2,276,13.8929,0.6364

Epoch: 277Epoch: 277

ogbn-arxiv,CAGNET-1D,2,277,13.9431,0.6356

Epoch: 278Epoch: 278

ogbn-arxiv,CAGNET-1D,2,278,13.9930,0.6355

Epoch: 279
Epoch: 279
ogbn-arxiv,CAGNET-1D,2,279,14.0427,0.6365

Epoch: 280
Epoch: 280
ogbn-arxiv,CAGNET-1D,2,280,14.0927,0.6361

Epoch: 281
Epoch: 281
ogbn-arxiv,CAGNET-1D,2,281,14.1428,0.6365

Epoch: 282
Epoch: 282
ogbn-arxiv,CAGNET-1D,2,282,14.1927,0.6367

Epoch: 283
Epoch: 283
ogbn-arxiv,CAGNET-1D,2,283,14.2424,0.6365

Epoch: 284Epoch: 284

ogbn-arxiv,CAGNET-1D,2,284,14.2920,0.6373

Epoch: 285
Epoch: 285
ogbn-arxiv,CAGNET-1D,2,285,14.3417,0.6371

Epoch: 286Epoch: 286

ogbn-arxiv,CAGNET-1D,2,286,14.3917,0.6369

Epoch: 287
Epoch: 287
ogbn-arxiv,CAGNET-1D,2,287,14.4415,0.6368

Epoch: 288
Epoch: 288
ogbn-arxiv,CAGNET-1D,2,288,14.4918,0.6383

Epoch: 289
Epoch: 289
ogbn-arxiv,CAGNET-1D,2,289,14.5416,0.6371

Epoch: 290
Epoch: 290
ogbn-arxiv,CAGNET-1D,2,290,14.5914,0.6369

Epoch: 291
Epoch: 291
ogbn-arxiv,CAGNET-1D,2,291,14.6415,0.6387

Epoch: 292
Epoch: 292
ogbn-arxiv,CAGNET-1D,2,292,14.6957,0.6380

Epoch: 293
Epoch: 293
ogbn-arxiv,CAGNET-1D,2,293,14.7459,0.6374

Epoch: 294
Epoch: 294
ogbn-arxiv,CAGNET-1D,2,294,14.7957,0.6393

Epoch: 295
Epoch: 295
ogbn-arxiv,CAGNET-1D,2,295,14.8458,0.6382

Epoch: 296
Epoch: 296
ogbn-arxiv,CAGNET-1D,2,296,14.8957,0.6377

Epoch: 297
Epoch: 297
ogbn-arxiv,CAGNET-1D,2,297,14.9457,0.6399

Epoch: 298
Epoch: 298
ogbn-arxiv,CAGNET-1D,2,298,14.9955,0.6382

Epoch: 299
Epoch: 299
ogbn-arxiv,CAGNET-1D,2,299,15.0454,0.6382

Epoch: 300
Epoch: 300
ogbn-arxiv,CAGNET-1D,2,300,15.0956,0.6400

Epoch: 301
Epoch: 301
ogbn-arxiv,CAGNET-1D,2,301,15.1461,0.6385

Epoch: 302
Epoch: 302
ogbn-arxiv,CAGNET-1D,2,302,15.1957,0.6381

Epoch: 303
Epoch: 303
ogbn-arxiv,CAGNET-1D,2,303,15.2456,0.6396

Epoch: 304
Epoch: 304
ogbn-arxiv,CAGNET-1D,2,304,15.2956,0.6388

Epoch: 305
Epoch: 305
ogbn-arxiv,CAGNET-1D,2,305,15.3455,0.6385

Epoch: 306
Epoch: 306
ogbn-arxiv,CAGNET-1D,2,306,15.3951,0.6403

Epoch: 307
Epoch: 307
ogbn-arxiv,CAGNET-1D,2,307,15.4456,0.6393

Epoch: 308
Epoch: 308
ogbn-arxiv,CAGNET-1D,2,308,15.4964,0.6386

Epoch: 309
Epoch: 309
ogbn-arxiv,CAGNET-1D,2,309,15.5470,0.6407

Epoch: 310
Epoch: 310
ogbn-arxiv,CAGNET-1D,2,310,15.5994,0.6392

Epoch: 311
Epoch: 311
ogbn-arxiv,CAGNET-1D,2,311,15.6506,0.6402

Epoch: 312
Epoch: 312
ogbn-arxiv,CAGNET-1D,2,312,15.7004,0.6407

Epoch: 313
Epoch: 313
ogbn-arxiv,CAGNET-1D,2,313,15.7503,0.6398

Epoch: 314
Epoch: 314
ogbn-arxiv,CAGNET-1D,2,314,15.8005,0.6405

Epoch: 315
Epoch: 315
ogbn-arxiv,CAGNET-1D,2,315,15.8506,0.6410

Epoch: 316
Epoch: 316
ogbn-arxiv,CAGNET-1D,2,316,15.9002,0.6403

Epoch: 317
Epoch: 317
ogbn-arxiv,CAGNET-1D,2,317,15.9503,0.6409

Epoch: 318
Epoch: 318
ogbn-arxiv,CAGNET-1D,2,318,15.9999,0.6408

Epoch: 319
Epoch: 319
ogbn-arxiv,CAGNET-1D,2,319,16.0496,0.6408

Epoch: 320
Epoch: 320
ogbn-arxiv,CAGNET-1D,2,320,16.0995,0.6415

Epoch: 321
Epoch: 321
ogbn-arxiv,CAGNET-1D,2,321,16.1493,0.6411

Epoch: 322
Epoch: 322
ogbn-arxiv,CAGNET-1D,2,322,16.1994,0.6408

Epoch: 323
Epoch: 323
ogbn-arxiv,CAGNET-1D,2,323,16.2492,0.6414

Epoch: 324
Epoch: 324
ogbn-arxiv,CAGNET-1D,2,324,16.2991,0.6414

Epoch: 325
Epoch: 325
ogbn-arxiv,CAGNET-1D,2,325,16.3494,0.6412

Epoch: 326
Epoch: 326
ogbn-arxiv,CAGNET-1D,2,326,16.3994,0.6417

Epoch: 327
Epoch: 327
ogbn-arxiv,CAGNET-1D,2,327,16.4493,0.6420

Epoch: 328
Epoch: 328
ogbn-arxiv,CAGNET-1D,2,328,16.4992,0.6414

Epoch: 329
Epoch: 329
ogbn-arxiv,CAGNET-1D,2,329,16.5535,0.6418

Epoch: 330
Epoch: 330
ogbn-arxiv,CAGNET-1D,2,330,16.6039,0.6416

Epoch: 331
Epoch: 331
ogbn-arxiv,CAGNET-1D,2,331,16.6539,0.6414

Epoch: 332
Epoch: 332
ogbn-arxiv,CAGNET-1D,2,332,16.7042,0.6419

Epoch: 333
Epoch: 333
ogbn-arxiv,CAGNET-1D,2,333,16.7545,0.6419

Epoch: 334
Epoch: 334
ogbn-arxiv,CAGNET-1D,2,334,16.8043,0.6420

Epoch: 335
Epoch: 335
ogbn-arxiv,CAGNET-1D,2,335,16.8542,0.6419

Epoch: 336Epoch: 336

ogbn-arxiv,CAGNET-1D,2,336,16.9043,0.6416

Epoch: 337
Epoch: 337
ogbn-arxiv,CAGNET-1D,2,337,16.9544,0.6423

Epoch: 338
Epoch: 338
ogbn-arxiv,CAGNET-1D,2,338,17.0046,0.6422

Epoch: 339
Epoch: 339
ogbn-arxiv,CAGNET-1D,2,339,17.0546,0.6424

Epoch: 340
Epoch: 340
ogbn-arxiv,CAGNET-1D,2,340,17.1043,0.6424

Epoch: 341
Epoch: 341
ogbn-arxiv,CAGNET-1D,2,341,17.1547,0.6425

Epoch: 342
Epoch: 342
ogbn-arxiv,CAGNET-1D,2,342,17.2045,0.6425

Epoch: 343
Epoch: 343
ogbn-arxiv,CAGNET-1D,2,343,17.2547,0.6425

Epoch: 344
Epoch: 344
ogbn-arxiv,CAGNET-1D,2,344,17.3054,0.6428

Epoch: 345
Epoch: 345
ogbn-arxiv,CAGNET-1D,2,345,17.3562,0.6422

Epoch: 346
Epoch: 346
ogbn-arxiv,CAGNET-1D,2,346,17.4062,0.6429

Epoch: 347
Epoch: 347
ogbn-arxiv,CAGNET-1D,2,347,17.4622,0.6428

Epoch: 348
Epoch: 348
ogbn-arxiv,CAGNET-1D,2,348,17.5131,0.6431

Epoch: 349
Epoch: 349
ogbn-arxiv,CAGNET-1D,2,349,17.5634,0.6429

Epoch: 350
Epoch: 350
ogbn-arxiv,CAGNET-1D,2,350,17.6136,0.6431

Epoch: 351
Epoch: 351
ogbn-arxiv,CAGNET-1D,2,351,17.6638,0.6432

Epoch: 352
Epoch: 352
ogbn-arxiv,CAGNET-1D,2,352,17.7138,0.6429

Epoch: 353
Epoch: 353
ogbn-arxiv,CAGNET-1D,2,353,17.7636,0.6435

Epoch: 354
Epoch: 354
ogbn-arxiv,CAGNET-1D,2,354,17.8131,0.6437

Epoch: 355
Epoch: 355
ogbn-arxiv,CAGNET-1D,2,355,17.8632,0.6434

Epoch: 356
Epoch: 356
ogbn-arxiv,CAGNET-1D,2,356,17.9133,0.6436

Epoch: 357
Epoch: 357
ogbn-arxiv,CAGNET-1D,2,357,17.9633,0.6438

Epoch: 358
Epoch: 358
ogbn-arxiv,CAGNET-1D,2,358,18.0132,0.6441

Epoch: 359
Epoch: 359
ogbn-arxiv,CAGNET-1D,2,359,18.0632,0.6440

Epoch: 360
Epoch: 360
ogbn-arxiv,CAGNET-1D,2,360,18.1132,0.6440

Epoch: 361
Epoch: 361
ogbn-arxiv,CAGNET-1D,2,361,18.1634,0.6440

Epoch: 362Epoch: 362

ogbn-arxiv,CAGNET-1D,2,362,18.2129,0.6438

Epoch: 363
Epoch: 363
ogbn-arxiv,CAGNET-1D,2,363,18.2624,0.6443

Epoch: 364
Epoch: 364
ogbn-arxiv,CAGNET-1D,2,364,18.3124,0.6441

Epoch: 365
Epoch: 365
ogbn-arxiv,CAGNET-1D,2,365,18.3628,0.6443

Epoch: 366
Epoch: 366
ogbn-arxiv,CAGNET-1D,2,366,18.4177,0.6447

Epoch: 367
Epoch: 367
ogbn-arxiv,CAGNET-1D,2,367,18.4680,0.6429

Epoch: 368
Epoch: 368
ogbn-arxiv,CAGNET-1D,2,368,18.5177,0.6458

Epoch: 369
Epoch: 369
ogbn-arxiv,CAGNET-1D,2,369,18.5676,0.6433

Epoch: 370
Epoch: 370
ogbn-arxiv,CAGNET-1D,2,370,18.6176,0.6448

Epoch: 371
Epoch: 371
ogbn-arxiv,CAGNET-1D,2,371,18.6677,0.6446

Epoch: 372
Epoch: 372
ogbn-arxiv,CAGNET-1D,2,372,18.7174,0.6436

Epoch: 373
Epoch: 373
ogbn-arxiv,CAGNET-1D,2,373,18.7669,0.6454

Epoch: 374
Epoch: 374
ogbn-arxiv,CAGNET-1D,2,374,18.8169,0.6441

Epoch: 375
Epoch: 375
ogbn-arxiv,CAGNET-1D,2,375,18.8671,0.6448

Epoch: 376
Epoch: 376
ogbn-arxiv,CAGNET-1D,2,376,18.9171,0.6451

Epoch: 377
Epoch: 377
ogbn-arxiv,CAGNET-1D,2,377,18.9669,0.6439

Epoch: 378
Epoch: 378
ogbn-arxiv,CAGNET-1D,2,378,19.0169,0.6458

Epoch: 379
Epoch: 379
ogbn-arxiv,CAGNET-1D,2,379,19.0671,0.6444

Epoch: 380
Epoch: 380
ogbn-arxiv,CAGNET-1D,2,380,19.1170,0.6452

Epoch: 381
Epoch: 381
ogbn-arxiv,CAGNET-1D,2,381,19.1665,0.6452

Epoch: 382
Epoch: 382
ogbn-arxiv,CAGNET-1D,2,382,19.2164,0.6449

Epoch: 383
Epoch: 383
ogbn-arxiv,CAGNET-1D,2,383,19.2665,0.6455

Epoch: 384
Epoch: 384
ogbn-arxiv,CAGNET-1D,2,384,19.3214,0.6456

Epoch: 385
Epoch: 385
ogbn-arxiv,CAGNET-1D,2,385,19.3715,0.6449

Epoch: 386
Epoch: 386
ogbn-arxiv,CAGNET-1D,2,386,19.4215,0.6457

Epoch: 387
Epoch: 387
ogbn-arxiv,CAGNET-1D,2,387,19.4716,0.6454

Epoch: 388
Epoch: 388
ogbn-arxiv,CAGNET-1D,2,388,19.5214,0.6458

Epoch: 389
Epoch: 389
ogbn-arxiv,CAGNET-1D,2,389,19.5710,0.6462

Epoch: 390
Epoch: 390
ogbn-arxiv,CAGNET-1D,2,390,19.6209,0.6454

Epoch: 391
Epoch: 391
ogbn-arxiv,CAGNET-1D,2,391,19.6706,0.6460

Epoch: 392
Epoch: 392
ogbn-arxiv,CAGNET-1D,2,392,19.7207,0.6458

Epoch: 393
Epoch: 393
ogbn-arxiv,CAGNET-1D,2,393,19.7706,0.6464

Epoch: 394
Epoch: 394
ogbn-arxiv,CAGNET-1D,2,394,19.8207,0.6458

Epoch: 395
Epoch: 395
ogbn-arxiv,CAGNET-1D,2,395,19.8707,0.6462

Epoch: 396
Epoch: 396
ogbn-arxiv,CAGNET-1D,2,396,19.9211,0.6458

Epoch: 397Epoch: 397

ogbn-arxiv,CAGNET-1D,2,397,19.9711,0.6465

Epoch: 398
Epoch: 398
ogbn-arxiv,CAGNET-1D,2,398,20.0212,0.6461

Epoch: 399
Epoch: 399
ogbn-arxiv,CAGNET-1D,2,399,20.0711,0.6464

Epoch: 400
Epoch: 400
ogbn-arxiv,CAGNET-1D,2,400,20.1208,0.6467

Epoch: 401
Epoch: 401
ogbn-arxiv,CAGNET-1D,2,401,20.1710,0.6460

Epoch: 402
Epoch: 402
ogbn-arxiv,CAGNET-1D,2,402,20.2210,0.6465

Epoch: 403
Epoch: 403
ogbn-arxiv,CAGNET-1D,2,403,20.2735,0.6457

Epoch: 404
Epoch: 404
ogbn-arxiv,CAGNET-1D,2,404,20.3236,0.6472

Epoch: 405
Epoch: 405
ogbn-arxiv,CAGNET-1D,2,405,20.3735,0.6457

Epoch: 406
Epoch: 406
ogbn-arxiv,CAGNET-1D,2,406,20.4235,0.6474

Epoch: 407
Epoch: 407
ogbn-arxiv,CAGNET-1D,2,407,20.4734,0.6457

Epoch: 408
Epoch: 408
ogbn-arxiv,CAGNET-1D,2,408,20.5235,0.6467

Epoch: 409
Epoch: 409
ogbn-arxiv,CAGNET-1D,2,409,20.5746,0.6468

Epoch: 410
Epoch: 410
ogbn-arxiv,CAGNET-1D,2,410,20.6256,0.6461

Epoch: 411
Epoch: 411
ogbn-arxiv,CAGNET-1D,2,411,20.6763,0.6473

Epoch: 412
Epoch: 412
ogbn-arxiv,CAGNET-1D,2,412,20.7267,0.6453

Epoch: 413
Epoch: 413
ogbn-arxiv,CAGNET-1D,2,413,20.7768,0.6480

Epoch: 414
Epoch: 414
ogbn-arxiv,CAGNET-1D,2,414,20.8267,0.6458

Epoch: 415
Epoch: 415
ogbn-arxiv,CAGNET-1D,2,415,20.8767,0.6472

Epoch: 416
Epoch: 416
ogbn-arxiv,CAGNET-1D,2,416,20.9264,0.6469

Epoch: 417
Epoch: 417
ogbn-arxiv,CAGNET-1D,2,417,20.9761,0.6458

Epoch: 418
Epoch: 418
ogbn-arxiv,CAGNET-1D,2,418,21.0265,0.6481

Epoch: 419
Epoch: 419
ogbn-arxiv,CAGNET-1D,2,419,21.0767,0.6457

Epoch: 420
Epoch: 420
ogbn-arxiv,CAGNET-1D,2,420,21.1265,0.6481

Epoch: 421
Epoch: 421
ogbn-arxiv,CAGNET-1D,2,421,21.1810,0.6463

Epoch: 422
Epoch: 422
ogbn-arxiv,CAGNET-1D,2,422,21.2311,0.6475

Epoch: 423
Epoch: 423
ogbn-arxiv,CAGNET-1D,2,423,21.2812,0.6474

Epoch: 424Epoch: 424

ogbn-arxiv,CAGNET-1D,2,424,21.3314,0.6470

Epoch: 425Epoch: 425

ogbn-arxiv,CAGNET-1D,2,425,21.3816,0.6480

Epoch: 426
Epoch: 426
ogbn-arxiv,CAGNET-1D,2,426,21.4315,0.6467

Epoch: 427
Epoch: 427
ogbn-arxiv,CAGNET-1D,2,427,21.4812,0.6479

Epoch: 428
Epoch: 428
ogbn-arxiv,CAGNET-1D,2,428,21.5313,0.6471

Epoch: 429
Epoch: 429
ogbn-arxiv,CAGNET-1D,2,429,21.5816,0.6476

Epoch: 430
Epoch: 430
ogbn-arxiv,CAGNET-1D,2,430,21.6314,0.6476

Epoch: 431
Epoch: 431
ogbn-arxiv,CAGNET-1D,2,431,21.6813,0.6473

Epoch: 432Epoch: 432

ogbn-arxiv,CAGNET-1D,2,432,21.7313,0.6477

Epoch: 433
Epoch: 433
ogbn-arxiv,CAGNET-1D,2,433,21.7810,0.6480

Epoch: 434
Epoch: 434
ogbn-arxiv,CAGNET-1D,2,434,21.8308,0.6475

Epoch: 435
Epoch: 435
ogbn-arxiv,CAGNET-1D,2,435,21.8808,0.6480

Epoch: 436
Epoch: 436
ogbn-arxiv,CAGNET-1D,2,436,21.9309,0.6487

Epoch: 437
Epoch: 437
ogbn-arxiv,CAGNET-1D,2,437,21.9809,0.6466

Epoch: 438
Epoch: 438
ogbn-arxiv,CAGNET-1D,2,438,22.0309,0.6501

Epoch: 439
Epoch: 439
ogbn-arxiv,CAGNET-1D,2,439,22.0806,0.6468

Epoch: 440Epoch: 440

ogbn-arxiv,CAGNET-1D,2,440,22.1331,0.6490

Epoch: 441
Epoch: 441
ogbn-arxiv,CAGNET-1D,2,441,22.1828,0.6482

Epoch: 442
Epoch: 442
ogbn-arxiv,CAGNET-1D,2,442,22.2326,0.6479

Epoch: 443
Epoch: 443
ogbn-arxiv,CAGNET-1D,2,443,22.2824,0.6500

Epoch: 444
Epoch: 444
ogbn-arxiv,CAGNET-1D,2,444,22.3323,0.6474

Epoch: 445
Epoch: 445
ogbn-arxiv,CAGNET-1D,2,445,22.3821,0.6502

Epoch: 446
Epoch: 446
ogbn-arxiv,CAGNET-1D,2,446,22.4318,0.6478

Epoch: 447
Epoch: 447
ogbn-arxiv,CAGNET-1D,2,447,22.4816,0.6491

Epoch: 448
Epoch: 448
ogbn-arxiv,CAGNET-1D,2,448,22.5315,0.6488

Epoch: 449
Epoch: 449
ogbn-arxiv,CAGNET-1D,2,449,22.5812,0.6486

Epoch: 450
Epoch: 450
ogbn-arxiv,CAGNET-1D,2,450,22.6310,0.6498

Epoch: 451
Epoch: 451
ogbn-arxiv,CAGNET-1D,2,451,22.6804,0.6486

Epoch: 452
Epoch: 452
ogbn-arxiv,CAGNET-1D,2,452,22.7305,0.6495

Epoch: 453
Epoch: 453
ogbn-arxiv,CAGNET-1D,2,453,22.7806,0.6485

Epoch: 454
Epoch: 454
ogbn-arxiv,CAGNET-1D,2,454,22.8304,0.6497

Epoch: 455
Epoch: 455
ogbn-arxiv,CAGNET-1D,2,455,22.8803,0.6491

Epoch: 456Epoch: 456

ogbn-arxiv,CAGNET-1D,2,456,22.9298,0.6498

Epoch: 457
Epoch: 457
ogbn-arxiv,CAGNET-1D,2,457,22.9797,0.6489

Epoch: 458
Epoch: 458
ogbn-arxiv,CAGNET-1D,2,458,23.0349,0.6499

Epoch: 459
Epoch: 459
ogbn-arxiv,CAGNET-1D,2,459,23.0852,0.6487

Epoch: 460
Epoch: 460
ogbn-arxiv,CAGNET-1D,2,460,23.1353,0.6498

Epoch: 461
Epoch: 461
ogbn-arxiv,CAGNET-1D,2,461,23.1851,0.6495

Epoch: 462
Epoch: 462
ogbn-arxiv,CAGNET-1D,2,462,23.2349,0.6492

Epoch: 463
Epoch: 463
ogbn-arxiv,CAGNET-1D,2,463,23.2845,0.6507

Epoch: 464
Epoch: 464
ogbn-arxiv,CAGNET-1D,2,464,23.3344,0.6481

Epoch: 465
Epoch: 465
ogbn-arxiv,CAGNET-1D,2,465,23.3843,0.6517

Epoch: 466
Epoch: 466
ogbn-arxiv,CAGNET-1D,2,466,23.4344,0.6475

Epoch: 467
Epoch: 467
ogbn-arxiv,CAGNET-1D,2,467,23.4842,0.6517

Epoch: 468
Epoch: 468
ogbn-arxiv,CAGNET-1D,2,468,23.5341,0.6486

Epoch: 469
Epoch: 469
ogbn-arxiv,CAGNET-1D,2,469,23.5847,0.6505

Epoch: 470
Epoch: 470
ogbn-arxiv,CAGNET-1D,2,470,23.6357,0.6506

Epoch: 471
Epoch: 471
ogbn-arxiv,CAGNET-1D,2,471,23.6864,0.6485

Epoch: 472
Epoch: 472
ogbn-arxiv,CAGNET-1D,2,472,23.7368,0.6515

Epoch: 473
Epoch: 473
ogbn-arxiv,CAGNET-1D,2,473,23.7870,0.6485

Epoch: 474
Epoch: 474
ogbn-arxiv,CAGNET-1D,2,474,23.8371,0.6513

Epoch: 475
Epoch: 475
ogbn-arxiv,CAGNET-1D,2,475,23.8873,0.6502

Epoch: 476
Epoch: 476
ogbn-arxiv,CAGNET-1D,2,476,23.9404,0.6497

Epoch: 477
Epoch: 477
ogbn-arxiv,CAGNET-1D,2,477,23.9920,0.6513

Epoch: 478
Epoch: 478
ogbn-arxiv,CAGNET-1D,2,478,24.0418,0.6487

Epoch: 479
Epoch: 479
ogbn-arxiv,CAGNET-1D,2,479,24.0914,0.6520

Epoch: 480
Epoch: 480
ogbn-arxiv,CAGNET-1D,2,480,24.1413,0.6495

Epoch: 481
Epoch: 481
ogbn-arxiv,CAGNET-1D,2,481,24.1916,0.6515

Epoch: 482
Epoch: 482
ogbn-arxiv,CAGNET-1D,2,482,24.2415,0.6503

Epoch: 483
Epoch: 483
ogbn-arxiv,CAGNET-1D,2,483,24.2916,0.6507

Epoch: 484Epoch: 484

ogbn-arxiv,CAGNET-1D,2,484,24.3415,0.6514

Epoch: 485
Epoch: 485
ogbn-arxiv,CAGNET-1D,2,485,24.3916,0.6501

Epoch: 486
Epoch: 486
ogbn-arxiv,CAGNET-1D,2,486,24.4415,0.6518

Epoch: 487
Epoch: 487
ogbn-arxiv,CAGNET-1D,2,487,24.4912,0.6494

Epoch: 488
Epoch: 488
ogbn-arxiv,CAGNET-1D,2,488,24.5410,0.6520

Epoch: 489
Epoch: 489
ogbn-arxiv,CAGNET-1D,2,489,24.5913,0.6507

Epoch: 490
Epoch: 490
ogbn-arxiv,CAGNET-1D,2,490,24.6417,0.6515

Epoch: 491
Epoch: 491
ogbn-arxiv,CAGNET-1D,2,491,24.6915,0.6507

Epoch: 492
Epoch: 492
ogbn-arxiv,CAGNET-1D,2,492,24.7412,0.6516

Epoch: 493
Epoch: 493
ogbn-arxiv,CAGNET-1D,2,493,24.7908,0.6509

Epoch: 494
Epoch: 494
ogbn-arxiv,CAGNET-1D,2,494,24.8408,0.6516

Epoch: 495
Epoch: 495
ogbn-arxiv,CAGNET-1D,2,495,24.8945,0.6513

Epoch: 496
Epoch: 496
ogbn-arxiv,CAGNET-1D,2,496,24.9445,0.6514

Epoch: 497Epoch: 497

ogbn-arxiv,CAGNET-1D,2,497,24.9946,0.6509

Epoch: 498Epoch: 498

ogbn-arxiv,CAGNET-1D,2,498,25.0448,0.6519

Epoch: 499
Epoch: 499
ogbn-arxiv,CAGNET-1D,2,499,25.0947,0.6513

Epoch: 500
Epoch: 500
ogbn-arxiv,CAGNET-1D,2,500,25.1444,0.6520

Epoch: 501
Epoch: 501
ogbn-arxiv,CAGNET-1D,2,501,25.1946,0.6514

Epoch: 502
Epoch: 502
ogbn-arxiv,CAGNET-1D,2,502,25.2447,0.6522

Epoch: 503
Epoch: 503
ogbn-arxiv,CAGNET-1D,2,503,25.2945,0.6513

Epoch: 504
Epoch: 504
ogbn-arxiv,CAGNET-1D,2,504,25.3442,0.6514

Epoch: 505
Epoch: 505
ogbn-arxiv,CAGNET-1D,2,505,25.3939,0.6525

Epoch: 506
Epoch: 506
ogbn-arxiv,CAGNET-1D,2,506,25.4442,0.6506

Epoch: 507
Epoch: 507
ogbn-arxiv,CAGNET-1D,2,507,25.4943,0.6538

Epoch: 508
Epoch: 508
ogbn-arxiv,CAGNET-1D,2,508,25.5440,0.6493

Epoch: 509
Epoch: 509
ogbn-arxiv,CAGNET-1D,2,509,25.5938,0.6542

Epoch: 510Epoch: 510

ogbn-arxiv,CAGNET-1D,2,510,25.6436,0.6501

Epoch: 511Epoch: 511

ogbn-arxiv,CAGNET-1D,2,511,25.6937,0.6523

Epoch: 512
Epoch: 512
ogbn-arxiv,CAGNET-1D,2,512,25.7439,0.6529

Epoch: 513
Epoch: 513
ogbn-arxiv,CAGNET-1D,2,513,25.7937,0.6508

Epoch: 514
Epoch: 514
ogbn-arxiv,CAGNET-1D,2,514,25.8456,0.6536

Epoch: 515
Epoch: 515
ogbn-arxiv,CAGNET-1D,2,515,25.8958,0.6509

Epoch: 516
Epoch: 516
ogbn-arxiv,CAGNET-1D,2,516,25.9458,0.6527

Epoch: 517
Epoch: 517
ogbn-arxiv,CAGNET-1D,2,517,25.9955,0.6522

Epoch: 518
Epoch: 518
ogbn-arxiv,CAGNET-1D,2,518,26.0454,0.6523

Epoch: 519
Epoch: 519
ogbn-arxiv,CAGNET-1D,2,519,26.0957,0.6529

Epoch: 520
Epoch: 520
ogbn-arxiv,CAGNET-1D,2,520,26.1455,0.6518

Epoch: 521
Epoch: 521
ogbn-arxiv,CAGNET-1D,2,521,26.1955,0.6530

Epoch: 522
Epoch: 522
ogbn-arxiv,CAGNET-1D,2,522,26.2463,0.6521

Epoch: 523
Epoch: 523
ogbn-arxiv,CAGNET-1D,2,523,26.2962,0.6527

Epoch: 524
Epoch: 524
ogbn-arxiv,CAGNET-1D,2,524,26.3461,0.6530

Epoch: 525
Epoch: 525
ogbn-arxiv,CAGNET-1D,2,525,26.3967,0.6518

Epoch: 526
Epoch: 526
ogbn-arxiv,CAGNET-1D,2,526,26.4474,0.6538

Epoch: 527
Epoch: 527
ogbn-arxiv,CAGNET-1D,2,527,26.4978,0.6515

Epoch: 528
Epoch: 528
ogbn-arxiv,CAGNET-1D,2,528,26.5478,0.6539

Epoch: 529
Epoch: 529
ogbn-arxiv,CAGNET-1D,2,529,26.5977,0.6515

Epoch: 530
Epoch: 530
ogbn-arxiv,CAGNET-1D,2,530,26.6477,0.6545

Epoch: 531
Epoch: 531
ogbn-arxiv,CAGNET-1D,2,531,26.6975,0.6515

Epoch: 532
Epoch: 532
ogbn-arxiv,CAGNET-1D,2,532,26.7522,0.6541

Epoch: 533
Epoch: 533
ogbn-arxiv,CAGNET-1D,2,533,26.8021,0.6521

Epoch: 534
Epoch: 534
ogbn-arxiv,CAGNET-1D,2,534,26.8518,0.6531

Epoch: 535
Epoch: 535
ogbn-arxiv,CAGNET-1D,2,535,26.9015,0.6538

Epoch: 536
Epoch: 536
ogbn-arxiv,CAGNET-1D,2,536,26.9517,0.6521

Epoch: 537
Epoch: 537
ogbn-arxiv,CAGNET-1D,2,537,27.0018,0.6545

Epoch: 538
Epoch: 538
ogbn-arxiv,CAGNET-1D,2,538,27.0517,0.6518

Epoch: 539
Epoch: 539
ogbn-arxiv,CAGNET-1D,2,539,27.1015,0.6547

Epoch: 540
Epoch: 540
ogbn-arxiv,CAGNET-1D,2,540,27.1514,0.6519

Epoch: 541
Epoch: 541
ogbn-arxiv,CAGNET-1D,2,541,27.2014,0.6541

Epoch: 542
Epoch: 542
ogbn-arxiv,CAGNET-1D,2,542,27.2514,0.6538

Epoch: 543
Epoch: 543
ogbn-arxiv,CAGNET-1D,2,543,27.3012,0.6527

Epoch: 544
Epoch: 544
ogbn-arxiv,CAGNET-1D,2,544,27.3517,0.6544

Epoch: 545
Epoch: 545
ogbn-arxiv,CAGNET-1D,2,545,27.4018,0.6523

Epoch: 546
Epoch: 546
ogbn-arxiv,CAGNET-1D,2,546,27.4516,0.6549

Epoch: 547
Epoch: 547
ogbn-arxiv,CAGNET-1D,2,547,27.5014,0.6525

Epoch: 548
Epoch: 548
ogbn-arxiv,CAGNET-1D,2,548,27.5512,0.6545

Epoch: 549
Epoch: 549
ogbn-arxiv,CAGNET-1D,2,549,27.6012,0.6529

Epoch: 550
Epoch: 550
ogbn-arxiv,CAGNET-1D,2,550,27.6514,0.6545

Epoch: 551
Epoch: 551
ogbn-arxiv,CAGNET-1D,2,551,27.7055,0.6534

Epoch: 552
Epoch: 552
ogbn-arxiv,CAGNET-1D,2,552,27.7555,0.6537

Epoch: 553
Epoch: 553
ogbn-arxiv,CAGNET-1D,2,553,27.8053,0.6546

Epoch: 554
Epoch: 554
ogbn-arxiv,CAGNET-1D,2,554,27.8551,0.6529

Epoch: 555
Epoch: 555
ogbn-arxiv,CAGNET-1D,2,555,27.9050,0.6547

Epoch: 556
Epoch: 556
ogbn-arxiv,CAGNET-1D,2,556,27.9549,0.6525

Epoch: 557
Epoch: 557
ogbn-arxiv,CAGNET-1D,2,557,28.0053,0.6555

Epoch: 558
Epoch: 558
ogbn-arxiv,CAGNET-1D,2,558,28.0557,0.6528

Epoch: 559
Epoch: 559
ogbn-arxiv,CAGNET-1D,2,559,28.1057,0.6553

Epoch: 560
Epoch: 560
ogbn-arxiv,CAGNET-1D,2,560,28.1555,0.6532

Epoch: 561Epoch: 561

ogbn-arxiv,CAGNET-1D,2,561,28.2054,0.6545

Epoch: 562
Epoch: 562
ogbn-arxiv,CAGNET-1D,2,562,28.2556,0.6537

Epoch: 563
Epoch: 563
ogbn-arxiv,CAGNET-1D,2,563,28.3055,0.6541

Epoch: 564
Epoch: 564
ogbn-arxiv,CAGNET-1D,2,564,28.3558,0.6545

Epoch: 565
Epoch: 565
ogbn-arxiv,CAGNET-1D,2,565,28.4056,0.6535

Epoch: 566
Epoch: 566
ogbn-arxiv,CAGNET-1D,2,566,28.4555,0.6549

Epoch: 567
Epoch: 567
ogbn-arxiv,CAGNET-1D,2,567,28.5055,0.6531

Epoch: 568
Epoch: 568
ogbn-arxiv,CAGNET-1D,2,568,28.5558,0.6555

Epoch: 569
Epoch: 569
ogbn-arxiv,CAGNET-1D,2,569,28.6107,0.6530

Epoch: 570
Epoch: 570
ogbn-arxiv,CAGNET-1D,2,570,28.6608,0.6558

Epoch: 571
Epoch: 571
ogbn-arxiv,CAGNET-1D,2,571,28.7105,0.6522

Epoch: 572Epoch: 572

ogbn-arxiv,CAGNET-1D,2,572,28.7601,0.6564

Epoch: 573Epoch: 573

ogbn-arxiv,CAGNET-1D,2,573,28.8099,0.6521

Epoch: 574
Epoch: 574
ogbn-arxiv,CAGNET-1D,2,574,28.8601,0.6560

Epoch: 575
Epoch: 575
ogbn-arxiv,CAGNET-1D,2,575,28.9101,0.6539

Epoch: 576
Epoch: 576
ogbn-arxiv,CAGNET-1D,2,576,28.9599,0.6540

Epoch: 577
Epoch: 577
ogbn-arxiv,CAGNET-1D,2,577,29.0104,0.6563

Epoch: 578
Epoch: 578
ogbn-arxiv,CAGNET-1D,2,578,29.0604,0.6525

Epoch: 579
Epoch: 579
ogbn-arxiv,CAGNET-1D,2,579,29.1103,0.6574

Epoch: 580
Epoch: 580
ogbn-arxiv,CAGNET-1D,2,580,29.1600,0.6523

Epoch: 581Epoch: 581

ogbn-arxiv,CAGNET-1D,2,581,29.2099,0.6571

Epoch: 582
Epoch: 582
ogbn-arxiv,CAGNET-1D,2,582,29.2603,0.6538

Epoch: 583
Epoch: 583
ogbn-arxiv,CAGNET-1D,2,583,29.3100,0.6546

Epoch: 584Epoch: 584

ogbn-arxiv,CAGNET-1D,2,584,29.3596,0.6563

Epoch: 585
Epoch: 585
ogbn-arxiv,CAGNET-1D,2,585,29.4095,0.6528

Epoch: 586
Epoch: 586
ogbn-arxiv,CAGNET-1D,2,586,29.4595,0.6577

Epoch: 587Epoch: 587

ogbn-arxiv,CAGNET-1D,2,587,29.5096,0.6523

Epoch: 588
Epoch: 588
ogbn-arxiv,CAGNET-1D,2,588,29.5651,0.6569

Epoch: 589
Epoch: 589
ogbn-arxiv,CAGNET-1D,2,589,29.6165,0.6538

Epoch: 590
Epoch: 590
ogbn-arxiv,CAGNET-1D,2,590,29.6666,0.6553

Epoch: 591
Epoch: 591
ogbn-arxiv,CAGNET-1D,2,591,29.7165,0.6555

Epoch: 592
Epoch: 592
ogbn-arxiv,CAGNET-1D,2,592,29.7666,0.6544

Epoch: 593
Epoch: 593
ogbn-arxiv,CAGNET-1D,2,593,29.8164,0.6554

Epoch: 594
Epoch: 594
ogbn-arxiv,CAGNET-1D,2,594,29.8660,0.6551

Epoch: 595Epoch: 595

ogbn-arxiv,CAGNET-1D,2,595,29.9161,0.6554

Epoch: 596
Epoch: 596
ogbn-arxiv,CAGNET-1D,2,596,29.9663,0.6548

Epoch: 597
Epoch: 597
ogbn-arxiv,CAGNET-1D,2,597,30.0160,0.6556

Epoch: 598
Epoch: 598
ogbn-arxiv,CAGNET-1D,2,598,30.0659,0.6545

Epoch: 599
Epoch: 599
ogbn-arxiv,CAGNET-1D,2,599,30.1160,0.6559

Epoch: 600
Epoch: 600
ogbn-arxiv,CAGNET-1D,2,600,30.1666,0.6543

Epoch: 601
Epoch: 601
ogbn-arxiv,CAGNET-1D,2,601,30.2165,0.6558

Epoch: 602
Epoch: 602
ogbn-arxiv,CAGNET-1D,2,602,30.2662,0.6545

Epoch: 603
Epoch: 603
ogbn-arxiv,CAGNET-1D,2,603,30.3161,0.6557

Epoch: 604Epoch: 604

ogbn-arxiv,CAGNET-1D,2,604,30.3660,0.6554

Epoch: 605
Epoch: 605
ogbn-arxiv,CAGNET-1D,2,605,30.4160,0.6550

Epoch: 606
Epoch: 606
ogbn-arxiv,CAGNET-1D,2,606,30.4692,0.6563

Epoch: 607
Epoch: 607
ogbn-arxiv,CAGNET-1D,2,607,30.5192,0.6536

Epoch: 608
Epoch: 608
ogbn-arxiv,CAGNET-1D,2,608,30.5690,0.6578

Epoch: 609
Epoch: 609
ogbn-arxiv,CAGNET-1D,2,609,30.6186,0.6529

Epoch: 610Epoch: 610

ogbn-arxiv,CAGNET-1D,2,610,30.6685,0.6573

Epoch: 611
Epoch: 611
ogbn-arxiv,CAGNET-1D,2,611,30.7187,0.6540

Epoch: 612
Epoch: 612
ogbn-arxiv,CAGNET-1D,2,612,30.7689,0.6567

Epoch: 613
Epoch: 613
ogbn-arxiv,CAGNET-1D,2,613,30.8184,0.6550

Epoch: 614
Epoch: 614
ogbn-arxiv,CAGNET-1D,2,614,30.8681,0.6554

Epoch: 615
Epoch: 615
ogbn-arxiv,CAGNET-1D,2,615,30.9183,0.6558

Epoch: 616
Epoch: 616
ogbn-arxiv,CAGNET-1D,2,616,30.9704,0.6549

Epoch: 617
Epoch: 617
ogbn-arxiv,CAGNET-1D,2,617,31.0218,0.6559

Epoch: 618
Epoch: 618
ogbn-arxiv,CAGNET-1D,2,618,31.0725,0.6555

Epoch: 619
Epoch: 619
ogbn-arxiv,CAGNET-1D,2,619,31.1227,0.6544

Epoch: 620
Epoch: 620
ogbn-arxiv,CAGNET-1D,2,620,31.1725,0.6571

Epoch: 621
Epoch: 621
ogbn-arxiv,CAGNET-1D,2,621,31.2226,0.6533

Epoch: 622
Epoch: 622
ogbn-arxiv,CAGNET-1D,2,622,31.2727,0.6575

Epoch: 623
Epoch: 623
ogbn-arxiv,CAGNET-1D,2,623,31.3223,0.6540

Epoch: 624
Epoch: 624
ogbn-arxiv,CAGNET-1D,2,624,31.3722,0.6571

Epoch: 625
Epoch: 625
ogbn-arxiv,CAGNET-1D,2,625,31.4238,0.6544

Epoch: 626Epoch: 626

ogbn-arxiv,CAGNET-1D,2,626,31.4737,0.6567

Epoch: 627
Epoch: 627
ogbn-arxiv,CAGNET-1D,2,627,31.5234,0.6548

Epoch: 628
Epoch: 628
ogbn-arxiv,CAGNET-1D,2,628,31.5732,0.6566

Epoch: 629
Epoch: 629
ogbn-arxiv,CAGNET-1D,2,629,31.6234,0.6557

Epoch: 630
Epoch: 630
ogbn-arxiv,CAGNET-1D,2,630,31.6735,0.6556

Epoch: 631
Epoch: 631
ogbn-arxiv,CAGNET-1D,2,631,31.7233,0.6564

Epoch: 632
Epoch: 632
ogbn-arxiv,CAGNET-1D,2,632,31.7734,0.6548

Epoch: 633
Epoch: 633
ogbn-arxiv,CAGNET-1D,2,633,31.8232,0.6574

Epoch: 634
Epoch: 634
ogbn-arxiv,CAGNET-1D,2,634,31.8732,0.6546

Epoch: 635
Epoch: 635
ogbn-arxiv,CAGNET-1D,2,635,31.9231,0.6576

Epoch: 636
Epoch: 636
ogbn-arxiv,CAGNET-1D,2,636,31.9728,0.6543

Epoch: 637Epoch: 637

ogbn-arxiv,CAGNET-1D,2,637,32.0228,0.6579

Epoch: 638
Epoch: 638
ogbn-arxiv,CAGNET-1D,2,638,32.0730,0.6535

Epoch: 639
Epoch: 639
ogbn-arxiv,CAGNET-1D,2,639,32.1228,0.6578

Epoch: 640
Epoch: 640
ogbn-arxiv,CAGNET-1D,2,640,32.1725,0.6545

Epoch: 641
Epoch: 641
ogbn-arxiv,CAGNET-1D,2,641,32.2220,0.6564

Epoch: 642Epoch: 642

ogbn-arxiv,CAGNET-1D,2,642,32.2722,0.6563

Epoch: 643Epoch: 643

ogbn-arxiv,CAGNET-1D,2,643,32.3269,0.6554

Epoch: 644
Epoch: 644
ogbn-arxiv,CAGNET-1D,2,644,32.3770,0.6566

Epoch: 645
Epoch: 645
ogbn-arxiv,CAGNET-1D,2,645,32.4271,0.6552

Epoch: 646
Epoch: 646
ogbn-arxiv,CAGNET-1D,2,646,32.4770,0.6566

Epoch: 647
Epoch: 647
ogbn-arxiv,CAGNET-1D,2,647,32.5265,0.6557

Epoch: 648
Epoch: 648
ogbn-arxiv,CAGNET-1D,2,648,32.5767,0.6561

Epoch: 649
Epoch: 649
ogbn-arxiv,CAGNET-1D,2,649,32.6270,0.6567

Epoch: 650
Epoch: 650
ogbn-arxiv,CAGNET-1D,2,650,32.6768,0.6555

Epoch: 651
Epoch: 651
ogbn-arxiv,CAGNET-1D,2,651,32.7263,0.6572

Epoch: 652
Epoch: 652
ogbn-arxiv,CAGNET-1D,2,652,32.7761,0.6550

Epoch: 653
Epoch: 653
ogbn-arxiv,CAGNET-1D,2,653,32.8261,0.6567

Epoch: 654
Epoch: 654
ogbn-arxiv,CAGNET-1D,2,654,32.8761,0.6561

Epoch: 655
Epoch: 655
ogbn-arxiv,CAGNET-1D,2,655,32.9270,0.6558

Epoch: 656
Epoch: 656
ogbn-arxiv,CAGNET-1D,2,656,32.9777,0.6572

Epoch: 657
Epoch: 657
ogbn-arxiv,CAGNET-1D,2,657,33.0283,0.6550

Epoch: 658
Epoch: 658
ogbn-arxiv,CAGNET-1D,2,658,33.0784,0.6574

Epoch: 659
Epoch: 659
ogbn-arxiv,CAGNET-1D,2,659,33.1284,0.6550

Epoch: 660
Epoch: 660
ogbn-arxiv,CAGNET-1D,2,660,33.1785,0.6568

Epoch: 661
Epoch: 661
ogbn-arxiv,CAGNET-1D,2,661,33.2281,0.6563

Epoch: 662
Epoch: 662
ogbn-arxiv,CAGNET-1D,2,662,33.2818,0.6557

Epoch: 663
Epoch: 663
ogbn-arxiv,CAGNET-1D,2,663,33.3336,0.6581

Epoch: 664
Epoch: 664
ogbn-arxiv,CAGNET-1D,2,664,33.3851,0.6541

Epoch: 665
Epoch: 665
ogbn-arxiv,CAGNET-1D,2,665,33.4360,0.6580

Epoch: 666Epoch: 666

ogbn-arxiv,CAGNET-1D,2,666,33.4859,0.6550

Epoch: 667
Epoch: 667
ogbn-arxiv,CAGNET-1D,2,667,33.5363,0.6574

Epoch: 668
Epoch: 668
ogbn-arxiv,CAGNET-1D,2,668,33.5865,0.6564

Epoch: 669
Epoch: 669
ogbn-arxiv,CAGNET-1D,2,669,33.6362,0.6564

Epoch: 670Epoch: 670

ogbn-arxiv,CAGNET-1D,2,670,33.6857,0.6571

Epoch: 671
Epoch: 671
ogbn-arxiv,CAGNET-1D,2,671,33.7354,0.6561

Epoch: 672
Epoch: 672
ogbn-arxiv,CAGNET-1D,2,672,33.7855,0.6561

Epoch: 673
Epoch: 673
ogbn-arxiv,CAGNET-1D,2,673,33.8355,0.6576

Epoch: 674
Epoch: 674
ogbn-arxiv,CAGNET-1D,2,674,33.8854,0.6548

Epoch: 675
Epoch: 675
ogbn-arxiv,CAGNET-1D,2,675,33.9350,0.6580

Epoch: 676
Epoch: 676
ogbn-arxiv,CAGNET-1D,2,676,33.9848,0.6548

Epoch: 677
Epoch: 677
ogbn-arxiv,CAGNET-1D,2,677,34.0357,0.6579

Epoch: 678
Epoch: 678
ogbn-arxiv,CAGNET-1D,2,678,34.0857,0.6558

Epoch: 679
Epoch: 679
ogbn-arxiv,CAGNET-1D,2,679,34.1356,0.6568

Epoch: 680
Epoch: 680
ogbn-arxiv,CAGNET-1D,2,680,34.1900,0.6573

Epoch: 681
Epoch: 681
ogbn-arxiv,CAGNET-1D,2,681,34.2402,0.6552

Epoch: 682
Epoch: 682
ogbn-arxiv,CAGNET-1D,2,682,34.2901,0.6585

Epoch: 683
Epoch: 683
ogbn-arxiv,CAGNET-1D,2,683,34.3399,0.6542

Epoch: 684
Epoch: 684
ogbn-arxiv,CAGNET-1D,2,684,34.3898,0.6591

Epoch: 685Epoch: 685

ogbn-arxiv,CAGNET-1D,2,685,34.4399,0.6548

Epoch: 686
Epoch: 686
ogbn-arxiv,CAGNET-1D,2,686,34.4901,0.6581

Epoch: 687
Epoch: 687
ogbn-arxiv,CAGNET-1D,2,687,34.5404,0.6556

Epoch: 688
Epoch: 688
ogbn-arxiv,CAGNET-1D,2,688,34.5904,0.6576

Epoch: 689
Epoch: 689
ogbn-arxiv,CAGNET-1D,2,689,34.6404,0.6567

Epoch: 690
Epoch: 690
ogbn-arxiv,CAGNET-1D,2,690,34.6905,0.6565

Epoch: 691
Epoch: 691
ogbn-arxiv,CAGNET-1D,2,691,34.7405,0.6575

Epoch: 692
Epoch: 692
ogbn-arxiv,CAGNET-1D,2,692,34.7907,0.6560

Epoch: 693
Epoch: 693
ogbn-arxiv,CAGNET-1D,2,693,34.8403,0.6572

Epoch: 694
Epoch: 694
ogbn-arxiv,CAGNET-1D,2,694,34.8905,0.6560

Epoch: 695
Epoch: 695
ogbn-arxiv,CAGNET-1D,2,695,34.9403,0.6574

Epoch: 696
Epoch: 696
ogbn-arxiv,CAGNET-1D,2,696,34.9902,0.6566

Epoch: 697Epoch: 697

ogbn-arxiv,CAGNET-1D,2,697,35.0400,0.6568

Epoch: 698
Epoch: 698
ogbn-arxiv,CAGNET-1D,2,698,35.0900,0.6573

Epoch: 699
Epoch: 699
ogbn-arxiv,CAGNET-1D,2,699,35.1444,0.6553

Epoch: 700
Epoch: 700
ogbn-arxiv,CAGNET-1D,2,700,35.1941,0.6594

Epoch: 701
Epoch: 701
ogbn-arxiv,CAGNET-1D,2,701,35.2443,0.6544

Epoch: 702Epoch: 702

ogbn-arxiv,CAGNET-1D,2,702,35.2941,0.6603

Epoch: 703
Epoch: 703
ogbn-arxiv,CAGNET-1D,2,703,35.3439,0.6516

Epoch: 704
Epoch: 704
ogbn-arxiv,CAGNET-1D,2,704,35.3936,0.6623

Epoch: 705
Epoch: 705
ogbn-arxiv,CAGNET-1D,2,705,35.4434,0.6514

Epoch: 706
Epoch: 706
ogbn-arxiv,CAGNET-1D,2,706,35.4937,0.6607

Epoch: 707
Epoch: 707
ogbn-arxiv,CAGNET-1D,2,707,35.5437,0.6548

Epoch: 708
Epoch: 708
ogbn-arxiv,CAGNET-1D,2,708,35.5940,0.6574

Epoch: 709
Epoch: 709
ogbn-arxiv,CAGNET-1D,2,709,35.6440,0.6582

Epoch: 710
Epoch: 710
ogbn-arxiv,CAGNET-1D,2,710,35.6940,0.6548

Epoch: 711
Epoch: 711
ogbn-arxiv,CAGNET-1D,2,711,35.7438,0.6595

Epoch: 712
Epoch: 712
ogbn-arxiv,CAGNET-1D,2,712,35.7939,0.6548

Epoch: 713
Epoch: 713
ogbn-arxiv,CAGNET-1D,2,713,35.8441,0.6587

Epoch: 714
Epoch: 714
ogbn-arxiv,CAGNET-1D,2,714,35.8940,0.6564

Epoch: 715
Epoch: 715
ogbn-arxiv,CAGNET-1D,2,715,35.9437,0.6570

Epoch: 716
Epoch: 716
ogbn-arxiv,CAGNET-1D,2,716,35.9938,0.6574

Epoch: 717
Epoch: 717
ogbn-arxiv,CAGNET-1D,2,717,36.0495,0.6558

Epoch: 718
Epoch: 718
ogbn-arxiv,CAGNET-1D,2,718,36.0998,0.6589

Epoch: 719
Epoch: 719
ogbn-arxiv,CAGNET-1D,2,719,36.1496,0.6555

Epoch: 720
Epoch: 720
ogbn-arxiv,CAGNET-1D,2,720,36.1995,0.6592

Epoch: 721
Epoch: 721
ogbn-arxiv,CAGNET-1D,2,721,36.2494,0.6562

Epoch: 722
Epoch: 722
ogbn-arxiv,CAGNET-1D,2,722,36.2994,0.6572

Epoch: 723
Epoch: 723
ogbn-arxiv,CAGNET-1D,2,723,36.3497,0.6584

Epoch: 724
Epoch: 724
ogbn-arxiv,CAGNET-1D,2,724,36.4002,0.6557

Epoch: 725
Epoch: 725
ogbn-arxiv,CAGNET-1D,2,725,36.4501,0.6590

Epoch: 726
Epoch: 726
ogbn-arxiv,CAGNET-1D,2,726,36.5004,0.6561

Epoch: 727
Epoch: 727
ogbn-arxiv,CAGNET-1D,2,727,36.5502,0.6586

Epoch: 728
Epoch: 728
ogbn-arxiv,CAGNET-1D,2,728,36.5999,0.6559

Epoch: 729
Epoch: 729
ogbn-arxiv,CAGNET-1D,2,729,36.6498,0.6591

Epoch: 730
Epoch: 730
ogbn-arxiv,CAGNET-1D,2,730,36.7001,0.6563

Epoch: 731
Epoch: 731
ogbn-arxiv,CAGNET-1D,2,731,36.7499,0.6581

Epoch: 732
Epoch: 732
ogbn-arxiv,CAGNET-1D,2,732,36.7996,0.6566

Epoch: 733
Epoch: 733
ogbn-arxiv,CAGNET-1D,2,733,36.8492,0.6583

Epoch: 734
Epoch: 734
ogbn-arxiv,CAGNET-1D,2,734,36.8995,0.6571

Epoch: 735
Epoch: 735
ogbn-arxiv,CAGNET-1D,2,735,36.9496,0.6576

Epoch: 736
Epoch: 736
ogbn-arxiv,CAGNET-1D,2,736,37.0025,0.6573

Epoch: 737
Epoch: 737
ogbn-arxiv,CAGNET-1D,2,737,37.0525,0.6576

Epoch: 738
Epoch: 738
ogbn-arxiv,CAGNET-1D,2,738,37.1025,0.6575

Epoch: 739
Epoch: 739
ogbn-arxiv,CAGNET-1D,2,739,37.1522,0.6570

Epoch: 740
Epoch: 740
ogbn-arxiv,CAGNET-1D,2,740,37.2019,0.6586

Epoch: 741
Epoch: 741
ogbn-arxiv,CAGNET-1D,2,741,37.2517,0.6553

Epoch: 742
Epoch: 742
ogbn-arxiv,CAGNET-1D,2,742,37.3020,0.6602

Epoch: 743
Epoch: 743
ogbn-arxiv,CAGNET-1D,2,743,37.3518,0.6545

Epoch: 744
Epoch: 744
ogbn-arxiv,CAGNET-1D,2,744,37.4019,0.6604

Epoch: 745
Epoch: 745
ogbn-arxiv,CAGNET-1D,2,745,37.4519,0.6546

Epoch: 746
Epoch: 746
ogbn-arxiv,CAGNET-1D,2,746,37.5018,0.6596

Epoch: 747
Epoch: 747
ogbn-arxiv,CAGNET-1D,2,747,37.5515,0.6565

Epoch: 748Epoch: 748

ogbn-arxiv,CAGNET-1D,2,748,37.6016,0.6578

Epoch: 749Epoch: 749

ogbn-arxiv,CAGNET-1D,2,749,37.6517,0.6578

Epoch: 750
Epoch: 750
ogbn-arxiv,CAGNET-1D,2,750,37.7016,0.6567

Epoch: 751
Epoch: 751
ogbn-arxiv,CAGNET-1D,2,751,37.7514,0.6588

Epoch: 752
Epoch: 752
ogbn-arxiv,CAGNET-1D,2,752,37.8012,0.6561

Epoch: 753
Epoch: 753
ogbn-arxiv,CAGNET-1D,2,753,37.8511,0.6600

Epoch: 754
Epoch: 754
ogbn-arxiv,CAGNET-1D,2,754,37.9063,0.6541

Epoch: 755
Epoch: 755
ogbn-arxiv,CAGNET-1D,2,755,37.9575,0.6619

Epoch: 756
Epoch: 756
ogbn-arxiv,CAGNET-1D,2,756,38.0078,0.6533

Epoch: 757
Epoch: 757
ogbn-arxiv,CAGNET-1D,2,757,38.0585,0.6615

Epoch: 758
Epoch: 758
ogbn-arxiv,CAGNET-1D,2,758,38.1086,0.6542

Epoch: 759
Epoch: 759
ogbn-arxiv,CAGNET-1D,2,759,38.1588,0.6599

Epoch: 760Epoch: 760

ogbn-arxiv,CAGNET-1D,2,760,38.2093,0.6563

Epoch: 761Epoch: 761

ogbn-arxiv,CAGNET-1D,2,761,38.2592,0.6587

Epoch: 762
Epoch: 762
ogbn-arxiv,CAGNET-1D,2,762,38.3090,0.6572

Epoch: 763
Epoch: 763
ogbn-arxiv,CAGNET-1D,2,763,38.3588,0.6581

Epoch: 764
Epoch: 764
ogbn-arxiv,CAGNET-1D,2,764,38.4091,0.6581

Epoch: 765Epoch: 765

ogbn-arxiv,CAGNET-1D,2,765,38.4594,0.6573

Epoch: 766
Epoch: 766
ogbn-arxiv,CAGNET-1D,2,766,38.5094,0.6591

Epoch: 767
Epoch: 767
ogbn-arxiv,CAGNET-1D,2,767,38.5589,0.6566

Epoch: 768Epoch: 768

ogbn-arxiv,CAGNET-1D,2,768,38.6089,0.6590

Epoch: 769
Epoch: 769
ogbn-arxiv,CAGNET-1D,2,769,38.6590,0.6570

Epoch: 770
Epoch: 770
ogbn-arxiv,CAGNET-1D,2,770,38.7089,0.6586

Epoch: 771
Epoch: 771
ogbn-arxiv,CAGNET-1D,2,771,38.7585,0.6574

Epoch: 772
Epoch: 772
ogbn-arxiv,CAGNET-1D,2,772,38.8118,0.6577

Epoch: 773
Epoch: 773
ogbn-arxiv,CAGNET-1D,2,773,38.8623,0.6586

Epoch: 774Epoch: 774

ogbn-arxiv,CAGNET-1D,2,774,38.9123,0.6573

Epoch: 775
Epoch: 775
ogbn-arxiv,CAGNET-1D,2,775,38.9624,0.6582

Epoch: 776
Epoch: 776
ogbn-arxiv,CAGNET-1D,2,776,39.0121,0.6584

Epoch: 777
Epoch: 777
ogbn-arxiv,CAGNET-1D,2,777,39.0620,0.6577

Epoch: 778
Epoch: 778
ogbn-arxiv,CAGNET-1D,2,778,39.1119,0.6579

Epoch: 779
Epoch: 779
ogbn-arxiv,CAGNET-1D,2,779,39.1620,0.6575

Epoch: 780
Epoch: 780
ogbn-arxiv,CAGNET-1D,2,780,39.2120,0.6591

Epoch: 781
Epoch: 781
ogbn-arxiv,CAGNET-1D,2,781,39.2618,0.6564

Epoch: 782
Epoch: 782
ogbn-arxiv,CAGNET-1D,2,782,39.3120,0.6607

Epoch: 783Epoch: 783

ogbn-arxiv,CAGNET-1D,2,783,39.3621,0.6543

Epoch: 784
Epoch: 784
ogbn-arxiv,CAGNET-1D,2,784,39.4122,0.6626

Epoch: 785
Epoch: 785
ogbn-arxiv,CAGNET-1D,2,785,39.4619,0.6526

Epoch: 786
Epoch: 786
ogbn-arxiv,CAGNET-1D,2,786,39.5115,0.6625

Epoch: 787Epoch: 787

ogbn-arxiv,CAGNET-1D,2,787,39.5616,0.6542

Epoch: 788
Epoch: 788
ogbn-arxiv,CAGNET-1D,2,788,39.6116,0.6604

Epoch: 789
Epoch: 789
ogbn-arxiv,CAGNET-1D,2,789,39.6615,0.6574

Epoch: 790
Epoch: 790
ogbn-arxiv,CAGNET-1D,2,790,39.7108,0.6572

Epoch: 791
Epoch: 791
ogbn-arxiv,CAGNET-1D,2,791,39.7629,0.6608

Epoch: 792
Epoch: 792
ogbn-arxiv,CAGNET-1D,2,792,39.8134,0.6538

Epoch: 793
Epoch: 793
ogbn-arxiv,CAGNET-1D,2,793,39.8632,0.6630

Epoch: 794
Epoch: 794
ogbn-arxiv,CAGNET-1D,2,794,39.9127,0.6528

Epoch: 795
Epoch: 795
ogbn-arxiv,CAGNET-1D,2,795,39.9625,0.6617

Epoch: 796
Epoch: 796
ogbn-arxiv,CAGNET-1D,2,796,40.0125,0.6563

Epoch: 797
Epoch: 797
ogbn-arxiv,CAGNET-1D,2,797,40.0625,0.6580

Epoch: 798
Epoch: 798
ogbn-arxiv,CAGNET-1D,2,798,40.1122,0.6606

Epoch: 799
Epoch: 799
ogbn-arxiv,CAGNET-1D,2,799,40.1622,0.6554

Epoch: 800Epoch: 800

ogbn-arxiv,CAGNET-1D,2,800,40.2121,0.6609

Epoch: 801
Epoch: 801
ogbn-arxiv,CAGNET-1D,2,801,40.2621,0.6550

Epoch: 802
Epoch: 802
ogbn-arxiv,CAGNET-1D,2,802,40.3121,0.6608

Epoch: 803
Epoch: 803
ogbn-arxiv,CAGNET-1D,2,803,40.3618,0.6566

Epoch: 804
Epoch: 804
ogbn-arxiv,CAGNET-1D,2,804,40.4119,0.6591

Epoch: 805
Epoch: 805
ogbn-arxiv,CAGNET-1D,2,805,40.4621,0.6591

Epoch: 806
Epoch: 806
ogbn-arxiv,CAGNET-1D,2,806,40.5120,0.6564

Epoch: 807
Epoch: 807
ogbn-arxiv,CAGNET-1D,2,807,40.5619,0.6611

Epoch: 808Epoch: 808

ogbn-arxiv,CAGNET-1D,2,808,40.6118,0.6561

Epoch: 809
Epoch: 809
ogbn-arxiv,CAGNET-1D,2,809,40.6619,0.6598

Epoch: 810
Epoch: 810
ogbn-arxiv,CAGNET-1D,2,810,40.7156,0.6583

Epoch: 811
Epoch: 811
ogbn-arxiv,CAGNET-1D,2,811,40.7654,0.6575

Epoch: 812
Epoch: 812
ogbn-arxiv,CAGNET-1D,2,812,40.8154,0.6598

Epoch: 813
Epoch: 813
ogbn-arxiv,CAGNET-1D,2,813,40.8654,0.6560

Epoch: 814
Epoch: 814
ogbn-arxiv,CAGNET-1D,2,814,40.9150,0.6615

Epoch: 815
Epoch: 815
ogbn-arxiv,CAGNET-1D,2,815,40.9648,0.6545

Epoch: 816
Epoch: 816
ogbn-arxiv,CAGNET-1D,2,816,41.0148,0.6622

Epoch: 817
Epoch: 817
ogbn-arxiv,CAGNET-1D,2,817,41.0648,0.6553

Epoch: 818
Epoch: 818
ogbn-arxiv,CAGNET-1D,2,818,41.1147,0.6605

Epoch: 819
Epoch: 819
ogbn-arxiv,CAGNET-1D,2,819,41.1644,0.6583

Epoch: 820Epoch: 820

ogbn-arxiv,CAGNET-1D,2,820,41.2143,0.6577

Epoch: 821
Epoch: 821
ogbn-arxiv,CAGNET-1D,2,821,41.2643,0.6608

Epoch: 822
Epoch: 822
ogbn-arxiv,CAGNET-1D,2,822,41.3143,0.6555

Epoch: 823
Epoch: 823
ogbn-arxiv,CAGNET-1D,2,823,41.3641,0.6616

Epoch: 824
Epoch: 824
ogbn-arxiv,CAGNET-1D,2,824,41.4140,0.6548

Epoch: 825
Epoch: 825
ogbn-arxiv,CAGNET-1D,2,825,41.4639,0.6618

Epoch: 826
Epoch: 826
ogbn-arxiv,CAGNET-1D,2,826,41.5139,0.6543

Epoch: 827
Epoch: 827
ogbn-arxiv,CAGNET-1D,2,827,41.5642,0.6617

Epoch: 828
Epoch: 828
ogbn-arxiv,CAGNET-1D,2,828,41.6194,0.6548

Epoch: 829
Epoch: 829
ogbn-arxiv,CAGNET-1D,2,829,41.6692,0.6616

Epoch: 830
Epoch: 830
ogbn-arxiv,CAGNET-1D,2,830,41.7189,0.6561

Epoch: 831
Epoch: 831
ogbn-arxiv,CAGNET-1D,2,831,41.7689,0.6609

Epoch: 832
Epoch: 832
ogbn-arxiv,CAGNET-1D,2,832,41.8189,0.6570

Epoch: 833
Epoch: 833
ogbn-arxiv,CAGNET-1D,2,833,41.8687,0.6595

Epoch: 834
Epoch: 834
ogbn-arxiv,CAGNET-1D,2,834,41.9186,0.6598

Epoch: 835
Epoch: 835
ogbn-arxiv,CAGNET-1D,2,835,41.9686,0.6564

Epoch: 836
Epoch: 836
ogbn-arxiv,CAGNET-1D,2,836,42.0186,0.6616

Epoch: 837
Epoch: 837
ogbn-arxiv,CAGNET-1D,2,837,42.0686,0.6543

Epoch: 838
Epoch: 838
ogbn-arxiv,CAGNET-1D,2,838,42.1184,0.6633

Epoch: 839
Epoch: 839
ogbn-arxiv,CAGNET-1D,2,839,42.1684,0.6540

Epoch: 840
Epoch: 840
ogbn-arxiv,CAGNET-1D,2,840,42.2183,0.6622

Epoch: 841
Epoch: 841
ogbn-arxiv,CAGNET-1D,2,841,42.2683,0.6563

Epoch: 842Epoch: 842

ogbn-arxiv,CAGNET-1D,2,842,42.3180,0.6602

Epoch: 843
Epoch: 843
ogbn-arxiv,CAGNET-1D,2,843,42.3681,0.6587

Epoch: 844
Epoch: 844
ogbn-arxiv,CAGNET-1D,2,844,42.4182,0.6579

Epoch: 845
Epoch: 845
ogbn-arxiv,CAGNET-1D,2,845,42.4680,0.6611

Epoch: 846Epoch: 846

ogbn-arxiv,CAGNET-1D,2,846,42.5182,0.6554

Epoch: 847
Epoch: 847
ogbn-arxiv,CAGNET-1D,2,847,42.5728,0.6629

Epoch: 848Epoch: 848

ogbn-arxiv,CAGNET-1D,2,848,42.6228,0.6547

Epoch: 849
Epoch: 849
ogbn-arxiv,CAGNET-1D,2,849,42.6727,0.6627

Epoch: 850
Epoch: 850
ogbn-arxiv,CAGNET-1D,2,850,42.7230,0.6555

Epoch: 851
Epoch: 851
ogbn-arxiv,CAGNET-1D,2,851,42.7729,0.6610

Epoch: 852
Epoch: 852
ogbn-arxiv,CAGNET-1D,2,852,42.8227,0.6574

Epoch: 853Epoch: 853

ogbn-arxiv,CAGNET-1D,2,853,42.8724,0.6594

Epoch: 854
Epoch: 854
ogbn-arxiv,CAGNET-1D,2,854,42.9223,0.6590

Epoch: 855
Epoch: 855
ogbn-arxiv,CAGNET-1D,2,855,42.9722,0.6585

Epoch: 856
Epoch: 856
ogbn-arxiv,CAGNET-1D,2,856,43.0225,0.6590

Epoch: 857
Epoch: 857
ogbn-arxiv,CAGNET-1D,2,857,43.0725,0.6586

Epoch: 858Epoch: 858

ogbn-arxiv,CAGNET-1D,2,858,43.1224,0.6578

Epoch: 859
Epoch: 859
ogbn-arxiv,CAGNET-1D,2,859,43.1721,0.6607

Epoch: 860
Epoch: 860
ogbn-arxiv,CAGNET-1D,2,860,43.2218,0.6560

Epoch: 861
Epoch: 861
ogbn-arxiv,CAGNET-1D,2,861,43.2718,0.6618

Epoch: 862
Epoch: 862
ogbn-arxiv,CAGNET-1D,2,862,43.3219,0.6554

Epoch: 863
Epoch: 863
ogbn-arxiv,CAGNET-1D,2,863,43.3719,0.6625

Epoch: 864
Epoch: 864
ogbn-arxiv,CAGNET-1D,2,864,43.4215,0.6539

Epoch: 865
Epoch: 865
ogbn-arxiv,CAGNET-1D,2,865,43.4753,0.6632

Epoch: 866
Epoch: 866
ogbn-arxiv,CAGNET-1D,2,866,43.5255,0.6545

Epoch: 867Epoch: 867

ogbn-arxiv,CAGNET-1D,2,867,43.5753,0.6618

Epoch: 868Epoch: 868

ogbn-arxiv,CAGNET-1D,2,868,43.6255,0.6570

Epoch: 869Epoch: 869

ogbn-arxiv,CAGNET-1D,2,869,43.6754,0.6594

Epoch: 870
Epoch: 870
ogbn-arxiv,CAGNET-1D,2,870,43.7253,0.6592

Epoch: 871Epoch: 871

ogbn-arxiv,CAGNET-1D,2,871,43.7753,0.6575

Epoch: 872Epoch: 872

ogbn-arxiv,CAGNET-1D,2,872,43.8254,0.6611

Epoch: 873Epoch: 873

ogbn-arxiv,CAGNET-1D,2,873,43.8756,0.6558

Epoch: 874
Epoch: 874
ogbn-arxiv,CAGNET-1D,2,874,43.9254,0.6616

Epoch: 875
Epoch: 875
ogbn-arxiv,CAGNET-1D,2,875,43.9754,0.6564

Epoch: 876Epoch: 876

ogbn-arxiv,CAGNET-1D,2,876,44.0254,0.6605

Epoch: 877Epoch: 877

ogbn-arxiv,CAGNET-1D,2,877,44.0755,0.6583

Epoch: 878
Epoch: 878
ogbn-arxiv,CAGNET-1D,2,878,44.1254,0.6589

Epoch: 879
Epoch: 879
ogbn-arxiv,CAGNET-1D,2,879,44.1751,0.6600

Epoch: 880Epoch: 880

ogbn-arxiv,CAGNET-1D,2,880,44.2252,0.6566

Epoch: 881
Epoch: 881
ogbn-arxiv,CAGNET-1D,2,881,44.2754,0.6620

Epoch: 882
Epoch: 882
ogbn-arxiv,CAGNET-1D,2,882,44.3251,0.6551

Epoch: 883
Epoch: 883
ogbn-arxiv,CAGNET-1D,2,883,44.3746,0.6624

Epoch: 884Epoch: 884

ogbn-arxiv,CAGNET-1D,2,884,44.4272,0.6548

Epoch: 885Epoch: 885

ogbn-arxiv,CAGNET-1D,2,885,44.4775,0.6634

Epoch: 886
Epoch: 886
ogbn-arxiv,CAGNET-1D,2,886,44.5271,0.6529

Epoch: 887
Epoch: 887
ogbn-arxiv,CAGNET-1D,2,887,44.5769,0.6647

Epoch: 888Epoch: 888

ogbn-arxiv,CAGNET-1D,2,888,44.6268,0.6510

Epoch: 889
Epoch: 889
ogbn-arxiv,CAGNET-1D,2,889,44.6767,0.6665

Epoch: 890Epoch: 890

ogbn-arxiv,CAGNET-1D,2,890,44.7269,0.6493

Epoch: 891Epoch: 891

ogbn-arxiv,CAGNET-1D,2,891,44.7765,0.6675

Epoch: 892
Epoch: 892
ogbn-arxiv,CAGNET-1D,2,892,44.8264,0.6507

Epoch: 893
Epoch: 893
ogbn-arxiv,CAGNET-1D,2,893,44.8767,0.6649

Epoch: 894
Epoch: 894
ogbn-arxiv,CAGNET-1D,2,894,44.9269,0.6556

Epoch: 895
Epoch: 895
ogbn-arxiv,CAGNET-1D,2,895,44.9766,0.6584

Epoch: 896
Epoch: 896
ogbn-arxiv,CAGNET-1D,2,896,45.0268,0.6621

Epoch: 897
Epoch: 897
ogbn-arxiv,CAGNET-1D,2,897,45.0766,0.6525

Epoch: 898
Epoch: 898
ogbn-arxiv,CAGNET-1D,2,898,45.1266,0.6660

Epoch: 899
Epoch: 899
ogbn-arxiv,CAGNET-1D,2,899,45.1764,0.6514

Epoch: 900
Epoch: 900
ogbn-arxiv,CAGNET-1D,2,900,45.2266,0.6646

Epoch: 901Epoch: 901

ogbn-arxiv,CAGNET-1D,2,901,45.2764,0.6555

Epoch: 902
Epoch: 902
ogbn-arxiv,CAGNET-1D,2,902,45.3297,0.6596

Epoch: 903
Epoch: 903
ogbn-arxiv,CAGNET-1D,2,903,45.3800,0.6611

Epoch: 904
Epoch: 904
ogbn-arxiv,CAGNET-1D,2,904,45.4299,0.6544

Epoch: 905
Epoch: 905
ogbn-arxiv,CAGNET-1D,2,905,45.4801,0.6649

Epoch: 906
Epoch: 906
ogbn-arxiv,CAGNET-1D,2,906,45.5300,0.6531

Epoch: 907
Epoch: 907
ogbn-arxiv,CAGNET-1D,2,907,45.5799,0.6624

Epoch: 908
Epoch: 908
ogbn-arxiv,CAGNET-1D,2,908,45.6298,0.6565

Epoch: 909
Epoch: 909
ogbn-arxiv,CAGNET-1D,2,909,45.6800,0.6597

Epoch: 910
Epoch: 910
ogbn-arxiv,CAGNET-1D,2,910,45.7299,0.6592

Epoch: 911
Epoch: 911
ogbn-arxiv,CAGNET-1D,2,911,45.7801,0.6582

Epoch: 912
Epoch: 912
ogbn-arxiv,CAGNET-1D,2,912,45.8301,0.6606

Epoch: 913
Epoch: 913
ogbn-arxiv,CAGNET-1D,2,913,45.8800,0.6568

Epoch: 914
Epoch: 914
ogbn-arxiv,CAGNET-1D,2,914,45.9299,0.6604

Epoch: 915
Epoch: 915
ogbn-arxiv,CAGNET-1D,2,915,45.9800,0.6574

Epoch: 916
Epoch: 916
ogbn-arxiv,CAGNET-1D,2,916,46.0300,0.6603

Epoch: 917
Epoch: 917
ogbn-arxiv,CAGNET-1D,2,917,46.0797,0.6571

Epoch: 918
Epoch: 918
ogbn-arxiv,CAGNET-1D,2,918,46.1296,0.6605

Epoch: 919
Epoch: 919
ogbn-arxiv,CAGNET-1D,2,919,46.1796,0.6577

Epoch: 920
Epoch: 920
ogbn-arxiv,CAGNET-1D,2,920,46.2296,0.6598

Epoch: 921
Epoch: 921
ogbn-arxiv,CAGNET-1D,2,921,46.2816,0.6586

Epoch: 922
Epoch: 922
ogbn-arxiv,CAGNET-1D,2,922,46.3317,0.6582

Epoch: 923
Epoch: 923
ogbn-arxiv,CAGNET-1D,2,923,46.3818,0.6597

Epoch: 924
Epoch: 924
ogbn-arxiv,CAGNET-1D,2,924,46.4320,0.6579

Epoch: 925
Epoch: 925
ogbn-arxiv,CAGNET-1D,2,925,46.4819,0.6600

Epoch: 926
Epoch: 926
ogbn-arxiv,CAGNET-1D,2,926,46.5320,0.6579

Epoch: 927
Epoch: 927
ogbn-arxiv,CAGNET-1D,2,927,46.5818,0.6592

Epoch: 928
Epoch: 928
ogbn-arxiv,CAGNET-1D,2,928,46.6316,0.6591

Epoch: 929
Epoch: 929
ogbn-arxiv,CAGNET-1D,2,929,46.6815,0.6574

Epoch: 930
Epoch: 930
ogbn-arxiv,CAGNET-1D,2,930,46.7315,0.6616

Epoch: 931
Epoch: 931
ogbn-arxiv,CAGNET-1D,2,931,46.7814,0.6564

Epoch: 932
Epoch: 932
ogbn-arxiv,CAGNET-1D,2,932,46.8312,0.6613

Epoch: 933
Epoch: 933
ogbn-arxiv,CAGNET-1D,2,933,46.8811,0.6571

Epoch: 934
Epoch: 934
ogbn-arxiv,CAGNET-1D,2,934,46.9311,0.6607

Epoch: 935
Epoch: 935
ogbn-arxiv,CAGNET-1D,2,935,46.9812,0.6576

Epoch: 936
Epoch: 936
ogbn-arxiv,CAGNET-1D,2,936,47.0316,0.6594

Epoch: 937
Epoch: 937
ogbn-arxiv,CAGNET-1D,2,937,47.0813,0.6588

Epoch: 938Epoch: 938

ogbn-arxiv,CAGNET-1D,2,938,47.1308,0.6589

Epoch: 939
Epoch: 939
ogbn-arxiv,CAGNET-1D,2,939,47.1845,0.6597

Epoch: 940
Epoch: 940
ogbn-arxiv,CAGNET-1D,2,940,47.2346,0.6583

Epoch: 941
Epoch: 941
ogbn-arxiv,CAGNET-1D,2,941,47.2848,0.6593

Epoch: 942
Epoch: 942
ogbn-arxiv,CAGNET-1D,2,942,47.3349,0.6589

Epoch: 943
Epoch: 943
ogbn-arxiv,CAGNET-1D,2,943,47.3847,0.6590

Epoch: 944
Epoch: 944
ogbn-arxiv,CAGNET-1D,2,944,47.4345,0.6588

Epoch: 945
Epoch: 945
ogbn-arxiv,CAGNET-1D,2,945,47.4845,0.6593

Epoch: 946
Epoch: 946
ogbn-arxiv,CAGNET-1D,2,946,47.5345,0.6586

Epoch: 947
Epoch: 947
ogbn-arxiv,CAGNET-1D,2,947,47.5845,0.6596

Epoch: 948
Epoch: 948
ogbn-arxiv,CAGNET-1D,2,948,47.6340,0.6584

Epoch: 949
Epoch: 949
ogbn-arxiv,CAGNET-1D,2,949,47.6837,0.6606

Epoch: 950
Epoch: 950
ogbn-arxiv,CAGNET-1D,2,950,47.7339,0.6570

Epoch: 951
Epoch: 951
ogbn-arxiv,CAGNET-1D,2,951,47.7842,0.6623

Epoch: 952
Epoch: 952
ogbn-arxiv,CAGNET-1D,2,952,47.8339,0.6549

Epoch: 953
Epoch: 953
ogbn-arxiv,CAGNET-1D,2,953,47.8836,0.6646

Epoch: 954
Epoch: 954
ogbn-arxiv,CAGNET-1D,2,954,47.9338,0.6511

Epoch: 955
Epoch: 955
ogbn-arxiv,CAGNET-1D,2,955,47.9838,0.6681

Epoch: 956
Epoch: 956
ogbn-arxiv,CAGNET-1D,2,956,48.0338,0.6468

Epoch: 957
Epoch: 957
ogbn-arxiv,CAGNET-1D,2,957,48.0832,0.6713

Epoch: 958
Epoch: 958
ogbn-arxiv,CAGNET-1D,2,958,48.1361,0.6441

Epoch: 959
Epoch: 959
ogbn-arxiv,CAGNET-1D,2,959,48.1861,0.6701

Epoch: 960
Epoch: 960
ogbn-arxiv,CAGNET-1D,2,960,48.2359,0.6487

Epoch: 961
Epoch: 961
ogbn-arxiv,CAGNET-1D,2,961,48.2854,0.6650

Epoch: 962
Epoch: 962
ogbn-arxiv,CAGNET-1D,2,962,48.3354,0.6567

Epoch: 963
Epoch: 963
ogbn-arxiv,CAGNET-1D,2,963,48.3858,0.6580

Epoch: 964
Epoch: 964
ogbn-arxiv,CAGNET-1D,2,964,48.4356,0.6635

Epoch: 965
Epoch: 965
ogbn-arxiv,CAGNET-1D,2,965,48.4857,0.6530

Epoch: 966
Epoch: 966
ogbn-arxiv,CAGNET-1D,2,966,48.5356,0.6664

Epoch: 967Epoch: 967

ogbn-arxiv,CAGNET-1D,2,967,48.5858,0.6518

Epoch: 968
Epoch: 968
ogbn-arxiv,CAGNET-1D,2,968,48.6358,0.6653

Epoch: 969
Epoch: 969
ogbn-arxiv,CAGNET-1D,2,969,48.6858,0.6539

Epoch: 970
Epoch: 970
ogbn-arxiv,CAGNET-1D,2,970,48.7359,0.6627

Epoch: 971Epoch: 971

ogbn-arxiv,CAGNET-1D,2,971,48.7858,0.6578

Epoch: 972
Epoch: 972
ogbn-arxiv,CAGNET-1D,2,972,48.8361,0.6578

Epoch: 973
Epoch: 973
ogbn-arxiv,CAGNET-1D,2,973,48.8863,0.6623

Epoch: 974
Epoch: 974
ogbn-arxiv,CAGNET-1D,2,974,48.9360,0.6544

Epoch: 975
Epoch: 975
ogbn-arxiv,CAGNET-1D,2,975,48.9859,0.6647

Epoch: 976
Epoch: 976
ogbn-arxiv,CAGNET-1D,2,976,49.0374,0.6538

Epoch: 977
Epoch: 977
ogbn-arxiv,CAGNET-1D,2,977,49.0876,0.6639

Epoch: 978
Epoch: 978
ogbn-arxiv,CAGNET-1D,2,978,49.1374,0.6554

Epoch: 979
Epoch: 979
ogbn-arxiv,CAGNET-1D,2,979,49.1875,0.6610

Epoch: 980
Epoch: 980
ogbn-arxiv,CAGNET-1D,2,980,49.2375,0.6584

Epoch: 981
Epoch: 981
ogbn-arxiv,CAGNET-1D,2,981,49.2874,0.6580

Epoch: 982
Epoch: 982
ogbn-arxiv,CAGNET-1D,2,982,49.3373,0.6610

Epoch: 983
Epoch: 983
ogbn-arxiv,CAGNET-1D,2,983,49.3871,0.6567

Epoch: 984
Epoch: 984
ogbn-arxiv,CAGNET-1D,2,984,49.4378,0.6616

Epoch: 985
Epoch: 985
ogbn-arxiv,CAGNET-1D,2,985,49.4877,0.6562

Epoch: 986
Epoch: 986
ogbn-arxiv,CAGNET-1D,2,986,49.5378,0.6619

Epoch: 987
Epoch: 987
ogbn-arxiv,CAGNET-1D,2,987,49.5877,0.6565

Epoch: 988
Epoch: 988
ogbn-arxiv,CAGNET-1D,2,988,49.6376,0.6616

Epoch: 989
Epoch: 989
ogbn-arxiv,CAGNET-1D,2,989,49.6875,0.6566

Epoch: 990
Epoch: 990
ogbn-arxiv,CAGNET-1D,2,990,49.7377,0.6614

Epoch: 991Epoch: 991

ogbn-arxiv,CAGNET-1D,2,991,49.7878,0.6566

Epoch: 992
Epoch: 992
ogbn-arxiv,CAGNET-1D,2,992,49.8375,0.6610

Epoch: 993
Epoch: 993
ogbn-arxiv,CAGNET-1D,2,993,49.8874,0.6576

Epoch: 994
Epoch: 994
ogbn-arxiv,CAGNET-1D,2,994,49.9372,0.6601

Epoch: 995
Epoch: 995
ogbn-arxiv,CAGNET-1D,2,995,49.9910,0.6582

Epoch: 996
Epoch: 996
ogbn-arxiv,CAGNET-1D,2,996,50.0409,0.6589

Epoch: 997
Epoch: 997
ogbn-arxiv,CAGNET-1D,2,997,50.0908,0.6589

Epoch: 998
Epoch: 998
ogbn-arxiv,CAGNET-1D,2,998,50.1408,0.6588

Epoch: 999
Epoch: 999
ogbn-arxiv,CAGNET-1D,2,999,50.1909,0.6588

Epoch: 1000
Epoch: 1000
ogbn-arxiv,CAGNET-1D,2,1000,50.2408,0.6587

Epoch: 1001
Epoch: 1001
ogbn-arxiv,CAGNET-1D,2,1001,50.2907,0.6588

Epoch: 1002
Epoch: 1002
ogbn-arxiv,CAGNET-1D,2,1002,50.3408,0.6586

Epoch: 1003
Epoch: 1003
ogbn-arxiv,CAGNET-1D,2,1003,50.3907,0.6591

Epoch: 1004
Epoch: 1004
ogbn-arxiv,CAGNET-1D,2,1004,50.4409,0.6580

Epoch: 1005
Epoch: 1005
ogbn-arxiv,CAGNET-1D,2,1005,50.4905,0.6592

Epoch: 1006
Epoch: 1006
ogbn-arxiv,CAGNET-1D,2,1006,50.5404,0.6585

Epoch: 1007
Epoch: 1007
ogbn-arxiv,CAGNET-1D,2,1007,50.5903,0.6589

Epoch: 1008
Epoch: 1008
ogbn-arxiv,CAGNET-1D,2,1008,50.6402,0.6587

Epoch: 1009
Epoch: 1009
ogbn-arxiv,CAGNET-1D,2,1009,50.6897,0.6580

Epoch: 1010
Epoch: 1010
ogbn-arxiv,CAGNET-1D,2,1010,50.7397,0.6608

Epoch: 1011Epoch: 1011

ogbn-arxiv,CAGNET-1D,2,1011,50.7896,0.6558

Epoch: 1012
Epoch: 1012
ogbn-arxiv,CAGNET-1D,2,1012,50.8396,0.6639

Epoch: 1013
Epoch: 1013
ogbn-arxiv,CAGNET-1D,2,1013,50.8952,0.6520

Epoch: 1014
Epoch: 1014
ogbn-arxiv,CAGNET-1D,2,1014,50.9456,0.6676

Epoch: 1015
Epoch: 1015
ogbn-arxiv,CAGNET-1D,2,1015,50.9954,0.6481

Epoch: 1016
Epoch: 1016
ogbn-arxiv,CAGNET-1D,2,1016,51.0453,0.6707

Epoch: 1017
Epoch: 1017
ogbn-arxiv,CAGNET-1D,2,1017,51.0954,0.6436

Epoch: 1018
Epoch: 1018
ogbn-arxiv,CAGNET-1D,2,1018,51.1453,0.6710

Epoch: 1019
Epoch: 1019
ogbn-arxiv,CAGNET-1D,2,1019,51.1950,0.6466

Epoch: 1020
Epoch: 1020
ogbn-arxiv,CAGNET-1D,2,1020,51.2453,0.6677

Epoch: 1021
Epoch: 1021
ogbn-arxiv,CAGNET-1D,2,1021,51.2954,0.6531

Epoch: 1022
Epoch: 1022
ogbn-arxiv,CAGNET-1D,2,1022,51.3454,0.6613

Epoch: 1023
Epoch: 1023
ogbn-arxiv,CAGNET-1D,2,1023,51.3951,0.6593

Epoch: 1024
Epoch: 1024
ogbn-arxiv,CAGNET-1D,2,1024,51.4451,0.6562

Epoch: 1025
Epoch: 1025
ogbn-arxiv,CAGNET-1D,2,1025,51.4949,0.6640

Epoch: 1026
Epoch: 1026
ogbn-arxiv,CAGNET-1D,2,1026,51.5451,0.6529

Epoch: 1027
Epoch: 1027
ogbn-arxiv,CAGNET-1D,2,1027,51.5951,0.6655

Epoch: 1028
Epoch: 1028
ogbn-arxiv,CAGNET-1D,2,1028,51.6451,0.6534

Epoch: 1029
Epoch: 1029
ogbn-arxiv,CAGNET-1D,2,1029,51.6948,0.6632

Epoch: 1030
Epoch: 1030
ogbn-arxiv,CAGNET-1D,2,1030,51.7447,0.6563

Epoch: 1031
Epoch: 1031
ogbn-arxiv,CAGNET-1D,2,1031,51.7947,0.6594

Epoch: 1032
Epoch: 1032
ogbn-arxiv,CAGNET-1D,2,1032,51.8476,0.6597

Epoch: 1033
Epoch: 1033
ogbn-arxiv,CAGNET-1D,2,1033,51.8975,0.6555

Epoch: 1034
Epoch: 1034
ogbn-arxiv,CAGNET-1D,2,1034,51.9476,0.6628

Epoch: 1035
Epoch: 1035
ogbn-arxiv,CAGNET-1D,2,1035,51.9976,0.6542

Epoch: 1036
Epoch: 1036
ogbn-arxiv,CAGNET-1D,2,1036,52.0473,0.6634

Epoch: 1037
Epoch: 1037
ogbn-arxiv,CAGNET-1D,2,1037,52.0968,0.6544

Epoch: 1038
Epoch: 1038
ogbn-arxiv,CAGNET-1D,2,1038,52.1468,0.6628

Epoch: 1039
Epoch: 1039
ogbn-arxiv,CAGNET-1D,2,1039,52.1968,0.6556

Epoch: 1040
Epoch: 1040
ogbn-arxiv,CAGNET-1D,2,1040,52.2467,0.6604

Epoch: 1041
Epoch: 1041
ogbn-arxiv,CAGNET-1D,2,1041,52.2965,0.6578

Epoch: 1042
Epoch: 1042
ogbn-arxiv,CAGNET-1D,2,1042,52.3475,0.6583

Epoch: 1043
Epoch: 1043
ogbn-arxiv,CAGNET-1D,2,1043,52.3988,0.6599

Epoch: 1044
Epoch: 1044
ogbn-arxiv,CAGNET-1D,2,1044,52.4491,0.6564

Epoch: 1045Epoch: 1045

ogbn-arxiv,CAGNET-1D,2,1045,52.4994,0.6617

Epoch: 1046
Epoch: 1046
ogbn-arxiv,CAGNET-1D,2,1046,52.5495,0.6542

Epoch: 1047
Epoch: 1047
ogbn-arxiv,CAGNET-1D,2,1047,52.5993,0.6642

Epoch: 1048
Epoch: 1048
ogbn-arxiv,CAGNET-1D,2,1048,52.6489,0.6525

Epoch: 1049
Epoch: 1049
ogbn-arxiv,CAGNET-1D,2,1049,52.6985,0.6650

Epoch: 1050
Epoch: 1050
ogbn-arxiv,CAGNET-1D,2,1050,52.7503,0.6521

Epoch: 1051
Epoch: 1051
ogbn-arxiv,CAGNET-1D,2,1051,52.8006,0.6651

Epoch: 1052
Epoch: 1052
ogbn-arxiv,CAGNET-1D,2,1052,52.8504,0.6526

Epoch: 1053
Epoch: 1053
ogbn-arxiv,CAGNET-1D,2,1053,52.9004,0.6641

Epoch: 1054
Epoch: 1054
ogbn-arxiv,CAGNET-1D,2,1054,52.9505,0.6544

Epoch: 1055
Epoch: 1055
ogbn-arxiv,CAGNET-1D,2,1055,53.0003,0.6622

Epoch: 1056
Epoch: 1056
ogbn-arxiv,CAGNET-1D,2,1056,53.0505,0.6560

Epoch: 1057Epoch: 1057

ogbn-arxiv,CAGNET-1D,2,1057,53.1004,0.6607

Epoch: 1058
Epoch: 1058
ogbn-arxiv,CAGNET-1D,2,1058,53.1507,0.6577

Epoch: 1059
Epoch: 1059
ogbn-arxiv,CAGNET-1D,2,1059,53.2003,0.6580

Epoch: 1060
Epoch: 1060
ogbn-arxiv,CAGNET-1D,2,1060,53.2501,0.6600

Epoch: 1061
Epoch: 1061
ogbn-arxiv,CAGNET-1D,2,1061,53.3000,0.6557

Epoch: 1062
Epoch: 1062
ogbn-arxiv,CAGNET-1D,2,1062,53.3501,0.6617

Epoch: 1063
Epoch: 1063
ogbn-arxiv,CAGNET-1D,2,1063,53.4003,0.6544

Epoch: 1064
Epoch: 1064
ogbn-arxiv,CAGNET-1D,2,1064,53.4503,0.6637

Epoch: 1065
Epoch: 1065
ogbn-arxiv,CAGNET-1D,2,1065,53.5002,0.6522

Epoch: 1066
Epoch: 1066
ogbn-arxiv,CAGNET-1D,2,1066,53.5502,0.6657

Epoch: 1067
Epoch: 1067
ogbn-arxiv,CAGNET-1D,2,1067,53.6003,0.6498

Epoch: 1068
Epoch: 1068
ogbn-arxiv,CAGNET-1D,2,1068,53.6501,0.6677

Epoch: 1069
Epoch: 1069
ogbn-arxiv,CAGNET-1D,2,1069,53.7035,0.6480

Epoch: 1070
Epoch: 1070
ogbn-arxiv,CAGNET-1D,2,1070,53.7534,0.6682

Epoch: 1071
Epoch: 1071
ogbn-arxiv,CAGNET-1D,2,1071,53.8033,0.6478

Epoch: 1072
Epoch: 1072
ogbn-arxiv,CAGNET-1D,2,1072,53.8533,0.6683

Epoch: 1073
Epoch: 1073
ogbn-arxiv,CAGNET-1D,2,1073,53.9033,0.6486

Epoch: 1074
Epoch: 1074
ogbn-arxiv,CAGNET-1D,2,1074,53.9534,0.6669

Epoch: 1075
Epoch: 1075
ogbn-arxiv,CAGNET-1D,2,1075,54.0032,0.6515

Epoch: 1076
Epoch: 1076
ogbn-arxiv,CAGNET-1D,2,1076,54.0528,0.6641

Epoch: 1077
Epoch: 1077
ogbn-arxiv,CAGNET-1D,2,1077,54.1027,0.6547

Epoch: 1078Epoch: 1078

ogbn-arxiv,CAGNET-1D,2,1078,54.1528,0.6601

Epoch: 1079
Epoch: 1079
ogbn-arxiv,CAGNET-1D,2,1079,54.2029,0.6586

Epoch: 1080
Epoch: 1080
ogbn-arxiv,CAGNET-1D,2,1080,54.2528,0.6566

Epoch: 1081
Epoch: 1081
ogbn-arxiv,CAGNET-1D,2,1081,54.3025,0.6616

Epoch: 1082
Epoch: 1082
ogbn-arxiv,CAGNET-1D,2,1082,54.3523,0.6537

Epoch: 1083
Epoch: 1083
ogbn-arxiv,CAGNET-1D,2,1083,54.4025,0.6650

Epoch: 1084
Epoch: 1084
ogbn-arxiv,CAGNET-1D,2,1084,54.4522,0.6500

Epoch: 1085
Epoch: 1085
ogbn-arxiv,CAGNET-1D,2,1085,54.5020,0.6672

Epoch: 1086
Epoch: 1086
ogbn-arxiv,CAGNET-1D,2,1086,54.5520,0.6491

Epoch: 1087
Epoch: 1087
ogbn-arxiv,CAGNET-1D,2,1087,54.6059,0.6664

Epoch: 1088
Epoch: 1088
ogbn-arxiv,CAGNET-1D,2,1088,54.6564,0.6517

Epoch: 1089
Epoch: 1089
ogbn-arxiv,CAGNET-1D,2,1089,54.7065,0.6634

Epoch: 1090
Epoch: 1090
ogbn-arxiv,CAGNET-1D,2,1090,54.7561,0.6552

Epoch: 1091
Epoch: 1091
ogbn-arxiv,CAGNET-1D,2,1091,54.8061,0.6608

Epoch: 1092Epoch: 1092

ogbn-arxiv,CAGNET-1D,2,1092,54.8562,0.6570

Epoch: 1093
Epoch: 1093
ogbn-arxiv,CAGNET-1D,2,1093,54.9064,0.6593

Epoch: 1094
Epoch: 1094
ogbn-arxiv,CAGNET-1D,2,1094,54.9562,0.6590

Epoch: 1095
Epoch: 1095
ogbn-arxiv,CAGNET-1D,2,1095,55.0068,0.6581

Epoch: 1096
Epoch: 1096
ogbn-arxiv,CAGNET-1D,2,1096,55.0566,0.6595

Epoch: 1097
Epoch: 1097
ogbn-arxiv,CAGNET-1D,2,1097,55.1066,0.6575

Epoch: 1098
Epoch: 1098
ogbn-arxiv,CAGNET-1D,2,1098,55.1575,0.6592

Epoch: 1099
Epoch: 1099
ogbn-arxiv,CAGNET-1D,2,1099,55.2078,0.6584

Epoch: 1100
Epoch: 1100
ogbn-arxiv,CAGNET-1D,2,1100,55.2582,0.6587

Epoch: 1101
Epoch: 1101
ogbn-arxiv,CAGNET-1D,2,1101,55.3084,0.6590

Epoch: 1102
Epoch: 1102
ogbn-arxiv,CAGNET-1D,2,1102,55.3583,0.6585

Epoch: 1103Epoch: 1103

ogbn-arxiv,CAGNET-1D,2,1103,55.4083,0.6579

Epoch: 1104
Epoch: 1104
ogbn-arxiv,CAGNET-1D,2,1104,55.4585,0.6593

Epoch: 1105
Epoch: 1105
ogbn-arxiv,CAGNET-1D,2,1105,55.5084,0.6570

Epoch: 1106
Epoch: 1106
ogbn-arxiv,CAGNET-1D,2,1106,55.5599,0.6611

Epoch: 1107Epoch: 1107

ogbn-arxiv,CAGNET-1D,2,1107,55.6099,0.6549

Epoch: 1108
Epoch: 1108
ogbn-arxiv,CAGNET-1D,2,1108,55.6600,0.6628

Epoch: 1109
Epoch: 1109
ogbn-arxiv,CAGNET-1D,2,1109,55.7098,0.6535

Epoch: 1110Epoch: 1110

ogbn-arxiv,CAGNET-1D,2,1110,55.7594,0.6639

Epoch: 1111
Epoch: 1111
ogbn-arxiv,CAGNET-1D,2,1111,55.8093,0.6522

Epoch: 1112
Epoch: 1112
ogbn-arxiv,CAGNET-1D,2,1112,55.8593,0.6651

Epoch: 1113
Epoch: 1113
ogbn-arxiv,CAGNET-1D,2,1113,55.9091,0.6512

Epoch: 1114
Epoch: 1114
ogbn-arxiv,CAGNET-1D,2,1114,55.9586,0.6653

Epoch: 1115
Epoch: 1115
ogbn-arxiv,CAGNET-1D,2,1115,56.0084,0.6505

Epoch: 1116
Epoch: 1116
ogbn-arxiv,CAGNET-1D,2,1116,56.0585,0.6666

Epoch: 1117
Epoch: 1117
ogbn-arxiv,CAGNET-1D,2,1117,56.1085,0.6491

Epoch: 1118
Epoch: 1118
ogbn-arxiv,CAGNET-1D,2,1118,56.1582,0.6678

Epoch: 1119
Epoch: 1119
ogbn-arxiv,CAGNET-1D,2,1119,56.2078,0.6465

Epoch: 1120
Epoch: 1120
ogbn-arxiv,CAGNET-1D,2,1120,56.2578,0.6694

Epoch: 1121
Epoch: 1121
ogbn-arxiv,CAGNET-1D,2,1121,56.3079,0.6458

Epoch: 1122
Epoch: 1122
ogbn-arxiv,CAGNET-1D,2,1122,56.3581,0.6691

Epoch: 1123
Epoch: 1123
ogbn-arxiv,CAGNET-1D,2,1123,56.4078,0.6482

Epoch: 1124
Epoch: 1124
ogbn-arxiv,CAGNET-1D,2,1124,56.4612,0.6658

Epoch: 1125
Epoch: 1125
ogbn-arxiv,CAGNET-1D,2,1125,56.5119,0.6538

Epoch: 1126
Epoch: 1126
ogbn-arxiv,CAGNET-1D,2,1126,56.5615,0.6599

Epoch: 1127
Epoch: 1127
ogbn-arxiv,CAGNET-1D,2,1127,56.6111,0.6598

Epoch: 1128
Epoch: 1128
ogbn-arxiv,CAGNET-1D,2,1128,56.6611,0.6544

Epoch: 1129
Epoch: 1129
ogbn-arxiv,CAGNET-1D,2,1129,56.7111,0.6652

Epoch: 1130
Epoch: 1130
ogbn-arxiv,CAGNET-1D,2,1130,56.7612,0.6495

Epoch: 1131
Epoch: 1131
ogbn-arxiv,CAGNET-1D,2,1131,56.8122,0.6678

Epoch: 1132
Epoch: 1132
ogbn-arxiv,CAGNET-1D,2,1132,56.8626,0.6476

Epoch: 1133Epoch: 1133

ogbn-arxiv,CAGNET-1D,2,1133,56.9126,0.6672

Epoch: 1134
Epoch: 1134
ogbn-arxiv,CAGNET-1D,2,1134,56.9625,0.6500

Epoch: 1135
Epoch: 1135
ogbn-arxiv,CAGNET-1D,2,1135,57.0125,0.6644

Epoch: 1136
Epoch: 1136
ogbn-arxiv,CAGNET-1D,2,1136,57.0622,0.6549

Epoch: 1137
Epoch: 1137
ogbn-arxiv,CAGNET-1D,2,1137,57.1120,0.6595

Epoch: 1138
Epoch: 1138
ogbn-arxiv,CAGNET-1D,2,1138,57.1621,0.6596

Epoch: 1139
Epoch: 1139
ogbn-arxiv,CAGNET-1D,2,1139,57.2121,0.6547

Epoch: 1140
Epoch: 1140
ogbn-arxiv,CAGNET-1D,2,1140,57.2629,0.6636

Epoch: 1141
Epoch: 1141
ogbn-arxiv,CAGNET-1D,2,1141,57.3136,0.6523

Epoch: 1142
Epoch: 1142
ogbn-arxiv,CAGNET-1D,2,1142,57.3644,0.6646

Epoch: 1143
Epoch: 1143
ogbn-arxiv,CAGNET-1D,2,1143,57.4155,0.6520

Epoch: 1144
Epoch: 1144
ogbn-arxiv,CAGNET-1D,2,1144,57.4662,0.6635

Epoch: 1145
Epoch: 1145
ogbn-arxiv,CAGNET-1D,2,1145,57.5174,0.6531

Epoch: 1146
Epoch: 1146
ogbn-arxiv,CAGNET-1D,2,1146,57.5675,0.6625

Epoch: 1147
Epoch: 1147
ogbn-arxiv,CAGNET-1D,2,1147,57.6174,0.6547

Epoch: 1148
Epoch: 1148
ogbn-arxiv,CAGNET-1D,2,1148,57.6676,0.6608

Epoch: 1149
Epoch: 1149
ogbn-arxiv,CAGNET-1D,2,1149,57.7176,0.6568

Epoch: 1150Epoch: 1150

ogbn-arxiv,CAGNET-1D,2,1150,57.7675,0.6583

Epoch: 1151
Epoch: 1151
ogbn-arxiv,CAGNET-1D,2,1151,57.8178,0.6597

Epoch: 1152
Epoch: 1152
ogbn-arxiv,CAGNET-1D,2,1152,57.8677,0.6563

Epoch: 1153
Epoch: 1153
ogbn-arxiv,CAGNET-1D,2,1153,57.9179,0.6601

Epoch: 1154
Epoch: 1154
ogbn-arxiv,CAGNET-1D,2,1154,57.9680,0.6562

Epoch: 1155
Epoch: 1155
ogbn-arxiv,CAGNET-1D,2,1155,58.0179,0.6603

Epoch: 1156
Epoch: 1156
ogbn-arxiv,CAGNET-1D,2,1156,58.0677,0.6558

Epoch: 1157
Epoch: 1157
ogbn-arxiv,CAGNET-1D,2,1157,58.1175,0.6614

Epoch: 1158
Epoch: 1158
ogbn-arxiv,CAGNET-1D,2,1158,58.1676,0.6537

Epoch: 1159
Epoch: 1159
ogbn-arxiv,CAGNET-1D,2,1159,58.2175,0.6630

Epoch: 1160
Epoch: 1160
ogbn-arxiv,CAGNET-1D,2,1160,58.2673,0.6528

Epoch: 1161
Epoch: 1161
ogbn-arxiv,CAGNET-1D,2,1161,58.3200,0.6635

Epoch: 1162
Epoch: 1162
ogbn-arxiv,CAGNET-1D,2,1162,58.3703,0.6522

Epoch: 1163
Epoch: 1163
ogbn-arxiv,CAGNET-1D,2,1163,58.4200,0.6644

Epoch: 1164
Epoch: 1164
ogbn-arxiv,CAGNET-1D,2,1164,58.4698,0.6509

Epoch: 1165
Epoch: 1165
ogbn-arxiv,CAGNET-1D,2,1165,58.5195,0.6663

Epoch: 1166Epoch: 1166

ogbn-arxiv,CAGNET-1D,2,1166,58.5694,0.6471

Epoch: 1167Epoch: 1167

ogbn-arxiv,CAGNET-1D,2,1167,58.6196,0.6687

Epoch: 1168
Epoch: 1168
ogbn-arxiv,CAGNET-1D,2,1168,58.6695,0.6449

Epoch: 1169
Epoch: 1169
ogbn-arxiv,CAGNET-1D,2,1169,58.7196,0.6703

Epoch: 1170
Epoch: 1170
ogbn-arxiv,CAGNET-1D,2,1170,58.7692,0.6441

Epoch: 1171Epoch: 1171

ogbn-arxiv,CAGNET-1D,2,1171,58.8193,0.6702

Epoch: 1172
Epoch: 1172
ogbn-arxiv,CAGNET-1D,2,1172,58.8695,0.6458

Epoch: 1173
Epoch: 1173
ogbn-arxiv,CAGNET-1D,2,1173,58.9195,0.6683

Epoch: 1174
Epoch: 1174
ogbn-arxiv,CAGNET-1D,2,1174,58.9696,0.6497

Epoch: 1175Epoch: 1175

ogbn-arxiv,CAGNET-1D,2,1175,59.0197,0.6641

Epoch: 1176
Epoch: 1176
ogbn-arxiv,CAGNET-1D,2,1176,59.0698,0.6553

Epoch: 1177
Epoch: 1177
ogbn-arxiv,CAGNET-1D,2,1177,59.1195,0.6588

Epoch: 1178
Epoch: 1178
ogbn-arxiv,CAGNET-1D,2,1178,59.1694,0.6595

Epoch: 1179
Epoch: 1179
ogbn-arxiv,CAGNET-1D,2,1179,59.2197,0.6556

Epoch: 1180
Epoch: 1180
ogbn-arxiv,CAGNET-1D,2,1180,59.2720,0.6633

Epoch: 1181
Epoch: 1181
ogbn-arxiv,CAGNET-1D,2,1181,59.3222,0.6516

Epoch: 1182
Epoch: 1182
ogbn-arxiv,CAGNET-1D,2,1182,59.3724,0.6658

Epoch: 1183
Epoch: 1183
ogbn-arxiv,CAGNET-1D,2,1183,59.4224,0.6505

Epoch: 1184
Epoch: 1184
ogbn-arxiv,CAGNET-1D,2,1184,59.4720,0.6658

Epoch: 1185
Epoch: 1185
ogbn-arxiv,CAGNET-1D,2,1185,59.5217,0.6503

Epoch: 1186
Epoch: 1186
ogbn-arxiv,CAGNET-1D,2,1186,59.5719,0.6650

Epoch: 1187
Epoch: 1187
ogbn-arxiv,CAGNET-1D,2,1187,59.6219,0.6520

Epoch: 1188
Epoch: 1188
ogbn-arxiv,CAGNET-1D,2,1188,59.6716,0.6628

Epoch: 1189
Epoch: 1189
ogbn-arxiv,CAGNET-1D,2,1189,59.7217,0.6539

Epoch: 1190
Epoch: 1190
ogbn-arxiv,CAGNET-1D,2,1190,59.7715,0.6608

Epoch: 1191
Epoch: 1191
ogbn-arxiv,CAGNET-1D,2,1191,59.8209,0.6573

Epoch: 1192
Epoch: 1192
ogbn-arxiv,CAGNET-1D,2,1192,59.8707,0.6578

Epoch: 1193
Epoch: 1193
ogbn-arxiv,CAGNET-1D,2,1193,59.9208,0.6599

Epoch: 1194
Epoch: 1194
ogbn-arxiv,CAGNET-1D,2,1194,59.9709,0.6558

Epoch: 1195
Epoch: 1195
ogbn-arxiv,CAGNET-1D,2,1195,60.0211,0.6608

Epoch: 1196
Epoch: 1196
ogbn-arxiv,CAGNET-1D,2,1196,60.0709,0.6548

Epoch: 1197
Epoch: 1197
ogbn-arxiv,CAGNET-1D,2,1197,60.1205,0.6616

Epoch: 1198
Epoch: 1198
ogbn-arxiv,CAGNET-1D,2,1198,60.1735,0.6540

Epoch: 1199
Epoch: 1199
ogbn-arxiv,CAGNET-1D,2,1199,60.2240,0.6624

Epoch: 1200
Epoch: 1200
ogbn-arxiv,CAGNET-1D,2,1200,60.2739,0.6526

Epoch: 1201
Epoch: 1201
ogbn-arxiv,CAGNET-1D,2,1201,60.3244,0.6641

Epoch: 1202
Epoch: 1202
ogbn-arxiv,CAGNET-1D,2,1202,60.3747,0.6520

Epoch: 1203
Epoch: 1203
ogbn-arxiv,CAGNET-1D,2,1203,60.4242,0.6648

Epoch: 1204
Epoch: 1204
ogbn-arxiv,CAGNET-1D,2,1204,60.4740,0.6509

Epoch: 1205
Epoch: 1205
ogbn-arxiv,CAGNET-1D,2,1205,60.5239,0.6652

Epoch: 1206
Epoch: 1206
ogbn-arxiv,CAGNET-1D,2,1206,60.5741,0.6494

Epoch: 1207
Epoch: 1207
ogbn-arxiv,CAGNET-1D,2,1207,60.6239,0.6663

Epoch: 1208
Epoch: 1208
ogbn-arxiv,CAGNET-1D,2,1208,60.6738,0.6489

Epoch: 1209
Epoch: 1209
ogbn-arxiv,CAGNET-1D,2,1209,60.7237,0.6669

Epoch: 1210
Epoch: 1210
ogbn-arxiv,CAGNET-1D,2,1210,60.7735,0.6488

Epoch: 1211
Epoch: 1211
ogbn-arxiv,CAGNET-1D,2,1211,60.8238,0.6664

Epoch: 1212
Epoch: 1212
ogbn-arxiv,CAGNET-1D,2,1212,60.8738,0.6492

Epoch: 1213
Epoch: 1213
ogbn-arxiv,CAGNET-1D,2,1213,60.9241,0.6660

Epoch: 1214
Epoch: 1214
ogbn-arxiv,CAGNET-1D,2,1214,60.9738,0.6499

Epoch: 1215
Epoch: 1215
ogbn-arxiv,CAGNET-1D,2,1215,61.0236,0.6653

Epoch: 1216
Epoch: 1216
ogbn-arxiv,CAGNET-1D,2,1216,61.0736,0.6503

Epoch: 1217
Epoch: 1217
ogbn-arxiv,CAGNET-1D,2,1217,61.1290,0.6646

Epoch: 1218
Epoch: 1218
ogbn-arxiv,CAGNET-1D,2,1218,61.1792,0.6514

Epoch: 1219Epoch: 1219

ogbn-arxiv,CAGNET-1D,2,1219,61.2297,0.6629

Epoch: 1220
Epoch: 1220
ogbn-arxiv,CAGNET-1D,2,1220,61.2796,0.6541

Epoch: 1221
Epoch: 1221
ogbn-arxiv,CAGNET-1D,2,1221,61.3293,0.6607

Epoch: 1222
Epoch: 1222
ogbn-arxiv,CAGNET-1D,2,1222,61.3793,0.6564

Epoch: 1223
Epoch: 1223
ogbn-arxiv,CAGNET-1D,2,1223,61.4292,0.6590

Epoch: 1224Epoch: 1224

ogbn-arxiv,CAGNET-1D,2,1224,61.4791,0.6578

Epoch: 1225Epoch: 1225

ogbn-arxiv,CAGNET-1D,2,1225,61.5288,0.6577

Epoch: 1226
Epoch: 1226
ogbn-arxiv,CAGNET-1D,2,1226,61.5786,0.6588

Epoch: 1227
Epoch: 1227
ogbn-arxiv,CAGNET-1D,2,1227,61.6286,0.6569

Epoch: 1228Epoch: 1228

ogbn-arxiv,CAGNET-1D,2,1228,61.6788,0.6596

Epoch: 1229
Epoch: 1229
ogbn-arxiv,CAGNET-1D,2,1229,61.7285,0.6561

Epoch: 1230
Epoch: 1230
ogbn-arxiv,CAGNET-1D,2,1230,61.7781,0.6608

Epoch: 1231Epoch: 1231

ogbn-arxiv,CAGNET-1D,2,1231,61.8282,0.6541

Epoch: 1232
Epoch: 1232
ogbn-arxiv,CAGNET-1D,2,1232,61.8789,0.6627

Epoch: 1233
Epoch: 1233
ogbn-arxiv,CAGNET-1D,2,1233,61.9293,0.6522

Epoch: 1234
Epoch: 1234
ogbn-arxiv,CAGNET-1D,2,1234,61.9789,0.6650

Epoch: 1235Epoch: 1235

ogbn-arxiv,CAGNET-1D,2,1235,62.0290,0.6489

Epoch: 1236
Epoch: 1236
ogbn-arxiv,CAGNET-1D,2,1236,62.0811,0.6672

Epoch: 1237Epoch: 1237

ogbn-arxiv,CAGNET-1D,2,1237,62.1308,0.6467

Epoch: 1238Epoch: 1238

ogbn-arxiv,CAGNET-1D,2,1238,62.1803,0.6686

Epoch: 1239Epoch: 1239

ogbn-arxiv,CAGNET-1D,2,1239,62.2304,0.6443

Epoch: 1240
Epoch: 1240
ogbn-arxiv,CAGNET-1D,2,1240,62.2804,0.6707

Epoch: 1241
Epoch: 1241
ogbn-arxiv,CAGNET-1D,2,1241,62.3305,0.6399

Epoch: 1242Epoch: 1242

ogbn-arxiv,CAGNET-1D,2,1242,62.3801,0.6726

Epoch: 1243Epoch: 1243

ogbn-arxiv,CAGNET-1D,2,1243,62.4298,0.6379

Epoch: 1244Epoch: 1244

ogbn-arxiv,CAGNET-1D,2,1244,62.4799,0.6718

Epoch: 1245Epoch: 1245

ogbn-arxiv,CAGNET-1D,2,1245,62.5297,0.6437

Epoch: 1246
Epoch: 1246
ogbn-arxiv,CAGNET-1D,2,1246,62.5794,0.6659

Epoch: 1247Epoch: 1247

ogbn-arxiv,CAGNET-1D,2,1247,62.6288,0.6544

Epoch: 1248
Epoch: 1248
ogbn-arxiv,CAGNET-1D,2,1248,62.6787,0.6567

Epoch: 1249
Epoch: 1249
ogbn-arxiv,CAGNET-1D,2,1249,62.7287,0.6634

Epoch: 1250
Epoch: 1250
ogbn-arxiv,CAGNET-1D,2,1250,62.7785,0.6488

Epoch: 1251
Epoch: 1251
ogbn-arxiv,CAGNET-1D,2,1251,62.8280,0.6676

Epoch: 1252
Epoch: 1252
ogbn-arxiv,CAGNET-1D,2,1252,62.8776,0.6458

Epoch: 1253
Epoch: 1253
ogbn-arxiv,CAGNET-1D,2,1253,62.9278,0.6677

Epoch: 1254
Epoch: 1254
ogbn-arxiv,CAGNET-1D,2,1254,62.9825,0.6495

Epoch: 1255
Epoch: 1255
ogbn-arxiv,CAGNET-1D,2,1255,63.0326,0.6623

Epoch: 1256
Epoch: 1256
ogbn-arxiv,CAGNET-1D,2,1256,63.0827,0.6567

Epoch: 1257
Epoch: 1257
ogbn-arxiv,CAGNET-1D,2,1257,63.1324,0.6560

Epoch: 1258
Epoch: 1258
ogbn-arxiv,CAGNET-1D,2,1258,63.1819,0.6627

Epoch: 1259
Epoch: 1259
ogbn-arxiv,CAGNET-1D,2,1259,63.2314,0.6504

Epoch: 1260
Epoch: 1260
ogbn-arxiv,CAGNET-1D,2,1260,63.2816,0.6658

Epoch: 1261
Epoch: 1261
ogbn-arxiv,CAGNET-1D,2,1261,63.3316,0.6479

Epoch: 1262
Epoch: 1262
ogbn-arxiv,CAGNET-1D,2,1262,63.3818,0.6662

Epoch: 1263
Epoch: 1263
ogbn-arxiv,CAGNET-1D,2,1263,63.4312,0.6496

Epoch: 1264
Epoch: 1264
ogbn-arxiv,CAGNET-1D,2,1264,63.4811,0.6634

Epoch: 1265
Epoch: 1265
ogbn-arxiv,CAGNET-1D,2,1265,63.5311,0.6548

Epoch: 1266Epoch: 1266

ogbn-arxiv,CAGNET-1D,2,1266,63.5811,0.6587

Epoch: 1267
Epoch: 1267
ogbn-arxiv,CAGNET-1D,2,1267,63.6309,0.6593

Epoch: 1268
Epoch: 1268
ogbn-arxiv,CAGNET-1D,2,1268,63.6806,0.6546

Epoch: 1269
Epoch: 1269
ogbn-arxiv,CAGNET-1D,2,1269,63.7314,0.6624

Epoch: 1270
Epoch: 1270
ogbn-arxiv,CAGNET-1D,2,1270,63.7827,0.6522

Epoch: 1271
Epoch: 1271
ogbn-arxiv,CAGNET-1D,2,1271,63.8326,0.6646

Epoch: 1272
Epoch: 1272
ogbn-arxiv,CAGNET-1D,2,1272,63.8826,0.6495

Epoch: 1273
Epoch: 1273
ogbn-arxiv,CAGNET-1D,2,1273,63.9349,0.6659

Epoch: 1274
Epoch: 1274
ogbn-arxiv,CAGNET-1D,2,1274,63.9844,0.6487

Epoch: 1275Epoch: 1275

ogbn-arxiv,CAGNET-1D,2,1275,64.0345,0.6650

Epoch: 1276
Epoch: 1276
ogbn-arxiv,CAGNET-1D,2,1276,64.0847,0.6507

Epoch: 1277
Epoch: 1277
ogbn-arxiv,CAGNET-1D,2,1277,64.1350,0.6630

Epoch: 1278Epoch: 1278

ogbn-arxiv,CAGNET-1D,2,1278,64.1846,0.6531

Epoch: 1279
Epoch: 1279
ogbn-arxiv,CAGNET-1D,2,1279,64.2346,0.6611

Epoch: 1280
Epoch: 1280
ogbn-arxiv,CAGNET-1D,2,1280,64.2847,0.6562

Epoch: 1281
Epoch: 1281
ogbn-arxiv,CAGNET-1D,2,1281,64.3344,0.6579

Epoch: 1282
Epoch: 1282
ogbn-arxiv,CAGNET-1D,2,1282,64.3841,0.6580

Epoch: 1283
Epoch: 1283
ogbn-arxiv,CAGNET-1D,2,1283,64.4340,0.6564

Epoch: 1284
Epoch: 1284
ogbn-arxiv,CAGNET-1D,2,1284,64.4839,0.6591

Epoch: 1285
Epoch: 1285
ogbn-arxiv,CAGNET-1D,2,1285,64.5339,0.6549

Epoch: 1286
Epoch: 1286
ogbn-arxiv,CAGNET-1D,2,1286,64.5838,0.6606

Epoch: 1287
Epoch: 1287
ogbn-arxiv,CAGNET-1D,2,1287,64.6334,0.6543

Epoch: 1288Epoch: 1288

ogbn-arxiv,CAGNET-1D,2,1288,64.6834,0.6609

Epoch: 1289Epoch: 1289

ogbn-arxiv,CAGNET-1D,2,1289,64.7336,0.6541

Epoch: 1290
Epoch: 1290
ogbn-arxiv,CAGNET-1D,2,1290,64.7837,0.6613

Epoch: 1291
Epoch: 1291
ogbn-arxiv,CAGNET-1D,2,1291,64.8403,0.6541

Epoch: 1292Epoch: 1292

ogbn-arxiv,CAGNET-1D,2,1292,64.8902,0.6617

Epoch: 1293
Epoch: 1293
ogbn-arxiv,CAGNET-1D,2,1293,64.9403,0.6533

Epoch: 1294
Epoch: 1294
ogbn-arxiv,CAGNET-1D,2,1294,64.9905,0.6625

Epoch: 1295
Epoch: 1295
ogbn-arxiv,CAGNET-1D,2,1295,65.0404,0.6524

Epoch: 1296
Epoch: 1296
ogbn-arxiv,CAGNET-1D,2,1296,65.0900,0.6627

Epoch: 1297Epoch: 1297

ogbn-arxiv,CAGNET-1D,2,1297,65.1401,0.6515

Epoch: 1298Epoch: 1298

ogbn-arxiv,CAGNET-1D,2,1298,65.1902,0.6629

Epoch: 1299Epoch: 1299

ogbn-arxiv,CAGNET-1D,2,1299,65.2401,0.6499

Epoch: 1300
Epoch: 1300
ogbn-arxiv,CAGNET-1D,2,1300,65.2902,0.6641

Epoch: 1301
Epoch: 1301
ogbn-arxiv,CAGNET-1D,2,1301,65.3396,0.6482

Epoch: 1302Epoch: 1302

ogbn-arxiv,CAGNET-1D,2,1302,65.3894,0.6655

Epoch: 1303
Epoch: 1303
ogbn-arxiv,CAGNET-1D,2,1303,65.4395,0.6475

Epoch: 1304
Epoch: 1304
ogbn-arxiv,CAGNET-1D,2,1304,65.4896,0.6660

Epoch: 1305
Epoch: 1305
ogbn-arxiv,CAGNET-1D,2,1305,65.5394,0.6473

Epoch: 1306
Epoch: 1306
ogbn-arxiv,CAGNET-1D,2,1306,65.5889,0.6666

Epoch: 1307
Epoch: 1307
ogbn-arxiv,CAGNET-1D,2,1307,65.6389,0.6474

Epoch: 1308
Epoch: 1308
ogbn-arxiv,CAGNET-1D,2,1308,65.6893,0.6653

Epoch: 1309
Epoch: 1309
ogbn-arxiv,CAGNET-1D,2,1309,65.7394,0.6499

Epoch: 1310
Epoch: 1310
ogbn-arxiv,CAGNET-1D,2,1310,65.7909,0.6625

Epoch: 1311
Epoch: 1311
ogbn-arxiv,CAGNET-1D,2,1311,65.8410,0.6541

Epoch: 1312
Epoch: 1312
ogbn-arxiv,CAGNET-1D,2,1312,65.8912,0.6595

Epoch: 1313
Epoch: 1313
ogbn-arxiv,CAGNET-1D,2,1313,65.9411,0.6556

Epoch: 1314
Epoch: 1314
ogbn-arxiv,CAGNET-1D,2,1314,65.9914,0.6587

Epoch: 1315
Epoch: 1315
ogbn-arxiv,CAGNET-1D,2,1315,66.0418,0.6569

Epoch: 1316
Epoch: 1316
ogbn-arxiv,CAGNET-1D,2,1316,66.0916,0.6575

Epoch: 1317Epoch: 1317

ogbn-arxiv,CAGNET-1D,2,1317,66.1419,0.6584

Epoch: 1318
Epoch: 1318
ogbn-arxiv,CAGNET-1D,2,1318,66.1917,0.6563

Epoch: 1319
Epoch: 1319
ogbn-arxiv,CAGNET-1D,2,1319,66.2417,0.6591

Epoch: 1320
Epoch: 1320
ogbn-arxiv,CAGNET-1D,2,1320,66.2916,0.6554

Epoch: 1321
Epoch: 1321
ogbn-arxiv,CAGNET-1D,2,1321,66.3412,0.6608

Epoch: 1322
Epoch: 1322
ogbn-arxiv,CAGNET-1D,2,1322,66.3910,0.6528

Epoch: 1323
Epoch: 1323
ogbn-arxiv,CAGNET-1D,2,1323,66.4410,0.6631

Epoch: 1324
Epoch: 1324
ogbn-arxiv,CAGNET-1D,2,1324,66.4911,0.6497

Epoch: 1325
Epoch: 1325
ogbn-arxiv,CAGNET-1D,2,1325,66.5409,0.6661

Epoch: 1326
Epoch: 1326
ogbn-arxiv,CAGNET-1D,2,1326,66.5907,0.6458

Epoch: 1327
Epoch: 1327
ogbn-arxiv,CAGNET-1D,2,1327,66.6405,0.6677

Epoch: 1328
Epoch: 1328
ogbn-arxiv,CAGNET-1D,2,1328,66.6964,0.6412

Epoch: 1329
Epoch: 1329
ogbn-arxiv,CAGNET-1D,2,1329,66.7466,0.6706

Epoch: 1330
Epoch: 1330
ogbn-arxiv,CAGNET-1D,2,1330,66.7966,0.6382

Epoch: 1331
Epoch: 1331
ogbn-arxiv,CAGNET-1D,2,1331,66.8467,0.6714

Epoch: 1332
Epoch: 1332
ogbn-arxiv,CAGNET-1D,2,1332,66.8967,0.6382

Epoch: 1333
Epoch: 1333
ogbn-arxiv,CAGNET-1D,2,1333,66.9469,0.6703

Epoch: 1334
Epoch: 1334
ogbn-arxiv,CAGNET-1D,2,1334,66.9967,0.6420

Epoch: 1335
Epoch: 1335
ogbn-arxiv,CAGNET-1D,2,1335,67.0466,0.6670

Epoch: 1336
Epoch: 1336
ogbn-arxiv,CAGNET-1D,2,1336,67.0965,0.6485

Epoch: 1337
Epoch: 1337
ogbn-arxiv,CAGNET-1D,2,1337,67.1464,0.6603

Epoch: 1338
Epoch: 1338
ogbn-arxiv,CAGNET-1D,2,1338,67.1959,0.6571

Epoch: 1339Epoch: 1339

ogbn-arxiv,CAGNET-1D,2,1339,67.2460,0.6537

Epoch: 1340Epoch: 1340

ogbn-arxiv,CAGNET-1D,2,1340,67.2962,0.6632

Epoch: 1341
Epoch: 1341
ogbn-arxiv,CAGNET-1D,2,1341,67.3462,0.6490

Epoch: 1342Epoch: 1342

ogbn-arxiv,CAGNET-1D,2,1342,67.3961,0.6657

Epoch: 1343
Epoch: 1343
ogbn-arxiv,CAGNET-1D,2,1343,67.4462,0.6462

Epoch: 1344
Epoch: 1344
ogbn-arxiv,CAGNET-1D,2,1344,67.4963,0.6671

Epoch: 1345
Epoch: 1345
ogbn-arxiv,CAGNET-1D,2,1345,67.5458,0.6470

Epoch: 1346
Epoch: 1346
ogbn-arxiv,CAGNET-1D,2,1346,67.5954,0.6655

Epoch: 1347Epoch: 1347

ogbn-arxiv,CAGNET-1D,2,1347,67.6496,0.6500

Epoch: 1348
Epoch: 1348
ogbn-arxiv,CAGNET-1D,2,1348,67.6995,0.6619

Epoch: 1349
Epoch: 1349
ogbn-arxiv,CAGNET-1D,2,1349,67.7496,0.6540

Epoch: 1350
Epoch: 1350
ogbn-arxiv,CAGNET-1D,2,1350,67.7999,0.6588

Epoch: 1351
Epoch: 1351
ogbn-arxiv,CAGNET-1D,2,1351,67.8498,0.6571

Epoch: 1352Epoch: 1352

ogbn-arxiv,CAGNET-1D,2,1352,67.8997,0.6555

Epoch: 1353
Epoch: 1353
ogbn-arxiv,CAGNET-1D,2,1353,67.9491,0.6600

Epoch: 1354
Epoch: 1354
ogbn-arxiv,CAGNET-1D,2,1354,67.9988,0.6535

Epoch: 1355
Epoch: 1355
ogbn-arxiv,CAGNET-1D,2,1355,68.0490,0.6615

Epoch: 1356
Epoch: 1356
ogbn-arxiv,CAGNET-1D,2,1356,68.0992,0.6521

Epoch: 1357
Epoch: 1357
ogbn-arxiv,CAGNET-1D,2,1357,68.1488,0.6613

Epoch: 1358
Epoch: 1358
ogbn-arxiv,CAGNET-1D,2,1358,68.1985,0.6527

Epoch: 1359
Epoch: 1359
ogbn-arxiv,CAGNET-1D,2,1359,68.2486,0.6610

Epoch: 1360
Epoch: 1360
ogbn-arxiv,CAGNET-1D,2,1360,68.2986,0.6531

Epoch: 1361
Epoch: 1361
ogbn-arxiv,CAGNET-1D,2,1361,68.3489,0.6606

Epoch: 1362
Epoch: 1362
ogbn-arxiv,CAGNET-1D,2,1362,68.3985,0.6531

Epoch: 1363
Epoch: 1363
ogbn-arxiv,CAGNET-1D,2,1363,68.4487,0.6604

Epoch: 1364
Epoch: 1364
ogbn-arxiv,CAGNET-1D,2,1364,68.4988,0.6535

Epoch: 1365
Epoch: 1365
ogbn-arxiv,CAGNET-1D,2,1365,68.5540,0.6602

Epoch: 1366
Epoch: 1366
ogbn-arxiv,CAGNET-1D,2,1366,68.6041,0.6535

Epoch: 1367
Epoch: 1367
ogbn-arxiv,CAGNET-1D,2,1367,68.6540,0.6599

Epoch: 1368
Epoch: 1368
ogbn-arxiv,CAGNET-1D,2,1368,68.7036,0.6540

Epoch: 1369
Epoch: 1369
ogbn-arxiv,CAGNET-1D,2,1369,68.7538,0.6597

Epoch: 1370
Epoch: 1370
ogbn-arxiv,CAGNET-1D,2,1370,68.8038,0.6542

Epoch: 1371
Epoch: 1371
ogbn-arxiv,CAGNET-1D,2,1371,68.8538,0.6585

Epoch: 1372
Epoch: 1372
ogbn-arxiv,CAGNET-1D,2,1372,68.9036,0.6551

Epoch: 1373
Epoch: 1373
ogbn-arxiv,CAGNET-1D,2,1373,68.9535,0.6583

Epoch: 1374
Epoch: 1374
ogbn-arxiv,CAGNET-1D,2,1374,69.0035,0.6560

Epoch: 1375
Epoch: 1375
ogbn-arxiv,CAGNET-1D,2,1375,69.0536,0.6577

Epoch: 1376
Epoch: 1376
ogbn-arxiv,CAGNET-1D,2,1376,69.1038,0.6570

Epoch: 1377
Epoch: 1377
ogbn-arxiv,CAGNET-1D,2,1377,69.1536,0.6563

Epoch: 1378
Epoch: 1378
ogbn-arxiv,CAGNET-1D,2,1378,69.2040,0.6579

Epoch: 1379
Epoch: 1379
ogbn-arxiv,CAGNET-1D,2,1379,69.2538,0.6568

Epoch: 1380
Epoch: 1380
ogbn-arxiv,CAGNET-1D,2,1380,69.3034,0.6567

Epoch: 1381
Epoch: 1381
ogbn-arxiv,CAGNET-1D,2,1381,69.3532,0.6558

Epoch: 1382
Epoch: 1382
ogbn-arxiv,CAGNET-1D,2,1382,69.4033,0.6570

Epoch: 1383
Epoch: 1383
ogbn-arxiv,CAGNET-1D,2,1383,69.4533,0.6563

Epoch: 1384
Epoch: 1384
ogbn-arxiv,CAGNET-1D,2,1384,69.5056,0.6570

Epoch: 1385
Epoch: 1385
ogbn-arxiv,CAGNET-1D,2,1385,69.5554,0.6564

Epoch: 1386
Epoch: 1386
ogbn-arxiv,CAGNET-1D,2,1386,69.6057,0.6563

Epoch: 1387
Epoch: 1387
ogbn-arxiv,CAGNET-1D,2,1387,69.6557,0.6573

Epoch: 1388
Epoch: 1388
ogbn-arxiv,CAGNET-1D,2,1388,69.7063,0.6554

Epoch: 1389
Epoch: 1389
ogbn-arxiv,CAGNET-1D,2,1389,69.7572,0.6590

Epoch: 1390
Epoch: 1390
ogbn-arxiv,CAGNET-1D,2,1390,69.8072,0.6534

Epoch: 1391
Epoch: 1391
ogbn-arxiv,CAGNET-1D,2,1391,69.8574,0.6610

Epoch: 1392
Epoch: 1392
ogbn-arxiv,CAGNET-1D,2,1392,69.9073,0.6505

Epoch: 1393
Epoch: 1393
ogbn-arxiv,CAGNET-1D,2,1393,69.9573,0.6644

Epoch: 1394
Epoch: 1394
ogbn-arxiv,CAGNET-1D,2,1394,70.0072,0.6457

Epoch: 1395
Epoch: 1395
ogbn-arxiv,CAGNET-1D,2,1395,70.0570,0.6685

Epoch: 1396
Epoch: 1396
ogbn-arxiv,CAGNET-1D,2,1396,70.1069,0.6373

Epoch: 1397
Epoch: 1397
ogbn-arxiv,CAGNET-1D,2,1397,70.1571,0.6729

Epoch: 1398
Epoch: 1398
ogbn-arxiv,CAGNET-1D,2,1398,70.2073,0.6253

Epoch: 1399
Epoch: 1399
ogbn-arxiv,CAGNET-1D,2,1399,70.2569,0.6754

Epoch: 1400
Epoch: 1400
ogbn-arxiv,CAGNET-1D,2,1400,70.3066,0.6184

Epoch: 1401
Epoch: 1401
ogbn-arxiv,CAGNET-1D,2,1401,70.3564,0.6753

Epoch: 1402
Epoch: 1402
ogbn-arxiv,CAGNET-1D,2,1402,70.4129,0.6347

Epoch: 1403
Epoch: 1403
ogbn-arxiv,CAGNET-1D,2,1403,70.4631,0.6629

Epoch: 1404
Epoch: 1404
ogbn-arxiv,CAGNET-1D,2,1404,70.5133,0.6597

Epoch: 1405
Epoch: 1405
ogbn-arxiv,CAGNET-1D,2,1405,70.5631,0.6418

Epoch: 1406
Epoch: 1406
ogbn-arxiv,CAGNET-1D,2,1406,70.6128,0.6703

Epoch: 1407
Epoch: 1407
ogbn-arxiv,CAGNET-1D,2,1407,70.6631,0.6351

Epoch: 1408
Epoch: 1408
ogbn-arxiv,CAGNET-1D,2,1408,70.7130,0.6680

Epoch: 1409
Epoch: 1409
ogbn-arxiv,CAGNET-1D,2,1409,70.7632,0.6509

Epoch: 1410
Epoch: 1410
ogbn-arxiv,CAGNET-1D,2,1410,70.8133,0.6524

Epoch: 1411
Epoch: 1411
ogbn-arxiv,CAGNET-1D,2,1411,70.8631,0.6646

Epoch: 1412
Epoch: 1412
ogbn-arxiv,CAGNET-1D,2,1412,70.9129,0.6414

Epoch: 1413
Epoch: 1413
ogbn-arxiv,CAGNET-1D,2,1413,70.9631,0.6673

Epoch: 1414Epoch: 1414

ogbn-arxiv,CAGNET-1D,2,1414,71.0132,0.6479

Epoch: 1415
Epoch: 1415
ogbn-arxiv,CAGNET-1D,2,1415,71.0636,0.6587

Epoch: 1416
Epoch: 1416
ogbn-arxiv,CAGNET-1D,2,1416,71.1143,0.6606

Epoch: 1417
Epoch: 1417
ogbn-arxiv,CAGNET-1D,2,1417,71.1651,0.6477

Epoch: 1418
Epoch: 1418
ogbn-arxiv,CAGNET-1D,2,1418,71.2159,0.6629

Epoch: 1419
Epoch: 1419
ogbn-arxiv,CAGNET-1D,2,1419,71.2658,0.6507

Epoch: 1420
Epoch: 1420
ogbn-arxiv,CAGNET-1D,2,1420,71.3159,0.6566

Epoch: 1421
Epoch: 1421
ogbn-arxiv,CAGNET-1D,2,1421,71.3670,0.6611

Epoch: 1422
Epoch: 1422
ogbn-arxiv,CAGNET-1D,2,1422,71.4169,0.6476

Epoch: 1423
Epoch: 1423
ogbn-arxiv,CAGNET-1D,2,1423,71.4666,0.6652

Epoch: 1424
Epoch: 1424
ogbn-arxiv,CAGNET-1D,2,1424,71.5167,0.6471

Epoch: 1425
Epoch: 1425
ogbn-arxiv,CAGNET-1D,2,1425,71.5669,0.6622

Epoch: 1426
Epoch: 1426
ogbn-arxiv,CAGNET-1D,2,1426,71.6168,0.6556

Epoch: 1427
Epoch: 1427
ogbn-arxiv,CAGNET-1D,2,1427,71.6665,0.6525

Epoch: 1428Epoch: 1428

ogbn-arxiv,CAGNET-1D,2,1428,71.7164,0.6636

Epoch: 1429Epoch: 1429

ogbn-arxiv,CAGNET-1D,2,1429,71.7665,0.6467

Epoch: 1430
Epoch: 1430
ogbn-arxiv,CAGNET-1D,2,1430,71.8164,0.6638

Epoch: 1431
Epoch: 1431
ogbn-arxiv,CAGNET-1D,2,1431,71.8663,0.6520

Epoch: 1432
Epoch: 1432
ogbn-arxiv,CAGNET-1D,2,1432,71.9163,0.6576

Epoch: 1433
Epoch: 1433
ogbn-arxiv,CAGNET-1D,2,1433,71.9663,0.6582

Epoch: 1434
Epoch: 1434
ogbn-arxiv,CAGNET-1D,2,1434,72.0159,0.6521

Epoch: 1435
Epoch: 1435
ogbn-arxiv,CAGNET-1D,2,1435,72.0656,0.6607

Epoch: 1436
Epoch: 1436
ogbn-arxiv,CAGNET-1D,2,1436,72.1155,0.6532

Epoch: 1437
Epoch: 1437
ogbn-arxiv,CAGNET-1D,2,1437,72.1654,0.6573

Epoch: 1438
Epoch: 1438
ogbn-arxiv,CAGNET-1D,2,1438,72.2152,0.6586

Epoch: 1439
Epoch: 1439
ogbn-arxiv,CAGNET-1D,2,1439,72.2693,0.6519

Epoch: 1440
Epoch: 1440
ogbn-arxiv,CAGNET-1D,2,1440,72.3193,0.6627

Epoch: 1441
Epoch: 1441
ogbn-arxiv,CAGNET-1D,2,1441,72.3695,0.6483

Epoch: 1442
Epoch: 1442
ogbn-arxiv,CAGNET-1D,2,1442,72.4193,0.6636

Epoch: 1443
Epoch: 1443
ogbn-arxiv,CAGNET-1D,2,1443,72.4691,0.6497

Epoch: 1444
Epoch: 1444
ogbn-arxiv,CAGNET-1D,2,1444,72.5191,0.6601

Epoch: 1445
Epoch: 1445
ogbn-arxiv,CAGNET-1D,2,1445,72.5692,0.6549

Epoch: 1446
Epoch: 1446
ogbn-arxiv,CAGNET-1D,2,1446,72.6192,0.6565

Epoch: 1447Epoch: 1447

ogbn-arxiv,CAGNET-1D,2,1447,72.6692,0.6576

Epoch: 1448Epoch: 1448

ogbn-arxiv,CAGNET-1D,2,1448,72.7194,0.6537

Epoch: 1449
Epoch: 1449
ogbn-arxiv,CAGNET-1D,2,1449,72.7696,0.6591

Epoch: 1450
Epoch: 1450
ogbn-arxiv,CAGNET-1D,2,1450,72.8195,0.6527

Epoch: 1451
Epoch: 1451
ogbn-arxiv,CAGNET-1D,2,1451,72.8693,0.6598

Epoch: 1452
Epoch: 1452
ogbn-arxiv,CAGNET-1D,2,1452,72.9193,0.6512

Epoch: 1453
Epoch: 1453
ogbn-arxiv,CAGNET-1D,2,1453,72.9697,0.6621

Epoch: 1454
Epoch: 1454
ogbn-arxiv,CAGNET-1D,2,1454,73.0196,0.6470

Epoch: 1455
Epoch: 1455
ogbn-arxiv,CAGNET-1D,2,1455,73.0695,0.6648

Epoch: 1456
Epoch: 1456
ogbn-arxiv,CAGNET-1D,2,1456,73.1195,0.6425

Epoch: 1457
Epoch: 1457
ogbn-arxiv,CAGNET-1D,2,1457,73.1695,0.6679

Epoch: 1458
Epoch: 1458
ogbn-arxiv,CAGNET-1D,2,1458,73.2233,0.6409

Epoch: 1459
Epoch: 1459
ogbn-arxiv,CAGNET-1D,2,1459,73.2733,0.6673

Epoch: 1460
Epoch: 1460
ogbn-arxiv,CAGNET-1D,2,1460,73.3233,0.6421

Epoch: 1461
Epoch: 1461
ogbn-arxiv,CAGNET-1D,2,1461,73.3732,0.6657

Epoch: 1462Epoch: 1462

ogbn-arxiv,CAGNET-1D,2,1462,73.4232,0.6473

Epoch: 1463
Epoch: 1463
ogbn-arxiv,CAGNET-1D,2,1463,73.4732,0.6604

Epoch: 1464
Epoch: 1464
ogbn-arxiv,CAGNET-1D,2,1464,73.5231,0.6548

Epoch: 1465
Epoch: 1465
ogbn-arxiv,CAGNET-1D,2,1465,73.5728,0.6539

Epoch: 1466
Epoch: 1466
ogbn-arxiv,CAGNET-1D,2,1466,73.6226,0.6615

Epoch: 1467Epoch: 1467

ogbn-arxiv,CAGNET-1D,2,1467,73.6726,0.6462

Epoch: 1468
Epoch: 1468
ogbn-arxiv,CAGNET-1D,2,1468,73.7227,0.6662

Epoch: 1469
Epoch: 1469
ogbn-arxiv,CAGNET-1D,2,1469,73.7725,0.6433

Epoch: 1470
Epoch: 1470
ogbn-arxiv,CAGNET-1D,2,1470,73.8223,0.6664

Epoch: 1471Epoch: 1471

ogbn-arxiv,CAGNET-1D,2,1471,73.8721,0.6447

Epoch: 1472
Epoch: 1472
ogbn-arxiv,CAGNET-1D,2,1472,73.9223,0.6636

Epoch: 1473
Epoch: 1473
ogbn-arxiv,CAGNET-1D,2,1473,73.9719,0.6500

Epoch: 1474Epoch: 1474

ogbn-arxiv,CAGNET-1D,2,1474,74.0215,0.6579

Epoch: 1475Epoch: 1475

ogbn-arxiv,CAGNET-1D,2,1475,74.0714,0.6563

Epoch: 1476
Epoch: 1476
ogbn-arxiv,CAGNET-1D,2,1476,74.1265,0.6530

Epoch: 1477
Epoch: 1477
ogbn-arxiv,CAGNET-1D,2,1477,74.1766,0.6600

Epoch: 1478Epoch: 1478

ogbn-arxiv,CAGNET-1D,2,1478,74.2270,0.6493

Epoch: 1479
Epoch: 1479
ogbn-arxiv,CAGNET-1D,2,1479,74.2771,0.6631

Epoch: 1480
Epoch: 1480
ogbn-arxiv,CAGNET-1D,2,1480,74.3268,0.6466

Epoch: 1481
Epoch: 1481
ogbn-arxiv,CAGNET-1D,2,1481,74.3767,0.6643

Epoch: 1482
Epoch: 1482
ogbn-arxiv,CAGNET-1D,2,1482,74.4268,0.6459

Epoch: 1483
Epoch: 1483
ogbn-arxiv,CAGNET-1D,2,1483,74.4770,0.6628

Epoch: 1484
Epoch: 1484
ogbn-arxiv,CAGNET-1D,2,1484,74.5269,0.6496

Epoch: 1485
Epoch: 1485
ogbn-arxiv,CAGNET-1D,2,1485,74.5768,0.6601

Epoch: 1486Epoch: 1486

ogbn-arxiv,CAGNET-1D,2,1486,74.6268,0.6521

Epoch: 1487
Epoch: 1487
ogbn-arxiv,CAGNET-1D,2,1487,74.6769,0.6581

Epoch: 1488
Epoch: 1488
ogbn-arxiv,CAGNET-1D,2,1488,74.7273,0.6539

Epoch: 1489
Epoch: 1489
ogbn-arxiv,CAGNET-1D,2,1489,74.7770,0.6566

Epoch: 1490Epoch: 1490

ogbn-arxiv,CAGNET-1D,2,1490,74.8269,0.6541

Epoch: 1491
Epoch: 1491
ogbn-arxiv,CAGNET-1D,2,1491,74.8771,0.6561

Epoch: 1492
Epoch: 1492
ogbn-arxiv,CAGNET-1D,2,1492,74.9275,0.6552

Epoch: 1493
Epoch: 1493
ogbn-arxiv,CAGNET-1D,2,1493,74.9770,0.6550

Epoch: 1494
Epoch: 1494
ogbn-arxiv,CAGNET-1D,2,1494,75.0269,0.6567

Epoch: 1495
Epoch: 1495
ogbn-arxiv,CAGNET-1D,2,1495,75.0787,0.6545

Epoch: 1496
Epoch: 1496
ogbn-arxiv,CAGNET-1D,2,1496,75.1286,0.6573

Epoch: 1497
Epoch: 1497
ogbn-arxiv,CAGNET-1D,2,1497,75.1783,0.6535

Epoch: 1498
Epoch: 1498
ogbn-arxiv,CAGNET-1D,2,1498,75.2282,0.6581

Epoch: 1499
Epoch: 1499
ogbn-arxiv,CAGNET-1D,2,1499,75.2781,0.6513

Epoch: 1500
Epoch: 1500
ogbn-arxiv,CAGNET-1D,2,1500,75.3278,0.6603

Epoch: 1501
Epoch: 1501
ogbn-arxiv,CAGNET-1D,2,1501,75.3780,0.6486

Epoch: 1502
Epoch: 1502
ogbn-arxiv,CAGNET-1D,2,1502,75.4283,0.6612

Epoch: 1503
Epoch: 1503
ogbn-arxiv,CAGNET-1D,2,1503,75.4780,0.6486

Epoch: 1504
Epoch: 1504
ogbn-arxiv,CAGNET-1D,2,1504,75.5278,0.6609

Epoch: 1505
Epoch: 1505
ogbn-arxiv,CAGNET-1D,2,1505,75.5778,0.6483

Epoch: 1506
Epoch: 1506
ogbn-arxiv,CAGNET-1D,2,1506,75.6279,0.6613

Epoch: 1507
Epoch: 1507
ogbn-arxiv,CAGNET-1D,2,1507,75.6781,0.6490

Epoch: 1508
Epoch: 1508
ogbn-arxiv,CAGNET-1D,2,1508,75.7295,0.6606

Epoch: 1509
Epoch: 1509
ogbn-arxiv,CAGNET-1D,2,1509,75.7802,0.6491

Epoch: 1510
Epoch: 1510
ogbn-arxiv,CAGNET-1D,2,1510,75.8307,0.6604

Epoch: 1511
Epoch: 1511
ogbn-arxiv,CAGNET-1D,2,1511,75.8805,0.6506

Epoch: 1512
Epoch: 1512
ogbn-arxiv,CAGNET-1D,2,1512,75.9306,0.6577

Epoch: 1513
Epoch: 1513
ogbn-arxiv,CAGNET-1D,2,1513,75.9831,0.6543

Epoch: 1514
Epoch: 1514
ogbn-arxiv,CAGNET-1D,2,1514,76.0329,0.6538

Epoch: 1515
Epoch: 1515
ogbn-arxiv,CAGNET-1D,2,1515,76.0828,0.6587

Epoch: 1516
Epoch: 1516
ogbn-arxiv,CAGNET-1D,2,1516,76.1327,0.6493

Epoch: 1517
Epoch: 1517
ogbn-arxiv,CAGNET-1D,2,1517,76.1828,0.6620

Epoch: 1518
Epoch: 1518
ogbn-arxiv,CAGNET-1D,2,1518,76.2328,0.6465

Epoch: 1519
Epoch: 1519
ogbn-arxiv,CAGNET-1D,2,1519,76.2828,0.6646

Epoch: 1520Epoch: 1520

ogbn-arxiv,CAGNET-1D,2,1520,76.3330,0.6424

Epoch: 1521
Epoch: 1521
ogbn-arxiv,CAGNET-1D,2,1521,76.3829,0.6677

Epoch: 1522
Epoch: 1522
ogbn-arxiv,CAGNET-1D,2,1522,76.4332,0.6361

Epoch: 1523
Epoch: 1523
ogbn-arxiv,CAGNET-1D,2,1523,76.4829,0.6710

Epoch: 1524
Epoch: 1524
ogbn-arxiv,CAGNET-1D,2,1524,76.5330,0.6300

Epoch: 1525
Epoch: 1525
ogbn-arxiv,CAGNET-1D,2,1525,76.5830,0.6724

Epoch: 1526
Epoch: 1526
ogbn-arxiv,CAGNET-1D,2,1526,76.6329,0.6293

Epoch: 1527
Epoch: 1527
ogbn-arxiv,CAGNET-1D,2,1527,76.6829,0.6706

Epoch: 1528
Epoch: 1528
ogbn-arxiv,CAGNET-1D,2,1528,76.7331,0.6383

Epoch: 1529
Epoch: 1529
ogbn-arxiv,CAGNET-1D,2,1529,76.7831,0.6626

Epoch: 1530
Epoch: 1530
ogbn-arxiv,CAGNET-1D,2,1530,76.8332,0.6514

Epoch: 1531
Epoch: 1531
ogbn-arxiv,CAGNET-1D,2,1531,76.8832,0.6516

Epoch: 1532
Epoch: 1532
ogbn-arxiv,CAGNET-1D,2,1532,76.9336,0.6628

Epoch: 1533Epoch: 1533

ogbn-arxiv,CAGNET-1D,2,1533,76.9836,0.6419

Epoch: 1534Epoch: 1534

ogbn-arxiv,CAGNET-1D,2,1534,77.0338,0.6678

Epoch: 1535
Epoch: 1535
ogbn-arxiv,CAGNET-1D,2,1535,77.0836,0.6379

Epoch: 1536
Epoch: 1536
ogbn-arxiv,CAGNET-1D,2,1536,77.1337,0.6671

Epoch: 1537
Epoch: 1537
ogbn-arxiv,CAGNET-1D,2,1537,77.1842,0.6432

Epoch: 1538
Epoch: 1538
ogbn-arxiv,CAGNET-1D,2,1538,77.2343,0.6611

Epoch: 1539
Epoch: 1539
ogbn-arxiv,CAGNET-1D,2,1539,77.2840,0.6524

Epoch: 1540
Epoch: 1540
ogbn-arxiv,CAGNET-1D,2,1540,77.3341,0.6520

Epoch: 1541
Epoch: 1541
ogbn-arxiv,CAGNET-1D,2,1541,77.3840,0.6619

Epoch: 1542
Epoch: 1542
ogbn-arxiv,CAGNET-1D,2,1542,77.4340,0.6431

Epoch: 1543
Epoch: 1543
ogbn-arxiv,CAGNET-1D,2,1543,77.4838,0.6662

Epoch: 1544
Epoch: 1544
ogbn-arxiv,CAGNET-1D,2,1544,77.5335,0.6410

Epoch: 1545Epoch: 1545

ogbn-arxiv,CAGNET-1D,2,1545,77.5834,0.6645

Epoch: 1546Epoch: 1546

ogbn-arxiv,CAGNET-1D,2,1546,77.6335,0.6457

Epoch: 1547
Epoch: 1547
ogbn-arxiv,CAGNET-1D,2,1547,77.6833,0.6594

Epoch: 1548
Epoch: 1548
ogbn-arxiv,CAGNET-1D,2,1548,77.7329,0.6526

Epoch: 1549Epoch: 1549

ogbn-arxiv,CAGNET-1D,2,1549,77.7828,0.6537

Epoch: 1550
Epoch: 1550
ogbn-arxiv,CAGNET-1D,2,1550,77.8367,0.6578

Epoch: 1551
Epoch: 1551
ogbn-arxiv,CAGNET-1D,2,1551,77.8868,0.6488

Epoch: 1552
Epoch: 1552
ogbn-arxiv,CAGNET-1D,2,1552,77.9370,0.6617

Epoch: 1553
Epoch: 1553
ogbn-arxiv,CAGNET-1D,2,1553,77.9869,0.6457

Epoch: 1554
Epoch: 1554
ogbn-arxiv,CAGNET-1D,2,1554,78.0369,0.6624

Epoch: 1555
Epoch: 1555
ogbn-arxiv,CAGNET-1D,2,1555,78.0871,0.6464

Epoch: 1556
Epoch: 1556
ogbn-arxiv,CAGNET-1D,2,1556,78.1370,0.6604

Epoch: 1557
Epoch: 1557
ogbn-arxiv,CAGNET-1D,2,1557,78.1869,0.6492

Epoch: 1558
Epoch: 1558
ogbn-arxiv,CAGNET-1D,2,1558,78.2365,0.6583

Epoch: 1559
Epoch: 1559
ogbn-arxiv,CAGNET-1D,2,1559,78.2863,0.6512

Epoch: 1560Epoch: 1560

ogbn-arxiv,CAGNET-1D,2,1560,78.3366,0.6563

Epoch: 1561
Epoch: 1561
ogbn-arxiv,CAGNET-1D,2,1561,78.3866,0.6540

Epoch: 1562
Epoch: 1562
ogbn-arxiv,CAGNET-1D,2,1562,78.4367,0.6537

Epoch: 1563
Epoch: 1563
ogbn-arxiv,CAGNET-1D,2,1563,78.4862,0.6555

Epoch: 1564Epoch: 1564

ogbn-arxiv,CAGNET-1D,2,1564,78.5361,0.6531

Epoch: 1565Epoch: 1565

ogbn-arxiv,CAGNET-1D,2,1565,78.5861,0.6557

Epoch: 1566
Epoch: 1566
ogbn-arxiv,CAGNET-1D,2,1566,78.6361,0.6534

Epoch: 1567Epoch: 1567

ogbn-arxiv,CAGNET-1D,2,1567,78.6856,0.6556

Epoch: 1568Epoch: 1568

ogbn-arxiv,CAGNET-1D,2,1568,78.7357,0.6540

Epoch: 1569
Epoch: 1569
ogbn-arxiv,CAGNET-1D,2,1569,78.7878,0.6541

Epoch: 1570
Epoch: 1570
ogbn-arxiv,CAGNET-1D,2,1570,78.8373,0.6555

Epoch: 1571
Epoch: 1571
ogbn-arxiv,CAGNET-1D,2,1571,78.8873,0.6530

Epoch: 1572Epoch: 1572

ogbn-arxiv,CAGNET-1D,2,1572,78.9375,0.6562

Epoch: 1573
Epoch: 1573
ogbn-arxiv,CAGNET-1D,2,1573,78.9880,0.6518

Epoch: 1574
Epoch: 1574
ogbn-arxiv,CAGNET-1D,2,1574,79.0386,0.6574

Epoch: 1575
Epoch: 1575
ogbn-arxiv,CAGNET-1D,2,1575,79.0891,0.6517

Epoch: 1576
Epoch: 1576
ogbn-arxiv,CAGNET-1D,2,1576,79.1390,0.6559

Epoch: 1577
Epoch: 1577
ogbn-arxiv,CAGNET-1D,2,1577,79.1890,0.6523

Epoch: 1578
Epoch: 1578
ogbn-arxiv,CAGNET-1D,2,1578,79.2390,0.6534

Epoch: 1579
Epoch: 1579
ogbn-arxiv,CAGNET-1D,2,1579,79.2894,0.6547

Epoch: 1580
Epoch: 1580
ogbn-arxiv,CAGNET-1D,2,1580,79.3391,0.6509

Epoch: 1581
Epoch: 1581
ogbn-arxiv,CAGNET-1D,2,1581,79.3895,0.6566

Epoch: 1582
Epoch: 1582
ogbn-arxiv,CAGNET-1D,2,1582,79.4395,0.6481

Epoch: 1583
Epoch: 1583
ogbn-arxiv,CAGNET-1D,2,1583,79.4894,0.6583

Epoch: 1584
Epoch: 1584
ogbn-arxiv,CAGNET-1D,2,1584,79.5393,0.6465

Epoch: 1585
Epoch: 1585
ogbn-arxiv,CAGNET-1D,2,1585,79.5893,0.6609

Epoch: 1586Epoch: 1586

ogbn-arxiv,CAGNET-1D,2,1586,79.6394,0.6449

Epoch: 1587
Epoch: 1587
ogbn-arxiv,CAGNET-1D,2,1587,79.6947,0.6631

Epoch: 1588
Epoch: 1588
ogbn-arxiv,CAGNET-1D,2,1588,79.7448,0.6441

Epoch: 1589
Epoch: 1589
ogbn-arxiv,CAGNET-1D,2,1589,79.7948,0.6639

Epoch: 1590
Epoch: 1590
ogbn-arxiv,CAGNET-1D,2,1590,79.8452,0.6423

Epoch: 1591
Epoch: 1591
ogbn-arxiv,CAGNET-1D,2,1591,79.8949,0.6643

Epoch: 1592Epoch: 1592

ogbn-arxiv,CAGNET-1D,2,1592,79.9449,0.6418

Epoch: 1593
Epoch: 1593
ogbn-arxiv,CAGNET-1D,2,1593,79.9953,0.6641

Epoch: 1594
Epoch: 1594
ogbn-arxiv,CAGNET-1D,2,1594,80.0453,0.6418

Epoch: 1595
Epoch: 1595
ogbn-arxiv,CAGNET-1D,2,1595,80.0951,0.6640

Epoch: 1596
Epoch: 1596
ogbn-arxiv,CAGNET-1D,2,1596,80.1454,0.6434

Epoch: 1597
Epoch: 1597
ogbn-arxiv,CAGNET-1D,2,1597,80.1954,0.6632

Epoch: 1598
Epoch: 1598
ogbn-arxiv,CAGNET-1D,2,1598,80.2452,0.6448

Epoch: 1599
Epoch: 1599
ogbn-arxiv,CAGNET-1D,2,1599,80.2950,0.6604

Epoch: 1600
Epoch: 1600
ogbn-arxiv,CAGNET-1D,2,1600,80.3450,0.6493

Epoch: 1601
Epoch: 1601
ogbn-arxiv,CAGNET-1D,2,1601,80.3950,0.6555

Epoch: 1602
Epoch: 1602
ogbn-arxiv,CAGNET-1D,2,1602,80.4451,0.6534

Epoch: 1603
Epoch: 1603
ogbn-arxiv,CAGNET-1D,2,1603,80.4949,0.6529

Epoch: 1604Epoch: 1604

ogbn-arxiv,CAGNET-1D,2,1604,80.5449,0.6556

Epoch: 1605
Epoch: 1605
ogbn-arxiv,CAGNET-1D,2,1605,80.5981,0.6519

Epoch: 1606
Epoch: 1606
ogbn-arxiv,CAGNET-1D,2,1606,80.6488,0.6572

Epoch: 1607
Epoch: 1607
ogbn-arxiv,CAGNET-1D,2,1607,80.6987,0.6501

Epoch: 1608
Epoch: 1608
ogbn-arxiv,CAGNET-1D,2,1608,80.7488,0.6579

Epoch: 1609
Epoch: 1609
ogbn-arxiv,CAGNET-1D,2,1609,80.7989,0.6490

Epoch: 1610
Epoch: 1610
ogbn-arxiv,CAGNET-1D,2,1610,80.8487,0.6590

Epoch: 1611
Epoch: 1611
ogbn-arxiv,CAGNET-1D,2,1611,80.8988,0.6478

Epoch: 1612
Epoch: 1612
ogbn-arxiv,CAGNET-1D,2,1612,80.9488,0.6598

Epoch: 1613
Epoch: 1613
ogbn-arxiv,CAGNET-1D,2,1613,80.9985,0.6463

Epoch: 1614
Epoch: 1614
ogbn-arxiv,CAGNET-1D,2,1614,81.0491,0.6611

Epoch: 1615
Epoch: 1615
ogbn-arxiv,CAGNET-1D,2,1615,81.0992,0.6438

Epoch: 1616
Epoch: 1616
ogbn-arxiv,CAGNET-1D,2,1616,81.1489,0.6641

Epoch: 1617Epoch: 1617

ogbn-arxiv,CAGNET-1D,2,1617,81.1986,0.6389

Epoch: 1618
Epoch: 1618
ogbn-arxiv,CAGNET-1D,2,1618,81.2488,0.6674

Epoch: 1619
Epoch: 1619
ogbn-arxiv,CAGNET-1D,2,1619,81.2987,0.6336

Epoch: 1620
Epoch: 1620
ogbn-arxiv,CAGNET-1D,2,1620,81.3486,0.6694

Epoch: 1621
Epoch: 1621
ogbn-arxiv,CAGNET-1D,2,1621,81.3985,0.6307

Epoch: 1622
Epoch: 1622
ogbn-arxiv,CAGNET-1D,2,1622,81.4483,0.6696

Epoch: 1623
Epoch: 1623
ogbn-arxiv,CAGNET-1D,2,1623,81.4983,0.6321

Epoch: 1624
Epoch: 1624
ogbn-arxiv,CAGNET-1D,2,1624,81.5523,0.6680

Epoch: 1625
Epoch: 1625
ogbn-arxiv,CAGNET-1D,2,1625,81.6034,0.6380

Epoch: 1626
Epoch: 1626
ogbn-arxiv,CAGNET-1D,2,1626,81.6538,0.6629

Epoch: 1627
Epoch: 1627
ogbn-arxiv,CAGNET-1D,2,1627,81.7038,0.6480

Epoch: 1628
Epoch: 1628
ogbn-arxiv,CAGNET-1D,2,1628,81.7541,0.6544

Epoch: 1629
Epoch: 1629
ogbn-arxiv,CAGNET-1D,2,1629,81.8042,0.6580

Epoch: 1630
Epoch: 1630
ogbn-arxiv,CAGNET-1D,2,1630,81.8550,0.6452

Epoch: 1631
Epoch: 1631
ogbn-arxiv,CAGNET-1D,2,1631,81.9063,0.6643

Epoch: 1632
Epoch: 1632
ogbn-arxiv,CAGNET-1D,2,1632,81.9569,0.6391

Epoch: 1633
Epoch: 1633
ogbn-arxiv,CAGNET-1D,2,1633,82.0069,0.6665

Epoch: 1634
Epoch: 1634
ogbn-arxiv,CAGNET-1D,2,1634,82.0571,0.6374

Epoch: 1635
Epoch: 1635
ogbn-arxiv,CAGNET-1D,2,1635,82.1072,0.6659

Epoch: 1636
Epoch: 1636
ogbn-arxiv,CAGNET-1D,2,1636,82.1570,0.6417

Epoch: 1637
Epoch: 1637
ogbn-arxiv,CAGNET-1D,2,1637,82.2069,0.6619

Epoch: 1638
Epoch: 1638
ogbn-arxiv,CAGNET-1D,2,1638,82.2569,0.6480

Epoch: 1639
Epoch: 1639
ogbn-arxiv,CAGNET-1D,2,1639,82.3068,0.6558

Epoch: 1640Epoch: 1640

ogbn-arxiv,CAGNET-1D,2,1640,82.3566,0.6544

Epoch: 1641
Epoch: 1641
ogbn-arxiv,CAGNET-1D,2,1641,82.4065,0.6500

Epoch: 1642
Epoch: 1642
ogbn-arxiv,CAGNET-1D,2,1642,82.4598,0.6594

Epoch: 1643
Epoch: 1643
ogbn-arxiv,CAGNET-1D,2,1643,82.5098,0.6447

Epoch: 1644
Epoch: 1644
ogbn-arxiv,CAGNET-1D,2,1644,82.5596,0.6631

Epoch: 1645
Epoch: 1645
ogbn-arxiv,CAGNET-1D,2,1645,82.6094,0.6414

Epoch: 1646
Epoch: 1646
ogbn-arxiv,CAGNET-1D,2,1646,82.6596,0.6643

Epoch: 1647
Epoch: 1647
ogbn-arxiv,CAGNET-1D,2,1647,82.7100,0.6410

Epoch: 1648
Epoch: 1648
ogbn-arxiv,CAGNET-1D,2,1648,82.7597,0.6639

Epoch: 1649
Epoch: 1649
ogbn-arxiv,CAGNET-1D,2,1649,82.8095,0.6427

Epoch: 1650
Epoch: 1650
ogbn-arxiv,CAGNET-1D,2,1650,82.8595,0.6623

Epoch: 1651
Epoch: 1651
ogbn-arxiv,CAGNET-1D,2,1651,82.9096,0.6461

Epoch: 1652
Epoch: 1652
ogbn-arxiv,CAGNET-1D,2,1652,82.9592,0.6580

Epoch: 1653
Epoch: 1653
ogbn-arxiv,CAGNET-1D,2,1653,83.0090,0.6515

Epoch: 1654Epoch: 1654

ogbn-arxiv,CAGNET-1D,2,1654,83.0591,0.6537

Epoch: 1655
Epoch: 1655
ogbn-arxiv,CAGNET-1D,2,1655,83.1093,0.6546

Epoch: 1656
Epoch: 1656
ogbn-arxiv,CAGNET-1D,2,1656,83.1592,0.6494

Epoch: 1657
Epoch: 1657
ogbn-arxiv,CAGNET-1D,2,1657,83.2089,0.6588

Epoch: 1658Epoch: 1658

ogbn-arxiv,CAGNET-1D,2,1658,83.2588,0.6438

Epoch: 1659Epoch: 1659

ogbn-arxiv,CAGNET-1D,2,1659,83.3091,0.6598

Epoch: 1660
Epoch: 1660
ogbn-arxiv,CAGNET-1D,2,1660,83.3596,0.6403

Epoch: 1661
Epoch: 1661
ogbn-arxiv,CAGNET-1D,2,1661,83.4132,0.6596

Epoch: 1662
Epoch: 1662
ogbn-arxiv,CAGNET-1D,2,1662,83.4633,0.6441

Epoch: 1663
Epoch: 1663
ogbn-arxiv,CAGNET-1D,2,1663,83.5132,0.6566

Epoch: 1664Epoch: 1664

ogbn-arxiv,CAGNET-1D,2,1664,83.5633,0.6514

Epoch: 1665
Epoch: 1665
ogbn-arxiv,CAGNET-1D,2,1665,83.6131,0.6513

Epoch: 1666
Epoch: 1666
ogbn-arxiv,CAGNET-1D,2,1666,83.6633,0.6578

Epoch: 1667
Epoch: 1667
ogbn-arxiv,CAGNET-1D,2,1667,83.7132,0.6461

Epoch: 1668
Epoch: 1668
ogbn-arxiv,CAGNET-1D,2,1668,83.7634,0.6618

Epoch: 1669
Epoch: 1669
ogbn-arxiv,CAGNET-1D,2,1669,83.8133,0.6426

Epoch: 1670
Epoch: 1670
ogbn-arxiv,CAGNET-1D,2,1670,83.8633,0.6605

Epoch: 1671
Epoch: 1671
ogbn-arxiv,CAGNET-1D,2,1671,83.9131,0.6451

Epoch: 1672
Epoch: 1672
ogbn-arxiv,CAGNET-1D,2,1672,83.9632,0.6570

Epoch: 1673
Epoch: 1673
ogbn-arxiv,CAGNET-1D,2,1673,84.0132,0.6515

Epoch: 1674
Epoch: 1674
ogbn-arxiv,CAGNET-1D,2,1674,84.0629,0.6508

Epoch: 1675
Epoch: 1675
ogbn-arxiv,CAGNET-1D,2,1675,84.1127,0.6581

Epoch: 1676
Epoch: 1676
ogbn-arxiv,CAGNET-1D,2,1676,84.1627,0.6441

Epoch: 1677
Epoch: 1677
ogbn-arxiv,CAGNET-1D,2,1677,84.2129,0.6635

Epoch: 1678
Epoch: 1678
ogbn-arxiv,CAGNET-1D,2,1678,84.2627,0.6392

Epoch: 1679
Epoch: 1679
ogbn-arxiv,CAGNET-1D,2,1679,84.3150,0.6647

Epoch: 1680
Epoch: 1680
ogbn-arxiv,CAGNET-1D,2,1680,84.3651,0.6386

Epoch: 1681
Epoch: 1681
ogbn-arxiv,CAGNET-1D,2,1681,84.4150,0.6642

Epoch: 1682
Epoch: 1682
ogbn-arxiv,CAGNET-1D,2,1682,84.4648,0.6406

Epoch: 1683
Epoch: 1683
ogbn-arxiv,CAGNET-1D,2,1683,84.5148,0.6628

Epoch: 1684
Epoch: 1684
ogbn-arxiv,CAGNET-1D,2,1684,84.5649,0.6452

Epoch: 1685
Epoch: 1685
ogbn-arxiv,CAGNET-1D,2,1685,84.6151,0.6580

Epoch: 1686
Epoch: 1686
ogbn-arxiv,CAGNET-1D,2,1686,84.6647,0.6503

Epoch: 1687Epoch: 1687

ogbn-arxiv,CAGNET-1D,2,1687,84.7145,0.6535

Epoch: 1688
Epoch: 1688
ogbn-arxiv,CAGNET-1D,2,1688,84.7644,0.6541

Epoch: 1689
Epoch: 1689
ogbn-arxiv,CAGNET-1D,2,1689,84.8145,0.6504

Epoch: 1690
Epoch: 1690
ogbn-arxiv,CAGNET-1D,2,1690,84.8645,0.6554

Epoch: 1691
Epoch: 1691
ogbn-arxiv,CAGNET-1D,2,1691,84.9140,0.6501

Epoch: 1692
Epoch: 1692
ogbn-arxiv,CAGNET-1D,2,1692,84.9637,0.6542

Epoch: 1693
Epoch: 1693
ogbn-arxiv,CAGNET-1D,2,1693,85.0137,0.6509

Epoch: 1694
Epoch: 1694
ogbn-arxiv,CAGNET-1D,2,1694,85.0639,0.6548

Epoch: 1695
Epoch: 1695
ogbn-arxiv,CAGNET-1D,2,1695,85.1143,0.6509

Epoch: 1696
Epoch: 1696
ogbn-arxiv,CAGNET-1D,2,1696,85.1640,0.6544

Epoch: 1697
Epoch: 1697
ogbn-arxiv,CAGNET-1D,2,1697,85.2137,0.6505

Epoch: 1698Epoch: 1698

ogbn-arxiv,CAGNET-1D,2,1698,85.2652,0.6553

Epoch: 1699
Epoch: 1699
ogbn-arxiv,CAGNET-1D,2,1699,85.3152,0.6476

Epoch: 1700
Epoch: 1700
ogbn-arxiv,CAGNET-1D,2,1700,85.3648,0.6590

Epoch: 1701
Epoch: 1701
ogbn-arxiv,CAGNET-1D,2,1701,85.4144,0.6427

Epoch: 1702
Epoch: 1702
ogbn-arxiv,CAGNET-1D,2,1702,85.4648,0.6622

Epoch: 1703
Epoch: 1703
ogbn-arxiv,CAGNET-1D,2,1703,85.5153,0.6390

Epoch: 1704
Epoch: 1704
ogbn-arxiv,CAGNET-1D,2,1704,85.5652,0.6652

Epoch: 1705
Epoch: 1705
ogbn-arxiv,CAGNET-1D,2,1705,85.6152,0.6354

Epoch: 1706
Epoch: 1706
ogbn-arxiv,CAGNET-1D,2,1706,85.6649,0.6670

Epoch: 1707
Epoch: 1707
ogbn-arxiv,CAGNET-1D,2,1707,85.7149,0.6315

Epoch: 1708
Epoch: 1708
ogbn-arxiv,CAGNET-1D,2,1708,85.7650,0.6671

Epoch: 1709
Epoch: 1709
ogbn-arxiv,CAGNET-1D,2,1709,85.8150,0.6334

Epoch: 1710
Epoch: 1710
ogbn-arxiv,CAGNET-1D,2,1710,85.8649,0.6643

Epoch: 1711
Epoch: 1711
ogbn-arxiv,CAGNET-1D,2,1711,85.9151,0.6431

Epoch: 1712
Epoch: 1712
ogbn-arxiv,CAGNET-1D,2,1712,85.9655,0.6577

Epoch: 1713
Epoch: 1713
ogbn-arxiv,CAGNET-1D,2,1713,86.0153,0.6519

Epoch: 1714
Epoch: 1714
ogbn-arxiv,CAGNET-1D,2,1714,86.0653,0.6490

Epoch: 1715
Epoch: 1715
ogbn-arxiv,CAGNET-1D,2,1715,86.1152,0.6601

Epoch: 1716
Epoch: 1716
ogbn-arxiv,CAGNET-1D,2,1716,86.1670,0.6409

Epoch: 1717
Epoch: 1717
ogbn-arxiv,CAGNET-1D,2,1717,86.2176,0.6648

Epoch: 1718
Epoch: 1718
ogbn-arxiv,CAGNET-1D,2,1718,86.2676,0.6360

Epoch: 1719
Epoch: 1719
ogbn-arxiv,CAGNET-1D,2,1719,86.3175,0.6650

Epoch: 1720
Epoch: 1720
ogbn-arxiv,CAGNET-1D,2,1720,86.3673,0.6383

Epoch: 1721
Epoch: 1721
ogbn-arxiv,CAGNET-1D,2,1721,86.4171,0.6629

Epoch: 1722
Epoch: 1722
ogbn-arxiv,CAGNET-1D,2,1722,86.4672,0.6429

Epoch: 1723
Epoch: 1723
ogbn-arxiv,CAGNET-1D,2,1723,86.5171,0.6591

Epoch: 1724
Epoch: 1724
ogbn-arxiv,CAGNET-1D,2,1724,86.5670,0.6482

Epoch: 1725
Epoch: 1725
ogbn-arxiv,CAGNET-1D,2,1725,86.6170,0.6537

Epoch: 1726
Epoch: 1726
ogbn-arxiv,CAGNET-1D,2,1726,86.6668,0.6534

Epoch: 1727
Epoch: 1727
ogbn-arxiv,CAGNET-1D,2,1727,86.7169,0.6498

Epoch: 1728
Epoch: 1728
ogbn-arxiv,CAGNET-1D,2,1728,86.7672,0.6566

Epoch: 1729
Epoch: 1729
ogbn-arxiv,CAGNET-1D,2,1729,86.8171,0.6473

Epoch: 1730
Epoch: 1730
ogbn-arxiv,CAGNET-1D,2,1730,86.8675,0.6595

Epoch: 1731
Epoch: 1731
ogbn-arxiv,CAGNET-1D,2,1731,86.9174,0.6435

Epoch: 1732
Epoch: 1732
ogbn-arxiv,CAGNET-1D,2,1732,86.9670,0.6620

Epoch: 1733
Epoch: 1733
ogbn-arxiv,CAGNET-1D,2,1733,87.0167,0.6411

Epoch: 1734
Epoch: 1734
ogbn-arxiv,CAGNET-1D,2,1734,87.0669,0.6621

Epoch: 1735
Epoch: 1735
ogbn-arxiv,CAGNET-1D,2,1735,87.1208,0.6411

Epoch: 1736
Epoch: 1736
ogbn-arxiv,CAGNET-1D,2,1736,87.1709,0.6612

Epoch: 1737
Epoch: 1737
ogbn-arxiv,CAGNET-1D,2,1737,87.2208,0.6437

Epoch: 1738
Epoch: 1738
ogbn-arxiv,CAGNET-1D,2,1738,87.2719,0.6575

Epoch: 1739
Epoch: 1739
ogbn-arxiv,CAGNET-1D,2,1739,87.3217,0.6485

Epoch: 1740
Epoch: 1740
ogbn-arxiv,CAGNET-1D,2,1740,87.3715,0.6521

Epoch: 1741
Epoch: 1741
ogbn-arxiv,CAGNET-1D,2,1741,87.4217,0.6540

Epoch: 1742
Epoch: 1742
ogbn-arxiv,CAGNET-1D,2,1742,87.4720,0.6469

Epoch: 1743
Epoch: 1743
ogbn-arxiv,CAGNET-1D,2,1743,87.5215,0.6585

Epoch: 1744
Epoch: 1744
ogbn-arxiv,CAGNET-1D,2,1744,87.5711,0.6445

Epoch: 1745
Epoch: 1745
ogbn-arxiv,CAGNET-1D,2,1745,87.6209,0.6597

Epoch: 1746
Epoch: 1746
ogbn-arxiv,CAGNET-1D,2,1746,87.6710,0.6435

Epoch: 1747Epoch: 1747

ogbn-arxiv,CAGNET-1D,2,1747,87.7210,0.6597

Epoch: 1748Epoch: 1748

ogbn-arxiv,CAGNET-1D,2,1748,87.7707,0.6437

Epoch: 1749
Epoch: 1749
ogbn-arxiv,CAGNET-1D,2,1749,87.8202,0.6592

Epoch: 1750
Epoch: 1750
ogbn-arxiv,CAGNET-1D,2,1750,87.8703,0.6464

Epoch: 1751
Epoch: 1751
ogbn-arxiv,CAGNET-1D,2,1751,87.9203,0.6558

Epoch: 1752
Epoch: 1752
ogbn-arxiv,CAGNET-1D,2,1752,87.9702,0.6495

Epoch: 1753
Epoch: 1753
ogbn-arxiv,CAGNET-1D,2,1753,88.0199,0.6531

Epoch: 1754
Epoch: 1754
ogbn-arxiv,CAGNET-1D,2,1754,88.0722,0.6524

Epoch: 1755
Epoch: 1755
ogbn-arxiv,CAGNET-1D,2,1755,88.1221,0.6509

Epoch: 1756
Epoch: 1756
ogbn-arxiv,CAGNET-1D,2,1756,88.1726,0.6538

Epoch: 1757
Epoch: 1757
ogbn-arxiv,CAGNET-1D,2,1757,88.2229,0.6502

Epoch: 1758
Epoch: 1758
ogbn-arxiv,CAGNET-1D,2,1758,88.2726,0.6545

Epoch: 1759Epoch: 1759

ogbn-arxiv,CAGNET-1D,2,1759,88.3223,0.6496

Epoch: 1760
Epoch: 1760
ogbn-arxiv,CAGNET-1D,2,1760,88.3727,0.6545

Epoch: 1761
Epoch: 1761
ogbn-arxiv,CAGNET-1D,2,1761,88.4229,0.6491

Epoch: 1762Epoch: 1762

ogbn-arxiv,CAGNET-1D,2,1762,88.4724,0.6544

Epoch: 1763
Epoch: 1763
ogbn-arxiv,CAGNET-1D,2,1763,88.5222,0.6495

Epoch: 1764
Epoch: 1764
ogbn-arxiv,CAGNET-1D,2,1764,88.5721,0.6536

Epoch: 1765
Epoch: 1765
ogbn-arxiv,CAGNET-1D,2,1765,88.6222,0.6493

Epoch: 1766
Epoch: 1766
ogbn-arxiv,CAGNET-1D,2,1766,88.6721,0.6538

Epoch: 1767
Epoch: 1767
ogbn-arxiv,CAGNET-1D,2,1767,88.7217,0.6495

Epoch: 1768
Epoch: 1768
ogbn-arxiv,CAGNET-1D,2,1768,88.7716,0.6532

Epoch: 1769
Epoch: 1769
ogbn-arxiv,CAGNET-1D,2,1769,88.8219,0.6502

Epoch: 1770
Epoch: 1770
ogbn-arxiv,CAGNET-1D,2,1770,88.8724,0.6528

Epoch: 1771
Epoch: 1771
ogbn-arxiv,CAGNET-1D,2,1771,88.9221,0.6515

Epoch: 1772
Epoch: 1772
ogbn-arxiv,CAGNET-1D,2,1772,88.9757,0.6513

Epoch: 1773
Epoch: 1773
ogbn-arxiv,CAGNET-1D,2,1773,89.0255,0.6537

Epoch: 1774
Epoch: 1774
ogbn-arxiv,CAGNET-1D,2,1774,89.0750,0.6474

Epoch: 1775Epoch: 1775

ogbn-arxiv,CAGNET-1D,2,1775,89.1246,0.6576

Epoch: 1776
Epoch: 1776
ogbn-arxiv,CAGNET-1D,2,1776,89.1748,0.6413

Epoch: 1777
Epoch: 1777
ogbn-arxiv,CAGNET-1D,2,1777,89.2248,0.6631

Epoch: 1778
Epoch: 1778
ogbn-arxiv,CAGNET-1D,2,1778,89.2745,0.6323

Epoch: 1779
Epoch: 1779
ogbn-arxiv,CAGNET-1D,2,1779,89.3240,0.6677

Epoch: 1780
Epoch: 1780
ogbn-arxiv,CAGNET-1D,2,1780,89.3741,0.6244

Epoch: 1781
Epoch: 1781
ogbn-arxiv,CAGNET-1D,2,1781,89.4241,0.6698

Epoch: 1782
Epoch: 1782
ogbn-arxiv,CAGNET-1D,2,1782,89.4741,0.6249

Epoch: 1783
Epoch: 1783
ogbn-arxiv,CAGNET-1D,2,1783,89.5237,0.6672

Epoch: 1784
Epoch: 1784
ogbn-arxiv,CAGNET-1D,2,1784,89.5737,0.6320

Epoch: 1785
Epoch: 1785
ogbn-arxiv,CAGNET-1D,2,1785,89.6236,0.6620

Epoch: 1786
Epoch: 1786
ogbn-arxiv,CAGNET-1D,2,1786,89.6733,0.6443

Epoch: 1787
Epoch: 1787
ogbn-arxiv,CAGNET-1D,2,1787,89.7228,0.6550

Epoch: 1788
Epoch: 1788
ogbn-arxiv,CAGNET-1D,2,1788,89.7726,0.6528

Epoch: 1789
Epoch: 1789
ogbn-arxiv,CAGNET-1D,2,1789,89.8225,0.6463

Epoch: 1790
Epoch: 1790
ogbn-arxiv,CAGNET-1D,2,1790,89.8727,0.6597

Epoch: 1791
Epoch: 1791
ogbn-arxiv,CAGNET-1D,2,1791,89.9232,0.6402

Epoch: 1792Epoch: 1792

ogbn-arxiv,CAGNET-1D,2,1792,89.9727,0.6629

Epoch: 1793
Epoch: 1793
ogbn-arxiv,CAGNET-1D,2,1793,90.0229,0.6361

Epoch: 1794
Epoch: 1794
ogbn-arxiv,CAGNET-1D,2,1794,90.0730,0.6640

Epoch: 1795
Epoch: 1795
ogbn-arxiv,CAGNET-1D,2,1795,90.1242,0.6383

Epoch: 1796
Epoch: 1796
ogbn-arxiv,CAGNET-1D,2,1796,90.1752,0.6610

Epoch: 1797
Epoch: 1797
ogbn-arxiv,CAGNET-1D,2,1797,90.2261,0.6460

Epoch: 1798
Epoch: 1798
ogbn-arxiv,CAGNET-1D,2,1798,90.2758,0.6528

Epoch: 1799Epoch: 1799

ogbn-arxiv,CAGNET-1D,2,1799,90.3254,0.6546

Epoch: 1800
Epoch: 1800
ogbn-arxiv,CAGNET-1D,2,1800,90.3756,0.6452

Epoch: 1801
Epoch: 1801
ogbn-arxiv,CAGNET-1D,2,1801,90.4256,0.6594

Epoch: 1802
Epoch: 1802
ogbn-arxiv,CAGNET-1D,2,1802,90.4753,0.6406

Epoch: 1803
Epoch: 1803
ogbn-arxiv,CAGNET-1D,2,1803,90.5247,0.6622

Epoch: 1804
Epoch: 1804
ogbn-arxiv,CAGNET-1D,2,1804,90.5745,0.6375

Epoch: 1805
Epoch: 1805
ogbn-arxiv,CAGNET-1D,2,1805,90.6246,0.6627

Epoch: 1806
Epoch: 1806
ogbn-arxiv,CAGNET-1D,2,1806,90.6746,0.6391

Epoch: 1807
Epoch: 1807
ogbn-arxiv,CAGNET-1D,2,1807,90.7243,0.6609

Epoch: 1808Epoch: 1808

ogbn-arxiv,CAGNET-1D,2,1808,90.7744,0.6416

Epoch: 1809
Epoch: 1809
ogbn-arxiv,CAGNET-1D,2,1809,90.8287,0.6589

Epoch: 1810
Epoch: 1810
ogbn-arxiv,CAGNET-1D,2,1810,90.8786,0.6459

Epoch: 1811
Epoch: 1811
ogbn-arxiv,CAGNET-1D,2,1811,90.9289,0.6541

Epoch: 1812
Epoch: 1812
ogbn-arxiv,CAGNET-1D,2,1812,90.9793,0.6500

Epoch: 1813
Epoch: 1813
ogbn-arxiv,CAGNET-1D,2,1813,91.0293,0.6507

Epoch: 1814
Epoch: 1814
ogbn-arxiv,CAGNET-1D,2,1814,91.0794,0.6520

Epoch: 1815
Epoch: 1815
ogbn-arxiv,CAGNET-1D,2,1815,91.1289,0.6482

Epoch: 1816
Epoch: 1816
ogbn-arxiv,CAGNET-1D,2,1816,91.1787,0.6538

Epoch: 1817
Epoch: 1817
ogbn-arxiv,CAGNET-1D,2,1817,91.2288,0.6456

Epoch: 1818
Epoch: 1818
ogbn-arxiv,CAGNET-1D,2,1818,91.2790,0.6538

Epoch: 1819
Epoch: 1819
ogbn-arxiv,CAGNET-1D,2,1819,91.3285,0.6466

Epoch: 1820
Epoch: 1820
ogbn-arxiv,CAGNET-1D,2,1820,91.3781,0.6539

Epoch: 1821
Epoch: 1821
ogbn-arxiv,CAGNET-1D,2,1821,91.4283,0.6478

Epoch: 1822
Epoch: 1822
ogbn-arxiv,CAGNET-1D,2,1822,91.4784,0.6525

Epoch: 1823
Epoch: 1823
ogbn-arxiv,CAGNET-1D,2,1823,91.5280,0.6520

Epoch: 1824
Epoch: 1824
ogbn-arxiv,CAGNET-1D,2,1824,91.5777,0.6498

Epoch: 1825
Epoch: 1825
ogbn-arxiv,CAGNET-1D,2,1825,91.6276,0.6540

Epoch: 1826
Epoch: 1826
ogbn-arxiv,CAGNET-1D,2,1826,91.6776,0.6475

Epoch: 1827
Epoch: 1827
ogbn-arxiv,CAGNET-1D,2,1827,91.7275,0.6551

Epoch: 1828
Epoch: 1828
ogbn-arxiv,CAGNET-1D,2,1828,91.7802,0.6462

Epoch: 1829
Epoch: 1829
ogbn-arxiv,CAGNET-1D,2,1829,91.8301,0.6556

Epoch: 1830
Epoch: 1830
ogbn-arxiv,CAGNET-1D,2,1830,91.8801,0.6462

Epoch: 1831
Epoch: 1831
ogbn-arxiv,CAGNET-1D,2,1831,91.9300,0.6561

Epoch: 1832
Epoch: 1832
ogbn-arxiv,CAGNET-1D,2,1832,91.9799,0.6456

Epoch: 1833
Epoch: 1833
ogbn-arxiv,CAGNET-1D,2,1833,92.0298,0.6558

Epoch: 1834
Epoch: 1834
ogbn-arxiv,CAGNET-1D,2,1834,92.0798,0.6460

Epoch: 1835
Epoch: 1835
ogbn-arxiv,CAGNET-1D,2,1835,92.1297,0.6551

Epoch: 1836
Epoch: 1836
ogbn-arxiv,CAGNET-1D,2,1836,92.1797,0.6478

Epoch: 1837
Epoch: 1837
ogbn-arxiv,CAGNET-1D,2,1837,92.2298,0.6539

Epoch: 1838
Epoch: 1838
ogbn-arxiv,CAGNET-1D,2,1838,92.2795,0.6491

Epoch: 1839
Epoch: 1839
ogbn-arxiv,CAGNET-1D,2,1839,92.3293,0.6535

Epoch: 1840Epoch: 1840

ogbn-arxiv,CAGNET-1D,2,1840,92.3794,0.6495

Epoch: 1841
Epoch: 1841
ogbn-arxiv,CAGNET-1D,2,1841,92.4305,0.6527

Epoch: 1842
Epoch: 1842
ogbn-arxiv,CAGNET-1D,2,1842,92.4810,0.6509

Epoch: 1843
Epoch: 1843
ogbn-arxiv,CAGNET-1D,2,1843,92.5315,0.6508

Epoch: 1844
Epoch: 1844
ogbn-arxiv,CAGNET-1D,2,1844,92.5814,0.6522

Epoch: 1845Epoch: 1845

ogbn-arxiv,CAGNET-1D,2,1845,92.6308,0.6503

Epoch: 1846
Epoch: 1846
ogbn-arxiv,CAGNET-1D,2,1846,92.6860,0.6522

Epoch: 1847Epoch: 1847

ogbn-arxiv,CAGNET-1D,2,1847,92.7359,0.6505

Epoch: 1848
Epoch: 1848
ogbn-arxiv,CAGNET-1D,2,1848,92.7860,0.6524

Epoch: 1849
Epoch: 1849
ogbn-arxiv,CAGNET-1D,2,1849,92.8358,0.6506

Epoch: 1850
Epoch: 1850
ogbn-arxiv,CAGNET-1D,2,1850,92.8856,0.6516

Epoch: 1851
Epoch: 1851
ogbn-arxiv,CAGNET-1D,2,1851,92.9349,0.6509

Epoch: 1852
Epoch: 1852
ogbn-arxiv,CAGNET-1D,2,1852,92.9851,0.6503

Epoch: 1853
Epoch: 1853
ogbn-arxiv,CAGNET-1D,2,1853,93.0351,0.6498

Epoch: 1854
Epoch: 1854
ogbn-arxiv,CAGNET-1D,2,1854,93.0850,0.6506

Epoch: 1855
Epoch: 1855
ogbn-arxiv,CAGNET-1D,2,1855,93.1348,0.6493

Epoch: 1856
Epoch: 1856
ogbn-arxiv,CAGNET-1D,2,1856,93.1845,0.6495

Epoch: 1857
Epoch: 1857
ogbn-arxiv,CAGNET-1D,2,1857,93.2347,0.6490

Epoch: 1858
Epoch: 1858
ogbn-arxiv,CAGNET-1D,2,1858,93.2847,0.6493

Epoch: 1859
Epoch: 1859
ogbn-arxiv,CAGNET-1D,2,1859,93.3343,0.6483

Epoch: 1860
Epoch: 1860
ogbn-arxiv,CAGNET-1D,2,1860,93.3845,0.6498

Epoch: 1861
Epoch: 1861
ogbn-arxiv,CAGNET-1D,2,1861,93.4347,0.6506

Epoch: 1862
Epoch: 1862
ogbn-arxiv,CAGNET-1D,2,1862,93.4846,0.6504

Epoch: 1863
Epoch: 1863
ogbn-arxiv,CAGNET-1D,2,1863,93.5347,0.6522

Epoch: 1864
Epoch: 1864
ogbn-arxiv,CAGNET-1D,2,1864,93.5861,0.6480

Epoch: 1865
Epoch: 1865
ogbn-arxiv,CAGNET-1D,2,1865,93.6388,0.6543

Epoch: 1866
Epoch: 1866
ogbn-arxiv,CAGNET-1D,2,1866,93.6889,0.6443

Epoch: 1867
Epoch: 1867
ogbn-arxiv,CAGNET-1D,2,1867,93.7390,0.6551

Epoch: 1868
Epoch: 1868
ogbn-arxiv,CAGNET-1D,2,1868,93.7891,0.6449

Epoch: 1869
Epoch: 1869
ogbn-arxiv,CAGNET-1D,2,1869,93.8389,0.6548

Epoch: 1870
Epoch: 1870
ogbn-arxiv,CAGNET-1D,2,1870,93.8890,0.6461

Epoch: 1871
Epoch: 1871
ogbn-arxiv,CAGNET-1D,2,1871,93.9390,0.6528

Epoch: 1872
Epoch: 1872
ogbn-arxiv,CAGNET-1D,2,1872,93.9888,0.6516

Epoch: 1873Epoch: 1873

ogbn-arxiv,CAGNET-1D,2,1873,94.0383,0.6476

Epoch: 1874
Epoch: 1874
ogbn-arxiv,CAGNET-1D,2,1874,94.0880,0.6570

Epoch: 1875
Epoch: 1875
ogbn-arxiv,CAGNET-1D,2,1875,94.1380,0.6418

Epoch: 1876
Epoch: 1876
ogbn-arxiv,CAGNET-1D,2,1876,94.1879,0.6611

Epoch: 1877
Epoch: 1877
ogbn-arxiv,CAGNET-1D,2,1877,94.2378,0.6350

Epoch: 1878
Epoch: 1878
ogbn-arxiv,CAGNET-1D,2,1878,94.2873,0.6644

Epoch: 1879
Epoch: 1879
ogbn-arxiv,CAGNET-1D,2,1879,94.3370,0.6266

Epoch: 1880
Epoch: 1880
ogbn-arxiv,CAGNET-1D,2,1880,94.3872,0.6693

Epoch: 1881
Epoch: 1881
ogbn-arxiv,CAGNET-1D,2,1881,94.4373,0.6160

Epoch: 1882
Epoch: 1882
ogbn-arxiv,CAGNET-1D,2,1882,94.4871,0.6729

Epoch: 1883
Epoch: 1883
ogbn-arxiv,CAGNET-1D,2,1883,94.5410,0.6099

Epoch: 1884
Epoch: 1884
ogbn-arxiv,CAGNET-1D,2,1884,94.5913,0.6711

Epoch: 1885
Epoch: 1885
ogbn-arxiv,CAGNET-1D,2,1885,94.6412,0.6234

Epoch: 1886
Epoch: 1886
ogbn-arxiv,CAGNET-1D,2,1886,94.6912,0.6608

Epoch: 1887
Epoch: 1887
ogbn-arxiv,CAGNET-1D,2,1887,94.7413,0.6497

Epoch: 1888
Epoch: 1888
ogbn-arxiv,CAGNET-1D,2,1888,94.7915,0.6406

Epoch: 1889
Epoch: 1889
ogbn-arxiv,CAGNET-1D,2,1889,94.8415,0.6647

Epoch: 1890
Epoch: 1890
ogbn-arxiv,CAGNET-1D,2,1890,94.8914,0.6259

Epoch: 1891
Epoch: 1891
ogbn-arxiv,CAGNET-1D,2,1891,94.9413,0.6677

Epoch: 1892
Epoch: 1892
ogbn-arxiv,CAGNET-1D,2,1892,94.9914,0.6296

Epoch: 1893
Epoch: 1893
ogbn-arxiv,CAGNET-1D,2,1893,95.0413,0.6605

Epoch: 1894
Epoch: 1894
ogbn-arxiv,CAGNET-1D,2,1894,95.0910,0.6472

Epoch: 1895
Epoch: 1895
ogbn-arxiv,CAGNET-1D,2,1895,95.1408,0.6464

Epoch: 1896
Epoch: 1896
ogbn-arxiv,CAGNET-1D,2,1896,95.1910,0.6605

Epoch: 1897
Epoch: 1897
ogbn-arxiv,CAGNET-1D,2,1897,95.2412,0.6353

Epoch: 1898
Epoch: 1898
ogbn-arxiv,CAGNET-1D,2,1898,95.2910,0.6640

Epoch: 1899
Epoch: 1899
ogbn-arxiv,CAGNET-1D,2,1899,95.3407,0.6371

Epoch: 1900Epoch: 1900

ogbn-arxiv,CAGNET-1D,2,1900,95.3910,0.6578

Epoch: 1901
Epoch: 1901
ogbn-arxiv,CAGNET-1D,2,1901,95.4412,0.6486

Epoch: 1902
Epoch: 1902
ogbn-arxiv,CAGNET-1D,2,1902,95.4943,0.6471

Epoch: 1903
Epoch: 1903
ogbn-arxiv,CAGNET-1D,2,1903,95.5463,0.6587

Epoch: 1904
Epoch: 1904
ogbn-arxiv,CAGNET-1D,2,1904,95.5974,0.6398

Epoch: 1905
Epoch: 1905
ogbn-arxiv,CAGNET-1D,2,1905,95.6482,0.6601

Epoch: 1906
Epoch: 1906
ogbn-arxiv,CAGNET-1D,2,1906,95.6987,0.6411

Epoch: 1907
Epoch: 1907
ogbn-arxiv,CAGNET-1D,2,1907,95.7487,0.6562

Epoch: 1908
Epoch: 1908
ogbn-arxiv,CAGNET-1D,2,1908,95.7983,0.6473

Epoch: 1909
Epoch: 1909
ogbn-arxiv,CAGNET-1D,2,1909,95.8485,0.6509

Epoch: 1910
Epoch: 1910
ogbn-arxiv,CAGNET-1D,2,1910,95.8986,0.6525

Epoch: 1911
Epoch: 1911
ogbn-arxiv,CAGNET-1D,2,1911,95.9485,0.6471

Epoch: 1912Epoch: 1912

ogbn-arxiv,CAGNET-1D,2,1912,95.9980,0.6553

Epoch: 1913
Epoch: 1913
ogbn-arxiv,CAGNET-1D,2,1913,96.0476,0.6452

Epoch: 1914
Epoch: 1914
ogbn-arxiv,CAGNET-1D,2,1914,96.0978,0.6552

Epoch: 1915
Epoch: 1915
ogbn-arxiv,CAGNET-1D,2,1915,96.1477,0.6464

Epoch: 1916
Epoch: 1916
ogbn-arxiv,CAGNET-1D,2,1916,96.1979,0.6539

Epoch: 1917
Epoch: 1917
ogbn-arxiv,CAGNET-1D,2,1917,96.2477,0.6486

Epoch: 1918
Epoch: 1918
ogbn-arxiv,CAGNET-1D,2,1918,96.2980,0.6522

Epoch: 1919
Epoch: 1919
ogbn-arxiv,CAGNET-1D,2,1919,96.3479,0.6508

Epoch: 1920
Epoch: 1920
ogbn-arxiv,CAGNET-1D,2,1920,96.4027,0.6490

Epoch: 1921
Epoch: 1921
ogbn-arxiv,CAGNET-1D,2,1921,96.4527,0.6529

Epoch: 1922
Epoch: 1922
ogbn-arxiv,CAGNET-1D,2,1922,96.5027,0.6478

Epoch: 1923
Epoch: 1923
ogbn-arxiv,CAGNET-1D,2,1923,96.5523,0.6531

Epoch: 1924
Epoch: 1924
ogbn-arxiv,CAGNET-1D,2,1924,96.6032,0.6479

Epoch: 1925
Epoch: 1925
ogbn-arxiv,CAGNET-1D,2,1925,96.6544,0.6528

Epoch: 1926Epoch: 1926

ogbn-arxiv,CAGNET-1D,2,1926,96.7042,0.6484

Epoch: 1927
Epoch: 1927
ogbn-arxiv,CAGNET-1D,2,1927,96.7542,0.6526

Epoch: 1928
Epoch: 1928
ogbn-arxiv,CAGNET-1D,2,1928,96.8045,0.6496

Epoch: 1929
Epoch: 1929
ogbn-arxiv,CAGNET-1D,2,1929,96.8542,0.6509

Epoch: 1930
Epoch: 1930
ogbn-arxiv,CAGNET-1D,2,1930,96.9038,0.6509

Epoch: 1931
Epoch: 1931
ogbn-arxiv,CAGNET-1D,2,1931,96.9538,0.6492

Epoch: 1932
Epoch: 1932
ogbn-arxiv,CAGNET-1D,2,1932,97.0038,0.6527

Epoch: 1933
Epoch: 1933
ogbn-arxiv,CAGNET-1D,2,1933,97.0539,0.6483

Epoch: 1934
Epoch: 1934
ogbn-arxiv,CAGNET-1D,2,1934,97.1034,0.6533

Epoch: 1935
Epoch: 1935
ogbn-arxiv,CAGNET-1D,2,1935,97.1534,0.6476

Epoch: 1936
Epoch: 1936
ogbn-arxiv,CAGNET-1D,2,1936,97.2035,0.6534

Epoch: 1937
Epoch: 1937
ogbn-arxiv,CAGNET-1D,2,1937,97.2537,0.6476

Epoch: 1938
Epoch: 1938
ogbn-arxiv,CAGNET-1D,2,1938,97.3048,0.6533

Epoch: 1939
Epoch: 1939
ogbn-arxiv,CAGNET-1D,2,1939,97.3552,0.6487

Epoch: 1940
Epoch: 1940
ogbn-arxiv,CAGNET-1D,2,1940,97.4053,0.6523

Epoch: 1941
Epoch: 1941
ogbn-arxiv,CAGNET-1D,2,1941,97.4552,0.6491

Epoch: 1942
Epoch: 1942
ogbn-arxiv,CAGNET-1D,2,1942,97.5047,0.6517

Epoch: 1943
Epoch: 1943
ogbn-arxiv,CAGNET-1D,2,1943,97.5547,0.6488

Epoch: 1944
Epoch: 1944
ogbn-arxiv,CAGNET-1D,2,1944,97.6049,0.6512

Epoch: 1945
Epoch: 1945
ogbn-arxiv,CAGNET-1D,2,1945,97.6548,0.6491

Epoch: 1946
Epoch: 1946
ogbn-arxiv,CAGNET-1D,2,1946,97.7047,0.6516

Epoch: 1947
Epoch: 1947
ogbn-arxiv,CAGNET-1D,2,1947,97.7550,0.6491

Epoch: 1948
Epoch: 1948
ogbn-arxiv,CAGNET-1D,2,1948,97.8053,0.6501

Epoch: 1949
Epoch: 1949
ogbn-arxiv,CAGNET-1D,2,1949,97.8550,0.6506

Epoch: 1950
Epoch: 1950
ogbn-arxiv,CAGNET-1D,2,1950,97.9049,0.6473

Epoch: 1951
Epoch: 1951
ogbn-arxiv,CAGNET-1D,2,1951,97.9546,0.6520

Epoch: 1952
Epoch: 1952
ogbn-arxiv,CAGNET-1D,2,1952,98.0047,0.6413

Epoch: 1953
Epoch: 1953
ogbn-arxiv,CAGNET-1D,2,1953,98.0548,0.6562

Epoch: 1954
Epoch: 1954
ogbn-arxiv,CAGNET-1D,2,1954,98.1045,0.6344

Epoch: 1955
Epoch: 1955
ogbn-arxiv,CAGNET-1D,2,1955,98.1543,0.6597

Epoch: 1956
Epoch: 1956
ogbn-arxiv,CAGNET-1D,2,1956,98.2043,0.6329

Epoch: 1957
Epoch: 1957
ogbn-arxiv,CAGNET-1D,2,1957,98.2584,0.6612

Epoch: 1958
Epoch: 1958
ogbn-arxiv,CAGNET-1D,2,1958,98.3085,0.6354

Epoch: 1959
Epoch: 1959
ogbn-arxiv,CAGNET-1D,2,1959,98.3585,0.6589

Epoch: 1960
Epoch: 1960
ogbn-arxiv,CAGNET-1D,2,1960,98.4081,0.6439

Epoch: 1961
Epoch: 1961
ogbn-arxiv,CAGNET-1D,2,1961,98.4576,0.6538

Epoch: 1962
Epoch: 1962
ogbn-arxiv,CAGNET-1D,2,1962,98.5077,0.6490

Epoch: 1963
Epoch: 1963
ogbn-arxiv,CAGNET-1D,2,1963,98.5578,0.6487

Epoch: 1964
Epoch: 1964
ogbn-arxiv,CAGNET-1D,2,1964,98.6079,0.6510

Epoch: 1965
Epoch: 1965
ogbn-arxiv,CAGNET-1D,2,1965,98.6574,0.6468

Epoch: 1966
Epoch: 1966
ogbn-arxiv,CAGNET-1D,2,1966,98.7070,0.6519

Epoch: 1967
Epoch: 1967
ogbn-arxiv,CAGNET-1D,2,1967,98.7572,0.6463

Epoch: 1968
Epoch: 1968
ogbn-arxiv,CAGNET-1D,2,1968,98.8072,0.6534

Epoch: 1969
Epoch: 1969
ogbn-arxiv,CAGNET-1D,2,1969,98.8571,0.6463

Epoch: 1970
Epoch: 1970
ogbn-arxiv,CAGNET-1D,2,1970,98.9070,0.6539

Epoch: 1971
Epoch: 1971
ogbn-arxiv,CAGNET-1D,2,1971,98.9568,0.6466

Epoch: 1972
Epoch: 1972
ogbn-arxiv,CAGNET-1D,2,1972,99.0070,0.6536

Epoch: 1973
Epoch: 1973
ogbn-arxiv,CAGNET-1D,2,1973,99.0570,0.6469

Epoch: 1974
Epoch: 1974
ogbn-arxiv,CAGNET-1D,2,1974,99.1068,0.6528

Epoch: 1975
Epoch: 1975
ogbn-arxiv,CAGNET-1D,2,1975,99.1579,0.6474

Epoch: 1976
Epoch: 1976
ogbn-arxiv,CAGNET-1D,2,1976,99.2109,0.6526

Epoch: 1977
Epoch: 1977
ogbn-arxiv,CAGNET-1D,2,1977,99.2611,0.6460

Epoch: 1978
Epoch: 1978
ogbn-arxiv,CAGNET-1D,2,1978,99.3110,0.6547

Epoch: 1979
Epoch: 1979
ogbn-arxiv,CAGNET-1D,2,1979,99.3611,0.6441

Epoch: 1980
Epoch: 1980
ogbn-arxiv,CAGNET-1D,2,1980,99.4115,0.6571

Epoch: 1981
Epoch: 1981
ogbn-arxiv,CAGNET-1D,2,1981,99.4613,0.6395

Epoch: 1982
Epoch: 1982
ogbn-arxiv,CAGNET-1D,2,1982,99.5112,0.6598

Epoch: 1983
Epoch: 1983
ogbn-arxiv,CAGNET-1D,2,1983,99.5610,0.6325

Epoch: 1984
Epoch: 1984
ogbn-arxiv,CAGNET-1D,2,1984,99.6112,0.6628

Epoch: 1985
Epoch: 1985
ogbn-arxiv,CAGNET-1D,2,1985,99.6611,0.6275

Epoch: 1986
Epoch: 1986
ogbn-arxiv,CAGNET-1D,2,1986,99.7111,0.6646

Epoch: 1987
Epoch: 1987
ogbn-arxiv,CAGNET-1D,2,1987,99.7609,0.6269

Epoch: 1988
Epoch: 1988
ogbn-arxiv,CAGNET-1D,2,1988,99.8108,0.6649

Epoch: 1989
Epoch: 1989
ogbn-arxiv,CAGNET-1D,2,1989,99.8606,0.6286

Epoch: 1990
Epoch: 1990
ogbn-arxiv,CAGNET-1D,2,1990,99.9105,0.6630

Epoch: 1991
Epoch: 1991
ogbn-arxiv,CAGNET-1D,2,1991,99.9607,0.6380

Epoch: 1992
Epoch: 1992
ogbn-arxiv,CAGNET-1D,2,1992,100.0111,0.6570

Epoch: 1993
Epoch: 1993
ogbn-arxiv,CAGNET-1D,2,1993,100.0607,0.6471

Epoch: 1994
Epoch: 1994
ogbn-arxiv,CAGNET-1D,2,1994,100.1164,0.6483

Epoch: 1995
Epoch: 1995
ogbn-arxiv,CAGNET-1D,2,1995,100.1662,0.6551

Epoch: 1996
Epoch: 1996
ogbn-arxiv,CAGNET-1D,2,1996,100.2165,0.6400

Epoch: 1997
Epoch: 1997
ogbn-arxiv,CAGNET-1D,2,1997,100.2666,0.6600

Epoch: 1998
Epoch: 1998
ogbn-arxiv,CAGNET-1D,2,1998,100.3164,0.6355

Epoch: 1999
Epoch: 1999
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
ogbn-arxiv,CAGNET-1D,2,1999,100.3661,0.6614

    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 916, in main
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    run)
  File "GNN-RDM/src/gcn_distr.py", line 740, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr.py", line 664, in run
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 916, in main
    total_time[i][rank] = tstop - tstart
KeyError: 1
    run)
  File "GNN-RDM/src/gcn_distr.py", line 740, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr.py", line 664, in run
    total_time[i][rank] = tstop - tstart
KeyError: 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2123027) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2123027 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:34:49
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2123027)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_02:34:49
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2123028)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='ogbn-arxiv', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: ogbn-arxiv timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='ogbn-arxiv', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: ogbn-arxiv timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 2
Processes: 2
tensor([[104447,  15858, 107156,  ...,  45118,  45118,  45118],
        [ 13091,  47283,  69161,  ..., 162473, 162537,  72717]],
       device='cuda:0')
tensor([[104447,  15858, 107156,  ...,  45118,  45118,  45118],
        [ 13091,  47283,  69161,  ..., 162473, 162537,  72717]],
       device='cuda:1')
tensor([[ 13091,  47283,  69161,  ..., 162473, 162537,  72717],
        [104447,  15858, 107156,  ...,  45118,  45118,  45118]])
tensor([[ 13091,  47283,  69161,  ..., 162473, 162537,  72717],
        [104447,  15858, 107156,  ...,  45118,  45118,  45118]])
rank: 1 adj_matrix_loc.size: torch.Size([169343, 169343])
rank: 1 inputs_loc.size: torch.Size([169343, 128])
rank: 0 adj_matrix_loc.size: torch.Size([169343, 169343])
rank: 0 inputs_loc.size: torch.Size([169343, 128])
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 1 run 0
Starting training... rank 0 run 0
Epoch: 000
Epoch: 000
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -84671: [-84671, 40]
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -84671: [-84671, 40]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2123175) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2123175 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:37:11
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2123175)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_02:37:11
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2123176)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='Reddit', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: Reddit timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 1
device: cuda:0
rank: 0 adj_matrix_loc.size: torch.Size([232965, 232965])
rank: 0 inputs.size: torch.Size([232965, 602])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 0 run 0
Epoch: 000
Reddit,CAGNET-1D,1,0,0.7233,0.1333

Epoch: 001
Reddit,CAGNET-1D,1,1,1.4477,0.1599

Epoch: 002
Reddit,CAGNET-1D,1,2,2.1711,0.1842

Epoch: 003
Reddit,CAGNET-1D,1,3,2.8951,0.2001

Epoch: 004
Reddit,CAGNET-1D,1,4,3.6185,0.2258

Epoch: 005
Reddit,CAGNET-1D,1,5,4.3434,0.2472

Epoch: 006
Reddit,CAGNET-1D,1,6,5.0685,0.2591

Epoch: 007
Reddit,CAGNET-1D,1,7,5.7912,0.2652

Epoch: 008
Reddit,CAGNET-1D,1,8,6.5156,0.2774

Epoch: 009
Reddit,CAGNET-1D,1,9,7.2396,0.2963

Epoch: 010
Reddit,CAGNET-1D,1,10,7.9641,0.3146

Epoch: 011
Reddit,CAGNET-1D,1,11,8.6876,0.3406

Epoch: 012
Reddit,CAGNET-1D,1,12,9.4120,0.3651

Epoch: 013
Reddit,CAGNET-1D,1,13,10.1356,0.3917

Epoch: 014
Reddit,CAGNET-1D,1,14,10.8588,0.4369

Epoch: 015
Reddit,CAGNET-1D,1,15,11.5817,0.5129

Epoch: 016
Reddit,CAGNET-1D,1,16,12.3052,0.5755

Epoch: 017
Reddit,CAGNET-1D,1,17,13.0294,0.6063

Epoch: 018
Reddit,CAGNET-1D,1,18,13.7530,0.6209

Epoch: 019
Reddit,CAGNET-1D,1,19,14.4768,0.6378

Epoch: 020
Reddit,CAGNET-1D,1,20,15.2002,0.6478

Epoch: 021
Reddit,CAGNET-1D,1,21,15.9245,0.6725

Epoch: 022
Reddit,CAGNET-1D,1,22,16.6488,0.7109

Epoch: 023
Reddit,CAGNET-1D,1,23,17.3722,0.7323

Epoch: 024
Reddit,CAGNET-1D,1,24,18.0972,0.7482

Epoch: 025
Reddit,CAGNET-1D,1,25,18.8203,0.7746

Epoch: 026
Reddit,CAGNET-1D,1,26,19.5448,0.8088

Epoch: 027
Reddit,CAGNET-1D,1,27,20.2691,0.8401

Epoch: 028
Reddit,CAGNET-1D,1,28,20.9922,0.8643

Epoch: 029
Reddit,CAGNET-1D,1,29,21.7151,0.8761

Epoch: 030
Reddit,CAGNET-1D,1,30,22.4390,0.8824

Epoch: 031
Reddit,CAGNET-1D,1,31,23.1616,0.8865

Epoch: 032
Reddit,CAGNET-1D,1,32,23.8854,0.8895

Epoch: 033
Reddit,CAGNET-1D,1,33,24.6084,0.8920

Epoch: 034
Reddit,CAGNET-1D,1,34,25.3323,0.8946

Epoch: 035
Reddit,CAGNET-1D,1,35,26.0563,0.8990

Epoch: 036
Reddit,CAGNET-1D,1,36,26.7800,0.9048

Epoch: 037
Reddit,CAGNET-1D,1,37,27.5029,0.9080

Epoch: 038
Reddit,CAGNET-1D,1,38,28.2264,0.9101

Epoch: 039
Reddit,CAGNET-1D,1,39,28.9501,0.9120

Epoch: 040
Reddit,CAGNET-1D,1,40,29.6736,0.9173

Epoch: 041
Reddit,CAGNET-1D,1,41,30.3982,0.9256

Epoch: 042
Reddit,CAGNET-1D,1,42,31.1204,0.9284

Epoch: 043
Reddit,CAGNET-1D,1,43,31.8444,0.9291

Epoch: 044
Reddit,CAGNET-1D,1,44,32.5682,0.9294

Epoch: 045
Reddit,CAGNET-1D,1,45,33.2923,0.9302

Epoch: 046
Reddit,CAGNET-1D,1,46,34.0148,0.9312

Epoch: 047
Reddit,CAGNET-1D,1,47,34.7385,0.9318

Epoch: 048
Reddit,CAGNET-1D,1,48,35.4615,0.9328

Epoch: 049
Reddit,CAGNET-1D,1,49,36.1860,0.9335

Epoch: 050
Reddit,CAGNET-1D,1,50,36.9099,0.9341

Epoch: 051
Reddit,CAGNET-1D,1,51,37.6328,0.9341

Epoch: 052
Reddit,CAGNET-1D,1,52,38.3561,0.9349

Epoch: 053
Reddit,CAGNET-1D,1,53,39.0801,0.9352

Epoch: 054
Reddit,CAGNET-1D,1,54,39.8053,0.9352

Epoch: 055
Reddit,CAGNET-1D,1,55,40.5286,0.9352

Epoch: 056
Reddit,CAGNET-1D,1,56,41.2510,0.9358

Epoch: 057
Reddit,CAGNET-1D,1,57,41.9756,0.9359

Epoch: 058
Reddit,CAGNET-1D,1,58,42.6990,0.9364

Epoch: 059
Reddit,CAGNET-1D,1,59,43.4233,0.9369

Epoch: 060
Reddit,CAGNET-1D,1,60,44.1461,0.9368

Epoch: 061
Reddit,CAGNET-1D,1,61,44.8688,0.9372

Epoch: 062
Reddit,CAGNET-1D,1,62,45.5920,0.9374

Epoch: 063
Reddit,CAGNET-1D,1,63,46.3162,0.9379

Epoch: 064
Reddit,CAGNET-1D,1,64,47.0414,0.9379

Epoch: 065
Reddit,CAGNET-1D,1,65,47.7661,0.9379

Epoch: 066
Reddit,CAGNET-1D,1,66,48.4898,0.9381

Epoch: 067
Reddit,CAGNET-1D,1,67,49.2136,0.9381

Epoch: 068
Reddit,CAGNET-1D,1,68,49.9368,0.9385

Epoch: 069
Reddit,CAGNET-1D,1,69,50.6608,0.9389

Epoch: 070
Reddit,CAGNET-1D,1,70,51.3836,0.9390

Epoch: 071
Reddit,CAGNET-1D,1,71,52.1070,0.9392

Epoch: 072
Reddit,CAGNET-1D,1,72,52.8313,0.9394

Epoch: 073
Reddit,CAGNET-1D,1,73,53.5547,0.9396

Epoch: 074
Reddit,CAGNET-1D,1,74,54.2779,0.9396

Epoch: 075
Reddit,CAGNET-1D,1,75,55.0020,0.9398

Epoch: 076
Reddit,CAGNET-1D,1,76,55.7249,0.9397

Epoch: 077
Reddit,CAGNET-1D,1,77,56.4486,0.9398

Epoch: 078
Reddit,CAGNET-1D,1,78,57.1732,0.9398

Epoch: 079
Reddit,CAGNET-1D,1,79,57.8962,0.9398

Epoch: 080
Reddit,CAGNET-1D,1,80,58.6205,0.9398

Epoch: 081
Reddit,CAGNET-1D,1,81,59.3439,0.9400

Epoch: 082
Reddit,CAGNET-1D,1,82,60.0689,0.9401

Epoch: 083
Reddit,CAGNET-1D,1,83,60.7935,0.9403

Epoch: 084
Reddit,CAGNET-1D,1,84,61.5179,0.9406

Epoch: 085
Reddit,CAGNET-1D,1,85,62.2420,0.9407

Epoch: 086
Reddit,CAGNET-1D,1,86,62.9652,0.9408

Epoch: 087
Reddit,CAGNET-1D,1,87,63.6890,0.9412

Epoch: 088
Reddit,CAGNET-1D,1,88,64.4121,0.9413

Epoch: 089
Reddit,CAGNET-1D,1,89,65.1372,0.9414

Epoch: 090
Reddit,CAGNET-1D,1,90,65.8612,0.9413

Epoch: 091
Reddit,CAGNET-1D,1,91,66.5855,0.9414

Epoch: 092
Reddit,CAGNET-1D,1,92,67.3085,0.9416

Epoch: 093
Reddit,CAGNET-1D,1,93,68.0325,0.9419

Epoch: 094
Reddit,CAGNET-1D,1,94,68.7576,0.9419

Epoch: 095
Reddit,CAGNET-1D,1,95,69.4818,0.9418

Epoch: 096
Reddit,CAGNET-1D,1,96,70.2055,0.9420

Epoch: 097
Reddit,CAGNET-1D,1,97,70.9291,0.9420

Epoch: 098
Reddit,CAGNET-1D,1,98,71.6524,0.9421

Epoch: 099
Reddit,CAGNET-1D,1,99,72.3759,0.9421

Epoch: 100
Reddit,CAGNET-1D,1,100,73.0998,0.9421

Epoch: 101
Reddit,CAGNET-1D,1,101,73.8235,0.9422

Epoch: 102
Reddit,CAGNET-1D,1,102,74.5470,0.9422

Epoch: 103
Reddit,CAGNET-1D,1,103,75.2710,0.9423

Epoch: 104
Reddit,CAGNET-1D,1,104,75.9961,0.9424

Epoch: 105
Reddit,CAGNET-1D,1,105,76.7190,0.9426

Epoch: 106
Reddit,CAGNET-1D,1,106,77.4423,0.9426

Epoch: 107
Reddit,CAGNET-1D,1,107,78.1660,0.9427

Epoch: 108
Reddit,CAGNET-1D,1,108,78.8905,0.9428

Epoch: 109
Reddit,CAGNET-1D,1,109,79.6140,0.9429

Epoch: 110
Reddit,CAGNET-1D,1,110,80.3375,0.9431

Epoch: 111
Reddit,CAGNET-1D,1,111,81.0613,0.9432

Epoch: 112
Reddit,CAGNET-1D,1,112,81.7845,0.9432

Epoch: 113
Reddit,CAGNET-1D,1,113,82.5098,0.9433

Epoch: 114
Reddit,CAGNET-1D,1,114,83.2338,0.9433

Epoch: 115
Reddit,CAGNET-1D,1,115,83.9574,0.9434

Epoch: 116
Reddit,CAGNET-1D,1,116,84.6808,0.9434

Epoch: 117
Reddit,CAGNET-1D,1,117,85.4041,0.9435

Epoch: 118
Reddit,CAGNET-1D,1,118,86.1282,0.9435

Epoch: 119
Reddit,CAGNET-1D,1,119,86.8532,0.9436

Epoch: 120
Reddit,CAGNET-1D,1,120,87.5767,0.9434

Epoch: 121
Reddit,CAGNET-1D,1,121,88.3000,0.9435

Epoch: 122
Reddit,CAGNET-1D,1,122,89.0238,0.9435

Epoch: 123
Reddit,CAGNET-1D,1,123,89.7488,0.9436

Epoch: 124
Reddit,CAGNET-1D,1,124,90.4725,0.9438

Epoch: 125
Reddit,CAGNET-1D,1,125,91.1958,0.9438

Epoch: 126
Reddit,CAGNET-1D,1,126,91.9197,0.9438

Epoch: 127
Reddit,CAGNET-1D,1,127,92.6435,0.9439

Epoch: 128
Reddit,CAGNET-1D,1,128,93.3673,0.9440

Epoch: 129
Reddit,CAGNET-1D,1,129,94.0909,0.9440

Epoch: 130
Reddit,CAGNET-1D,1,130,94.8149,0.9441

Epoch: 131
Reddit,CAGNET-1D,1,131,95.5390,0.9442

Epoch: 132
Reddit,CAGNET-1D,1,132,96.2636,0.9443

Epoch: 133
Reddit,CAGNET-1D,1,133,96.9883,0.9443

Epoch: 134
Reddit,CAGNET-1D,1,134,97.7118,0.9444

Epoch: 135
Reddit,CAGNET-1D,1,135,98.4364,0.9444

Epoch: 136
Reddit,CAGNET-1D,1,136,99.1605,0.9443

Epoch: 137
Reddit,CAGNET-1D,1,137,99.8844,0.9443

Epoch: 138
Reddit,CAGNET-1D,1,138,100.6073,0.9443

Epoch: 139
Reddit,CAGNET-1D,1,139,101.3313,0.9444

Epoch: 140
Reddit,CAGNET-1D,1,140,102.0552,0.9443

Epoch: 141
Reddit,CAGNET-1D,1,141,102.7786,0.9443

Epoch: 142
Reddit,CAGNET-1D,1,142,103.5031,0.9443

Epoch: 143
Reddit,CAGNET-1D,1,143,104.2273,0.9443

Epoch: 144
Reddit,CAGNET-1D,1,144,104.9511,0.9443

Epoch: 145
Reddit,CAGNET-1D,1,145,105.6741,0.9443

Epoch: 146
Reddit,CAGNET-1D,1,146,106.3982,0.9444

Epoch: 147
Reddit,CAGNET-1D,1,147,107.1218,0.9444

Epoch: 148
Reddit,CAGNET-1D,1,148,107.8466,0.9444

Epoch: 149
Reddit,CAGNET-1D,1,149,108.5704,0.9444

Epoch: 150
Reddit,CAGNET-1D,1,150,109.2960,0.9444

Epoch: 151
Reddit,CAGNET-1D,1,151,110.0210,0.9445

Epoch: 152
Reddit,CAGNET-1D,1,152,110.7453,0.9445

Epoch: 153
Reddit,CAGNET-1D,1,153,111.4696,0.9445

Epoch: 154
Reddit,CAGNET-1D,1,154,112.1952,0.9445

Epoch: 155
Reddit,CAGNET-1D,1,155,112.9192,0.9446

Epoch: 156
Reddit,CAGNET-1D,1,156,113.6430,0.9447

Epoch: 157
Reddit,CAGNET-1D,1,157,114.3670,0.9447

Epoch: 158
Reddit,CAGNET-1D,1,158,115.0911,0.9447

Epoch: 159
Reddit,CAGNET-1D,1,159,115.8155,0.9449

Epoch: 160
Reddit,CAGNET-1D,1,160,116.5403,0.9449

Epoch: 161
Reddit,CAGNET-1D,1,161,117.2633,0.9450

Epoch: 162
Reddit,CAGNET-1D,1,162,117.9871,0.9451

Epoch: 163
Reddit,CAGNET-1D,1,163,118.7102,0.9452

Epoch: 164
Reddit,CAGNET-1D,1,164,119.4346,0.9452

Epoch: 165
Reddit,CAGNET-1D,1,165,120.1579,0.9453

Epoch: 166
Reddit,CAGNET-1D,1,166,120.8831,0.9453

Epoch: 167
Reddit,CAGNET-1D,1,167,121.6072,0.9453

Epoch: 168
Reddit,CAGNET-1D,1,168,122.3309,0.9454

Epoch: 169
Reddit,CAGNET-1D,1,169,123.0546,0.9454

Epoch: 170
Reddit,CAGNET-1D,1,170,123.7802,0.9455

Epoch: 171
Reddit,CAGNET-1D,1,171,124.5034,0.9455

Epoch: 172
Reddit,CAGNET-1D,1,172,125.2287,0.9456

Epoch: 173
Reddit,CAGNET-1D,1,173,125.9533,0.9456

Epoch: 174
Reddit,CAGNET-1D,1,174,126.6776,0.9457

Epoch: 175
Reddit,CAGNET-1D,1,175,127.4009,0.9456

Epoch: 176
Reddit,CAGNET-1D,1,176,128.1245,0.9456

Epoch: 177
Reddit,CAGNET-1D,1,177,128.8487,0.9457

Epoch: 178
Reddit,CAGNET-1D,1,178,129.5719,0.9457

Epoch: 179
Reddit,CAGNET-1D,1,179,130.2965,0.9457

Epoch: 180
Reddit,CAGNET-1D,1,180,131.0210,0.9457

Epoch: 181
Reddit,CAGNET-1D,1,181,131.7444,0.9458

Epoch: 182
Reddit,CAGNET-1D,1,182,132.4681,0.9457

Epoch: 183
Reddit,CAGNET-1D,1,183,133.1908,0.9458

Epoch: 184
Reddit,CAGNET-1D,1,184,133.9148,0.9458

Epoch: 185
Reddit,CAGNET-1D,1,185,134.6391,0.9457

Epoch: 186
Reddit,CAGNET-1D,1,186,135.3623,0.9458

Epoch: 187
Reddit,CAGNET-1D,1,187,136.0851,0.9458

Epoch: 188
Reddit,CAGNET-1D,1,188,136.8099,0.9458

Epoch: 189
Reddit,CAGNET-1D,1,189,137.5340,0.9458

Epoch: 190
Reddit,CAGNET-1D,1,190,138.2583,0.9458

Epoch: 191
Reddit,CAGNET-1D,1,191,138.9818,0.9458

Epoch: 192
Reddit,CAGNET-1D,1,192,139.7069,0.9459

Epoch: 193
Reddit,CAGNET-1D,1,193,140.4308,0.9460

Epoch: 194
Reddit,CAGNET-1D,1,194,141.1538,0.9460

Epoch: 195
Reddit,CAGNET-1D,1,195,141.8768,0.9460

Epoch: 196
Reddit,CAGNET-1D,1,196,142.6006,0.9460

Epoch: 197
Reddit,CAGNET-1D,1,197,143.3244,0.9459

Epoch: 198
Reddit,CAGNET-1D,1,198,144.0479,0.9459

Epoch: 199
Reddit,CAGNET-1D,1,199,144.7710,0.9459

Epoch: 200
Reddit,CAGNET-1D,1,200,145.4944,0.9459

Epoch: 201
Reddit,CAGNET-1D,1,201,146.2190,0.9459

Epoch: 202
Reddit,CAGNET-1D,1,202,146.9427,0.9459

Epoch: 203
Reddit,CAGNET-1D,1,203,147.6670,0.9459

Epoch: 204
Reddit,CAGNET-1D,1,204,148.3907,0.9459

Epoch: 205
Reddit,CAGNET-1D,1,205,149.1146,0.9459

Epoch: 206
Reddit,CAGNET-1D,1,206,149.8382,0.9459

Epoch: 207
Reddit,CAGNET-1D,1,207,150.5617,0.9461

Epoch: 208
Reddit,CAGNET-1D,1,208,151.2862,0.9461

Epoch: 209
Reddit,CAGNET-1D,1,209,152.0084,0.9459

Epoch: 210
Reddit,CAGNET-1D,1,210,152.7322,0.9460

Epoch: 211
Reddit,CAGNET-1D,1,211,153.4558,0.9460

Epoch: 212
Reddit,CAGNET-1D,1,212,154.1807,0.9460

Epoch: 213
Reddit,CAGNET-1D,1,213,154.9051,0.9460

Epoch: 214
Reddit,CAGNET-1D,1,214,155.6287,0.9460

Epoch: 215
Reddit,CAGNET-1D,1,215,156.3517,0.9460

Epoch: 216
Reddit,CAGNET-1D,1,216,157.0747,0.9460

Epoch: 217
Reddit,CAGNET-1D,1,217,157.8003,0.9461

Epoch: 218
Reddit,CAGNET-1D,1,218,158.5237,0.9460

Epoch: 219
Reddit,CAGNET-1D,1,219,159.2475,0.9461

Epoch: 220
Reddit,CAGNET-1D,1,220,159.9725,0.9461

Epoch: 221
Reddit,CAGNET-1D,1,221,160.6990,0.9461

Epoch: 222
Reddit,CAGNET-1D,1,222,161.4227,0.9462

Epoch: 223
Reddit,CAGNET-1D,1,223,162.1467,0.9462

Epoch: 224
Reddit,CAGNET-1D,1,224,162.8708,0.9461

Epoch: 225
Reddit,CAGNET-1D,1,225,163.5939,0.9462

Epoch: 226
Reddit,CAGNET-1D,1,226,164.3187,0.9463

Epoch: 227
Reddit,CAGNET-1D,1,227,165.0419,0.9463

Epoch: 228
Reddit,CAGNET-1D,1,228,165.7662,0.9464

Epoch: 229
Reddit,CAGNET-1D,1,229,166.4900,0.9465

Epoch: 230
Reddit,CAGNET-1D,1,230,167.2136,0.9464

Epoch: 231
Reddit,CAGNET-1D,1,231,167.9378,0.9464

Epoch: 232
Reddit,CAGNET-1D,1,232,168.6619,0.9464

Epoch: 233
Reddit,CAGNET-1D,1,233,169.3853,0.9465

Epoch: 234
Reddit,CAGNET-1D,1,234,170.1090,0.9465

Epoch: 235
Reddit,CAGNET-1D,1,235,170.8316,0.9465

Epoch: 236
Reddit,CAGNET-1D,1,236,171.5558,0.9465

Epoch: 237
Reddit,CAGNET-1D,1,237,172.2793,0.9465

Epoch: 238
Reddit,CAGNET-1D,1,238,173.0031,0.9465

Epoch: 239
Reddit,CAGNET-1D,1,239,173.7268,0.9466

Epoch: 240
Reddit,CAGNET-1D,1,240,174.4517,0.9465

Epoch: 241
Reddit,CAGNET-1D,1,241,175.1756,0.9466

Epoch: 242
Reddit,CAGNET-1D,1,242,175.9002,0.9467

Epoch: 243
Reddit,CAGNET-1D,1,243,176.6243,0.9466

Epoch: 244
Reddit,CAGNET-1D,1,244,177.3486,0.9467

Epoch: 245
Reddit,CAGNET-1D,1,245,178.0713,0.9467

Epoch: 246
Reddit,CAGNET-1D,1,246,178.7952,0.9466

Epoch: 247
Reddit,CAGNET-1D,1,247,179.5191,0.9467

Epoch: 248
Reddit,CAGNET-1D,1,248,180.2421,0.9465

Epoch: 249
Reddit,CAGNET-1D,1,249,180.9659,0.9466

Epoch: 250
Reddit,CAGNET-1D,1,250,181.6901,0.9465

Epoch: 251
Reddit,CAGNET-1D,1,251,182.4141,0.9465

Epoch: 252
Reddit,CAGNET-1D,1,252,183.1386,0.9466

Epoch: 253
Reddit,CAGNET-1D,1,253,183.8626,0.9466

Epoch: 254
Reddit,CAGNET-1D,1,254,184.5857,0.9466

Epoch: 255
Reddit,CAGNET-1D,1,255,185.3102,0.9466

Epoch: 256
Reddit,CAGNET-1D,1,256,186.0349,0.9466

Epoch: 257
Reddit,CAGNET-1D,1,257,186.7589,0.9466

Epoch: 258
Reddit,CAGNET-1D,1,258,187.4823,0.9466

Epoch: 259
Reddit,CAGNET-1D,1,259,188.2061,0.9468

Epoch: 260
Reddit,CAGNET-1D,1,260,188.9311,0.9466

Epoch: 261
Reddit,CAGNET-1D,1,261,189.6546,0.9467

Epoch: 262
Reddit,CAGNET-1D,1,262,190.3783,0.9468

Epoch: 263
Reddit,CAGNET-1D,1,263,191.1015,0.9467

Epoch: 264
Reddit,CAGNET-1D,1,264,191.8247,0.9468

Epoch: 265
Reddit,CAGNET-1D,1,265,192.5480,0.9467

Epoch: 266
Reddit,CAGNET-1D,1,266,193.2720,0.9469

Epoch: 267
Reddit,CAGNET-1D,1,267,193.9953,0.9467

Epoch: 268
Reddit,CAGNET-1D,1,268,194.7184,0.9468

Epoch: 269
Reddit,CAGNET-1D,1,269,195.4429,0.9466

Epoch: 270
Reddit,CAGNET-1D,1,270,196.1666,0.9468

Epoch: 271
Reddit,CAGNET-1D,1,271,196.8903,0.9467

Epoch: 272
Reddit,CAGNET-1D,1,272,197.6139,0.9468

Epoch: 273
Reddit,CAGNET-1D,1,273,198.3374,0.9467

Epoch: 274
Reddit,CAGNET-1D,1,274,199.0615,0.9468

Epoch: 275
Reddit,CAGNET-1D,1,275,199.7849,0.9467

Epoch: 276
Reddit,CAGNET-1D,1,276,200.5080,0.9468

total_times_r0: [201.7177438735962]
rank: 0 median_run: 0
rank: 0 total_time: 201.7177438735962
rank: 0 comm_time: 0.19328737258911133
rank: 0 comp_time: 197.0623972415924
rank: 0 scomp_time: 196.05567383766174
rank: 0 dcomp_time: 1.006723403930664
rank: 0 bcast_comm_time: 0.15128827095031738
rank: 0 barrier_time: 0.869603157043457
rank: 0 barrier_subset_time: 0.0
rank: 0 op1_comm_time: 0.0
rank: 0 op2_comm_time: 0.041999101638793945
rank: 0 tensor([[-29.3672, -33.2459, -35.6323,  ..., -52.5068, -10.1117, -27.5095],
        [-16.5522, -19.6026, -21.9345,  ..., -21.6928, -17.0256, -20.6483],
        [ -5.5817, -12.4233, -17.0600,  ..., -15.6617, -12.4144, -12.8577],
        ...,
        [-41.6480, -57.5373, -67.9579,  ..., -48.4329, -41.9061, -51.3886],
        [-24.1854, -34.2785, -32.1781,  ..., -32.6567, -29.3483, -32.9063],
        [-24.8707, -33.0738, -29.4663,  ..., -45.6586, -19.5557, -39.7790]],
       device='cuda:0', grad_fn=<CatBackward>)
Epoch: 900, Train: 0.9585, Val: 0.9471, Test: 0.9468
None
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='Reddit', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: Reddit timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 582, in run
    row_groups, col_groups = get_proc_groups(rank, size)
  File "GNN-RDM/src/gcn_distr_15d.py", line 414, in get_proc_groups
    row_groups.append(dist.new_group(row_procs[i]))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 2690, in new_group
    raise RuntimeError("the new group's world size should be less or "
RuntimeError: the new group's world size should be less or equal to the world size set by init_process_group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2123694) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2123694 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:46:15
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2123694)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='Reddit', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: Reddit timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='Reddit', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: Reddit timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 2
device: cuda:0
Processes: 2
device: cuda:1
rank: 1 adj_matrix_loc.size: torch.Size([232965, 116482])
rank: 1 inputs.size: torch.Size([232965, 602])
rank: 0 adj_matrix_loc.size: torch.Size([232965, 116483])
rank: 0 inputs.size: torch.Size([232965, 602])
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 1 run 0
Starting training... rank 0 run 0
Epoch: 000
Epoch: 000
Reddit,CAGNET-1D,2,0,0.4192,0.1332

Epoch: 001Epoch: 001

Reddit,CAGNET-1D,2,1,0.8367,0.1637

Epoch: 002Epoch: 002

Reddit,CAGNET-1D,2,2,1.2538,0.1841

Epoch: 003Epoch: 003

Reddit,CAGNET-1D,2,3,1.6719,0.2067

Epoch: 004
Epoch: 004
Reddit,CAGNET-1D,2,4,2.0886,0.2280

Epoch: 005
Epoch: 005
Reddit,CAGNET-1D,2,5,2.5069,0.2425

Epoch: 006
Epoch: 006
Reddit,CAGNET-1D,2,6,2.9239,0.2728

Epoch: 007
Epoch: 007
Reddit,CAGNET-1D,2,7,3.3419,0.2947

Epoch: 008Epoch: 008

Reddit,CAGNET-1D,2,8,3.7588,0.3492

Epoch: 009Epoch: 009

Reddit,CAGNET-1D,2,9,4.1772,0.3949

Epoch: 010Epoch: 010

Reddit,CAGNET-1D,2,10,4.5938,0.4259

Epoch: 011Epoch: 011

Reddit,CAGNET-1D,2,11,5.0115,0.4430

Epoch: 012
Epoch: 012
Reddit,CAGNET-1D,2,12,5.4285,0.4660

Epoch: 013Epoch: 013

Reddit,CAGNET-1D,2,13,5.8470,0.5141

Epoch: 014Epoch: 014

Reddit,CAGNET-1D,2,14,6.2642,0.5345

Epoch: 015Epoch: 015

Reddit,CAGNET-1D,2,15,6.6813,0.5435

Epoch: 016Epoch: 016

Reddit,CAGNET-1D,2,16,7.0981,0.5424

Epoch: 017Epoch: 017

Reddit,CAGNET-1D,2,17,7.5153,0.5467

Epoch: 018
Epoch: 018
Reddit,CAGNET-1D,2,18,7.9331,0.5522

Epoch: 019Epoch: 019

Reddit,CAGNET-1D,2,19,8.3505,0.5579

Epoch: 020Epoch: 020

Reddit,CAGNET-1D,2,20,8.7692,0.5695

Epoch: 021Epoch: 021

Reddit,CAGNET-1D,2,21,9.1878,0.5929

Epoch: 022Epoch: 022

Reddit,CAGNET-1D,2,22,9.6052,0.6038

Epoch: 023Epoch: 023

Reddit,CAGNET-1D,2,23,10.0227,0.6132

Epoch: 024Epoch: 024

Reddit,CAGNET-1D,2,24,10.4399,0.6269

Epoch: 025Epoch: 025

Reddit,CAGNET-1D,2,25,10.8573,0.6474

Epoch: 026
Epoch: 026
Reddit,CAGNET-1D,2,26,11.2743,0.6550

Epoch: 027Epoch: 027

Reddit,CAGNET-1D,2,27,11.6917,0.6613

Epoch: 028Epoch: 028

Reddit,CAGNET-1D,2,28,12.1096,0.6701

Epoch: 029Epoch: 029

Reddit,CAGNET-1D,2,29,12.5272,0.6841

Epoch: 030
Epoch: 030
Reddit,CAGNET-1D,2,30,12.9444,0.6951

Epoch: 031Epoch: 031

Reddit,CAGNET-1D,2,31,13.3614,0.7313

Epoch: 032Epoch: 032

Reddit,CAGNET-1D,2,32,13.7788,0.7725

Epoch: 033Epoch: 033

Reddit,CAGNET-1D,2,33,14.1966,0.7816

Epoch: 034Epoch: 034

Reddit,CAGNET-1D,2,34,14.6138,0.7865

Epoch: 035Epoch: 035

Reddit,CAGNET-1D,2,35,15.0315,0.7925

Epoch: 036Epoch: 036

Reddit,CAGNET-1D,2,36,15.4492,0.8007

Epoch: 037
Epoch: 037
Reddit,CAGNET-1D,2,37,15.8668,0.8384

Epoch: 038Epoch: 038

Reddit,CAGNET-1D,2,38,16.2844,0.8690

Epoch: 039Epoch: 039

Reddit,CAGNET-1D,2,39,16.7020,0.8787

Epoch: 040Epoch: 040

Reddit,CAGNET-1D,2,40,17.1193,0.8841

Epoch: 041Epoch: 041

Reddit,CAGNET-1D,2,41,17.5366,0.8901

Epoch: 042Epoch: 042

Reddit,CAGNET-1D,2,42,17.9537,0.8956

Epoch: 043Epoch: 043

Reddit,CAGNET-1D,2,43,18.3707,0.8989

Epoch: 044Epoch: 044

Reddit,CAGNET-1D,2,44,18.7885,0.9011

Epoch: 045
Epoch: 045
Reddit,CAGNET-1D,2,45,19.2054,0.9019

Epoch: 046Epoch: 046

Reddit,CAGNET-1D,2,46,19.6242,0.9031

Epoch: 047Epoch: 047

Reddit,CAGNET-1D,2,47,20.0418,0.9042

Epoch: 048Epoch: 048

Reddit,CAGNET-1D,2,48,20.4586,0.9052

Epoch: 049
Epoch: 049
Reddit,CAGNET-1D,2,49,20.8758,0.9071

Epoch: 050
Epoch: 050
Reddit,CAGNET-1D,2,50,21.2938,0.9085

Epoch: 051Epoch: 051

Reddit,CAGNET-1D,2,51,21.7125,0.9095

Epoch: 052Epoch: 052

Reddit,CAGNET-1D,2,52,22.1300,0.9101

Epoch: 053Epoch: 053

Reddit,CAGNET-1D,2,53,22.5478,0.9105

Epoch: 054Epoch: 054

Reddit,CAGNET-1D,2,54,22.9648,0.9106

Epoch: 055Epoch: 055

Reddit,CAGNET-1D,2,55,23.3826,0.9106

Epoch: 056Epoch: 056

Reddit,CAGNET-1D,2,56,23.8001,0.9112

Epoch: 057Epoch: 057

Reddit,CAGNET-1D,2,57,24.2177,0.9118

Epoch: 058Epoch: 058

Reddit,CAGNET-1D,2,58,24.6349,0.9118

Epoch: 059Epoch: 059

Reddit,CAGNET-1D,2,59,25.0532,0.9120

Epoch: 060Epoch: 060

Reddit,CAGNET-1D,2,60,25.4715,0.9120

Epoch: 061Epoch: 061

Reddit,CAGNET-1D,2,61,25.8889,0.9121

Epoch: 062Epoch: 062

Reddit,CAGNET-1D,2,62,26.3058,0.9127

Epoch: 063Epoch: 063

Reddit,CAGNET-1D,2,63,26.7228,0.9130

Epoch: 064Epoch: 064

Reddit,CAGNET-1D,2,64,27.1414,0.9137

Epoch: 065Epoch: 065

Reddit,CAGNET-1D,2,65,27.5596,0.9156

Epoch: 066Epoch: 066

Reddit,CAGNET-1D,2,66,27.9774,0.9178

Epoch: 067
Epoch: 067
Reddit,CAGNET-1D,2,67,28.3948,0.9189

Epoch: 068Epoch: 068

Reddit,CAGNET-1D,2,68,28.8119,0.9200

Epoch: 069Epoch: 069

Reddit,CAGNET-1D,2,69,29.2295,0.9208

Epoch: 070Epoch: 070

Reddit,CAGNET-1D,2,70,29.6466,0.9215

Epoch: 071Epoch: 071

Reddit,CAGNET-1D,2,71,30.0644,0.9223

Epoch: 072Epoch: 072

Reddit,CAGNET-1D,2,72,30.4820,0.9225

Epoch: 073Epoch: 073

Reddit,CAGNET-1D,2,73,30.8993,0.9227

Epoch: 074
Epoch: 074
Reddit,CAGNET-1D,2,74,31.3176,0.9235

Epoch: 075Epoch: 075

Reddit,CAGNET-1D,2,75,31.7347,0.9238

Epoch: 076Epoch: 076

Reddit,CAGNET-1D,2,76,32.1523,0.9242

Epoch: 077Epoch: 077

Reddit,CAGNET-1D,2,77,32.5710,0.9245

Epoch: 078Epoch: 078

Reddit,CAGNET-1D,2,78,32.9880,0.9246

Epoch: 079Epoch: 079

Reddit,CAGNET-1D,2,79,33.4053,0.9249

Epoch: 080Epoch: 080

Reddit,CAGNET-1D,2,80,33.8239,0.9250

Epoch: 081
Epoch: 081
Reddit,CAGNET-1D,2,81,34.2414,0.9250

Epoch: 082Epoch: 082

Reddit,CAGNET-1D,2,82,34.6589,0.9250

Epoch: 083Epoch: 083

Reddit,CAGNET-1D,2,83,35.0762,0.9254

Epoch: 084Epoch: 084

Reddit,CAGNET-1D,2,84,35.4936,0.9254

Epoch: 085Epoch: 085

Reddit,CAGNET-1D,2,85,35.9105,0.9257

Epoch: 086Epoch: 086

Reddit,CAGNET-1D,2,86,36.3284,0.9259

Epoch: 087Epoch: 087

Reddit,CAGNET-1D,2,87,36.7467,0.9261

Epoch: 088Epoch: 088

Reddit,CAGNET-1D,2,88,37.1633,0.9262

Epoch: 089Epoch: 089

Reddit,CAGNET-1D,2,89,37.5816,0.9264

Epoch: 090Epoch: 090

Reddit,CAGNET-1D,2,90,37.9984,0.9259

Epoch: 091Epoch: 091

Reddit,CAGNET-1D,2,91,38.4167,0.9262

Epoch: 092Epoch: 092

Reddit,CAGNET-1D,2,92,38.8337,0.9262

Epoch: 093Epoch: 093

Reddit,CAGNET-1D,2,93,39.2513,0.9263

Epoch: 094
Epoch: 094
Reddit,CAGNET-1D,2,94,39.6696,0.9264

Epoch: 095
Epoch: 095
Reddit,CAGNET-1D,2,95,40.0868,0.9264

Epoch: 096Epoch: 096

Reddit,CAGNET-1D,2,96,40.5050,0.9264

Epoch: 097
Epoch: 097
Reddit,CAGNET-1D,2,97,40.9220,0.9262

Epoch: 098Epoch: 098

Reddit,CAGNET-1D,2,98,41.3397,0.9265

Epoch: 099Epoch: 099

Reddit,CAGNET-1D,2,99,41.7570,0.9257

Epoch: 100Epoch: 100

Reddit,CAGNET-1D,2,100,42.1746,0.9270

Epoch: 101Epoch: 101

Reddit,CAGNET-1D,2,101,42.5925,0.9259

Epoch: 102
Epoch: 102
Reddit,CAGNET-1D,2,102,43.0096,0.9262

Epoch: 103Epoch: 103

Reddit,CAGNET-1D,2,103,43.4281,0.9267

Epoch: 104Epoch: 104

Reddit,CAGNET-1D,2,104,43.8455,0.9260

Epoch: 105
Epoch: 105
Reddit,CAGNET-1D,2,105,44.2624,0.9261

Epoch: 106Epoch: 106

Reddit,CAGNET-1D,2,106,44.6801,0.9266

Epoch: 107Epoch: 107

Reddit,CAGNET-1D,2,107,45.0985,0.9258

Epoch: 108Epoch: 108

Reddit,CAGNET-1D,2,108,45.5154,0.9264

Epoch: 109
Epoch: 109
Reddit,CAGNET-1D,2,109,45.9323,0.9264

Epoch: 110Epoch: 110

Reddit,CAGNET-1D,2,110,46.3496,0.9258

Epoch: 111Epoch: 111

Reddit,CAGNET-1D,2,111,46.7670,0.9260

Epoch: 112Epoch: 112

Reddit,CAGNET-1D,2,112,47.1837,0.9259

Epoch: 113Epoch: 113

Reddit,CAGNET-1D,2,113,47.6012,0.9256

Epoch: 114Epoch: 114

Reddit,CAGNET-1D,2,114,48.0188,0.9257

Epoch: 115Epoch: 115

Reddit,CAGNET-1D,2,115,48.4378,0.9257

Epoch: 116Epoch: 116

Reddit,CAGNET-1D,2,116,48.8546,0.9252

Epoch: 117Epoch: 117

Reddit,CAGNET-1D,2,117,49.2725,0.9255

Epoch: 118Epoch: 118

Reddit,CAGNET-1D,2,118,49.6890,0.9252

Epoch: 119Epoch: 119

Reddit,CAGNET-1D,2,119,50.1064,0.9251

Epoch: 120Epoch: 120

Reddit,CAGNET-1D,2,120,50.5233,0.9252

Epoch: 121
Epoch: 121
Reddit,CAGNET-1D,2,121,50.9401,0.9248

Epoch: 122Epoch: 122

Reddit,CAGNET-1D,2,122,51.3573,0.9250

Epoch: 123
Epoch: 123
Reddit,CAGNET-1D,2,123,51.7744,0.9250

Epoch: 124
Epoch: 124
Reddit,CAGNET-1D,2,124,52.1914,0.9245

Epoch: 125
Epoch: 125
Reddit,CAGNET-1D,2,125,52.6088,0.9249

Epoch: 126Epoch: 126

Reddit,CAGNET-1D,2,126,53.0273,0.9248

Epoch: 127Epoch: 127

Reddit,CAGNET-1D,2,127,53.4450,0.9247

Epoch: 128Epoch: 128

Reddit,CAGNET-1D,2,128,53.8624,0.9249

Epoch: 129Epoch: 129

Reddit,CAGNET-1D,2,129,54.2793,0.9246

Epoch: 130Epoch: 130

Reddit,CAGNET-1D,2,130,54.6972,0.9248

Epoch: 131Epoch: 131

Reddit,CAGNET-1D,2,131,55.1145,0.9246

Epoch: 132Epoch: 132

Reddit,CAGNET-1D,2,132,55.5318,0.9244

Epoch: 133Epoch: 133

Reddit,CAGNET-1D,2,133,55.9498,0.9246

Epoch: 134Epoch: 134

Reddit,CAGNET-1D,2,134,56.3669,0.9242

Epoch: 135Epoch: 135

Reddit,CAGNET-1D,2,135,56.7846,0.9249

Epoch: 136Epoch: 136

Reddit,CAGNET-1D,2,136,57.2032,0.9238

Epoch: 137Epoch: 137

Reddit,CAGNET-1D,2,137,57.6208,0.9248

Epoch: 138Epoch: 138

Reddit,CAGNET-1D,2,138,58.0383,0.9240

Epoch: 139Epoch: 139

Reddit,CAGNET-1D,2,139,58.4554,0.9240

Epoch: 140Epoch: 140

Reddit,CAGNET-1D,2,140,58.8733,0.9243

Epoch: 141Epoch: 141

Reddit,CAGNET-1D,2,141,59.2912,0.9234

Epoch: 142Epoch: 142

Reddit,CAGNET-1D,2,142,59.7084,0.9243

Epoch: 143
Epoch: 143
Reddit,CAGNET-1D,2,143,60.1251,0.9238

Epoch: 144
Epoch: 144
Reddit,CAGNET-1D,2,144,60.5425,0.9237

Epoch: 145Epoch: 145

Reddit,CAGNET-1D,2,145,60.9594,0.9240

Epoch: 146Epoch: 146

Reddit,CAGNET-1D,2,146,61.3773,0.9237

Epoch: 147Epoch: 147

Reddit,CAGNET-1D,2,147,61.7947,0.9237

Epoch: 148Epoch: 148

Reddit,CAGNET-1D,2,148,62.2124,0.9238

Epoch: 149Epoch: 149

Reddit,CAGNET-1D,2,149,62.6302,0.9235

Epoch: 150Epoch: 150

Reddit,CAGNET-1D,2,150,63.0482,0.9236

Epoch: 151Epoch: 151

Reddit,CAGNET-1D,2,151,63.4662,0.9232

Epoch: 152Epoch: 152

Reddit,CAGNET-1D,2,152,63.8844,0.9235

Epoch: 153Epoch: 153

Reddit,CAGNET-1D,2,153,64.3014,0.9225

Epoch: 154Epoch: 154

Reddit,CAGNET-1D,2,154,64.7194,0.9234

Epoch: 155Epoch: 155

Reddit,CAGNET-1D,2,155,65.1367,0.9219

Epoch: 156Epoch: 156

Reddit,CAGNET-1D,2,156,65.5542,0.9234

Epoch: 157Epoch: 157

Reddit,CAGNET-1D,2,157,65.9716,0.9225

Epoch: 158
Epoch: 158
Reddit,CAGNET-1D,2,158,66.3894,0.9225

Epoch: 159Epoch: 159

Reddit,CAGNET-1D,2,159,66.8075,0.9228

Epoch: 160Epoch: 160

Reddit,CAGNET-1D,2,160,67.2251,0.9218

Epoch: 161Epoch: 161

Reddit,CAGNET-1D,2,161,67.6434,0.9228

Epoch: 162
Epoch: 162
Reddit,CAGNET-1D,2,162,68.0609,0.9221

Epoch: 163Epoch: 163

Reddit,CAGNET-1D,2,163,68.4789,0.9220

Epoch: 164Epoch: 164

Reddit,CAGNET-1D,2,164,68.8962,0.9223

Epoch: 165
Epoch: 165
Reddit,CAGNET-1D,2,165,69.3132,0.9217

Epoch: 166Epoch: 166

Reddit,CAGNET-1D,2,166,69.7305,0.9222

Epoch: 167Epoch: 167

Reddit,CAGNET-1D,2,167,70.1482,0.9218

Epoch: 168Epoch: 168

Reddit,CAGNET-1D,2,168,70.5659,0.9215

Epoch: 169Epoch: 169

Reddit,CAGNET-1D,2,169,70.9834,0.9216

Epoch: 170Epoch: 170

Reddit,CAGNET-1D,2,170,71.4012,0.9215

Epoch: 171Epoch: 171

Reddit,CAGNET-1D,2,171,71.8183,0.9214

Epoch: 172
Epoch: 172
Reddit,CAGNET-1D,2,172,72.2362,0.9217

Epoch: 173Epoch: 173

Reddit,CAGNET-1D,2,173,72.6531,0.9211

Epoch: 174Epoch: 174

Reddit,CAGNET-1D,2,174,73.0710,0.9216

Epoch: 175Epoch: 175

Reddit,CAGNET-1D,2,175,73.4876,0.9211

Epoch: 176
Epoch: 176
Reddit,CAGNET-1D,2,176,73.9063,0.9212

Epoch: 177Epoch: 177

Reddit,CAGNET-1D,2,177,74.3233,0.9210

Epoch: 178Epoch: 178

Reddit,CAGNET-1D,2,178,74.7420,0.9214

Epoch: 179
Epoch: 179
Reddit,CAGNET-1D,2,179,75.1593,0.9206

Epoch: 180Epoch: 180

Reddit,CAGNET-1D,2,180,75.5787,0.9211

Epoch: 181Epoch: 181

Reddit,CAGNET-1D,2,181,75.9964,0.9208

Epoch: 182Epoch: 182

Reddit,CAGNET-1D,2,182,76.4134,0.9210

Epoch: 183Epoch: 183

Reddit,CAGNET-1D,2,183,76.8311,0.9206

Epoch: 184Epoch: 184

Reddit,CAGNET-1D,2,184,77.2478,0.9207

Epoch: 185Epoch: 185

Reddit,CAGNET-1D,2,185,77.6655,0.9208

Epoch: 186
Epoch: 186
Reddit,CAGNET-1D,2,186,78.0827,0.9203

Epoch: 187
Epoch: 187
Reddit,CAGNET-1D,2,187,78.5008,0.9207

Epoch: 188Epoch: 188

Reddit,CAGNET-1D,2,188,78.9176,0.9206

Epoch: 189Epoch: 189

Reddit,CAGNET-1D,2,189,79.3357,0.9200

Epoch: 190Epoch: 190

Reddit,CAGNET-1D,2,190,79.7533,0.9203

Epoch: 191Epoch: 191

Reddit,CAGNET-1D,2,191,80.1716,0.9205

Epoch: 192
Epoch: 192
Reddit,CAGNET-1D,2,192,80.5905,0.9198

Epoch: 193
Epoch: 193
Reddit,CAGNET-1D,2,193,81.0074,0.9205

Epoch: 194Epoch: 194

Reddit,CAGNET-1D,2,194,81.4255,0.9193

Epoch: 195Epoch: 195

Reddit,CAGNET-1D,2,195,81.8431,0.9201

Epoch: 196Epoch: 196

Reddit,CAGNET-1D,2,196,82.2611,0.9188

Epoch: 197Epoch: 197

Reddit,CAGNET-1D,2,197,82.6791,0.9203

Epoch: 198Epoch: 198

Reddit,CAGNET-1D,2,198,83.0955,0.9191

Epoch: 199Epoch: 199

Reddit,CAGNET-1D,2,199,83.5126,0.9199

Epoch: 200
Epoch: 200
Reddit,CAGNET-1D,2,200,83.9300,0.9191

Epoch: 201
Epoch: 201
Reddit,CAGNET-1D,2,201,84.3473,0.9192

Epoch: 202Epoch: 202

Reddit,CAGNET-1D,2,202,84.7641,0.9193

Epoch: 203Epoch: 203

Reddit,CAGNET-1D,2,203,85.1813,0.9186

Epoch: 204
Epoch: 204
Reddit,CAGNET-1D,2,204,85.5993,0.9192

Epoch: 205Epoch: 205

Reddit,CAGNET-1D,2,205,86.0166,0.9183

Epoch: 206Epoch: 206

Reddit,CAGNET-1D,2,206,86.4335,0.9189

Epoch: 207Epoch: 207

Reddit,CAGNET-1D,2,207,86.8506,0.9176

Epoch: 208
Epoch: 208
Reddit,CAGNET-1D,2,208,87.2680,0.9183

Epoch: 209Epoch: 209

Reddit,CAGNET-1D,2,209,87.6855,0.9177

Epoch: 210Epoch: 210

Reddit,CAGNET-1D,2,210,88.1023,0.9184

Epoch: 211Epoch: 211

Reddit,CAGNET-1D,2,211,88.5201,0.9180

Epoch: 212
Epoch: 212
Reddit,CAGNET-1D,2,212,88.9379,0.9177

Epoch: 213Epoch: 213

Reddit,CAGNET-1D,2,213,89.3556,0.9180

Epoch: 214Epoch: 214

Reddit,CAGNET-1D,2,214,89.7732,0.9172

Epoch: 215Epoch: 215

Reddit,CAGNET-1D,2,215,90.1906,0.9177

Epoch: 216Epoch: 216

Reddit,CAGNET-1D,2,216,90.6087,0.9170

Epoch: 217Epoch: 217

Reddit,CAGNET-1D,2,217,91.0260,0.9173

Epoch: 218Epoch: 218

Reddit,CAGNET-1D,2,218,91.4442,0.9172

Epoch: 219Epoch: 219

Reddit,CAGNET-1D,2,219,91.8625,0.9168

Epoch: 220Epoch: 220

Reddit,CAGNET-1D,2,220,92.2808,0.9170

Epoch: 221Epoch: 221

Reddit,CAGNET-1D,2,221,92.6986,0.9174

Epoch: 222Epoch: 222

Reddit,CAGNET-1D,2,222,93.1165,0.9168

Epoch: 223Epoch: 223

Reddit,CAGNET-1D,2,223,93.5344,0.9160

Epoch: 224
Epoch: 224
Reddit,CAGNET-1D,2,224,93.9521,0.9169

Epoch: 225Epoch: 225

Reddit,CAGNET-1D,2,225,94.3690,0.9144

Epoch: 226Epoch: 226

Reddit,CAGNET-1D,2,226,94.7878,0.9168

Epoch: 227Epoch: 227

Reddit,CAGNET-1D,2,227,95.2057,0.9146

Epoch: 228Epoch: 228

Reddit,CAGNET-1D,2,228,95.6233,0.9165

Epoch: 229Epoch: 229

Reddit,CAGNET-1D,2,229,96.0412,0.9153

Epoch: 230Epoch: 230

Reddit,CAGNET-1D,2,230,96.4587,0.9152

Epoch: 231
Epoch: 231
Reddit,CAGNET-1D,2,231,96.8765,0.9165

Epoch: 232Epoch: 232

Reddit,CAGNET-1D,2,232,97.2940,0.9152

Epoch: 233Epoch: 233

Reddit,CAGNET-1D,2,233,97.7113,0.9159

Epoch: 234Epoch: 234

Reddit,CAGNET-1D,2,234,98.1283,0.9154

Epoch: 235Epoch: 235

Reddit,CAGNET-1D,2,235,98.5452,0.9151

Epoch: 236Epoch: 236

Reddit,CAGNET-1D,2,236,98.9635,0.9159

Epoch: 237
Epoch: 237
Reddit,CAGNET-1D,2,237,99.3810,0.9149

Epoch: 238
Epoch: 238
Reddit,CAGNET-1D,2,238,99.7983,0.9152

Epoch: 239Epoch: 239

Reddit,CAGNET-1D,2,239,100.2162,0.9149

Epoch: 240Epoch: 240

Reddit,CAGNET-1D,2,240,100.6339,0.9150

Epoch: 241Epoch: 241

Reddit,CAGNET-1D,2,241,101.0519,0.9149

Epoch: 242
Epoch: 242
Reddit,CAGNET-1D,2,242,101.4698,0.9155

Epoch: 243Epoch: 243

Reddit,CAGNET-1D,2,243,101.8878,0.9139

Epoch: 244Epoch: 244

Reddit,CAGNET-1D,2,244,102.3060,0.9148

Epoch: 245Epoch: 245

Reddit,CAGNET-1D,2,245,102.7237,0.9145

Epoch: 246Epoch: 246

Reddit,CAGNET-1D,2,246,103.1409,0.9142

Epoch: 247Epoch: 247

Reddit,CAGNET-1D,2,247,103.5590,0.9149

Epoch: 248Epoch: 248

Reddit,CAGNET-1D,2,248,103.9762,0.9133

Epoch: 249Epoch: 249

Reddit,CAGNET-1D,2,249,104.3939,0.9135

Epoch: 250Epoch: 250

Reddit,CAGNET-1D,2,250,104.8116,0.9142

Epoch: 251
Epoch: 251
Reddit,CAGNET-1D,2,251,105.2291,0.9135

Epoch: 252Epoch: 252

Reddit,CAGNET-1D,2,252,105.6462,0.9129

Epoch: 253Epoch: 253

Reddit,CAGNET-1D,2,253,106.0642,0.9136

Epoch: 254Epoch: 254

Reddit,CAGNET-1D,2,254,106.4822,0.9125

Epoch: 255Epoch: 255

Reddit,CAGNET-1D,2,255,106.9001,0.9134

Epoch: 256Epoch: 256

Reddit,CAGNET-1D,2,256,107.3179,0.9124

Epoch: 257
Epoch: 257
Reddit,CAGNET-1D,2,257,107.7366,0.9123

Epoch: 258
Epoch: 258
Reddit,CAGNET-1D,2,258,108.1549,0.9131

Epoch: 259Epoch: 259

Reddit,CAGNET-1D,2,259,108.5722,0.9117

Epoch: 260
Epoch: 260
Reddit,CAGNET-1D,2,260,108.9894,0.9130

Epoch: 261Epoch: 261

Reddit,CAGNET-1D,2,261,109.4070,0.9119

Epoch: 262
Epoch: 262
Reddit,CAGNET-1D,2,262,109.8241,0.9122

Epoch: 263Epoch: 263

Reddit,CAGNET-1D,2,263,110.2413,0.9121

Epoch: 264Epoch: 264

Reddit,CAGNET-1D,2,264,110.6588,0.9117

Epoch: 265Epoch: 265

Reddit,CAGNET-1D,2,265,111.0770,0.9115

Epoch: 266Epoch: 266

Reddit,CAGNET-1D,2,266,111.4944,0.9114

Epoch: 267
Epoch: 267
Reddit,CAGNET-1D,2,267,111.9118,0.9117

Epoch: 268
Epoch: 268
Reddit,CAGNET-1D,2,268,112.3289,0.9110

Epoch: 269
Epoch: 269
Reddit,CAGNET-1D,2,269,112.7480,0.9119

Epoch: 270Epoch: 270

Reddit,CAGNET-1D,2,270,113.1657,0.9100

Epoch: 271Epoch: 271

Reddit,CAGNET-1D,2,271,113.5845,0.9119

Epoch: 272Epoch: 272

Reddit,CAGNET-1D,2,272,114.0026,0.9096

Epoch: 273Epoch: 273

Reddit,CAGNET-1D,2,273,114.4194,0.9119

Epoch: 274Epoch: 274

Reddit,CAGNET-1D,2,274,114.8367,0.9095

Epoch: 275Epoch: 275

Reddit,CAGNET-1D,2,275,115.2546,0.9106

Epoch: 276Epoch: 276

Reddit,CAGNET-1D,2,276,115.6728,0.9109

Epoch: 277
Epoch: 277
Reddit,CAGNET-1D,2,277,116.0909,0.9094

Epoch: 278Epoch: 278

Reddit,CAGNET-1D,2,278,116.5079,0.9110

Epoch: 279Epoch: 279

Reddit,CAGNET-1D,2,279,116.9258,0.9093

Epoch: 280Epoch: 280

Reddit,CAGNET-1D,2,280,117.3427,0.9107

Epoch: 281Epoch: 281

Reddit,CAGNET-1D,2,281,117.7605,0.9096

Epoch: 282Epoch: 282

Reddit,CAGNET-1D,2,282,118.1780,0.9093

Epoch: 283Epoch: 283

Reddit,CAGNET-1D,2,283,118.5959,0.9102

Epoch: 284
Epoch: 284
Reddit,CAGNET-1D,2,284,119.0140,0.9087

Epoch: 285
Epoch: 285
Reddit,CAGNET-1D,2,285,119.4320,0.9102

Epoch: 286Epoch: 286

Reddit,CAGNET-1D,2,286,119.8497,0.9086

Epoch: 287
Epoch: 287
Reddit,CAGNET-1D,2,287,120.2678,0.9095

Epoch: 288Epoch: 288

Reddit,CAGNET-1D,2,288,120.6852,0.9083

Epoch: 289Epoch: 289

Reddit,CAGNET-1D,2,289,121.1039,0.9095

Epoch: 290
Epoch: 290
Reddit,CAGNET-1D,2,290,121.5207,0.9081

Epoch: 291
Epoch: 291
Reddit,CAGNET-1D,2,291,121.9381,0.9091

Epoch: 292Epoch: 292

Reddit,CAGNET-1D,2,292,122.3556,0.9078

Epoch: 293Epoch: 293

Reddit,CAGNET-1D,2,293,122.7729,0.9087

Epoch: 294Epoch: 294

Reddit,CAGNET-1D,2,294,123.1907,0.9082

Epoch: 295Epoch: 295

Reddit,CAGNET-1D,2,295,123.6079,0.9085

Epoch: 296Epoch: 296

Reddit,CAGNET-1D,2,296,124.0252,0.9084

Epoch: 297Epoch: 297

Reddit,CAGNET-1D,2,297,124.4426,0.9073

Epoch: 298
Epoch: 298
Reddit,CAGNET-1D,2,298,124.8613,0.9086

Epoch: 299
Epoch: 299
Reddit,CAGNET-1D,2,299,125.2796,0.9077

Epoch: 300Epoch: 300

Reddit,CAGNET-1D,2,300,125.6972,0.9074

Epoch: 301Epoch: 301

Reddit,CAGNET-1D,2,301,126.1150,0.9084

Epoch: 302Epoch: 302

Reddit,CAGNET-1D,2,302,126.5327,0.9066

Epoch: 303Epoch: 303

Reddit,CAGNET-1D,2,303,126.9497,0.9084

Epoch: 304Epoch: 304

Reddit,CAGNET-1D,2,304,127.3673,0.9062

Epoch: 305Epoch: 305

Reddit,CAGNET-1D,2,305,127.7848,0.9082

Epoch: 306Epoch: 306

Reddit,CAGNET-1D,2,306,128.2029,0.9072

Epoch: 307
Epoch: 307
Reddit,CAGNET-1D,2,307,128.6206,0.9068

Epoch: 308Epoch: 308

Reddit,CAGNET-1D,2,308,129.0384,0.9075

Epoch: 309Epoch: 309

Reddit,CAGNET-1D,2,309,129.4553,0.9062

Epoch: 310Epoch: 310

Reddit,CAGNET-1D,2,310,129.8729,0.9074

Epoch: 311Epoch: 311

Reddit,CAGNET-1D,2,311,130.2909,0.9062

Epoch: 312Epoch: 312

Reddit,CAGNET-1D,2,312,130.7085,0.9062

Epoch: 313
Epoch: 313
Reddit,CAGNET-1D,2,313,131.1262,0.9061

Epoch: 314Epoch: 314

Reddit,CAGNET-1D,2,314,131.5434,0.9068

Epoch: 315Epoch: 315

Reddit,CAGNET-1D,2,315,131.9605,0.9050

Epoch: 316Epoch: 316

Reddit,CAGNET-1D,2,316,132.3777,0.9074

Epoch: 317
Epoch: 317
Reddit,CAGNET-1D,2,317,132.7949,0.9041

Epoch: 318Epoch: 318

Reddit,CAGNET-1D,2,318,133.2124,0.9073

Epoch: 319Epoch: 319

Reddit,CAGNET-1D,2,319,133.6300,0.9044

Epoch: 320Epoch: 320

Reddit,CAGNET-1D,2,320,134.0476,0.9060

Epoch: 321
Epoch: 321
Reddit,CAGNET-1D,2,321,134.4652,0.9058

Epoch: 322Epoch: 322

Reddit,CAGNET-1D,2,322,134.8825,0.9040

Epoch: 323Epoch: 323

Reddit,CAGNET-1D,2,323,135.3004,0.9062

Epoch: 324Epoch: 324

Reddit,CAGNET-1D,2,324,135.7191,0.9051

Epoch: 325Epoch: 325

Reddit,CAGNET-1D,2,325,136.1361,0.9046

Epoch: 326
Epoch: 326
Reddit,CAGNET-1D,2,326,136.5536,0.9059

Epoch: 327
Epoch: 327
Reddit,CAGNET-1D,2,327,136.9721,0.9049

Epoch: 328Epoch: 328

Reddit,CAGNET-1D,2,328,137.3901,0.9043

Epoch: 329Epoch: 329

Reddit,CAGNET-1D,2,329,137.8079,0.9056

Epoch: 330Epoch: 330

Reddit,CAGNET-1D,2,330,138.2250,0.9038

Epoch: 331
Epoch: 331
Reddit,CAGNET-1D,2,331,138.6432,0.9037

Epoch: 332Epoch: 332

Reddit,CAGNET-1D,2,332,139.0603,0.9058

Epoch: 333Epoch: 333

Reddit,CAGNET-1D,2,333,139.4777,0.9021

Epoch: 334Epoch: 334

Reddit,CAGNET-1D,2,334,139.8947,0.9047

Epoch: 335Epoch: 335

Reddit,CAGNET-1D,2,335,140.3121,0.9048

Epoch: 336Epoch: 336

Reddit,CAGNET-1D,2,336,140.7294,0.9024

Epoch: 337Epoch: 337

Reddit,CAGNET-1D,2,337,141.1481,0.9055

Epoch: 338Epoch: 338

Reddit,CAGNET-1D,2,338,141.5660,0.9031

Epoch: 339Epoch: 339

Reddit,CAGNET-1D,2,339,141.9832,0.9033

Epoch: 340
Epoch: 340
Reddit,CAGNET-1D,2,340,142.4001,0.9047

Epoch: 341
Epoch: 341
Reddit,CAGNET-1D,2,341,142.8184,0.9014

Epoch: 342Epoch: 342

Reddit,CAGNET-1D,2,342,143.2362,0.9043

Epoch: 343Epoch: 343

Reddit,CAGNET-1D,2,343,143.6549,0.9021

Epoch: 344Epoch: 344

Reddit,CAGNET-1D,2,344,144.0735,0.9033

Epoch: 345Epoch: 345

Reddit,CAGNET-1D,2,345,144.4901,0.9016

Epoch: 346Epoch: 346

Reddit,CAGNET-1D,2,346,144.9069,0.9031

Epoch: 347Epoch: 347

Reddit,CAGNET-1D,2,347,145.3249,0.9025

Epoch: 348Epoch: 348

Reddit,CAGNET-1D,2,348,145.7416,0.9028

Epoch: 349Epoch: 349

Reddit,CAGNET-1D,2,349,146.1587,0.9033

Epoch: 350
Epoch: 350
Reddit,CAGNET-1D,2,350,146.5751,0.9021

Epoch: 351
Epoch: 351
Reddit,CAGNET-1D,2,351,146.9920,0.9025

Epoch: 352Epoch: 352

Reddit,CAGNET-1D,2,352,147.4095,0.9028

Epoch: 353
Epoch: 353
Reddit,CAGNET-1D,2,353,147.8263,0.9022

Epoch: 354Epoch: 354

Reddit,CAGNET-1D,2,354,148.2445,0.9024

Epoch: 355
Epoch: 355
Reddit,CAGNET-1D,2,355,148.6610,0.9028

Epoch: 356Epoch: 356

Reddit,CAGNET-1D,2,356,149.0785,0.9017

Epoch: 357
Epoch: 357
Reddit,CAGNET-1D,2,357,149.4957,0.9017

Epoch: 358Epoch: 358

Reddit,CAGNET-1D,2,358,149.9133,0.9027

Epoch: 359Epoch: 359

Reddit,CAGNET-1D,2,359,150.3310,0.9011

Epoch: 360Epoch: 360

Reddit,CAGNET-1D,2,360,150.7484,0.9025

Epoch: 361Epoch: 361

Reddit,CAGNET-1D,2,361,151.1670,0.9009

Epoch: 362Epoch: 362

Reddit,CAGNET-1D,2,362,151.5847,0.9019

Epoch: 363Epoch: 363

Reddit,CAGNET-1D,2,363,152.0024,0.9022

Epoch: 364Epoch: 364

Reddit,CAGNET-1D,2,364,152.4205,0.8998

Epoch: 365Epoch: 365

Reddit,CAGNET-1D,2,365,152.8380,0.9028

Epoch: 366Epoch: 366

Reddit,CAGNET-1D,2,366,153.2551,0.9002

Epoch: 367Epoch: 367

Reddit,CAGNET-1D,2,367,153.6741,0.9015

Epoch: 368
Epoch: 368
Reddit,CAGNET-1D,2,368,154.0917,0.9011

Epoch: 369
Epoch: 369
Reddit,CAGNET-1D,2,369,154.5091,0.9002

Epoch: 370Epoch: 370

Reddit,CAGNET-1D,2,370,154.9266,0.9010

Epoch: 371Epoch: 371

Reddit,CAGNET-1D,2,371,155.3432,0.9001

Epoch: 372Epoch: 372

Reddit,CAGNET-1D,2,372,155.7607,0.9001

Epoch: 373Epoch: 373

Reddit,CAGNET-1D,2,373,156.1783,0.9006

Epoch: 374Epoch: 374

Reddit,CAGNET-1D,2,374,156.5962,0.8988

Epoch: 375Epoch: 375

Reddit,CAGNET-1D,2,375,157.0131,0.9017

Epoch: 376
Epoch: 376
Reddit,CAGNET-1D,2,376,157.4299,0.8978

Epoch: 377Epoch: 377

Reddit,CAGNET-1D,2,377,157.8476,0.9008

Epoch: 378Epoch: 378

Reddit,CAGNET-1D,2,378,158.2651,0.9004

Epoch: 379Epoch: 379

Reddit,CAGNET-1D,2,379,158.6828,0.8989

Epoch: 380Epoch: 380

Reddit,CAGNET-1D,2,380,159.1007,0.9012

Epoch: 381Epoch: 381

Reddit,CAGNET-1D,2,381,159.5185,0.8977

Epoch: 382Epoch: 382

Reddit,CAGNET-1D,2,382,159.9367,0.9003

Epoch: 383Epoch: 383

Reddit,CAGNET-1D,2,383,160.3539,0.8994

Epoch: 384Epoch: 384

Reddit,CAGNET-1D,2,384,160.7715,0.8982

Epoch: 385
Epoch: 385
Reddit,CAGNET-1D,2,385,161.1885,0.8997

Epoch: 386Epoch: 386

Reddit,CAGNET-1D,2,386,161.6064,0.8980

Epoch: 387Epoch: 387

Reddit,CAGNET-1D,2,387,162.0237,0.8997

Epoch: 388Epoch: 388

Reddit,CAGNET-1D,2,388,162.4417,0.8990

Epoch: 389
Epoch: 389
Reddit,CAGNET-1D,2,389,162.8607,0.8980

Epoch: 390Epoch: 390

Reddit,CAGNET-1D,2,390,163.2788,0.8998

Epoch: 391Epoch: 391

Reddit,CAGNET-1D,2,391,163.6956,0.8968

Epoch: 392Epoch: 392

Reddit,CAGNET-1D,2,392,164.1133,0.8998

Epoch: 393Epoch: 393

Reddit,CAGNET-1D,2,393,164.5308,0.8979

Epoch: 394Epoch: 394

Reddit,CAGNET-1D,2,394,164.9493,0.8982

Epoch: 395Epoch: 395

Reddit,CAGNET-1D,2,395,165.3666,0.8991

Epoch: 396Epoch: 396

Reddit,CAGNET-1D,2,396,165.7841,0.8962

Epoch: 397Epoch: 397

Reddit,CAGNET-1D,2,397,166.2015,0.8996

Epoch: 398Epoch: 398

Reddit,CAGNET-1D,2,398,166.6193,0.8946

Epoch: 399Epoch: 399

Reddit,CAGNET-1D,2,399,167.0365,0.8998

Epoch: 400Epoch: 400

Reddit,CAGNET-1D,2,400,167.4545,0.8933

Epoch: 401Epoch: 401

Reddit,CAGNET-1D,2,401,167.8725,0.8997

Epoch: 402Epoch: 402

Reddit,CAGNET-1D,2,402,168.2905,0.8935

Epoch: 403Epoch: 403

Reddit,CAGNET-1D,2,403,168.7082,0.8991

Epoch: 404Epoch: 404

Reddit,CAGNET-1D,2,404,169.1257,0.8969

Epoch: 405Epoch: 405

Reddit,CAGNET-1D,2,405,169.5429,0.8963

Epoch: 406Epoch: 406

Reddit,CAGNET-1D,2,406,169.9606,0.8986

Epoch: 407Epoch: 407

Reddit,CAGNET-1D,2,407,170.3789,0.8945

Epoch: 408Epoch: 408

Reddit,CAGNET-1D,2,408,170.7956,0.8991

Epoch: 409Epoch: 409

Reddit,CAGNET-1D,2,409,171.2132,0.8959

Epoch: 410Epoch: 410

Reddit,CAGNET-1D,2,410,171.6307,0.8967

Epoch: 411Epoch: 411

Reddit,CAGNET-1D,2,411,172.0486,0.8973

Epoch: 412Epoch: 412

Reddit,CAGNET-1D,2,412,172.4658,0.8950

Epoch: 413Epoch: 413

Reddit,CAGNET-1D,2,413,172.8831,0.8981

Epoch: 414Epoch: 414

Reddit,CAGNET-1D,2,414,173.3004,0.8945

Epoch: 415Epoch: 415

Reddit,CAGNET-1D,2,415,173.7178,0.8966

Epoch: 416Epoch: 416

Reddit,CAGNET-1D,2,416,174.1353,0.8968

Epoch: 417
Epoch: 417
Reddit,CAGNET-1D,2,417,174.5526,0.8946

Epoch: 418
Epoch: 418
Reddit,CAGNET-1D,2,418,174.9698,0.8967

Epoch: 419Epoch: 419

Reddit,CAGNET-1D,2,419,175.3878,0.8954

Epoch: 420Epoch: 420

Reddit,CAGNET-1D,2,420,175.8058,0.8947

Epoch: 421Epoch: 421

Reddit,CAGNET-1D,2,421,176.2232,0.8971

Epoch: 422Epoch: 422

Reddit,CAGNET-1D,2,422,176.6422,0.8937

Epoch: 423Epoch: 423

Reddit,CAGNET-1D,2,423,177.0598,0.8962

Epoch: 424
Epoch: 424
Reddit,CAGNET-1D,2,424,177.4774,0.8963

Epoch: 425
Epoch: 425
Reddit,CAGNET-1D,2,425,177.8944,0.8933

Epoch: 426Epoch: 426

Reddit,CAGNET-1D,2,426,178.3129,0.8975

Epoch: 427Epoch: 427

Reddit,CAGNET-1D,2,427,178.7304,0.8913

Epoch: 428Epoch: 428

Reddit,CAGNET-1D,2,428,179.1475,0.8967

Epoch: 429Epoch: 429

Reddit,CAGNET-1D,2,429,179.5654,0.8933

Epoch: 430Epoch: 430

Reddit,CAGNET-1D,2,430,179.9828,0.8940

Epoch: 431
Epoch: 431
Reddit,CAGNET-1D,2,431,180.4002,0.8956

Epoch: 432Epoch: 432

Reddit,CAGNET-1D,2,432,180.8187,0.8940

Epoch: 433Epoch: 433

Reddit,CAGNET-1D,2,433,181.2354,0.8953

Epoch: 434Epoch: 434

Reddit,CAGNET-1D,2,434,181.6525,0.8943

Epoch: 435Epoch: 435

Reddit,CAGNET-1D,2,435,182.0713,0.8936

Epoch: 436Epoch: 436

Reddit,CAGNET-1D,2,436,182.4887,0.8951

Epoch: 437Epoch: 437

Reddit,CAGNET-1D,2,437,182.9070,0.8925

Epoch: 438
Epoch: 438
Reddit,CAGNET-1D,2,438,183.3249,0.8960

Epoch: 439Epoch: 439

Reddit,CAGNET-1D,2,439,183.7429,0.8926

Epoch: 440Epoch: 440

Reddit,CAGNET-1D,2,440,184.1605,0.8952

Epoch: 441Epoch: 441

Reddit,CAGNET-1D,2,441,184.5774,0.8920

Epoch: 442Epoch: 442

Reddit,CAGNET-1D,2,442,184.9953,0.8940

Epoch: 443Epoch: 443

Reddit,CAGNET-1D,2,443,185.4125,0.8947

Epoch: 444Epoch: 444

Reddit,CAGNET-1D,2,444,185.8305,0.8920

Epoch: 445Epoch: 445

Reddit,CAGNET-1D,2,445,186.2484,0.8954

Epoch: 446Epoch: 446

Reddit,CAGNET-1D,2,446,186.6660,0.8919

Epoch: 447Epoch: 447

Reddit,CAGNET-1D,2,447,187.0837,0.8941

Epoch: 448Epoch: 448

Reddit,CAGNET-1D,2,448,187.5020,0.8935

Epoch: 449
Epoch: 449
Reddit,CAGNET-1D,2,449,187.9189,0.8915

Epoch: 450
Epoch: 450
Reddit,CAGNET-1D,2,450,188.3367,0.8951

Epoch: 451Epoch: 451

Reddit,CAGNET-1D,2,451,188.7538,0.8908

Epoch: 452
Epoch: 452
Reddit,CAGNET-1D,2,452,189.1713,0.8942

Epoch: 453
Epoch: 453
Reddit,CAGNET-1D,2,453,189.5881,0.8898

Epoch: 454Epoch: 454

Reddit,CAGNET-1D,2,454,190.0052,0.8936

Epoch: 455Epoch: 455

Reddit,CAGNET-1D,2,455,190.4220,0.8897

Epoch: 456Epoch: 456

Reddit,CAGNET-1D,2,456,190.8400,0.8935

Epoch: 457Epoch: 457

Reddit,CAGNET-1D,2,457,191.2577,0.8902

Epoch: 458Epoch: 458

Reddit,CAGNET-1D,2,458,191.6758,0.8936

Epoch: 459Epoch: 459

Reddit,CAGNET-1D,2,459,192.0936,0.8900

Epoch: 460
Epoch: 460
Reddit,CAGNET-1D,2,460,192.5110,0.8932

Epoch: 461Epoch: 461

Reddit,CAGNET-1D,2,461,192.9291,0.8921

Epoch: 462
Epoch: 462
Reddit,CAGNET-1D,2,462,193.3480,0.8909

Epoch: 463Epoch: 463

Reddit,CAGNET-1D,2,463,193.7656,0.8929

Epoch: 464Epoch: 464

Reddit,CAGNET-1D,2,464,194.1835,0.8891

Epoch: 465Epoch: 465

Reddit,CAGNET-1D,2,465,194.6007,0.8931

Epoch: 466Epoch: 466

Reddit,CAGNET-1D,2,466,195.0192,0.8907

Epoch: 467
Epoch: 467
Reddit,CAGNET-1D,2,467,195.4372,0.8923

Epoch: 468Epoch: 468

Reddit,CAGNET-1D,2,468,195.8541,0.8905

Epoch: 469Epoch: 469

Reddit,CAGNET-1D,2,469,196.2712,0.8920

Epoch: 470Epoch: 470

Reddit,CAGNET-1D,2,470,196.6888,0.8900

Epoch: 471Epoch: 471

Reddit,CAGNET-1D,2,471,197.1065,0.8920

Epoch: 472Epoch: 472

Reddit,CAGNET-1D,2,472,197.5238,0.8894

Epoch: 473Epoch: 473

Reddit,CAGNET-1D,2,473,197.9408,0.8916

Epoch: 474Epoch: 474

Reddit,CAGNET-1D,2,474,198.3579,0.8890

Epoch: 475
Epoch: 475
Reddit,CAGNET-1D,2,475,198.7768,0.8914

Epoch: 476Epoch: 476

Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
Reddit,CAGNET-1D,2,476,199.1945,0.8894

    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 916, in main
    run)
  File "GNN-RDM/src/gcn_distr.py", line 740, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr.py", line 664, in run
    total_time[i][rank] = tstop - tstart
KeyError: 1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2123962) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2123962 (local_rank 1) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:54:02
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2123962)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='Reddit', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: Reddit timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='Reddit', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: Reddit timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 2
Processes: 2
rank: 0 adj_matrix_loc.size: torch.Size([232965, 232965])
rank: 0 inputs_loc.size: torch.Size([232965, 602])
rank: 1 adj_matrix_loc.size: torch.Size([232965, 232965])
rank: 1 inputs_loc.size: torch.Size([232965, 602])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 0 run 0
Starting training... rank 1 run 0
Epoch: 000Epoch: 000

Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -116482: [-116482, 41]
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -116482: [-116482, 41]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2124153) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2124153 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:59:50
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2124153)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_02:59:50
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2124154)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='meta', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: meta timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 1
device: cuda:0
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2124342) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2124342 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_02:59:56
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2124342)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='meta', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: meta timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 582, in run
    row_groups, col_groups = get_proc_groups(rank, size)
  File "GNN-RDM/src/gcn_distr_15d.py", line 414, in get_proc_groups
    row_groups.append(dist.new_group(row_procs[i]))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 2690, in new_group
    raise RuntimeError("the new group's world size should be less or "
RuntimeError: the new group's world size should be less or equal to the world size set by init_process_group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2124549) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2124549 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:00:08
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2124549)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='meta', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: meta timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='meta', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: meta timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 2
device: cuda:0
Processes: 2
device: cuda:1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2124816) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2124816 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:00:14
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2124816)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_03:00:14
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2124817)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='meta', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: meta timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='meta', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: meta timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 2
Processes: 2
rank: 0 adj_matrix_loc.size: torch.Size([500036, 500036])
rank: 0 inputs_loc.size: torch.Size([500036, 64])
rank: 1 adj_matrix_loc.size: torch.Size([500036, 500036])
rank: 1 inputs_loc.size: torch.Size([500036, 64])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 0 run 0Starting training... rank 1 run 0

Epoch: 000Epoch: 000

Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -250018: [-250018, 25]
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -250018: [-250018, 25]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2124906) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2124906 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:19:37
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2124906)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_03:19:37
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2124907)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='arctic25', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: arctic25 timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 1
device: cuda:0
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2125274) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2125274 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:19:43
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2125274)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='arctic25', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: arctic25 timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 582, in run
    row_groups, col_groups = get_proc_groups(rank, size)
  File "GNN-RDM/src/gcn_distr_15d.py", line 414, in get_proc_groups
    row_groups.append(dist.new_group(row_procs[i]))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 2690, in new_group
    raise RuntimeError("the new group's world size should be less or "
RuntimeError: the new group's world size should be less or equal to the world size set by init_process_group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2125481) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2125481 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:19:55
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2125481)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='arctic25', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: arctic25 timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='arctic25', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: arctic25 timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 2
device: cuda:0
Processes: 2
device: cuda:1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2125746) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2125746 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:20:01
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2125746)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_03:20:01
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2125747)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='arctic25', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: arctic25 timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='arctic25', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: arctic25 timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 2
Processes: 2
rank: 1 adj_matrix_loc.size: torch.Size([500044, 500044])
rank: 1 inputs_loc.size: torch.Size([500044, 64])
rank: 0 adj_matrix_loc.size: torch.Size([500044, 500044])
rank: 0 inputs_loc.size: torch.Size([500044, 64])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 1 run 0
Starting training... rank 0 run 0
Epoch: 000
Epoch: 000
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -250022: [-250022, 33]
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -250022: [-250022, 33]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2125837) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2125837 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:39:24
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2125837)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_03:39:24
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2125838)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='oral', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: oral timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 1
device: cuda:0
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2126251) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2126251 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:39:31
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2126251)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=1, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='oral', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: oral timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 582, in run
    row_groups, col_groups = get_proc_groups(rank, size)
  File "GNN-RDM/src/gcn_distr_15d.py", line 414, in get_proc_groups
    row_groups.append(dist.new_group(row_procs[i]))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 2690, in new_group
    raise RuntimeError("the new group's world size should be less or "
RuntimeError: the new group's world size should be less or equal to the world size set by init_process_group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2126458) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2126458 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:39:42
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2126458)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
  <NO_OTHER_FAILURES>
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='oral', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: oral timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='oral', local_rank=None, midlayer=128, normalization='True', runcount=1, timing='True')
Arguments: epochs: 2000 graph: oral timing: True mid: 128 norm: True act: True acc: True runs: 1
notch347
Processes: 2
device: cuda:0
Processes: 2
device: cuda:1
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr.py", line 959, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    print(main())
  File "GNN-RDM/src/gcn_distr.py", line 910, in main
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
    adj_matrix, _ = add_remaining_self_loops(edge_index, num_nodes=inputs.size(0))
UnboundLocalError: local variable 'edge_index' referenced before assignment
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2126723) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2126723 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    GNN-RDM/src/gcn_distr.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-07-12_03:39:48
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2126723)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_03:39:48
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2126724)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='oral', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: oral timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Namespace(acc_csv='full_new.csv', accperrank=2, accuracy='True', activations='True', csv='', download=None, epochs=2000, graphname='oral', local_rank=None, midlayer=128, normalization='True', replication=2, runcount=1, timing='True')
Arguments: epochs: 2000 graph: oral timing: True mid: 128 norm: True act: True acc: True runs: 1 rep: 2
notch347
Processes: 2
Processes: 2
rank: 0 adj_matrix_loc.size: torch.Size([1000000, 1000000])
rank: 0 inputs_loc.size: torch.Size([1000000, 32])
rank: 1 adj_matrix_loc.size: torch.Size([1000000, 1000000])
rank: 1 inputs_loc.size: torch.Size([1000000, 32])
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Starting training... rank 1 run 0Starting training... rank 0 run 0

Epoch: 000Epoch: 000

Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
Traceback (most recent call last):
  File "GNN-RDM/src/gcn_distr_15d.py", line 1053, in <module>
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    print(main())
  File "GNN-RDM/src/gcn_distr_15d.py", line 1008, in main
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run)
  File "GNN-RDM/src/gcn_distr_15d.py", line 796, in init_process
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -500000: [-500000, 32]
    run_outputs = fn(rank, size, inputs, adj_matrix, data, features, classes, device)
  File "GNN-RDM/src/gcn_distr_15d.py", line 672, in run
    outputs = torch.cat((outputs, torch.cuda.FloatTensor(pad_row, classes, device=device)), dim=0)
RuntimeError: Trying to create tensor with negative dimension -500000: [-500000, 32]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2126817) of binary: /uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/bin/python
/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2126817 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/run.py", line 692, in run
    )(*cmd_args)
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/uufs/chpc.utah.edu/common/home/u1320844/anaconda3/envs/ogb/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
  GNN-RDM/src/gcn_distr_15d.py FAILED  
=======================================
Root Cause:
[0]:
  time: 2022-07-12_04:54:19
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2126817)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-07-12_04:54:19
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2126818)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

[master 29708c4] Jul12-02:27:13-MDT2022
 6 files changed, 25176 insertions(+), 3 deletions(-)
remote: error: Trace: a6224616fd909bff5e2e7242291eafbbd989e6829d6af2e1c0cfc438578d5421        
remote: error: See http://git.io/iEPt8g for more information.        
remote: error: File meta_adj_part_rk0 is 994.57 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: File oral_adj_part_rk0 is 414.56 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: File arctic25_adj_part_rk0 is 880.77 MB; this exceeds GitHub's file size limit of 100.00 MB        
remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.        
To github.com:JoseYan/GNNs.git
 ! [remote rejected] master -> master (pre-receive hook declined)
error: failed to push some refs to 'github.com:JoseYan/GNNs.git'
run.sh: line 28: pushd: ../GNN_logs: No such file or directory
